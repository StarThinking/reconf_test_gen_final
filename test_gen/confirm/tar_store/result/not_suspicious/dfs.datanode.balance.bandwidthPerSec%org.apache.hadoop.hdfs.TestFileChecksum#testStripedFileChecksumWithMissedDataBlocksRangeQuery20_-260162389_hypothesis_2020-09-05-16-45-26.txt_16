reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-844733342-172.17.0.2-1599324336469:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41466,DS-1bb77e47-58fe-4efd-adf7-3c0015af6427,DISK], DatanodeInfoWithStorage[127.0.0.1:39688,DS-fadeafbc-7cff-4611-8fb8-e95fa54f447d,DISK], DatanodeInfoWithStorage[127.0.0.1:40866,DS-15e8936e-0c87-4b25-b134-4d21a0050360,DISK], DatanodeInfoWithStorage[127.0.0.1:32909,DS-577f7112-a85f-4f53-8752-8c83dc732809,DISK], DatanodeInfoWithStorage[127.0.0.1:35240,DS-ad19b450-d25d-4d92-958b-4b43633e08f3,DISK], DatanodeInfoWithStorage[127.0.0.1:39464,DS-3c5f1312-117b-451f-b8ca-af7bd264b082,DISK], DatanodeInfoWithStorage[127.0.0.1:38753,DS-e678f2b6-08ca-41a9-807f-b78bb848be4e,DISK], DatanodeInfoWithStorage[127.0.0.1:33776,DS-225c340c-8635-4986-ae07-f466d721d955,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-844733342-172.17.0.2-1599324336469:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41466,DS-1bb77e47-58fe-4efd-adf7-3c0015af6427,DISK], DatanodeInfoWithStorage[127.0.0.1:39688,DS-fadeafbc-7cff-4611-8fb8-e95fa54f447d,DISK], DatanodeInfoWithStorage[127.0.0.1:40866,DS-15e8936e-0c87-4b25-b134-4d21a0050360,DISK], DatanodeInfoWithStorage[127.0.0.1:32909,DS-577f7112-a85f-4f53-8752-8c83dc732809,DISK], DatanodeInfoWithStorage[127.0.0.1:35240,DS-ad19b450-d25d-4d92-958b-4b43633e08f3,DISK], DatanodeInfoWithStorage[127.0.0.1:39464,DS-3c5f1312-117b-451f-b8ca-af7bd264b082,DISK], DatanodeInfoWithStorage[127.0.0.1:38753,DS-e678f2b6-08ca-41a9-807f-b78bb848be4e,DISK], DatanodeInfoWithStorage[127.0.0.1:33776,DS-225c340c-8635-4986-ae07-f466d721d955,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1208206776-172.17.0.2-1599324656379:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40870,DS-c41f1867-8da2-48b2-beab-632a6f8432cf,DISK], DatanodeInfoWithStorage[127.0.0.1:40294,DS-7c6ea959-af0f-4151-b000-53cd80e31140,DISK], DatanodeInfoWithStorage[127.0.0.1:35613,DS-a7980b39-53a2-476a-aa93-7b8b163af2e2,DISK], DatanodeInfoWithStorage[127.0.0.1:41807,DS-18f423b9-e863-42c2-be1e-3098c463b835,DISK], DatanodeInfoWithStorage[127.0.0.1:41645,DS-6f3f9575-b883-45c4-8085-4816cdb647bd,DISK], DatanodeInfoWithStorage[127.0.0.1:39128,DS-889e2ac2-95be-4225-9613-d1b5a6db0574,DISK], DatanodeInfoWithStorage[127.0.0.1:37075,DS-3361e208-11d9-4b0e-a59e-c235776998c2,DISK], DatanodeInfoWithStorage[127.0.0.1:45576,DS-05b97a0b-89e3-4ecb-828a-ea31175e8457,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1208206776-172.17.0.2-1599324656379:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40870,DS-c41f1867-8da2-48b2-beab-632a6f8432cf,DISK], DatanodeInfoWithStorage[127.0.0.1:40294,DS-7c6ea959-af0f-4151-b000-53cd80e31140,DISK], DatanodeInfoWithStorage[127.0.0.1:35613,DS-a7980b39-53a2-476a-aa93-7b8b163af2e2,DISK], DatanodeInfoWithStorage[127.0.0.1:41807,DS-18f423b9-e863-42c2-be1e-3098c463b835,DISK], DatanodeInfoWithStorage[127.0.0.1:41645,DS-6f3f9575-b883-45c4-8085-4816cdb647bd,DISK], DatanodeInfoWithStorage[127.0.0.1:39128,DS-889e2ac2-95be-4225-9613-d1b5a6db0574,DISK], DatanodeInfoWithStorage[127.0.0.1:37075,DS-3361e208-11d9-4b0e-a59e-c235776998c2,DISK], DatanodeInfoWithStorage[127.0.0.1:45576,DS-05b97a0b-89e3-4ecb-828a-ea31175e8457,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-736149647-172.17.0.2-1599324818798:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39259,DS-e3b316c4-665c-4213-904a-c53ffc753e0d,DISK], DatanodeInfoWithStorage[127.0.0.1:35801,DS-7f2baacc-1839-4cc1-a3e0-bc04b5b71119,DISK], DatanodeInfoWithStorage[127.0.0.1:40248,DS-489ba652-cb4b-4bc3-87fa-df77c0a7c9d9,DISK], DatanodeInfoWithStorage[127.0.0.1:35712,DS-54a6318e-61a9-496e-b5c2-a63e1b9c108c,DISK], DatanodeInfoWithStorage[127.0.0.1:35197,DS-fd1ad644-a1da-4633-b004-4dc076c6ce93,DISK], DatanodeInfoWithStorage[127.0.0.1:40643,DS-135a98f1-8583-4bc8-9d4f-a2bf19a51241,DISK], DatanodeInfoWithStorage[127.0.0.1:36013,DS-829b6660-1a68-46e9-b496-bdc42fa2d8c0,DISK], DatanodeInfoWithStorage[127.0.0.1:36026,DS-d8e8d1bb-31ad-4bd0-a2b2-e58d58817617,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-736149647-172.17.0.2-1599324818798:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39259,DS-e3b316c4-665c-4213-904a-c53ffc753e0d,DISK], DatanodeInfoWithStorage[127.0.0.1:35801,DS-7f2baacc-1839-4cc1-a3e0-bc04b5b71119,DISK], DatanodeInfoWithStorage[127.0.0.1:40248,DS-489ba652-cb4b-4bc3-87fa-df77c0a7c9d9,DISK], DatanodeInfoWithStorage[127.0.0.1:35712,DS-54a6318e-61a9-496e-b5c2-a63e1b9c108c,DISK], DatanodeInfoWithStorage[127.0.0.1:35197,DS-fd1ad644-a1da-4633-b004-4dc076c6ce93,DISK], DatanodeInfoWithStorage[127.0.0.1:40643,DS-135a98f1-8583-4bc8-9d4f-a2bf19a51241,DISK], DatanodeInfoWithStorage[127.0.0.1:36013,DS-829b6660-1a68-46e9-b496-bdc42fa2d8c0,DISK], DatanodeInfoWithStorage[127.0.0.1:36026,DS-d8e8d1bb-31ad-4bd0-a2b2-e58d58817617,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-459751042-172.17.0.2-1599324922612:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44588,DS-4242b0a8-a2f7-471a-a9aa-eb7bef635b27,DISK], DatanodeInfoWithStorage[127.0.0.1:35651,DS-b47bbe22-5ddd-4c30-8eac-94a54d5fab49,DISK], DatanodeInfoWithStorage[127.0.0.1:35503,DS-8235ba1b-ec94-40cf-b9d6-0df8d5d4168a,DISK], DatanodeInfoWithStorage[127.0.0.1:43883,DS-9ae20196-d98e-49b5-9f6c-f8f540d1dd03,DISK], DatanodeInfoWithStorage[127.0.0.1:41725,DS-3560a8c0-c1b0-44e9-9d4f-d0e9bf1a7242,DISK], DatanodeInfoWithStorage[127.0.0.1:34768,DS-0900baf3-0df6-438c-865a-6032ca35751a,DISK], DatanodeInfoWithStorage[127.0.0.1:33679,DS-14001aa0-1ad2-42d6-9ee7-1471ec553673,DISK], DatanodeInfoWithStorage[127.0.0.1:34437,DS-7c55df92-e212-48a2-949b-715ab3b2e111,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-459751042-172.17.0.2-1599324922612:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44588,DS-4242b0a8-a2f7-471a-a9aa-eb7bef635b27,DISK], DatanodeInfoWithStorage[127.0.0.1:35651,DS-b47bbe22-5ddd-4c30-8eac-94a54d5fab49,DISK], DatanodeInfoWithStorage[127.0.0.1:35503,DS-8235ba1b-ec94-40cf-b9d6-0df8d5d4168a,DISK], DatanodeInfoWithStorage[127.0.0.1:43883,DS-9ae20196-d98e-49b5-9f6c-f8f540d1dd03,DISK], DatanodeInfoWithStorage[127.0.0.1:41725,DS-3560a8c0-c1b0-44e9-9d4f-d0e9bf1a7242,DISK], DatanodeInfoWithStorage[127.0.0.1:34768,DS-0900baf3-0df6-438c-865a-6032ca35751a,DISK], DatanodeInfoWithStorage[127.0.0.1:33679,DS-14001aa0-1ad2-42d6-9ee7-1471ec553673,DISK], DatanodeInfoWithStorage[127.0.0.1:34437,DS-7c55df92-e212-48a2-949b-715ab3b2e111,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: File /striped/stripedFileChecksum3 could only be written to 5 of the 6 required nodes for RS-6-3-1024k. There are 5 datanode(s) running and 5 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2226)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

stackTrace: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /striped/stripedFileChecksum3 could only be written to 5 of the 6 required nodes for RS-6-3-1024k. There are 5 datanode(s) running and 5 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2226)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy26.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy27.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.allocateNewBlock(DFSStripedOutputStream.java:480)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:526)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:164)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:145)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1178)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
	at org.apache.hadoop.hdfs.TestFileChecksum.prepareTestFiles(TestFileChecksum.java:602)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:532)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1675135874-172.17.0.2-1599325634160:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45820,DS-169493bf-11d3-413f-925d-bff9cc074f8a,DISK], DatanodeInfoWithStorage[127.0.0.1:46766,DS-fb9ca51d-03a8-4475-9fd3-7725dd8d9b1f,DISK], DatanodeInfoWithStorage[127.0.0.1:33615,DS-f0b93784-a194-40ff-9e15-cbdd3933242b,DISK], DatanodeInfoWithStorage[127.0.0.1:45700,DS-bc89ab6e-15b6-4329-94f7-42e9c6a31424,DISK], DatanodeInfoWithStorage[127.0.0.1:42698,DS-c4be4b62-6fde-4c9a-b735-e105532b30ef,DISK], DatanodeInfoWithStorage[127.0.0.1:40883,DS-52814242-b5dc-4f24-b252-ba018d9ef433,DISK], DatanodeInfoWithStorage[127.0.0.1:39710,DS-5a7370c2-82e0-442e-a81f-74933f42bf43,DISK], DatanodeInfoWithStorage[127.0.0.1:38691,DS-e1355e75-c25e-43aa-976c-25750f39613c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1675135874-172.17.0.2-1599325634160:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45820,DS-169493bf-11d3-413f-925d-bff9cc074f8a,DISK], DatanodeInfoWithStorage[127.0.0.1:46766,DS-fb9ca51d-03a8-4475-9fd3-7725dd8d9b1f,DISK], DatanodeInfoWithStorage[127.0.0.1:33615,DS-f0b93784-a194-40ff-9e15-cbdd3933242b,DISK], DatanodeInfoWithStorage[127.0.0.1:45700,DS-bc89ab6e-15b6-4329-94f7-42e9c6a31424,DISK], DatanodeInfoWithStorage[127.0.0.1:42698,DS-c4be4b62-6fde-4c9a-b735-e105532b30ef,DISK], DatanodeInfoWithStorage[127.0.0.1:40883,DS-52814242-b5dc-4f24-b252-ba018d9ef433,DISK], DatanodeInfoWithStorage[127.0.0.1:39710,DS-5a7370c2-82e0-442e-a81f-74933f42bf43,DISK], DatanodeInfoWithStorage[127.0.0.1:38691,DS-e1355e75-c25e-43aa-976c-25750f39613c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-903245740-172.17.0.2-1599326082660:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38804,DS-d0965f10-c0f8-42d8-b1c1-fd59ba11a5ad,DISK], DatanodeInfoWithStorage[127.0.0.1:41482,DS-c1f9302f-f1b4-46a1-bcae-1200b4fb094e,DISK], DatanodeInfoWithStorage[127.0.0.1:38208,DS-820f772a-4b8b-4d9e-98da-3ad41bc3c8d6,DISK], DatanodeInfoWithStorage[127.0.0.1:38537,DS-57c21d08-5d10-4562-9900-e979ba3eb719,DISK], DatanodeInfoWithStorage[127.0.0.1:45065,DS-13df4938-7b14-4c92-82df-15e9d585eed7,DISK], DatanodeInfoWithStorage[127.0.0.1:46490,DS-4d16945d-0137-492b-bbfb-fbd9a74e82ed,DISK], DatanodeInfoWithStorage[127.0.0.1:33042,DS-09b603e2-9170-4e10-b284-f90f039685a1,DISK], DatanodeInfoWithStorage[127.0.0.1:36901,DS-7010c428-e469-4217-a632-a89fd567dd20,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-903245740-172.17.0.2-1599326082660:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38804,DS-d0965f10-c0f8-42d8-b1c1-fd59ba11a5ad,DISK], DatanodeInfoWithStorage[127.0.0.1:41482,DS-c1f9302f-f1b4-46a1-bcae-1200b4fb094e,DISK], DatanodeInfoWithStorage[127.0.0.1:38208,DS-820f772a-4b8b-4d9e-98da-3ad41bc3c8d6,DISK], DatanodeInfoWithStorage[127.0.0.1:38537,DS-57c21d08-5d10-4562-9900-e979ba3eb719,DISK], DatanodeInfoWithStorage[127.0.0.1:45065,DS-13df4938-7b14-4c92-82df-15e9d585eed7,DISK], DatanodeInfoWithStorage[127.0.0.1:46490,DS-4d16945d-0137-492b-bbfb-fbd9a74e82ed,DISK], DatanodeInfoWithStorage[127.0.0.1:33042,DS-09b603e2-9170-4e10-b284-f90f039685a1,DISK], DatanodeInfoWithStorage[127.0.0.1:36901,DS-7010c428-e469-4217-a632-a89fd567dd20,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2000376971-172.17.0.2-1599326748571:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40040,DS-1b0600d8-b2e6-459b-8a77-8043b56c09c1,DISK], DatanodeInfoWithStorage[127.0.0.1:41630,DS-3a218749-7314-4aa8-a2e5-f37fc9eb380c,DISK], DatanodeInfoWithStorage[127.0.0.1:36349,DS-6266b3ff-bd01-42b1-9813-512b1da4169f,DISK], DatanodeInfoWithStorage[127.0.0.1:43392,DS-72ecaf3b-6fb8-41a6-93f5-d18f8d9309cb,DISK], DatanodeInfoWithStorage[127.0.0.1:33836,DS-c7b2f92e-d003-4a76-a160-2751c1d9dc71,DISK], DatanodeInfoWithStorage[127.0.0.1:45800,DS-a954d3b3-9835-4dce-b5ab-2213b93cdf5b,DISK], DatanodeInfoWithStorage[127.0.0.1:35872,DS-ea04e036-ed49-42b4-ba4a-b6ca21f171d0,DISK], DatanodeInfoWithStorage[127.0.0.1:39032,DS-0c407990-af63-4bbf-b91f-2147bcabab16,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2000376971-172.17.0.2-1599326748571:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40040,DS-1b0600d8-b2e6-459b-8a77-8043b56c09c1,DISK], DatanodeInfoWithStorage[127.0.0.1:41630,DS-3a218749-7314-4aa8-a2e5-f37fc9eb380c,DISK], DatanodeInfoWithStorage[127.0.0.1:36349,DS-6266b3ff-bd01-42b1-9813-512b1da4169f,DISK], DatanodeInfoWithStorage[127.0.0.1:43392,DS-72ecaf3b-6fb8-41a6-93f5-d18f8d9309cb,DISK], DatanodeInfoWithStorage[127.0.0.1:33836,DS-c7b2f92e-d003-4a76-a160-2751c1d9dc71,DISK], DatanodeInfoWithStorage[127.0.0.1:45800,DS-a954d3b3-9835-4dce-b5ab-2213b93cdf5b,DISK], DatanodeInfoWithStorage[127.0.0.1:35872,DS-ea04e036-ed49-42b4-ba4a-b6ca21f171d0,DISK], DatanodeInfoWithStorage[127.0.0.1:39032,DS-0c407990-af63-4bbf-b91f-2147bcabab16,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1519440525-172.17.0.2-1599326930201:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38732,DS-880221ff-9ae8-4b14-8084-2d1dfe6455e9,DISK], DatanodeInfoWithStorage[127.0.0.1:46162,DS-39a17ca1-c468-4c7c-9729-e5e190f9b0c3,DISK], DatanodeInfoWithStorage[127.0.0.1:43734,DS-40e661a8-ce6c-4e98-8c47-12ddd467364d,DISK], DatanodeInfoWithStorage[127.0.0.1:43372,DS-07609e2e-1a4f-4d7d-826f-dfd52ef3608f,DISK], DatanodeInfoWithStorage[127.0.0.1:45657,DS-4a68abe3-db28-4a95-b3e8-d08f3ae0de99,DISK], DatanodeInfoWithStorage[127.0.0.1:33570,DS-552aaa25-94b2-4f9c-ba2a-ceadf3c5ff16,DISK], DatanodeInfoWithStorage[127.0.0.1:37668,DS-54d602ad-3f0e-4aa4-857f-e642eba7abe2,DISK], DatanodeInfoWithStorage[127.0.0.1:36453,DS-bfc8189b-0827-4975-9304-e50cd75b661d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1519440525-172.17.0.2-1599326930201:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38732,DS-880221ff-9ae8-4b14-8084-2d1dfe6455e9,DISK], DatanodeInfoWithStorage[127.0.0.1:46162,DS-39a17ca1-c468-4c7c-9729-e5e190f9b0c3,DISK], DatanodeInfoWithStorage[127.0.0.1:43734,DS-40e661a8-ce6c-4e98-8c47-12ddd467364d,DISK], DatanodeInfoWithStorage[127.0.0.1:43372,DS-07609e2e-1a4f-4d7d-826f-dfd52ef3608f,DISK], DatanodeInfoWithStorage[127.0.0.1:45657,DS-4a68abe3-db28-4a95-b3e8-d08f3ae0de99,DISK], DatanodeInfoWithStorage[127.0.0.1:33570,DS-552aaa25-94b2-4f9c-ba2a-ceadf3c5ff16,DISK], DatanodeInfoWithStorage[127.0.0.1:37668,DS-54d602ad-3f0e-4aa4-857f-e642eba7abe2,DISK], DatanodeInfoWithStorage[127.0.0.1:36453,DS-bfc8189b-0827-4975-9304-e50cd75b661d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-892446353-172.17.0.2-1599326956118:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35457,DS-b26e0892-b895-4f22-8bbd-93fba78a8869,DISK], DatanodeInfoWithStorage[127.0.0.1:46423,DS-e007087d-104a-4225-be5e-242dfd3e0360,DISK], DatanodeInfoWithStorage[127.0.0.1:44133,DS-9a5289dd-74c8-4e7f-9dba-18f1228d7c9b,DISK], DatanodeInfoWithStorage[127.0.0.1:34868,DS-f24a85ea-e239-4701-a483-88cfa9909523,DISK], DatanodeInfoWithStorage[127.0.0.1:37895,DS-8bf9d71b-f6c7-4813-b13e-700e557e06fe,DISK], DatanodeInfoWithStorage[127.0.0.1:40527,DS-9ab2eaf6-0172-45db-8659-97a0149c8125,DISK], DatanodeInfoWithStorage[127.0.0.1:42025,DS-87c4a919-4d6a-45f6-87b6-921e717ebb31,DISK], DatanodeInfoWithStorage[127.0.0.1:41319,DS-66b5d550-2702-4129-8b48-46182931b811,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-892446353-172.17.0.2-1599326956118:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35457,DS-b26e0892-b895-4f22-8bbd-93fba78a8869,DISK], DatanodeInfoWithStorage[127.0.0.1:46423,DS-e007087d-104a-4225-be5e-242dfd3e0360,DISK], DatanodeInfoWithStorage[127.0.0.1:44133,DS-9a5289dd-74c8-4e7f-9dba-18f1228d7c9b,DISK], DatanodeInfoWithStorage[127.0.0.1:34868,DS-f24a85ea-e239-4701-a483-88cfa9909523,DISK], DatanodeInfoWithStorage[127.0.0.1:37895,DS-8bf9d71b-f6c7-4813-b13e-700e557e06fe,DISK], DatanodeInfoWithStorage[127.0.0.1:40527,DS-9ab2eaf6-0172-45db-8659-97a0149c8125,DISK], DatanodeInfoWithStorage[127.0.0.1:42025,DS-87c4a919-4d6a-45f6-87b6-921e717ebb31,DISK], DatanodeInfoWithStorage[127.0.0.1:41319,DS-66b5d550-2702-4129-8b48-46182931b811,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1045601508-172.17.0.2-1599327166823:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44914,DS-6e89e9bc-126d-4d7e-9a1b-c03a4a35ce8f,DISK], DatanodeInfoWithStorage[127.0.0.1:37655,DS-96a0f903-bd3e-4997-ad20-175001ae4327,DISK], DatanodeInfoWithStorage[127.0.0.1:35128,DS-7af16e4a-add2-46db-97f8-20477cbf7848,DISK], DatanodeInfoWithStorage[127.0.0.1:40523,DS-165fd6a8-b8c4-48e2-af4d-4d43ea64dc73,DISK], DatanodeInfoWithStorage[127.0.0.1:45587,DS-1ed8afc6-d0a3-440c-9736-712c96ced7cf,DISK], DatanodeInfoWithStorage[127.0.0.1:37156,DS-eec11cce-8079-4e07-ba9f-795130e03040,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-dff17e10-d303-4880-9daf-8cadc8811fa6,DISK], DatanodeInfoWithStorage[127.0.0.1:38806,DS-844ccdd3-7350-4b89-9ea8-2bb585a61d81,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1045601508-172.17.0.2-1599327166823:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44914,DS-6e89e9bc-126d-4d7e-9a1b-c03a4a35ce8f,DISK], DatanodeInfoWithStorage[127.0.0.1:37655,DS-96a0f903-bd3e-4997-ad20-175001ae4327,DISK], DatanodeInfoWithStorage[127.0.0.1:35128,DS-7af16e4a-add2-46db-97f8-20477cbf7848,DISK], DatanodeInfoWithStorage[127.0.0.1:40523,DS-165fd6a8-b8c4-48e2-af4d-4d43ea64dc73,DISK], DatanodeInfoWithStorage[127.0.0.1:45587,DS-1ed8afc6-d0a3-440c-9736-712c96ced7cf,DISK], DatanodeInfoWithStorage[127.0.0.1:37156,DS-eec11cce-8079-4e07-ba9f-795130e03040,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-dff17e10-d303-4880-9daf-8cadc8811fa6,DISK], DatanodeInfoWithStorage[127.0.0.1:38806,DS-844ccdd3-7350-4b89-9ea8-2bb585a61d81,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1120339617-172.17.0.2-1599327722341:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37285,DS-00b3481a-f541-4de6-ae61-f8b12beb9c0e,DISK], DatanodeInfoWithStorage[127.0.0.1:35805,DS-09ead1b6-12a2-45e0-b05e-5ae0cde639cc,DISK], DatanodeInfoWithStorage[127.0.0.1:40741,DS-b8150404-8965-4309-91ad-1d131bc44ef9,DISK], DatanodeInfoWithStorage[127.0.0.1:40600,DS-9ee4c0a7-746f-4ef4-8c66-356c4a454bad,DISK], DatanodeInfoWithStorage[127.0.0.1:34304,DS-324305af-cf41-467b-9a01-5f4747dd82cd,DISK], DatanodeInfoWithStorage[127.0.0.1:34262,DS-016ba496-b950-4c01-a4ac-5b49d89a1c2c,DISK], DatanodeInfoWithStorage[127.0.0.1:37157,DS-41062a2f-023a-4c46-ae9d-6c0bdbcad4ed,DISK], DatanodeInfoWithStorage[127.0.0.1:45810,DS-5ac9143a-868a-472c-89b0-6b66a5d8b799,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1120339617-172.17.0.2-1599327722341:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37285,DS-00b3481a-f541-4de6-ae61-f8b12beb9c0e,DISK], DatanodeInfoWithStorage[127.0.0.1:35805,DS-09ead1b6-12a2-45e0-b05e-5ae0cde639cc,DISK], DatanodeInfoWithStorage[127.0.0.1:40741,DS-b8150404-8965-4309-91ad-1d131bc44ef9,DISK], DatanodeInfoWithStorage[127.0.0.1:40600,DS-9ee4c0a7-746f-4ef4-8c66-356c4a454bad,DISK], DatanodeInfoWithStorage[127.0.0.1:34304,DS-324305af-cf41-467b-9a01-5f4747dd82cd,DISK], DatanodeInfoWithStorage[127.0.0.1:34262,DS-016ba496-b950-4c01-a4ac-5b49d89a1c2c,DISK], DatanodeInfoWithStorage[127.0.0.1:37157,DS-41062a2f-023a-4c46-ae9d-6c0bdbcad4ed,DISK], DatanodeInfoWithStorage[127.0.0.1:45810,DS-5ac9143a-868a-472c-89b0-6b66a5d8b799,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-816147434-172.17.0.2-1599328265331:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46158,DS-7efea7ef-52e3-4ea4-ae61-af659acd91e8,DISK], DatanodeInfoWithStorage[127.0.0.1:39729,DS-b66ec3b9-99a4-48fe-8e06-d28cbc85e970,DISK], DatanodeInfoWithStorage[127.0.0.1:37269,DS-3904761b-0ab7-41bf-a617-bd43e3f903ec,DISK], DatanodeInfoWithStorage[127.0.0.1:44518,DS-7a28987e-0c97-4383-8f2f-518b50b5c2eb,DISK], DatanodeInfoWithStorage[127.0.0.1:33050,DS-e7182ea5-c880-4dde-ad26-388975ead6d1,DISK], DatanodeInfoWithStorage[127.0.0.1:37775,DS-9aa313a6-7c68-4e16-9497-ab45228225de,DISK], DatanodeInfoWithStorage[127.0.0.1:39289,DS-bbc532b7-bd84-4587-a026-309cdf69021d,DISK], DatanodeInfoWithStorage[127.0.0.1:35943,DS-cefe3754-651d-4358-8e7c-c7d09a8fb105,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-816147434-172.17.0.2-1599328265331:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46158,DS-7efea7ef-52e3-4ea4-ae61-af659acd91e8,DISK], DatanodeInfoWithStorage[127.0.0.1:39729,DS-b66ec3b9-99a4-48fe-8e06-d28cbc85e970,DISK], DatanodeInfoWithStorage[127.0.0.1:37269,DS-3904761b-0ab7-41bf-a617-bd43e3f903ec,DISK], DatanodeInfoWithStorage[127.0.0.1:44518,DS-7a28987e-0c97-4383-8f2f-518b50b5c2eb,DISK], DatanodeInfoWithStorage[127.0.0.1:33050,DS-e7182ea5-c880-4dde-ad26-388975ead6d1,DISK], DatanodeInfoWithStorage[127.0.0.1:37775,DS-9aa313a6-7c68-4e16-9497-ab45228225de,DISK], DatanodeInfoWithStorage[127.0.0.1:39289,DS-bbc532b7-bd84-4587-a026-309cdf69021d,DISK], DatanodeInfoWithStorage[127.0.0.1:35943,DS-cefe3754-651d-4358-8e7c-c7d09a8fb105,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1195946456-172.17.0.2-1599328435874:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36363,DS-cab14439-be42-48ac-bacf-b6280422e8b6,DISK], DatanodeInfoWithStorage[127.0.0.1:38529,DS-531c2c1a-a65c-4351-ba86-1ba6c1eab90d,DISK], DatanodeInfoWithStorage[127.0.0.1:38741,DS-00e9d1f8-2acb-433a-9fcb-0be4f478652d,DISK], DatanodeInfoWithStorage[127.0.0.1:37783,DS-5a2a3952-6e2c-467a-a9aa-e670e8e47efc,DISK], DatanodeInfoWithStorage[127.0.0.1:41726,DS-ff2cfdc7-8464-4afd-9d76-c400bc976e25,DISK], DatanodeInfoWithStorage[127.0.0.1:41344,DS-6bc9d1aa-505b-4c62-a3a8-bfa475c7ee38,DISK], DatanodeInfoWithStorage[127.0.0.1:34584,DS-e9d2cf33-62b9-4832-8cc4-b89536cdf822,DISK], DatanodeInfoWithStorage[127.0.0.1:42048,DS-e2a4da11-30a3-407a-9bc3-0ac29799736d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1195946456-172.17.0.2-1599328435874:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36363,DS-cab14439-be42-48ac-bacf-b6280422e8b6,DISK], DatanodeInfoWithStorage[127.0.0.1:38529,DS-531c2c1a-a65c-4351-ba86-1ba6c1eab90d,DISK], DatanodeInfoWithStorage[127.0.0.1:38741,DS-00e9d1f8-2acb-433a-9fcb-0be4f478652d,DISK], DatanodeInfoWithStorage[127.0.0.1:37783,DS-5a2a3952-6e2c-467a-a9aa-e670e8e47efc,DISK], DatanodeInfoWithStorage[127.0.0.1:41726,DS-ff2cfdc7-8464-4afd-9d76-c400bc976e25,DISK], DatanodeInfoWithStorage[127.0.0.1:41344,DS-6bc9d1aa-505b-4c62-a3a8-bfa475c7ee38,DISK], DatanodeInfoWithStorage[127.0.0.1:34584,DS-e9d2cf33-62b9-4832-8cc4-b89536cdf822,DISK], DatanodeInfoWithStorage[127.0.0.1:42048,DS-e2a4da11-30a3-407a-9bc3-0ac29799736d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-536176571-172.17.0.2-1599328495409:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39824,DS-484c82cc-adea-4eb7-addb-0f097f43fb39,DISK], DatanodeInfoWithStorage[127.0.0.1:41725,DS-0666dd24-0299-4dbf-a315-e6ad6384036a,DISK], DatanodeInfoWithStorage[127.0.0.1:38138,DS-5d5c39c1-40db-4893-a654-1e76fa19ed84,DISK], DatanodeInfoWithStorage[127.0.0.1:34825,DS-7163bbac-7f68-430b-b220-645bb423c877,DISK], DatanodeInfoWithStorage[127.0.0.1:33126,DS-de764696-6d42-4e87-98f4-cff717262ac5,DISK], DatanodeInfoWithStorage[127.0.0.1:35124,DS-aa1cfd9e-eef6-415e-97f9-039f16782788,DISK], DatanodeInfoWithStorage[127.0.0.1:34123,DS-d8a2d9a3-7318-46e0-bb95-e8dd19aa49ce,DISK], DatanodeInfoWithStorage[127.0.0.1:34935,DS-0fa9be09-18d9-44ca-8946-4c7dd190f06a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-536176571-172.17.0.2-1599328495409:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39824,DS-484c82cc-adea-4eb7-addb-0f097f43fb39,DISK], DatanodeInfoWithStorage[127.0.0.1:41725,DS-0666dd24-0299-4dbf-a315-e6ad6384036a,DISK], DatanodeInfoWithStorage[127.0.0.1:38138,DS-5d5c39c1-40db-4893-a654-1e76fa19ed84,DISK], DatanodeInfoWithStorage[127.0.0.1:34825,DS-7163bbac-7f68-430b-b220-645bb423c877,DISK], DatanodeInfoWithStorage[127.0.0.1:33126,DS-de764696-6d42-4e87-98f4-cff717262ac5,DISK], DatanodeInfoWithStorage[127.0.0.1:35124,DS-aa1cfd9e-eef6-415e-97f9-039f16782788,DISK], DatanodeInfoWithStorage[127.0.0.1:34123,DS-d8a2d9a3-7318-46e0-bb95-e8dd19aa49ce,DISK], DatanodeInfoWithStorage[127.0.0.1:34935,DS-0fa9be09-18d9-44ca-8946-4c7dd190f06a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-549946554-172.17.0.2-1599328672449:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46792,DS-79ebd92a-1592-4c9b-aef8-3661a5c97dfe,DISK], DatanodeInfoWithStorage[127.0.0.1:34613,DS-bcfc528c-f37f-460b-934a-3e5a063d2a8d,DISK], DatanodeInfoWithStorage[127.0.0.1:44196,DS-9af6c6bb-62e9-4365-892e-b4e9104ab3db,DISK], DatanodeInfoWithStorage[127.0.0.1:37722,DS-284c8831-afcf-47be-9355-2d40136d1c08,DISK], DatanodeInfoWithStorage[127.0.0.1:34670,DS-9b9d924b-a98e-4aa1-88ea-3c04d34cf8b5,DISK], DatanodeInfoWithStorage[127.0.0.1:39510,DS-211bd8a6-50db-4f6a-af90-42d7086db4f6,DISK], DatanodeInfoWithStorage[127.0.0.1:39211,DS-0c6aa805-ef89-4f9d-9a30-44487521d75b,DISK], DatanodeInfoWithStorage[127.0.0.1:34974,DS-06cff23e-e12f-4022-8454-c57193ef57ac,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-549946554-172.17.0.2-1599328672449:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46792,DS-79ebd92a-1592-4c9b-aef8-3661a5c97dfe,DISK], DatanodeInfoWithStorage[127.0.0.1:34613,DS-bcfc528c-f37f-460b-934a-3e5a063d2a8d,DISK], DatanodeInfoWithStorage[127.0.0.1:44196,DS-9af6c6bb-62e9-4365-892e-b4e9104ab3db,DISK], DatanodeInfoWithStorage[127.0.0.1:37722,DS-284c8831-afcf-47be-9355-2d40136d1c08,DISK], DatanodeInfoWithStorage[127.0.0.1:34670,DS-9b9d924b-a98e-4aa1-88ea-3c04d34cf8b5,DISK], DatanodeInfoWithStorage[127.0.0.1:39510,DS-211bd8a6-50db-4f6a-af90-42d7086db4f6,DISK], DatanodeInfoWithStorage[127.0.0.1:39211,DS-0c6aa805-ef89-4f9d-9a30-44487521d75b,DISK], DatanodeInfoWithStorage[127.0.0.1:34974,DS-06cff23e-e12f-4022-8454-c57193ef57ac,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.datanode.balance.bandwidthPerSec
component: hdfs:DataNode
v1: 10m
v2: 20m
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1408270936-172.17.0.2-1599328703296:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42204,DS-3d315a2a-c2ff-4800-927f-62577d6ab5dc,DISK], DatanodeInfoWithStorage[127.0.0.1:32895,DS-922159e4-5ffa-4085-b227-c134c5e17634,DISK], DatanodeInfoWithStorage[127.0.0.1:33860,DS-9a949352-7def-4685-a103-65e3fd99d865,DISK], DatanodeInfoWithStorage[127.0.0.1:35500,DS-bc304e27-b34e-40cc-b739-5e9d7b959971,DISK], DatanodeInfoWithStorage[127.0.0.1:35704,DS-eb46dab8-f74e-40bf-8ecc-3042f5ef7dcc,DISK], DatanodeInfoWithStorage[127.0.0.1:43574,DS-ac1bb023-480e-40d0-9a8b-f92142ad4090,DISK], DatanodeInfoWithStorage[127.0.0.1:33212,DS-4f7763dd-5940-4332-a94d-7697d6b41e8d,DISK], DatanodeInfoWithStorage[127.0.0.1:45905,DS-a6a523d8-0903-454c-9dc1-9cf4882bb422,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1408270936-172.17.0.2-1599328703296:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42204,DS-3d315a2a-c2ff-4800-927f-62577d6ab5dc,DISK], DatanodeInfoWithStorage[127.0.0.1:32895,DS-922159e4-5ffa-4085-b227-c134c5e17634,DISK], DatanodeInfoWithStorage[127.0.0.1:33860,DS-9a949352-7def-4685-a103-65e3fd99d865,DISK], DatanodeInfoWithStorage[127.0.0.1:35500,DS-bc304e27-b34e-40cc-b739-5e9d7b959971,DISK], DatanodeInfoWithStorage[127.0.0.1:35704,DS-eb46dab8-f74e-40bf-8ecc-3042f5ef7dcc,DISK], DatanodeInfoWithStorage[127.0.0.1:43574,DS-ac1bb023-480e-40d0-9a8b-f92142ad4090,DISK], DatanodeInfoWithStorage[127.0.0.1:33212,DS-4f7763dd-5940-4332-a94d-7697d6b41e8d,DISK], DatanodeInfoWithStorage[127.0.0.1:45905,DS-a6a523d8-0903-454c-9dc1-9cf4882bb422,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 5 out of 50
v1v1v2v2 failed with probability 11 out of 50
result: false positive !!!
Total execution time in seconds : 4487

reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46130,DS-e14e379a-bedd-4f55-8285-5ae555611b71,DISK], DatanodeInfoWithStorage[127.0.0.1:36956,DS-84c978ff-e06a-4794-98d7-387164d67fdf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46130,DS-e14e379a-bedd-4f55-8285-5ae555611b71,DISK], DatanodeInfoWithStorage[127.0.0.1:36956,DS-84c978ff-e06a-4794-98d7-387164d67fdf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46130,DS-e14e379a-bedd-4f55-8285-5ae555611b71,DISK], DatanodeInfoWithStorage[127.0.0.1:36956,DS-84c978ff-e06a-4794-98d7-387164d67fdf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46130,DS-e14e379a-bedd-4f55-8285-5ae555611b71,DISK], DatanodeInfoWithStorage[127.0.0.1:36956,DS-84c978ff-e06a-4794-98d7-387164d67fdf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39285,DS-4d96b78a-2355-48cd-a832-b58f6eea9114,DISK], DatanodeInfoWithStorage[127.0.0.1:35525,DS-e766fc39-365f-4cd4-9918-e22852c18412,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35525,DS-e766fc39-365f-4cd4-9918-e22852c18412,DISK], DatanodeInfoWithStorage[127.0.0.1:39285,DS-4d96b78a-2355-48cd-a832-b58f6eea9114,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39285,DS-4d96b78a-2355-48cd-a832-b58f6eea9114,DISK], DatanodeInfoWithStorage[127.0.0.1:35525,DS-e766fc39-365f-4cd4-9918-e22852c18412,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35525,DS-e766fc39-365f-4cd4-9918-e22852c18412,DISK], DatanodeInfoWithStorage[127.0.0.1:39285,DS-4d96b78a-2355-48cd-a832-b58f6eea9114,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35014,DS-bb4ecae0-cdaa-40fb-b9d0-1cb5cad3eae4,DISK], DatanodeInfoWithStorage[127.0.0.1:37628,DS-c2a88914-9a6b-417a-9518-4792fb214ccd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35014,DS-bb4ecae0-cdaa-40fb-b9d0-1cb5cad3eae4,DISK], DatanodeInfoWithStorage[127.0.0.1:37628,DS-c2a88914-9a6b-417a-9518-4792fb214ccd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35014,DS-bb4ecae0-cdaa-40fb-b9d0-1cb5cad3eae4,DISK], DatanodeInfoWithStorage[127.0.0.1:37628,DS-c2a88914-9a6b-417a-9518-4792fb214ccd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35014,DS-bb4ecae0-cdaa-40fb-b9d0-1cb5cad3eae4,DISK], DatanodeInfoWithStorage[127.0.0.1:37628,DS-c2a88914-9a6b-417a-9518-4792fb214ccd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32803,DS-c93cbd5a-a722-4463-ae5e-f02c0615d905,DISK], DatanodeInfoWithStorage[127.0.0.1:34383,DS-aec2e432-6283-4276-966a-e232103365a4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32803,DS-c93cbd5a-a722-4463-ae5e-f02c0615d905,DISK], DatanodeInfoWithStorage[127.0.0.1:34383,DS-aec2e432-6283-4276-966a-e232103365a4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32803,DS-c93cbd5a-a722-4463-ae5e-f02c0615d905,DISK], DatanodeInfoWithStorage[127.0.0.1:34383,DS-aec2e432-6283-4276-966a-e232103365a4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32803,DS-c93cbd5a-a722-4463-ae5e-f02c0615d905,DISK], DatanodeInfoWithStorage[127.0.0.1:34383,DS-aec2e432-6283-4276-966a-e232103365a4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33742,DS-2adc5827-4683-4ffb-99ee-a284ec08bd64,DISK], DatanodeInfoWithStorage[127.0.0.1:39550,DS-6a7dc906-bd95-4df5-82a4-4a9be37849f2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33742,DS-2adc5827-4683-4ffb-99ee-a284ec08bd64,DISK], DatanodeInfoWithStorage[127.0.0.1:39550,DS-6a7dc906-bd95-4df5-82a4-4a9be37849f2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33742,DS-2adc5827-4683-4ffb-99ee-a284ec08bd64,DISK], DatanodeInfoWithStorage[127.0.0.1:39550,DS-6a7dc906-bd95-4df5-82a4-4a9be37849f2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33742,DS-2adc5827-4683-4ffb-99ee-a284ec08bd64,DISK], DatanodeInfoWithStorage[127.0.0.1:39550,DS-6a7dc906-bd95-4df5-82a4-4a9be37849f2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34607,DS-095ba09e-8621-4898-9e65-574ee37d361e,DISK], DatanodeInfoWithStorage[127.0.0.1:36921,DS-62c39f19-57e5-43e0-a477-9aba7071dfe3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36921,DS-62c39f19-57e5-43e0-a477-9aba7071dfe3,DISK], DatanodeInfoWithStorage[127.0.0.1:34607,DS-095ba09e-8621-4898-9e65-574ee37d361e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34607,DS-095ba09e-8621-4898-9e65-574ee37d361e,DISK], DatanodeInfoWithStorage[127.0.0.1:36921,DS-62c39f19-57e5-43e0-a477-9aba7071dfe3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36921,DS-62c39f19-57e5-43e0-a477-9aba7071dfe3,DISK], DatanodeInfoWithStorage[127.0.0.1:34607,DS-095ba09e-8621-4898-9e65-574ee37d361e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36091,DS-da818dd9-cb0e-40a7-9476-7979704e7506,DISK], DatanodeInfoWithStorage[127.0.0.1:33259,DS-69654278-b4de-4f70-ab0b-2ae2c8e36acb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33259,DS-69654278-b4de-4f70-ab0b-2ae2c8e36acb,DISK], DatanodeInfoWithStorage[127.0.0.1:36091,DS-da818dd9-cb0e-40a7-9476-7979704e7506,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36091,DS-da818dd9-cb0e-40a7-9476-7979704e7506,DISK], DatanodeInfoWithStorage[127.0.0.1:33259,DS-69654278-b4de-4f70-ab0b-2ae2c8e36acb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33259,DS-69654278-b4de-4f70-ab0b-2ae2c8e36acb,DISK], DatanodeInfoWithStorage[127.0.0.1:36091,DS-da818dd9-cb0e-40a7-9476-7979704e7506,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 3 out of 50
v1v1v2v2 failed with probability 4 out of 50
result: false positive !!!
Total execution time in seconds : 4010

reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1249845611-172.17.0.7-1599306822670:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34118,DS-1d48b9cd-b6af-457d-9954-f06bfe5421df,DISK], DatanodeInfoWithStorage[127.0.0.1:32906,DS-27880898-0e16-4356-8b6a-e50d62e9c9b5,DISK], DatanodeInfoWithStorage[127.0.0.1:41671,DS-71f16331-b3e5-48c0-85c4-bef1a57cdd3e,DISK], DatanodeInfoWithStorage[127.0.0.1:35171,DS-4d2ee207-bfab-482b-9fc4-db5b5cf86038,DISK], DatanodeInfoWithStorage[127.0.0.1:37046,DS-886b09da-1ae7-4944-a6a7-bae2fb0a9ac4,DISK], DatanodeInfoWithStorage[127.0.0.1:43488,DS-bb5d2d63-4f8d-47f5-9718-079b4e2c9635,DISK]]; indices=[2, 3, 4, 5, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1249845611-172.17.0.7-1599306822670:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34118,DS-1d48b9cd-b6af-457d-9954-f06bfe5421df,DISK], DatanodeInfoWithStorage[127.0.0.1:32906,DS-27880898-0e16-4356-8b6a-e50d62e9c9b5,DISK], DatanodeInfoWithStorage[127.0.0.1:41671,DS-71f16331-b3e5-48c0-85c4-bef1a57cdd3e,DISK], DatanodeInfoWithStorage[127.0.0.1:35171,DS-4d2ee207-bfab-482b-9fc4-db5b5cf86038,DISK], DatanodeInfoWithStorage[127.0.0.1:37046,DS-886b09da-1ae7-4944-a6a7-bae2fb0a9ac4,DISK], DatanodeInfoWithStorage[127.0.0.1:43488,DS-bb5d2d63-4f8d-47f5-9718-079b4e2c9635,DISK]]; indices=[2, 3, 4, 5, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1249845611-172.17.0.7-1599306822670:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34118,DS-1d48b9cd-b6af-457d-9954-f06bfe5421df,DISK], DatanodeInfoWithStorage[127.0.0.1:32906,DS-27880898-0e16-4356-8b6a-e50d62e9c9b5,DISK], DatanodeInfoWithStorage[127.0.0.1:41671,DS-71f16331-b3e5-48c0-85c4-bef1a57cdd3e,DISK], DatanodeInfoWithStorage[127.0.0.1:35171,DS-4d2ee207-bfab-482b-9fc4-db5b5cf86038,DISK], DatanodeInfoWithStorage[127.0.0.1:37046,DS-886b09da-1ae7-4944-a6a7-bae2fb0a9ac4,DISK], DatanodeInfoWithStorage[127.0.0.1:43488,DS-bb5d2d63-4f8d-47f5-9718-079b4e2c9635,DISK]]; indices=[2, 3, 4, 5, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1249845611-172.17.0.7-1599306822670:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34118,DS-1d48b9cd-b6af-457d-9954-f06bfe5421df,DISK], DatanodeInfoWithStorage[127.0.0.1:32906,DS-27880898-0e16-4356-8b6a-e50d62e9c9b5,DISK], DatanodeInfoWithStorage[127.0.0.1:41671,DS-71f16331-b3e5-48c0-85c4-bef1a57cdd3e,DISK], DatanodeInfoWithStorage[127.0.0.1:35171,DS-4d2ee207-bfab-482b-9fc4-db5b5cf86038,DISK], DatanodeInfoWithStorage[127.0.0.1:37046,DS-886b09da-1ae7-4944-a6a7-bae2fb0a9ac4,DISK], DatanodeInfoWithStorage[127.0.0.1:43488,DS-bb5d2d63-4f8d-47f5-9718-079b4e2c9635,DISK]]; indices=[2, 3, 4, 5, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1144570305-172.17.0.7-1599308584474:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35779,DS-1cd17290-50a8-42f6-954d-e51ac5fed0bd,DISK], DatanodeInfoWithStorage[127.0.0.1:39505,DS-7d5524bb-6b38-4695-a668-2a497ca347df,DISK], DatanodeInfoWithStorage[127.0.0.1:35687,DS-d4e8133f-27c6-40cd-b513-2b94bab67668,DISK], DatanodeInfoWithStorage[127.0.0.1:38532,DS-44eb67f9-6144-444e-aaef-dc0fd045f77b,DISK], DatanodeInfoWithStorage[127.0.0.1:35656,DS-b6c576fb-de4a-4460-a83f-f8b9ce36ecee,DISK], DatanodeInfoWithStorage[127.0.0.1:40376,DS-232eda86-4dc6-42ba-83f6-313071750394,DISK], DatanodeInfoWithStorage[127.0.0.1:45391,DS-ae125c5d-6fc3-407f-bd30-678830829e2b,DISK]]; indices=[0, 1, 2, 3, 4, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1144570305-172.17.0.7-1599308584474:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35779,DS-1cd17290-50a8-42f6-954d-e51ac5fed0bd,DISK], DatanodeInfoWithStorage[127.0.0.1:39505,DS-7d5524bb-6b38-4695-a668-2a497ca347df,DISK], DatanodeInfoWithStorage[127.0.0.1:35687,DS-d4e8133f-27c6-40cd-b513-2b94bab67668,DISK], DatanodeInfoWithStorage[127.0.0.1:38532,DS-44eb67f9-6144-444e-aaef-dc0fd045f77b,DISK], DatanodeInfoWithStorage[127.0.0.1:35656,DS-b6c576fb-de4a-4460-a83f-f8b9ce36ecee,DISK], DatanodeInfoWithStorage[127.0.0.1:40376,DS-232eda86-4dc6-42ba-83f6-313071750394,DISK], DatanodeInfoWithStorage[127.0.0.1:45391,DS-ae125c5d-6fc3-407f-bd30-678830829e2b,DISK]]; indices=[0, 1, 2, 3, 4, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1144570305-172.17.0.7-1599308584474:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35779,DS-1cd17290-50a8-42f6-954d-e51ac5fed0bd,DISK], DatanodeInfoWithStorage[127.0.0.1:39505,DS-7d5524bb-6b38-4695-a668-2a497ca347df,DISK], DatanodeInfoWithStorage[127.0.0.1:35687,DS-d4e8133f-27c6-40cd-b513-2b94bab67668,DISK], DatanodeInfoWithStorage[127.0.0.1:38532,DS-44eb67f9-6144-444e-aaef-dc0fd045f77b,DISK], DatanodeInfoWithStorage[127.0.0.1:35656,DS-b6c576fb-de4a-4460-a83f-f8b9ce36ecee,DISK], DatanodeInfoWithStorage[127.0.0.1:40376,DS-232eda86-4dc6-42ba-83f6-313071750394,DISK], DatanodeInfoWithStorage[127.0.0.1:45391,DS-ae125c5d-6fc3-407f-bd30-678830829e2b,DISK]]; indices=[0, 1, 2, 3, 4, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1144570305-172.17.0.7-1599308584474:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35779,DS-1cd17290-50a8-42f6-954d-e51ac5fed0bd,DISK], DatanodeInfoWithStorage[127.0.0.1:39505,DS-7d5524bb-6b38-4695-a668-2a497ca347df,DISK], DatanodeInfoWithStorage[127.0.0.1:35687,DS-d4e8133f-27c6-40cd-b513-2b94bab67668,DISK], DatanodeInfoWithStorage[127.0.0.1:38532,DS-44eb67f9-6144-444e-aaef-dc0fd045f77b,DISK], DatanodeInfoWithStorage[127.0.0.1:35656,DS-b6c576fb-de4a-4460-a83f-f8b9ce36ecee,DISK], DatanodeInfoWithStorage[127.0.0.1:40376,DS-232eda86-4dc6-42ba-83f6-313071750394,DISK], DatanodeInfoWithStorage[127.0.0.1:45391,DS-ae125c5d-6fc3-407f-bd30-678830829e2b,DISK]]; indices=[0, 1, 2, 3, 4, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-953974510-172.17.0.7-1599309154488:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41719,DS-ed402421-9eb0-4e81-9daf-59251717d987,DISK], DatanodeInfoWithStorage[127.0.0.1:35080,DS-ed8cecf0-9a3e-469a-bb4b-efe8206cc20f,DISK], DatanodeInfoWithStorage[127.0.0.1:45796,DS-3cec8f4e-7041-43dd-879d-392e534363f1,DISK], DatanodeInfoWithStorage[127.0.0.1:36486,DS-8d190074-a346-4d66-b67c-4bb4b8720479,DISK], DatanodeInfoWithStorage[127.0.0.1:41909,DS-bc1a7ccc-892b-4a96-813e-411da72a381f,DISK], DatanodeInfoWithStorage[127.0.0.1:41237,DS-0edf5552-b6ea-4326-b5fc-4a76a0059708,DISK]]; indices=[2, 3, 4, 5, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-953974510-172.17.0.7-1599309154488:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41719,DS-ed402421-9eb0-4e81-9daf-59251717d987,DISK], DatanodeInfoWithStorage[127.0.0.1:35080,DS-ed8cecf0-9a3e-469a-bb4b-efe8206cc20f,DISK], DatanodeInfoWithStorage[127.0.0.1:45796,DS-3cec8f4e-7041-43dd-879d-392e534363f1,DISK], DatanodeInfoWithStorage[127.0.0.1:36486,DS-8d190074-a346-4d66-b67c-4bb4b8720479,DISK], DatanodeInfoWithStorage[127.0.0.1:41909,DS-bc1a7ccc-892b-4a96-813e-411da72a381f,DISK], DatanodeInfoWithStorage[127.0.0.1:41237,DS-0edf5552-b6ea-4326-b5fc-4a76a0059708,DISK]]; indices=[2, 3, 4, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-953974510-172.17.0.7-1599309154488:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41719,DS-ed402421-9eb0-4e81-9daf-59251717d987,DISK], DatanodeInfoWithStorage[127.0.0.1:35080,DS-ed8cecf0-9a3e-469a-bb4b-efe8206cc20f,DISK], DatanodeInfoWithStorage[127.0.0.1:45796,DS-3cec8f4e-7041-43dd-879d-392e534363f1,DISK], DatanodeInfoWithStorage[127.0.0.1:36486,DS-8d190074-a346-4d66-b67c-4bb4b8720479,DISK], DatanodeInfoWithStorage[127.0.0.1:41909,DS-bc1a7ccc-892b-4a96-813e-411da72a381f,DISK], DatanodeInfoWithStorage[127.0.0.1:41237,DS-0edf5552-b6ea-4326-b5fc-4a76a0059708,DISK]]; indices=[2, 3, 4, 5, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-953974510-172.17.0.7-1599309154488:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41719,DS-ed402421-9eb0-4e81-9daf-59251717d987,DISK], DatanodeInfoWithStorage[127.0.0.1:35080,DS-ed8cecf0-9a3e-469a-bb4b-efe8206cc20f,DISK], DatanodeInfoWithStorage[127.0.0.1:45796,DS-3cec8f4e-7041-43dd-879d-392e534363f1,DISK], DatanodeInfoWithStorage[127.0.0.1:36486,DS-8d190074-a346-4d66-b67c-4bb4b8720479,DISK], DatanodeInfoWithStorage[127.0.0.1:41909,DS-bc1a7ccc-892b-4a96-813e-411da72a381f,DISK], DatanodeInfoWithStorage[127.0.0.1:41237,DS-0edf5552-b6ea-4326-b5fc-4a76a0059708,DISK]]; indices=[2, 3, 4, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1143554242-172.17.0.7-1599309323855:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36174,DS-ec06f169-be67-4cd9-9bbb-b6aaf09377a1,DISK], DatanodeInfoWithStorage[127.0.0.1:40692,DS-43934505-5f4c-4819-9d36-e6c6111c5a0d,DISK], DatanodeInfoWithStorage[127.0.0.1:42398,DS-2b78cc9d-2588-43f5-8d82-a87ceb33ecf2,DISK], DatanodeInfoWithStorage[127.0.0.1:42509,DS-af04a9d9-b009-4637-af12-2c5180f85cf3,DISK], DatanodeInfoWithStorage[127.0.0.1:43128,DS-4b247665-9327-4e95-a7cc-d4cb48b419f7,DISK], DatanodeInfoWithStorage[127.0.0.1:34160,DS-b8bfb306-845d-49af-bb3f-fe3cea99e9e7,DISK]]; indices=[0, 2, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1143554242-172.17.0.7-1599309323855:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36174,DS-ec06f169-be67-4cd9-9bbb-b6aaf09377a1,DISK], DatanodeInfoWithStorage[127.0.0.1:40692,DS-43934505-5f4c-4819-9d36-e6c6111c5a0d,DISK], DatanodeInfoWithStorage[127.0.0.1:42398,DS-2b78cc9d-2588-43f5-8d82-a87ceb33ecf2,DISK], DatanodeInfoWithStorage[127.0.0.1:42509,DS-af04a9d9-b009-4637-af12-2c5180f85cf3,DISK], DatanodeInfoWithStorage[127.0.0.1:43128,DS-4b247665-9327-4e95-a7cc-d4cb48b419f7,DISK], DatanodeInfoWithStorage[127.0.0.1:34160,DS-b8bfb306-845d-49af-bb3f-fe3cea99e9e7,DISK]]; indices=[0, 2, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1143554242-172.17.0.7-1599309323855:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36174,DS-ec06f169-be67-4cd9-9bbb-b6aaf09377a1,DISK], DatanodeInfoWithStorage[127.0.0.1:40692,DS-43934505-5f4c-4819-9d36-e6c6111c5a0d,DISK], DatanodeInfoWithStorage[127.0.0.1:42398,DS-2b78cc9d-2588-43f5-8d82-a87ceb33ecf2,DISK], DatanodeInfoWithStorage[127.0.0.1:42509,DS-af04a9d9-b009-4637-af12-2c5180f85cf3,DISK], DatanodeInfoWithStorage[127.0.0.1:43128,DS-4b247665-9327-4e95-a7cc-d4cb48b419f7,DISK], DatanodeInfoWithStorage[127.0.0.1:34160,DS-b8bfb306-845d-49af-bb3f-fe3cea99e9e7,DISK]]; indices=[0, 2, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1143554242-172.17.0.7-1599309323855:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36174,DS-ec06f169-be67-4cd9-9bbb-b6aaf09377a1,DISK], DatanodeInfoWithStorage[127.0.0.1:40692,DS-43934505-5f4c-4819-9d36-e6c6111c5a0d,DISK], DatanodeInfoWithStorage[127.0.0.1:42398,DS-2b78cc9d-2588-43f5-8d82-a87ceb33ecf2,DISK], DatanodeInfoWithStorage[127.0.0.1:42509,DS-af04a9d9-b009-4637-af12-2c5180f85cf3,DISK], DatanodeInfoWithStorage[127.0.0.1:43128,DS-4b247665-9327-4e95-a7cc-d4cb48b419f7,DISK], DatanodeInfoWithStorage[127.0.0.1:34160,DS-b8bfb306-845d-49af-bb3f-fe3cea99e9e7,DISK]]; indices=[0, 2, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:338)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1039465965-172.17.0.7-1599309678257:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34690,DS-c64d4f66-2626-4e2d-aadc-072222e00d3c,DISK], DatanodeInfoWithStorage[127.0.0.1:35360,DS-cab2fbb6-1d3b-444e-891a-ff6c7aa81925,DISK], DatanodeInfoWithStorage[127.0.0.1:43396,DS-b08830b8-62f2-49ec-ba59-b6e08b725264,DISK], DatanodeInfoWithStorage[127.0.0.1:45889,DS-5dbe505c-af6e-4746-806a-e72a985f506d,DISK], DatanodeInfoWithStorage[127.0.0.1:43108,DS-7d258e91-a23c-4f2b-b5a1-0deff4aaf015,DISK], DatanodeInfoWithStorage[127.0.0.1:39026,DS-8da13f11-c30b-45dd-a81e-0cdf1ca7b325,DISK]]; indices=[0, 1, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1039465965-172.17.0.7-1599309678257:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34690,DS-c64d4f66-2626-4e2d-aadc-072222e00d3c,DISK], DatanodeInfoWithStorage[127.0.0.1:35360,DS-cab2fbb6-1d3b-444e-891a-ff6c7aa81925,DISK], DatanodeInfoWithStorage[127.0.0.1:43396,DS-b08830b8-62f2-49ec-ba59-b6e08b725264,DISK], DatanodeInfoWithStorage[127.0.0.1:45889,DS-5dbe505c-af6e-4746-806a-e72a985f506d,DISK], DatanodeInfoWithStorage[127.0.0.1:43108,DS-7d258e91-a23c-4f2b-b5a1-0deff4aaf015,DISK], DatanodeInfoWithStorage[127.0.0.1:39026,DS-8da13f11-c30b-45dd-a81e-0cdf1ca7b325,DISK]]; indices=[0, 1, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1039465965-172.17.0.7-1599309678257:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34690,DS-c64d4f66-2626-4e2d-aadc-072222e00d3c,DISK], DatanodeInfoWithStorage[127.0.0.1:35360,DS-cab2fbb6-1d3b-444e-891a-ff6c7aa81925,DISK], DatanodeInfoWithStorage[127.0.0.1:43396,DS-b08830b8-62f2-49ec-ba59-b6e08b725264,DISK], DatanodeInfoWithStorage[127.0.0.1:45889,DS-5dbe505c-af6e-4746-806a-e72a985f506d,DISK], DatanodeInfoWithStorage[127.0.0.1:43108,DS-7d258e91-a23c-4f2b-b5a1-0deff4aaf015,DISK], DatanodeInfoWithStorage[127.0.0.1:39026,DS-8da13f11-c30b-45dd-a81e-0cdf1ca7b325,DISK]]; indices=[0, 1, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1039465965-172.17.0.7-1599309678257:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34690,DS-c64d4f66-2626-4e2d-aadc-072222e00d3c,DISK], DatanodeInfoWithStorage[127.0.0.1:35360,DS-cab2fbb6-1d3b-444e-891a-ff6c7aa81925,DISK], DatanodeInfoWithStorage[127.0.0.1:43396,DS-b08830b8-62f2-49ec-ba59-b6e08b725264,DISK], DatanodeInfoWithStorage[127.0.0.1:45889,DS-5dbe505c-af6e-4746-806a-e72a985f506d,DISK], DatanodeInfoWithStorage[127.0.0.1:43108,DS-7d258e91-a23c-4f2b-b5a1-0deff4aaf015,DISK], DatanodeInfoWithStorage[127.0.0.1:39026,DS-8da13f11-c30b-45dd-a81e-0cdf1ca7b325,DISK]]; indices=[0, 1, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:338)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-989972004-172.17.0.7-1599310061603:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39236,DS-e1ad332d-7373-43cf-be6c-72fa6a86536c,DISK], DatanodeInfoWithStorage[127.0.0.1:41539,DS-483b2796-0328-4f2a-8c5f-ca3d204db431,DISK], DatanodeInfoWithStorage[127.0.0.1:44929,DS-d8391b2b-540d-46be-a45a-dfb1bed00803,DISK], DatanodeInfoWithStorage[127.0.0.1:41361,DS-f2688ae5-7c1e-4225-baef-c984f37f7bea,DISK], DatanodeInfoWithStorage[127.0.0.1:45561,DS-de42ab01-9751-4936-bd7e-554671440e25,DISK], DatanodeInfoWithStorage[127.0.0.1:33196,DS-e8cb2fdf-7b7a-491a-b31c-6df73cd98e47,DISK]]; indices=[0, 1, 2, 3, 4, 6]}];  lastLocatedBlock=LocatedStripedBlock{BP-989972004-172.17.0.7-1599310061603:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39236,DS-e1ad332d-7373-43cf-be6c-72fa6a86536c,DISK], DatanodeInfoWithStorage[127.0.0.1:41539,DS-483b2796-0328-4f2a-8c5f-ca3d204db431,DISK], DatanodeInfoWithStorage[127.0.0.1:44929,DS-d8391b2b-540d-46be-a45a-dfb1bed00803,DISK], DatanodeInfoWithStorage[127.0.0.1:41361,DS-f2688ae5-7c1e-4225-baef-c984f37f7bea,DISK], DatanodeInfoWithStorage[127.0.0.1:45561,DS-de42ab01-9751-4936-bd7e-554671440e25,DISK], DatanodeInfoWithStorage[127.0.0.1:33196,DS-e8cb2fdf-7b7a-491a-b31c-6df73cd98e47,DISK]]; indices=[0, 1, 2, 3, 4, 6]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-989972004-172.17.0.7-1599310061603:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39236,DS-e1ad332d-7373-43cf-be6c-72fa6a86536c,DISK], DatanodeInfoWithStorage[127.0.0.1:41539,DS-483b2796-0328-4f2a-8c5f-ca3d204db431,DISK], DatanodeInfoWithStorage[127.0.0.1:44929,DS-d8391b2b-540d-46be-a45a-dfb1bed00803,DISK], DatanodeInfoWithStorage[127.0.0.1:41361,DS-f2688ae5-7c1e-4225-baef-c984f37f7bea,DISK], DatanodeInfoWithStorage[127.0.0.1:45561,DS-de42ab01-9751-4936-bd7e-554671440e25,DISK], DatanodeInfoWithStorage[127.0.0.1:33196,DS-e8cb2fdf-7b7a-491a-b31c-6df73cd98e47,DISK]]; indices=[0, 1, 2, 3, 4, 6]}];  lastLocatedBlock=LocatedStripedBlock{BP-989972004-172.17.0.7-1599310061603:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39236,DS-e1ad332d-7373-43cf-be6c-72fa6a86536c,DISK], DatanodeInfoWithStorage[127.0.0.1:41539,DS-483b2796-0328-4f2a-8c5f-ca3d204db431,DISK], DatanodeInfoWithStorage[127.0.0.1:44929,DS-d8391b2b-540d-46be-a45a-dfb1bed00803,DISK], DatanodeInfoWithStorage[127.0.0.1:41361,DS-f2688ae5-7c1e-4225-baef-c984f37f7bea,DISK], DatanodeInfoWithStorage[127.0.0.1:45561,DS-de42ab01-9751-4936-bd7e-554671440e25,DISK], DatanodeInfoWithStorage[127.0.0.1:33196,DS-e8cb2fdf-7b7a-491a-b31c-6df73cd98e47,DISK]]; indices=[0, 1, 2, 3, 4, 6]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-506740920-172.17.0.7-1599310540369:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35848,DS-2571a2d8-6572-4e13-a704-77539a3b4e6e,DISK], DatanodeInfoWithStorage[127.0.0.1:37340,DS-e592770a-ebe8-4148-bd79-07d4c3714a60,DISK], DatanodeInfoWithStorage[127.0.0.1:41956,DS-8b8befe2-0cbc-42a1-9187-8d55fa143a7f,DISK], DatanodeInfoWithStorage[127.0.0.1:35165,DS-7c942a17-2c8f-4c01-a538-2e6555da54a5,DISK], DatanodeInfoWithStorage[127.0.0.1:36636,DS-07c51a97-714f-4ea2-a717-27bedb48f5a9,DISK], DatanodeInfoWithStorage[127.0.0.1:44075,DS-af91ee84-0f9b-408e-91ac-1d50f358ef04,DISK], DatanodeInfoWithStorage[127.0.0.1:45863,DS-d1a9b578-3bae-4127-ac40-b3261440f4dc,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6]}];  lastLocatedBlock=LocatedStripedBlock{BP-506740920-172.17.0.7-1599310540369:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35848,DS-2571a2d8-6572-4e13-a704-77539a3b4e6e,DISK], DatanodeInfoWithStorage[127.0.0.1:37340,DS-e592770a-ebe8-4148-bd79-07d4c3714a60,DISK], DatanodeInfoWithStorage[127.0.0.1:41956,DS-8b8befe2-0cbc-42a1-9187-8d55fa143a7f,DISK], DatanodeInfoWithStorage[127.0.0.1:35165,DS-7c942a17-2c8f-4c01-a538-2e6555da54a5,DISK], DatanodeInfoWithStorage[127.0.0.1:36636,DS-07c51a97-714f-4ea2-a717-27bedb48f5a9,DISK], DatanodeInfoWithStorage[127.0.0.1:44075,DS-af91ee84-0f9b-408e-91ac-1d50f358ef04,DISK], DatanodeInfoWithStorage[127.0.0.1:45863,DS-d1a9b578-3bae-4127-ac40-b3261440f4dc,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-506740920-172.17.0.7-1599310540369:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35848,DS-2571a2d8-6572-4e13-a704-77539a3b4e6e,DISK], DatanodeInfoWithStorage[127.0.0.1:37340,DS-e592770a-ebe8-4148-bd79-07d4c3714a60,DISK], DatanodeInfoWithStorage[127.0.0.1:41956,DS-8b8befe2-0cbc-42a1-9187-8d55fa143a7f,DISK], DatanodeInfoWithStorage[127.0.0.1:35165,DS-7c942a17-2c8f-4c01-a538-2e6555da54a5,DISK], DatanodeInfoWithStorage[127.0.0.1:36636,DS-07c51a97-714f-4ea2-a717-27bedb48f5a9,DISK], DatanodeInfoWithStorage[127.0.0.1:44075,DS-af91ee84-0f9b-408e-91ac-1d50f358ef04,DISK], DatanodeInfoWithStorage[127.0.0.1:45863,DS-d1a9b578-3bae-4127-ac40-b3261440f4dc,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6]}];  lastLocatedBlock=LocatedStripedBlock{BP-506740920-172.17.0.7-1599310540369:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35848,DS-2571a2d8-6572-4e13-a704-77539a3b4e6e,DISK], DatanodeInfoWithStorage[127.0.0.1:37340,DS-e592770a-ebe8-4148-bd79-07d4c3714a60,DISK], DatanodeInfoWithStorage[127.0.0.1:41956,DS-8b8befe2-0cbc-42a1-9187-8d55fa143a7f,DISK], DatanodeInfoWithStorage[127.0.0.1:35165,DS-7c942a17-2c8f-4c01-a538-2e6555da54a5,DISK], DatanodeInfoWithStorage[127.0.0.1:36636,DS-07c51a97-714f-4ea2-a717-27bedb48f5a9,DISK], DatanodeInfoWithStorage[127.0.0.1:44075,DS-af91ee84-0f9b-408e-91ac-1d50f358ef04,DISK], DatanodeInfoWithStorage[127.0.0.1:45863,DS-d1a9b578-3bae-4127-ac40-b3261440f4dc,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-924317680-172.17.0.7-1599311721185:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36348,DS-3b9997e3-86a9-4a20-8220-d271b452212d,DISK], DatanodeInfoWithStorage[127.0.0.1:45671,DS-e2e3313e-c00d-46a0-9db7-8b854e5f9cd4,DISK], DatanodeInfoWithStorage[127.0.0.1:38459,DS-ea0a6c15-f127-4160-a02c-9b581bae3c98,DISK], DatanodeInfoWithStorage[127.0.0.1:37544,DS-2aea1221-0f9b-4bb2-a0b1-ce867c4336b2,DISK], DatanodeInfoWithStorage[127.0.0.1:45787,DS-7ab76eb9-f67d-40ba-b041-325062754c65,DISK], DatanodeInfoWithStorage[127.0.0.1:39399,DS-80e39575-05e6-483d-86ea-568a297d811c,DISK]]; indices=[0, 1, 2, 3, 4, 6]}];  lastLocatedBlock=LocatedStripedBlock{BP-924317680-172.17.0.7-1599311721185:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36348,DS-3b9997e3-86a9-4a20-8220-d271b452212d,DISK], DatanodeInfoWithStorage[127.0.0.1:45671,DS-e2e3313e-c00d-46a0-9db7-8b854e5f9cd4,DISK], DatanodeInfoWithStorage[127.0.0.1:38459,DS-ea0a6c15-f127-4160-a02c-9b581bae3c98,DISK], DatanodeInfoWithStorage[127.0.0.1:37544,DS-2aea1221-0f9b-4bb2-a0b1-ce867c4336b2,DISK], DatanodeInfoWithStorage[127.0.0.1:45787,DS-7ab76eb9-f67d-40ba-b041-325062754c65,DISK], DatanodeInfoWithStorage[127.0.0.1:39399,DS-80e39575-05e6-483d-86ea-568a297d811c,DISK]]; indices=[0, 1, 2, 3, 4, 6]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-924317680-172.17.0.7-1599311721185:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36348,DS-3b9997e3-86a9-4a20-8220-d271b452212d,DISK], DatanodeInfoWithStorage[127.0.0.1:45671,DS-e2e3313e-c00d-46a0-9db7-8b854e5f9cd4,DISK], DatanodeInfoWithStorage[127.0.0.1:38459,DS-ea0a6c15-f127-4160-a02c-9b581bae3c98,DISK], DatanodeInfoWithStorage[127.0.0.1:37544,DS-2aea1221-0f9b-4bb2-a0b1-ce867c4336b2,DISK], DatanodeInfoWithStorage[127.0.0.1:45787,DS-7ab76eb9-f67d-40ba-b041-325062754c65,DISK], DatanodeInfoWithStorage[127.0.0.1:39399,DS-80e39575-05e6-483d-86ea-568a297d811c,DISK]]; indices=[0, 1, 2, 3, 4, 6]}];  lastLocatedBlock=LocatedStripedBlock{BP-924317680-172.17.0.7-1599311721185:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36348,DS-3b9997e3-86a9-4a20-8220-d271b452212d,DISK], DatanodeInfoWithStorage[127.0.0.1:45671,DS-e2e3313e-c00d-46a0-9db7-8b854e5f9cd4,DISK], DatanodeInfoWithStorage[127.0.0.1:38459,DS-ea0a6c15-f127-4160-a02c-9b581bae3c98,DISK], DatanodeInfoWithStorage[127.0.0.1:37544,DS-2aea1221-0f9b-4bb2-a0b1-ce867c4336b2,DISK], DatanodeInfoWithStorage[127.0.0.1:45787,DS-7ab76eb9-f67d-40ba-b041-325062754c65,DISK], DatanodeInfoWithStorage[127.0.0.1:39399,DS-80e39575-05e6-483d-86ea-568a297d811c,DISK]]; indices=[0, 1, 2, 3, 4, 6]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-659105018-172.17.0.7-1599312403587:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45094,DS-f7643b01-d43c-41b6-9b32-206261daeb6f,DISK], DatanodeInfoWithStorage[127.0.0.1:43341,DS-8398dd76-ab55-4472-9822-d97206d5015b,DISK], DatanodeInfoWithStorage[127.0.0.1:37868,DS-4c29b600-c4e0-4c14-a41f-7ee438cb1714,DISK], DatanodeInfoWithStorage[127.0.0.1:33550,DS-6d902d00-c741-449d-8ee3-96e1aeb3cd38,DISK], DatanodeInfoWithStorage[127.0.0.1:39926,DS-c138b9e9-5681-49a0-be6e-cd4be626b397,DISK], DatanodeInfoWithStorage[127.0.0.1:43975,DS-42f02683-0fc2-47fd-a122-943e99ddc151,DISK]]; indices=[0, 1, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-659105018-172.17.0.7-1599312403587:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45094,DS-f7643b01-d43c-41b6-9b32-206261daeb6f,DISK], DatanodeInfoWithStorage[127.0.0.1:43341,DS-8398dd76-ab55-4472-9822-d97206d5015b,DISK], DatanodeInfoWithStorage[127.0.0.1:37868,DS-4c29b600-c4e0-4c14-a41f-7ee438cb1714,DISK], DatanodeInfoWithStorage[127.0.0.1:33550,DS-6d902d00-c741-449d-8ee3-96e1aeb3cd38,DISK], DatanodeInfoWithStorage[127.0.0.1:39926,DS-c138b9e9-5681-49a0-be6e-cd4be626b397,DISK], DatanodeInfoWithStorage[127.0.0.1:43975,DS-42f02683-0fc2-47fd-a122-943e99ddc151,DISK]]; indices=[0, 1, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-659105018-172.17.0.7-1599312403587:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45094,DS-f7643b01-d43c-41b6-9b32-206261daeb6f,DISK], DatanodeInfoWithStorage[127.0.0.1:43341,DS-8398dd76-ab55-4472-9822-d97206d5015b,DISK], DatanodeInfoWithStorage[127.0.0.1:37868,DS-4c29b600-c4e0-4c14-a41f-7ee438cb1714,DISK], DatanodeInfoWithStorage[127.0.0.1:33550,DS-6d902d00-c741-449d-8ee3-96e1aeb3cd38,DISK], DatanodeInfoWithStorage[127.0.0.1:39926,DS-c138b9e9-5681-49a0-be6e-cd4be626b397,DISK], DatanodeInfoWithStorage[127.0.0.1:43975,DS-42f02683-0fc2-47fd-a122-943e99ddc151,DISK]]; indices=[0, 1, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-659105018-172.17.0.7-1599312403587:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45094,DS-f7643b01-d43c-41b6-9b32-206261daeb6f,DISK], DatanodeInfoWithStorage[127.0.0.1:43341,DS-8398dd76-ab55-4472-9822-d97206d5015b,DISK], DatanodeInfoWithStorage[127.0.0.1:37868,DS-4c29b600-c4e0-4c14-a41f-7ee438cb1714,DISK], DatanodeInfoWithStorage[127.0.0.1:33550,DS-6d902d00-c741-449d-8ee3-96e1aeb3cd38,DISK], DatanodeInfoWithStorage[127.0.0.1:39926,DS-c138b9e9-5681-49a0-be6e-cd4be626b397,DISK], DatanodeInfoWithStorage[127.0.0.1:43975,DS-42f02683-0fc2-47fd-a122-943e99ddc151,DISK]]; indices=[0, 1, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:338)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 10000
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[3]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1014287728-172.17.0.7-1599314504407:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39441,DS-b6da444f-8839-408d-a0b3-e5072e0b3fd4,DISK], DatanodeInfoWithStorage[127.0.0.1:36736,DS-ed42433a-4651-478e-864a-10f7b1d2c331,DISK], DatanodeInfoWithStorage[127.0.0.1:38708,DS-6eae113f-f267-4662-9449-b80e87826b1d,DISK], DatanodeInfoWithStorage[127.0.0.1:42456,DS-8b190bdf-23b5-4f59-a606-d2498c291a16,DISK], DatanodeInfoWithStorage[127.0.0.1:37879,DS-fbb3e716-2b2c-4f75-894c-94d440333fff,DISK], DatanodeInfoWithStorage[127.0.0.1:35740,DS-0ba6042c-cae5-4350-adcd-0d107c76e87d,DISK]]; indices=[0, 1, 4, 5, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1014287728-172.17.0.7-1599314504407:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39441,DS-b6da444f-8839-408d-a0b3-e5072e0b3fd4,DISK], DatanodeInfoWithStorage[127.0.0.1:36736,DS-ed42433a-4651-478e-864a-10f7b1d2c331,DISK], DatanodeInfoWithStorage[127.0.0.1:38708,DS-6eae113f-f267-4662-9449-b80e87826b1d,DISK], DatanodeInfoWithStorage[127.0.0.1:42456,DS-8b190bdf-23b5-4f59-a606-d2498c291a16,DISK], DatanodeInfoWithStorage[127.0.0.1:37879,DS-fbb3e716-2b2c-4f75-894c-94d440333fff,DISK], DatanodeInfoWithStorage[127.0.0.1:35740,DS-0ba6042c-cae5-4350-adcd-0d107c76e87d,DISK]]; indices=[0, 1, 4, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1014287728-172.17.0.7-1599314504407:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39441,DS-b6da444f-8839-408d-a0b3-e5072e0b3fd4,DISK], DatanodeInfoWithStorage[127.0.0.1:36736,DS-ed42433a-4651-478e-864a-10f7b1d2c331,DISK], DatanodeInfoWithStorage[127.0.0.1:38708,DS-6eae113f-f267-4662-9449-b80e87826b1d,DISK], DatanodeInfoWithStorage[127.0.0.1:42456,DS-8b190bdf-23b5-4f59-a606-d2498c291a16,DISK], DatanodeInfoWithStorage[127.0.0.1:37879,DS-fbb3e716-2b2c-4f75-894c-94d440333fff,DISK], DatanodeInfoWithStorage[127.0.0.1:35740,DS-0ba6042c-cae5-4350-adcd-0d107c76e87d,DISK]]; indices=[0, 1, 4, 5, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1014287728-172.17.0.7-1599314504407:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39441,DS-b6da444f-8839-408d-a0b3-e5072e0b3fd4,DISK], DatanodeInfoWithStorage[127.0.0.1:36736,DS-ed42433a-4651-478e-864a-10f7b1d2c331,DISK], DatanodeInfoWithStorage[127.0.0.1:38708,DS-6eae113f-f267-4662-9449-b80e87826b1d,DISK], DatanodeInfoWithStorage[127.0.0.1:42456,DS-8b190bdf-23b5-4f59-a606-d2498c291a16,DISK], DatanodeInfoWithStorage[127.0.0.1:37879,DS-fbb3e716-2b2c-4f75-894c-94d440333fff,DISK], DatanodeInfoWithStorage[127.0.0.1:35740,DS-0ba6042c-cae5-4350-adcd-0d107c76e87d,DISK]]; indices=[0, 1, 4, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 11 out of 50
v1v1v2v2 failed with probability 8 out of 50
result: might be true error
Total execution time in seconds : 9747

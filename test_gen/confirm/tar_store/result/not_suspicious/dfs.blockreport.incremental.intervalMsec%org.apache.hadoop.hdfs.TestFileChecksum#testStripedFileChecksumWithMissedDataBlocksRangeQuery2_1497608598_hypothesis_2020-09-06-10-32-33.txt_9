reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-946877344-172.17.0.12-1599388900779:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33768,DS-1e853fa8-f7dd-4312-8d58-a2e72192d2db,DISK], DatanodeInfoWithStorage[127.0.0.1:40395,DS-70968ad1-d78b-4d11-b25e-1b43125e46c2,DISK], DatanodeInfoWithStorage[127.0.0.1:39444,DS-98abf97b-5237-4b8d-941d-758974910fc7,DISK], DatanodeInfoWithStorage[127.0.0.1:33150,DS-b2f2ef65-9c28-430e-8635-b1a998aed145,DISK], DatanodeInfoWithStorage[127.0.0.1:45054,DS-6e91238a-c96a-4b75-b022-10bb8d6c384e,DISK], DatanodeInfoWithStorage[127.0.0.1:37620,DS-1c656b3b-6b66-4156-bc9f-cb7f31a8acf5,DISK], DatanodeInfoWithStorage[127.0.0.1:43296,DS-fc576755-c108-4831-9f00-9d81554ddcd4,DISK], DatanodeInfoWithStorage[127.0.0.1:43269,DS-f4f981d7-38d3-4885-9857-aad8f6914db4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-946877344-172.17.0.12-1599388900779:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33768,DS-1e853fa8-f7dd-4312-8d58-a2e72192d2db,DISK], DatanodeInfoWithStorage[127.0.0.1:40395,DS-70968ad1-d78b-4d11-b25e-1b43125e46c2,DISK], DatanodeInfoWithStorage[127.0.0.1:39444,DS-98abf97b-5237-4b8d-941d-758974910fc7,DISK], DatanodeInfoWithStorage[127.0.0.1:33150,DS-b2f2ef65-9c28-430e-8635-b1a998aed145,DISK], DatanodeInfoWithStorage[127.0.0.1:45054,DS-6e91238a-c96a-4b75-b022-10bb8d6c384e,DISK], DatanodeInfoWithStorage[127.0.0.1:37620,DS-1c656b3b-6b66-4156-bc9f-cb7f31a8acf5,DISK], DatanodeInfoWithStorage[127.0.0.1:43296,DS-fc576755-c108-4831-9f00-9d81554ddcd4,DISK], DatanodeInfoWithStorage[127.0.0.1:43269,DS-f4f981d7-38d3-4885-9857-aad8f6914db4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery2(TestFileChecksum.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-15297602-172.17.0.12-1599388945027:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44365,DS-6e98dcec-afcb-47ca-b729-05288c5012c3,DISK], DatanodeInfoWithStorage[127.0.0.1:40887,DS-93f1774b-5504-43d0-af51-8390d9e82af0,DISK], DatanodeInfoWithStorage[127.0.0.1:32814,DS-c3499135-3152-4640-8ca3-ccdd675cb3b2,DISK], DatanodeInfoWithStorage[127.0.0.1:41883,DS-c15a5fb7-dd54-4109-a45b-fab60f4b8142,DISK], DatanodeInfoWithStorage[127.0.0.1:39713,DS-8f093c8e-0994-499c-b40a-691876f97203,DISK], DatanodeInfoWithStorage[127.0.0.1:35174,DS-150fbd6f-70cb-430a-80cc-75164593213b,DISK], DatanodeInfoWithStorage[127.0.0.1:46393,DS-e5232fca-198c-4c4e-93ee-9aaf8e2d5a28,DISK], DatanodeInfoWithStorage[127.0.0.1:32809,DS-78893b5e-a83d-447d-8c0a-d6759ae7ccc5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-15297602-172.17.0.12-1599388945027:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44365,DS-6e98dcec-afcb-47ca-b729-05288c5012c3,DISK], DatanodeInfoWithStorage[127.0.0.1:40887,DS-93f1774b-5504-43d0-af51-8390d9e82af0,DISK], DatanodeInfoWithStorage[127.0.0.1:32814,DS-c3499135-3152-4640-8ca3-ccdd675cb3b2,DISK], DatanodeInfoWithStorage[127.0.0.1:41883,DS-c15a5fb7-dd54-4109-a45b-fab60f4b8142,DISK], DatanodeInfoWithStorage[127.0.0.1:39713,DS-8f093c8e-0994-499c-b40a-691876f97203,DISK], DatanodeInfoWithStorage[127.0.0.1:35174,DS-150fbd6f-70cb-430a-80cc-75164593213b,DISK], DatanodeInfoWithStorage[127.0.0.1:46393,DS-e5232fca-198c-4c4e-93ee-9aaf8e2d5a28,DISK], DatanodeInfoWithStorage[127.0.0.1:32809,DS-78893b5e-a83d-447d-8c0a-d6759ae7ccc5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery2(TestFileChecksum.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2123470441-172.17.0.12-1599389154586:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43331,DS-ac1882a4-0cc8-4046-99ee-b166149ed08c,DISK], DatanodeInfoWithStorage[127.0.0.1:37372,DS-28d0ef2c-22b8-48c1-96ae-dde432ac9149,DISK], DatanodeInfoWithStorage[127.0.0.1:42587,DS-809ccb08-1b64-428f-bda5-cad7f702bd3e,DISK], DatanodeInfoWithStorage[127.0.0.1:36694,DS-2964fdda-0a58-4c59-a730-86350fdd6504,DISK], DatanodeInfoWithStorage[127.0.0.1:41959,DS-316b02c7-3528-42de-b7f7-51709478929f,DISK], DatanodeInfoWithStorage[127.0.0.1:44637,DS-bf554ce4-c05f-4814-b236-4e8b26fca523,DISK], DatanodeInfoWithStorage[127.0.0.1:39311,DS-a0000b08-e981-4563-b807-01e845d6923e,DISK], DatanodeInfoWithStorage[127.0.0.1:42645,DS-cdeea44a-0726-4f9c-8bfd-6d58affb3524,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2123470441-172.17.0.12-1599389154586:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43331,DS-ac1882a4-0cc8-4046-99ee-b166149ed08c,DISK], DatanodeInfoWithStorage[127.0.0.1:37372,DS-28d0ef2c-22b8-48c1-96ae-dde432ac9149,DISK], DatanodeInfoWithStorage[127.0.0.1:42587,DS-809ccb08-1b64-428f-bda5-cad7f702bd3e,DISK], DatanodeInfoWithStorage[127.0.0.1:36694,DS-2964fdda-0a58-4c59-a730-86350fdd6504,DISK], DatanodeInfoWithStorage[127.0.0.1:41959,DS-316b02c7-3528-42de-b7f7-51709478929f,DISK], DatanodeInfoWithStorage[127.0.0.1:44637,DS-bf554ce4-c05f-4814-b236-4e8b26fca523,DISK], DatanodeInfoWithStorage[127.0.0.1:39311,DS-a0000b08-e981-4563-b807-01e845d6923e,DISK], DatanodeInfoWithStorage[127.0.0.1:42645,DS-cdeea44a-0726-4f9c-8bfd-6d58affb3524,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery2(TestFileChecksum.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1292188852-172.17.0.12-1599389404185:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37993,DS-178ea8fc-5516-4bb6-bbc5-bbb7c186e062,DISK], DatanodeInfoWithStorage[127.0.0.1:38184,DS-296314ce-ddb0-4fd6-9de6-15c7442d6271,DISK], DatanodeInfoWithStorage[127.0.0.1:35112,DS-0e4ee77e-b450-4841-a214-493ff0933f2f,DISK], DatanodeInfoWithStorage[127.0.0.1:40858,DS-b09f0dc2-0d45-488b-b44f-5617b3c55ca9,DISK], DatanodeInfoWithStorage[127.0.0.1:33424,DS-8cd777a4-9b5e-40ce-b45e-6d621a756337,DISK], DatanodeInfoWithStorage[127.0.0.1:45763,DS-9237113c-6558-4195-9002-c4d390528a14,DISK], DatanodeInfoWithStorage[127.0.0.1:45046,DS-dd8c55d0-ef2c-4334-a519-f2fadc9035be,DISK], DatanodeInfoWithStorage[127.0.0.1:35899,DS-8533319e-7f74-410d-ab3d-5dc85707c0d9,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1292188852-172.17.0.12-1599389404185:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37993,DS-178ea8fc-5516-4bb6-bbc5-bbb7c186e062,DISK], DatanodeInfoWithStorage[127.0.0.1:38184,DS-296314ce-ddb0-4fd6-9de6-15c7442d6271,DISK], DatanodeInfoWithStorage[127.0.0.1:35112,DS-0e4ee77e-b450-4841-a214-493ff0933f2f,DISK], DatanodeInfoWithStorage[127.0.0.1:40858,DS-b09f0dc2-0d45-488b-b44f-5617b3c55ca9,DISK], DatanodeInfoWithStorage[127.0.0.1:33424,DS-8cd777a4-9b5e-40ce-b45e-6d621a756337,DISK], DatanodeInfoWithStorage[127.0.0.1:45763,DS-9237113c-6558-4195-9002-c4d390528a14,DISK], DatanodeInfoWithStorage[127.0.0.1:45046,DS-dd8c55d0-ef2c-4334-a519-f2fadc9035be,DISK], DatanodeInfoWithStorage[127.0.0.1:35899,DS-8533319e-7f74-410d-ab3d-5dc85707c0d9,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery2(TestFileChecksum.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1614484493-172.17.0.12-1599389764693:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36638,DS-c88e8615-3352-4c17-9025-a9bac6b774b2,DISK], DatanodeInfoWithStorage[127.0.0.1:37812,DS-08c38d98-6da5-4375-996f-700c98651209,DISK], DatanodeInfoWithStorage[127.0.0.1:44757,DS-c7dd7298-c107-4007-8654-44ee79d2fe19,DISK], DatanodeInfoWithStorage[127.0.0.1:41621,DS-297b17e0-2845-456a-868e-58a0b93878f0,DISK], DatanodeInfoWithStorage[127.0.0.1:42587,DS-429acc17-5079-4785-bb92-fa572388113b,DISK], DatanodeInfoWithStorage[127.0.0.1:42038,DS-71021b4c-c632-4de3-96f4-25260cc2871c,DISK], DatanodeInfoWithStorage[127.0.0.1:41326,DS-e7a7e48c-a6a5-4e74-839c-1d930c308603,DISK], DatanodeInfoWithStorage[127.0.0.1:42610,DS-0efc7d02-2877-46a6-9d68-0ccf1533276c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1614484493-172.17.0.12-1599389764693:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36638,DS-c88e8615-3352-4c17-9025-a9bac6b774b2,DISK], DatanodeInfoWithStorage[127.0.0.1:37812,DS-08c38d98-6da5-4375-996f-700c98651209,DISK], DatanodeInfoWithStorage[127.0.0.1:44757,DS-c7dd7298-c107-4007-8654-44ee79d2fe19,DISK], DatanodeInfoWithStorage[127.0.0.1:41621,DS-297b17e0-2845-456a-868e-58a0b93878f0,DISK], DatanodeInfoWithStorage[127.0.0.1:42587,DS-429acc17-5079-4785-bb92-fa572388113b,DISK], DatanodeInfoWithStorage[127.0.0.1:42038,DS-71021b4c-c632-4de3-96f4-25260cc2871c,DISK], DatanodeInfoWithStorage[127.0.0.1:41326,DS-e7a7e48c-a6a5-4e74-839c-1d930c308603,DISK], DatanodeInfoWithStorage[127.0.0.1:42610,DS-0efc7d02-2877-46a6-9d68-0ccf1533276c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery2(TestFileChecksum.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2122452009-172.17.0.12-1599389784863:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33644,DS-e6823dc6-2dcb-4fc6-a8f8-445eca87ec26,DISK], DatanodeInfoWithStorage[127.0.0.1:34348,DS-c36c264f-37b7-4f31-9e7d-73ffd190d8e4,DISK], DatanodeInfoWithStorage[127.0.0.1:45221,DS-51af7b07-c4cd-4f0c-9844-ef53092c30c0,DISK], DatanodeInfoWithStorage[127.0.0.1:34419,DS-8545be31-b545-4d53-91ac-6cd818cefc63,DISK], DatanodeInfoWithStorage[127.0.0.1:45686,DS-1af2c37b-3b94-4157-8f4a-069f7a98c9bc,DISK], DatanodeInfoWithStorage[127.0.0.1:41413,DS-998ae200-3cdb-48bc-b88f-d1e2d09ad37a,DISK], DatanodeInfoWithStorage[127.0.0.1:35404,DS-17549671-5a49-4406-8dba-2b1ba9f84635,DISK], DatanodeInfoWithStorage[127.0.0.1:36494,DS-0f00311b-4c01-4bcf-80fa-fa0aa821aa6c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2122452009-172.17.0.12-1599389784863:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33644,DS-e6823dc6-2dcb-4fc6-a8f8-445eca87ec26,DISK], DatanodeInfoWithStorage[127.0.0.1:34348,DS-c36c264f-37b7-4f31-9e7d-73ffd190d8e4,DISK], DatanodeInfoWithStorage[127.0.0.1:45221,DS-51af7b07-c4cd-4f0c-9844-ef53092c30c0,DISK], DatanodeInfoWithStorage[127.0.0.1:34419,DS-8545be31-b545-4d53-91ac-6cd818cefc63,DISK], DatanodeInfoWithStorage[127.0.0.1:45686,DS-1af2c37b-3b94-4157-8f4a-069f7a98c9bc,DISK], DatanodeInfoWithStorage[127.0.0.1:41413,DS-998ae200-3cdb-48bc-b88f-d1e2d09ad37a,DISK], DatanodeInfoWithStorage[127.0.0.1:35404,DS-17549671-5a49-4406-8dba-2b1ba9f84635,DISK], DatanodeInfoWithStorage[127.0.0.1:36494,DS-0f00311b-4c01-4bcf-80fa-fa0aa821aa6c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery2(TestFileChecksum.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1045431088-172.17.0.12-1599389862478:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38032,DS-ac88e6f2-7bbb-49f1-a32c-42186b66fc4e,DISK], DatanodeInfoWithStorage[127.0.0.1:32923,DS-3be5cb98-3993-4265-b5f5-9b2a44d9a0fd,DISK], DatanodeInfoWithStorage[127.0.0.1:35341,DS-8edd0e7d-ead1-4ec7-939d-d19c1f7a1ee1,DISK], DatanodeInfoWithStorage[127.0.0.1:41820,DS-d945cb68-e67e-4ec1-bd2b-48d68ec13abe,DISK], DatanodeInfoWithStorage[127.0.0.1:45399,DS-7efa56fa-2b95-4888-bcaa-dae386b41b85,DISK], DatanodeInfoWithStorage[127.0.0.1:39296,DS-aae873eb-a99d-441e-8cac-b3fe2075ded4,DISK], DatanodeInfoWithStorage[127.0.0.1:40900,DS-7423e107-8f3d-4737-b7ec-d41277cd4c18,DISK], DatanodeInfoWithStorage[127.0.0.1:34374,DS-95706358-38a6-4961-93fb-ae7c96e4beae,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1045431088-172.17.0.12-1599389862478:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38032,DS-ac88e6f2-7bbb-49f1-a32c-42186b66fc4e,DISK], DatanodeInfoWithStorage[127.0.0.1:32923,DS-3be5cb98-3993-4265-b5f5-9b2a44d9a0fd,DISK], DatanodeInfoWithStorage[127.0.0.1:35341,DS-8edd0e7d-ead1-4ec7-939d-d19c1f7a1ee1,DISK], DatanodeInfoWithStorage[127.0.0.1:41820,DS-d945cb68-e67e-4ec1-bd2b-48d68ec13abe,DISK], DatanodeInfoWithStorage[127.0.0.1:45399,DS-7efa56fa-2b95-4888-bcaa-dae386b41b85,DISK], DatanodeInfoWithStorage[127.0.0.1:39296,DS-aae873eb-a99d-441e-8cac-b3fe2075ded4,DISK], DatanodeInfoWithStorage[127.0.0.1:40900,DS-7423e107-8f3d-4737-b7ec-d41277cd4c18,DISK], DatanodeInfoWithStorage[127.0.0.1:34374,DS-95706358-38a6-4961-93fb-ae7c96e4beae,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery2(TestFileChecksum.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1012854806-172.17.0.12-1599389960994:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40475,DS-143f9cb7-1a92-4bdc-9f7a-e1c5857c7543,DISK], DatanodeInfoWithStorage[127.0.0.1:46029,DS-eb6d9527-d3a1-4539-8271-e74864461ac6,DISK], DatanodeInfoWithStorage[127.0.0.1:34915,DS-0a5cc83e-6e36-4a98-9c03-a53036a65a9f,DISK], DatanodeInfoWithStorage[127.0.0.1:36890,DS-9586379f-ca04-491d-bf70-82ea9b04a839,DISK], DatanodeInfoWithStorage[127.0.0.1:34390,DS-0cfe29cc-9705-48cf-91c5-8dde4cc0bfa4,DISK], DatanodeInfoWithStorage[127.0.0.1:34787,DS-3b5822b6-c5aa-4a6b-abb8-96f7c7ddd6a9,DISK], DatanodeInfoWithStorage[127.0.0.1:38491,DS-ea1aa55b-cf5e-4b0f-b64b-9d390519226b,DISK], DatanodeInfoWithStorage[127.0.0.1:33285,DS-dcbd2d79-f1d0-4848-805d-b0567cdbc85e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1012854806-172.17.0.12-1599389960994:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40475,DS-143f9cb7-1a92-4bdc-9f7a-e1c5857c7543,DISK], DatanodeInfoWithStorage[127.0.0.1:46029,DS-eb6d9527-d3a1-4539-8271-e74864461ac6,DISK], DatanodeInfoWithStorage[127.0.0.1:34915,DS-0a5cc83e-6e36-4a98-9c03-a53036a65a9f,DISK], DatanodeInfoWithStorage[127.0.0.1:36890,DS-9586379f-ca04-491d-bf70-82ea9b04a839,DISK], DatanodeInfoWithStorage[127.0.0.1:34390,DS-0cfe29cc-9705-48cf-91c5-8dde4cc0bfa4,DISK], DatanodeInfoWithStorage[127.0.0.1:34787,DS-3b5822b6-c5aa-4a6b-abb8-96f7c7ddd6a9,DISK], DatanodeInfoWithStorage[127.0.0.1:38491,DS-ea1aa55b-cf5e-4b0f-b64b-9d390519226b,DISK], DatanodeInfoWithStorage[127.0.0.1:33285,DS-dcbd2d79-f1d0-4848-805d-b0567cdbc85e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery2(TestFileChecksum.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1
failureMessage: File /striped/stripedFileChecksum1 could only be written to 5 of the 6 required nodes for RS-6-3-1024k. There are 5 datanode(s) running and 5 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2226)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

stackTrace: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /striped/stripedFileChecksum1 could only be written to 5 of the 6 required nodes for RS-6-3-1024k. There are 5 datanode(s) running and 5 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2226)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy28.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy29.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.allocateNewBlock(DFSStripedOutputStream.java:480)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:526)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:865)
	at org.apache.hadoop.hdfs.TestFileChecksum.prepareTestFiles(TestFileChecksum.java:602)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:292)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery2(TestFileChecksum.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1314414512-172.17.0.12-1599390465822:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42181,DS-b7c111f3-8ded-4dae-b393-8d0a56999b07,DISK], DatanodeInfoWithStorage[127.0.0.1:33259,DS-90f4a14a-b9a5-4a32-9dc2-21fad08f635c,DISK], DatanodeInfoWithStorage[127.0.0.1:37213,DS-3517a958-678e-4c26-a856-efdc97fca62b,DISK], DatanodeInfoWithStorage[127.0.0.1:36611,DS-728e7fec-5159-4ccf-843c-b98535e5d49e,DISK], DatanodeInfoWithStorage[127.0.0.1:34203,DS-1d99036c-a75b-4c87-bd6e-c38656dcef40,DISK], DatanodeInfoWithStorage[127.0.0.1:46562,DS-8fdde0d9-1f4a-43fe-9c1d-e3c7ef3b8daf,DISK], DatanodeInfoWithStorage[127.0.0.1:45022,DS-f8debc3f-1912-4f7c-bb21-24af4522571b,DISK], DatanodeInfoWithStorage[127.0.0.1:42418,DS-3ab80c30-8d3e-4763-8491-f64443bad236,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1314414512-172.17.0.12-1599390465822:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42181,DS-b7c111f3-8ded-4dae-b393-8d0a56999b07,DISK], DatanodeInfoWithStorage[127.0.0.1:33259,DS-90f4a14a-b9a5-4a32-9dc2-21fad08f635c,DISK], DatanodeInfoWithStorage[127.0.0.1:37213,DS-3517a958-678e-4c26-a856-efdc97fca62b,DISK], DatanodeInfoWithStorage[127.0.0.1:36611,DS-728e7fec-5159-4ccf-843c-b98535e5d49e,DISK], DatanodeInfoWithStorage[127.0.0.1:34203,DS-1d99036c-a75b-4c87-bd6e-c38656dcef40,DISK], DatanodeInfoWithStorage[127.0.0.1:46562,DS-8fdde0d9-1f4a-43fe-9c1d-e3c7ef3b8daf,DISK], DatanodeInfoWithStorage[127.0.0.1:45022,DS-f8debc3f-1912-4f7c-bb21-24af4522571b,DISK], DatanodeInfoWithStorage[127.0.0.1:42418,DS-3ab80c30-8d3e-4763-8491-f64443bad236,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery2(TestFileChecksum.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1229664723-172.17.0.12-1599390893564:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39772,DS-85bba847-6c2b-46c5-9f20-3fc211e0e1cb,DISK], DatanodeInfoWithStorage[127.0.0.1:36998,DS-07a35785-8069-48be-8991-87bbf028ec7a,DISK], DatanodeInfoWithStorage[127.0.0.1:33760,DS-600874e0-6ae1-449f-9754-3098922f21b1,DISK], DatanodeInfoWithStorage[127.0.0.1:43623,DS-79b52e03-a5f2-4390-a055-e96726266a1f,DISK], DatanodeInfoWithStorage[127.0.0.1:45054,DS-a8e2562b-8e0a-4747-9f2f-151496efbd37,DISK], DatanodeInfoWithStorage[127.0.0.1:34493,DS-699d335b-678b-455a-86a5-b6da8da82943,DISK], DatanodeInfoWithStorage[127.0.0.1:34054,DS-04e66ba8-b4ed-442a-a8dd-eae53ed88735,DISK], DatanodeInfoWithStorage[127.0.0.1:45890,DS-00a9f5e5-8e5d-44cd-9d1a-b6236e2b59b3,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1229664723-172.17.0.12-1599390893564:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39772,DS-85bba847-6c2b-46c5-9f20-3fc211e0e1cb,DISK], DatanodeInfoWithStorage[127.0.0.1:36998,DS-07a35785-8069-48be-8991-87bbf028ec7a,DISK], DatanodeInfoWithStorage[127.0.0.1:33760,DS-600874e0-6ae1-449f-9754-3098922f21b1,DISK], DatanodeInfoWithStorage[127.0.0.1:43623,DS-79b52e03-a5f2-4390-a055-e96726266a1f,DISK], DatanodeInfoWithStorage[127.0.0.1:45054,DS-a8e2562b-8e0a-4747-9f2f-151496efbd37,DISK], DatanodeInfoWithStorage[127.0.0.1:34493,DS-699d335b-678b-455a-86a5-b6da8da82943,DISK], DatanodeInfoWithStorage[127.0.0.1:34054,DS-04e66ba8-b4ed-442a-a8dd-eae53ed88735,DISK], DatanodeInfoWithStorage[127.0.0.1:45890,DS-00a9f5e5-8e5d-44cd-9d1a-b6236e2b59b3,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery2(TestFileChecksum.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery2
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-986130006-172.17.0.12-1599391313312:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35727,DS-1144ee0b-e958-4cad-87d3-ab39c788e16f,DISK], DatanodeInfoWithStorage[127.0.0.1:40752,DS-9e327d47-bce2-47c3-adf4-b9bf081da90e,DISK], DatanodeInfoWithStorage[127.0.0.1:43793,DS-29df9860-a29a-41fc-a95d-077448ae7c35,DISK], DatanodeInfoWithStorage[127.0.0.1:41808,DS-afc0efb7-5caa-478d-b4cd-b36d9c88ce72,DISK], DatanodeInfoWithStorage[127.0.0.1:42579,DS-f4c99158-1771-481f-b173-156c465c8d58,DISK], DatanodeInfoWithStorage[127.0.0.1:38763,DS-944bf909-c387-468e-8897-f01eabf95b42,DISK], DatanodeInfoWithStorage[127.0.0.1:33843,DS-05b4293d-f04b-4867-947d-929e994be038,DISK], DatanodeInfoWithStorage[127.0.0.1:45201,DS-3f0922f4-d076-444f-b5aa-6dd5dcefc648,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-986130006-172.17.0.12-1599391313312:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35727,DS-1144ee0b-e958-4cad-87d3-ab39c788e16f,DISK], DatanodeInfoWithStorage[127.0.0.1:40752,DS-9e327d47-bce2-47c3-adf4-b9bf081da90e,DISK], DatanodeInfoWithStorage[127.0.0.1:43793,DS-29df9860-a29a-41fc-a95d-077448ae7c35,DISK], DatanodeInfoWithStorage[127.0.0.1:41808,DS-afc0efb7-5caa-478d-b4cd-b36d9c88ce72,DISK], DatanodeInfoWithStorage[127.0.0.1:42579,DS-f4c99158-1771-481f-b173-156c465c8d58,DISK], DatanodeInfoWithStorage[127.0.0.1:38763,DS-944bf909-c387-468e-8897-f01eabf95b42,DISK], DatanodeInfoWithStorage[127.0.0.1:33843,DS-05b4293d-f04b-4867-947d-929e994be038,DISK], DatanodeInfoWithStorage[127.0.0.1:45201,DS-3f0922f4-d076-444f-b5aa-6dd5dcefc648,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery2(TestFileChecksum.java:322)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 2 out of 50
v1v1v2v2 failed with probability 10 out of 50
result: false positive !!!
Total execution time in seconds : 3050

reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: Invalid buffer found, not allowing null
stackTrace: org.apache.hadoop.HadoopIllegalArgumentException: Invalid buffer found, not allowing null
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.checkOutputBuffers(ByteBufferDecodingState.java:132)
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.<init>(ByteBufferDecodingState.java:48)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:86)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:170)
	at org.apache.hadoop.hdfs.StripeReader.decodeAndFillBuffer(StripeReader.java:433)
	at org.apache.hadoop.hdfs.StatefulStripeReader.decode(StatefulStripeReader.java:94)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:390)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readOneStripe(DFSStripedInputStream.java:326)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readWithStrategy(DFSStripedInputStream.java:419)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyStatefulRead(StripedFileTestUtil.java:126)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:141)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=2, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1310328025-172.17.0.10-1599296937148:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44935,DS-b3b4665a-ed74-4195-b3c0-835c1bef2b15,DISK], DatanodeInfoWithStorage[127.0.0.1:39025,DS-4007eb26-4cc4-47ed-9e9e-7f614225a5ad,DISK], DatanodeInfoWithStorage[127.0.0.1:45133,DS-20d5d3f9-4d95-45cb-912b-ac9e3ff5c1e5,DISK], DatanodeInfoWithStorage[127.0.0.1:34912,DS-bdb69b24-fdcb-4f65-a33b-5cbe22e99d21,DISK], DatanodeInfoWithStorage[127.0.0.1:42044,DS-d8a89cd9-e5ee-4a23-ac5c-070bc9c13b7d,DISK], DatanodeInfoWithStorage[127.0.0.1:46461,DS-2eee7ebd-a796-415f-9da9-63e85607f9a1,DISK], DatanodeInfoWithStorage[127.0.0.1:36988,DS-3ce2cf5f-191b-4003-96e9-583f8df4eb73,DISK]]; indices=[1, 2, 3, 4, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1310328025-172.17.0.10-1599296937148:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44935,DS-b3b4665a-ed74-4195-b3c0-835c1bef2b15,DISK], DatanodeInfoWithStorage[127.0.0.1:39025,DS-4007eb26-4cc4-47ed-9e9e-7f614225a5ad,DISK], DatanodeInfoWithStorage[127.0.0.1:45133,DS-20d5d3f9-4d95-45cb-912b-ac9e3ff5c1e5,DISK], DatanodeInfoWithStorage[127.0.0.1:34912,DS-bdb69b24-fdcb-4f65-a33b-5cbe22e99d21,DISK], DatanodeInfoWithStorage[127.0.0.1:42044,DS-d8a89cd9-e5ee-4a23-ac5c-070bc9c13b7d,DISK], DatanodeInfoWithStorage[127.0.0.1:46461,DS-2eee7ebd-a796-415f-9da9-63e85607f9a1,DISK], DatanodeInfoWithStorage[127.0.0.1:36988,DS-3ce2cf5f-191b-4003-96e9-583f8df4eb73,DISK]]; indices=[1, 2, 3, 4, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=2, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1310328025-172.17.0.10-1599296937148:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44935,DS-b3b4665a-ed74-4195-b3c0-835c1bef2b15,DISK], DatanodeInfoWithStorage[127.0.0.1:39025,DS-4007eb26-4cc4-47ed-9e9e-7f614225a5ad,DISK], DatanodeInfoWithStorage[127.0.0.1:45133,DS-20d5d3f9-4d95-45cb-912b-ac9e3ff5c1e5,DISK], DatanodeInfoWithStorage[127.0.0.1:34912,DS-bdb69b24-fdcb-4f65-a33b-5cbe22e99d21,DISK], DatanodeInfoWithStorage[127.0.0.1:42044,DS-d8a89cd9-e5ee-4a23-ac5c-070bc9c13b7d,DISK], DatanodeInfoWithStorage[127.0.0.1:46461,DS-2eee7ebd-a796-415f-9da9-63e85607f9a1,DISK], DatanodeInfoWithStorage[127.0.0.1:36988,DS-3ce2cf5f-191b-4003-96e9-583f8df4eb73,DISK]]; indices=[1, 2, 3, 4, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1310328025-172.17.0.10-1599296937148:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44935,DS-b3b4665a-ed74-4195-b3c0-835c1bef2b15,DISK], DatanodeInfoWithStorage[127.0.0.1:39025,DS-4007eb26-4cc4-47ed-9e9e-7f614225a5ad,DISK], DatanodeInfoWithStorage[127.0.0.1:45133,DS-20d5d3f9-4d95-45cb-912b-ac9e3ff5c1e5,DISK], DatanodeInfoWithStorage[127.0.0.1:34912,DS-bdb69b24-fdcb-4f65-a33b-5cbe22e99d21,DISK], DatanodeInfoWithStorage[127.0.0.1:42044,DS-d8a89cd9-e5ee-4a23-ac5c-070bc9c13b7d,DISK], DatanodeInfoWithStorage[127.0.0.1:46461,DS-2eee7ebd-a796-415f-9da9-63e85607f9a1,DISK], DatanodeInfoWithStorage[127.0.0.1:36988,DS-3ce2cf5f-191b-4003-96e9-583f8df4eb73,DISK]]; indices=[1, 2, 3, 4, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=1, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-674712066-172.17.0.10-1599297417709:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46559,DS-661034d2-3f17-489d-bd98-cbba521a755a,DISK], DatanodeInfoWithStorage[127.0.0.1:40470,DS-def27cbb-ae88-4edf-a82b-ba7c54b502ff,DISK], DatanodeInfoWithStorage[127.0.0.1:41146,DS-80dbe17e-c61e-4519-a02e-0edd8f690fff,DISK], DatanodeInfoWithStorage[127.0.0.1:37755,DS-52821a16-bd2f-4be4-8d75-40d34035902c,DISK], DatanodeInfoWithStorage[127.0.0.1:39496,DS-77adac53-09f1-4452-8f82-c5ffe4fac310,DISK], DatanodeInfoWithStorage[127.0.0.1:38243,DS-6cee7233-a1dd-4f95-9ec6-38f487b3bec5,DISK]]; indices=[0, 2, 4, 5, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-674712066-172.17.0.10-1599297417709:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46559,DS-661034d2-3f17-489d-bd98-cbba521a755a,DISK], DatanodeInfoWithStorage[127.0.0.1:40470,DS-def27cbb-ae88-4edf-a82b-ba7c54b502ff,DISK], DatanodeInfoWithStorage[127.0.0.1:41146,DS-80dbe17e-c61e-4519-a02e-0edd8f690fff,DISK], DatanodeInfoWithStorage[127.0.0.1:37755,DS-52821a16-bd2f-4be4-8d75-40d34035902c,DISK], DatanodeInfoWithStorage[127.0.0.1:39496,DS-77adac53-09f1-4452-8f82-c5ffe4fac310,DISK], DatanodeInfoWithStorage[127.0.0.1:38243,DS-6cee7233-a1dd-4f95-9ec6-38f487b3bec5,DISK]]; indices=[0, 2, 4, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=1, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-674712066-172.17.0.10-1599297417709:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46559,DS-661034d2-3f17-489d-bd98-cbba521a755a,DISK], DatanodeInfoWithStorage[127.0.0.1:40470,DS-def27cbb-ae88-4edf-a82b-ba7c54b502ff,DISK], DatanodeInfoWithStorage[127.0.0.1:41146,DS-80dbe17e-c61e-4519-a02e-0edd8f690fff,DISK], DatanodeInfoWithStorage[127.0.0.1:37755,DS-52821a16-bd2f-4be4-8d75-40d34035902c,DISK], DatanodeInfoWithStorage[127.0.0.1:39496,DS-77adac53-09f1-4452-8f82-c5ffe4fac310,DISK], DatanodeInfoWithStorage[127.0.0.1:38243,DS-6cee7233-a1dd-4f95-9ec6-38f487b3bec5,DISK]]; indices=[0, 2, 4, 5, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-674712066-172.17.0.10-1599297417709:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46559,DS-661034d2-3f17-489d-bd98-cbba521a755a,DISK], DatanodeInfoWithStorage[127.0.0.1:40470,DS-def27cbb-ae88-4edf-a82b-ba7c54b502ff,DISK], DatanodeInfoWithStorage[127.0.0.1:41146,DS-80dbe17e-c61e-4519-a02e-0edd8f690fff,DISK], DatanodeInfoWithStorage[127.0.0.1:37755,DS-52821a16-bd2f-4be4-8d75-40d34035902c,DISK], DatanodeInfoWithStorage[127.0.0.1:39496,DS-77adac53-09f1-4452-8f82-c5ffe4fac310,DISK], DatanodeInfoWithStorage[127.0.0.1:38243,DS-6cee7233-a1dd-4f95-9ec6-38f487b3bec5,DISK]]; indices=[0, 2, 4, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-215830594-172.17.0.10-1599298658486:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45016,DS-a253a3df-0c82-4ed1-bcb7-839e3ee85d87,DISK], DatanodeInfoWithStorage[127.0.0.1:45747,DS-1f6e53c8-4a85-4db8-a547-cffb3c8b8fa3,DISK], DatanodeInfoWithStorage[127.0.0.1:45916,DS-c55a77f2-f98a-45a7-9ec5-cda5c6ad5e86,DISK], DatanodeInfoWithStorage[127.0.0.1:37417,DS-6951f2f0-2f52-4af9-8be7-053158d870fc,DISK], DatanodeInfoWithStorage[127.0.0.1:36419,DS-fdf5289a-0c23-4cec-994d-45cb6025d75b,DISK], DatanodeInfoWithStorage[127.0.0.1:33625,DS-d9beb3a3-0fdc-4f6f-90d0-b6c730e41470,DISK], DatanodeInfoWithStorage[127.0.0.1:41741,DS-a62ebf9e-661d-4f7f-b533-99c41935eb0a,DISK]]; indices=[2, 3, 4, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-215830594-172.17.0.10-1599298658486:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45016,DS-a253a3df-0c82-4ed1-bcb7-839e3ee85d87,DISK], DatanodeInfoWithStorage[127.0.0.1:45747,DS-1f6e53c8-4a85-4db8-a547-cffb3c8b8fa3,DISK], DatanodeInfoWithStorage[127.0.0.1:45916,DS-c55a77f2-f98a-45a7-9ec5-cda5c6ad5e86,DISK], DatanodeInfoWithStorage[127.0.0.1:37417,DS-6951f2f0-2f52-4af9-8be7-053158d870fc,DISK], DatanodeInfoWithStorage[127.0.0.1:36419,DS-fdf5289a-0c23-4cec-994d-45cb6025d75b,DISK], DatanodeInfoWithStorage[127.0.0.1:33625,DS-d9beb3a3-0fdc-4f6f-90d0-b6c730e41470,DISK], DatanodeInfoWithStorage[127.0.0.1:41741,DS-a62ebf9e-661d-4f7f-b533-99c41935eb0a,DISK]]; indices=[2, 3, 4, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-215830594-172.17.0.10-1599298658486:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45016,DS-a253a3df-0c82-4ed1-bcb7-839e3ee85d87,DISK], DatanodeInfoWithStorage[127.0.0.1:45747,DS-1f6e53c8-4a85-4db8-a547-cffb3c8b8fa3,DISK], DatanodeInfoWithStorage[127.0.0.1:45916,DS-c55a77f2-f98a-45a7-9ec5-cda5c6ad5e86,DISK], DatanodeInfoWithStorage[127.0.0.1:37417,DS-6951f2f0-2f52-4af9-8be7-053158d870fc,DISK], DatanodeInfoWithStorage[127.0.0.1:36419,DS-fdf5289a-0c23-4cec-994d-45cb6025d75b,DISK], DatanodeInfoWithStorage[127.0.0.1:33625,DS-d9beb3a3-0fdc-4f6f-90d0-b6c730e41470,DISK], DatanodeInfoWithStorage[127.0.0.1:41741,DS-a62ebf9e-661d-4f7f-b533-99c41935eb0a,DISK]]; indices=[2, 3, 4, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-215830594-172.17.0.10-1599298658486:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45016,DS-a253a3df-0c82-4ed1-bcb7-839e3ee85d87,DISK], DatanodeInfoWithStorage[127.0.0.1:45747,DS-1f6e53c8-4a85-4db8-a547-cffb3c8b8fa3,DISK], DatanodeInfoWithStorage[127.0.0.1:45916,DS-c55a77f2-f98a-45a7-9ec5-cda5c6ad5e86,DISK], DatanodeInfoWithStorage[127.0.0.1:37417,DS-6951f2f0-2f52-4af9-8be7-053158d870fc,DISK], DatanodeInfoWithStorage[127.0.0.1:36419,DS-fdf5289a-0c23-4cec-994d-45cb6025d75b,DISK], DatanodeInfoWithStorage[127.0.0.1:33625,DS-d9beb3a3-0fdc-4f6f-90d0-b6c730e41470,DISK], DatanodeInfoWithStorage[127.0.0.1:41741,DS-a62ebf9e-661d-4f7f-b533-99c41935eb0a,DISK]]; indices=[2, 3, 4, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-548833674-172.17.0.10-1599299491921:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40886,DS-b52a99f7-23a9-4fa9-8ebb-180b6763d595,DISK], DatanodeInfoWithStorage[127.0.0.1:40838,DS-de84104f-a70d-4f8f-8009-2251fbe208f3,DISK], DatanodeInfoWithStorage[127.0.0.1:44824,DS-31c4a144-53bc-4ef3-b278-1d582c76ef1f,DISK], DatanodeInfoWithStorage[127.0.0.1:36454,DS-c7089dcf-ee8d-42c4-a555-094ae0553c62,DISK], DatanodeInfoWithStorage[127.0.0.1:38355,DS-5637849e-69b6-4a6d-b635-68cac790bd4f,DISK], DatanodeInfoWithStorage[127.0.0.1:34646,DS-b7074aa1-12de-4b49-9983-1d4f44ed0b5f,DISK], DatanodeInfoWithStorage[127.0.0.1:37062,DS-9c1b7d9f-b2c0-432e-b80f-a1f2bd8329b6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-548833674-172.17.0.10-1599299491921:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40886,DS-b52a99f7-23a9-4fa9-8ebb-180b6763d595,DISK], DatanodeInfoWithStorage[127.0.0.1:40838,DS-de84104f-a70d-4f8f-8009-2251fbe208f3,DISK], DatanodeInfoWithStorage[127.0.0.1:44824,DS-31c4a144-53bc-4ef3-b278-1d582c76ef1f,DISK], DatanodeInfoWithStorage[127.0.0.1:36454,DS-c7089dcf-ee8d-42c4-a555-094ae0553c62,DISK], DatanodeInfoWithStorage[127.0.0.1:38355,DS-5637849e-69b6-4a6d-b635-68cac790bd4f,DISK], DatanodeInfoWithStorage[127.0.0.1:34646,DS-b7074aa1-12de-4b49-9983-1d4f44ed0b5f,DISK], DatanodeInfoWithStorage[127.0.0.1:37062,DS-9c1b7d9f-b2c0-432e-b80f-a1f2bd8329b6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-548833674-172.17.0.10-1599299491921:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40886,DS-b52a99f7-23a9-4fa9-8ebb-180b6763d595,DISK], DatanodeInfoWithStorage[127.0.0.1:40838,DS-de84104f-a70d-4f8f-8009-2251fbe208f3,DISK], DatanodeInfoWithStorage[127.0.0.1:44824,DS-31c4a144-53bc-4ef3-b278-1d582c76ef1f,DISK], DatanodeInfoWithStorage[127.0.0.1:36454,DS-c7089dcf-ee8d-42c4-a555-094ae0553c62,DISK], DatanodeInfoWithStorage[127.0.0.1:38355,DS-5637849e-69b6-4a6d-b635-68cac790bd4f,DISK], DatanodeInfoWithStorage[127.0.0.1:34646,DS-b7074aa1-12de-4b49-9983-1d4f44ed0b5f,DISK], DatanodeInfoWithStorage[127.0.0.1:37062,DS-9c1b7d9f-b2c0-432e-b80f-a1f2bd8329b6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-548833674-172.17.0.10-1599299491921:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40886,DS-b52a99f7-23a9-4fa9-8ebb-180b6763d595,DISK], DatanodeInfoWithStorage[127.0.0.1:40838,DS-de84104f-a70d-4f8f-8009-2251fbe208f3,DISK], DatanodeInfoWithStorage[127.0.0.1:44824,DS-31c4a144-53bc-4ef3-b278-1d582c76ef1f,DISK], DatanodeInfoWithStorage[127.0.0.1:36454,DS-c7089dcf-ee8d-42c4-a555-094ae0553c62,DISK], DatanodeInfoWithStorage[127.0.0.1:38355,DS-5637849e-69b6-4a6d-b635-68cac790bd4f,DISK], DatanodeInfoWithStorage[127.0.0.1:34646,DS-b7074aa1-12de-4b49-9983-1d4f44ed0b5f,DISK], DatanodeInfoWithStorage[127.0.0.1:37062,DS-9c1b7d9f-b2c0-432e-b80f-a1f2bd8329b6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:378)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1476200550-172.17.0.10-1599300418495:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42187,DS-14047eb4-6922-4fc2-a3a0-021cab496c12,DISK], DatanodeInfoWithStorage[127.0.0.1:41768,DS-bffb3409-a427-41c0-98ea-5a50f68f74d6,DISK], DatanodeInfoWithStorage[127.0.0.1:37431,DS-870ec42a-5c31-435d-a8f6-13bbb053f2bb,DISK], DatanodeInfoWithStorage[127.0.0.1:42661,DS-35199372-06e1-4911-a71e-b0dfb46881dd,DISK], DatanodeInfoWithStorage[127.0.0.1:34869,DS-4346c850-ff8d-4ffd-80eb-9505af1d5461,DISK], DatanodeInfoWithStorage[127.0.0.1:45566,DS-2e0e2217-fc50-48d7-b28f-b16e2f68f3c0,DISK], DatanodeInfoWithStorage[127.0.0.1:42191,DS-3e749435-c1fd-4de7-8b02-5238b916512b,DISK]]; indices=[0, 1, 2, 3, 4, 5, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1476200550-172.17.0.10-1599300418495:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42187,DS-14047eb4-6922-4fc2-a3a0-021cab496c12,DISK], DatanodeInfoWithStorage[127.0.0.1:41768,DS-bffb3409-a427-41c0-98ea-5a50f68f74d6,DISK], DatanodeInfoWithStorage[127.0.0.1:37431,DS-870ec42a-5c31-435d-a8f6-13bbb053f2bb,DISK], DatanodeInfoWithStorage[127.0.0.1:42661,DS-35199372-06e1-4911-a71e-b0dfb46881dd,DISK], DatanodeInfoWithStorage[127.0.0.1:34869,DS-4346c850-ff8d-4ffd-80eb-9505af1d5461,DISK], DatanodeInfoWithStorage[127.0.0.1:45566,DS-2e0e2217-fc50-48d7-b28f-b16e2f68f3c0,DISK], DatanodeInfoWithStorage[127.0.0.1:42191,DS-3e749435-c1fd-4de7-8b02-5238b916512b,DISK]]; indices=[0, 1, 2, 3, 4, 5, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1476200550-172.17.0.10-1599300418495:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42187,DS-14047eb4-6922-4fc2-a3a0-021cab496c12,DISK], DatanodeInfoWithStorage[127.0.0.1:41768,DS-bffb3409-a427-41c0-98ea-5a50f68f74d6,DISK], DatanodeInfoWithStorage[127.0.0.1:37431,DS-870ec42a-5c31-435d-a8f6-13bbb053f2bb,DISK], DatanodeInfoWithStorage[127.0.0.1:42661,DS-35199372-06e1-4911-a71e-b0dfb46881dd,DISK], DatanodeInfoWithStorage[127.0.0.1:34869,DS-4346c850-ff8d-4ffd-80eb-9505af1d5461,DISK], DatanodeInfoWithStorage[127.0.0.1:45566,DS-2e0e2217-fc50-48d7-b28f-b16e2f68f3c0,DISK], DatanodeInfoWithStorage[127.0.0.1:42191,DS-3e749435-c1fd-4de7-8b02-5238b916512b,DISK]]; indices=[0, 1, 2, 3, 4, 5, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1476200550-172.17.0.10-1599300418495:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42187,DS-14047eb4-6922-4fc2-a3a0-021cab496c12,DISK], DatanodeInfoWithStorage[127.0.0.1:41768,DS-bffb3409-a427-41c0-98ea-5a50f68f74d6,DISK], DatanodeInfoWithStorage[127.0.0.1:37431,DS-870ec42a-5c31-435d-a8f6-13bbb053f2bb,DISK], DatanodeInfoWithStorage[127.0.0.1:42661,DS-35199372-06e1-4911-a71e-b0dfb46881dd,DISK], DatanodeInfoWithStorage[127.0.0.1:34869,DS-4346c850-ff8d-4ffd-80eb-9505af1d5461,DISK], DatanodeInfoWithStorage[127.0.0.1:45566,DS-2e0e2217-fc50-48d7-b28f-b16e2f68f3c0,DISK], DatanodeInfoWithStorage[127.0.0.1:42191,DS-3e749435-c1fd-4de7-8b02-5238b916512b,DISK]]; indices=[0, 1, 2, 3, 4, 5, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:378)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=2, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-717842451-172.17.0.10-1599300860913:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44586,DS-df705534-579e-4dda-bebc-993d1404f7fe,DISK], DatanodeInfoWithStorage[127.0.0.1:38611,DS-4478d30a-becb-4afb-8faa-08d5f22d8590,DISK], DatanodeInfoWithStorage[127.0.0.1:41266,DS-065d829f-62d1-40c0-83e0-73476a45b810,DISK], DatanodeInfoWithStorage[127.0.0.1:40096,DS-2ed34306-1f35-4700-b479-59dcbc7431c3,DISK], DatanodeInfoWithStorage[127.0.0.1:36903,DS-30075caa-88fa-492d-8c48-b1f2ae0e2461,DISK], DatanodeInfoWithStorage[127.0.0.1:37450,DS-b8852f01-0a0b-4551-9df3-f2168a64f55a,DISK], DatanodeInfoWithStorage[127.0.0.1:39789,DS-0118ec65-2326-451e-a2a1-cc2a6c01002e,DISK]]; indices=[0, 2, 3, 4, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-717842451-172.17.0.10-1599300860913:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44586,DS-df705534-579e-4dda-bebc-993d1404f7fe,DISK], DatanodeInfoWithStorage[127.0.0.1:38611,DS-4478d30a-becb-4afb-8faa-08d5f22d8590,DISK], DatanodeInfoWithStorage[127.0.0.1:41266,DS-065d829f-62d1-40c0-83e0-73476a45b810,DISK], DatanodeInfoWithStorage[127.0.0.1:40096,DS-2ed34306-1f35-4700-b479-59dcbc7431c3,DISK], DatanodeInfoWithStorage[127.0.0.1:36903,DS-30075caa-88fa-492d-8c48-b1f2ae0e2461,DISK], DatanodeInfoWithStorage[127.0.0.1:37450,DS-b8852f01-0a0b-4551-9df3-f2168a64f55a,DISK], DatanodeInfoWithStorage[127.0.0.1:39789,DS-0118ec65-2326-451e-a2a1-cc2a6c01002e,DISK]]; indices=[0, 2, 3, 4, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=2, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-717842451-172.17.0.10-1599300860913:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44586,DS-df705534-579e-4dda-bebc-993d1404f7fe,DISK], DatanodeInfoWithStorage[127.0.0.1:38611,DS-4478d30a-becb-4afb-8faa-08d5f22d8590,DISK], DatanodeInfoWithStorage[127.0.0.1:41266,DS-065d829f-62d1-40c0-83e0-73476a45b810,DISK], DatanodeInfoWithStorage[127.0.0.1:40096,DS-2ed34306-1f35-4700-b479-59dcbc7431c3,DISK], DatanodeInfoWithStorage[127.0.0.1:36903,DS-30075caa-88fa-492d-8c48-b1f2ae0e2461,DISK], DatanodeInfoWithStorage[127.0.0.1:37450,DS-b8852f01-0a0b-4551-9df3-f2168a64f55a,DISK], DatanodeInfoWithStorage[127.0.0.1:39789,DS-0118ec65-2326-451e-a2a1-cc2a6c01002e,DISK]]; indices=[0, 2, 3, 4, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-717842451-172.17.0.10-1599300860913:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44586,DS-df705534-579e-4dda-bebc-993d1404f7fe,DISK], DatanodeInfoWithStorage[127.0.0.1:38611,DS-4478d30a-becb-4afb-8faa-08d5f22d8590,DISK], DatanodeInfoWithStorage[127.0.0.1:41266,DS-065d829f-62d1-40c0-83e0-73476a45b810,DISK], DatanodeInfoWithStorage[127.0.0.1:40096,DS-2ed34306-1f35-4700-b479-59dcbc7431c3,DISK], DatanodeInfoWithStorage[127.0.0.1:36903,DS-30075caa-88fa-492d-8c48-b1f2ae0e2461,DISK], DatanodeInfoWithStorage[127.0.0.1:37450,DS-b8852f01-0a0b-4551-9df3-f2168a64f55a,DISK], DatanodeInfoWithStorage[127.0.0.1:39789,DS-0118ec65-2326-451e-a2a1-cc2a6c01002e,DISK]]; indices=[0, 2, 3, 4, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1376327125-172.17.0.10-1599301384890:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40155,DS-c36ddaf3-2a21-46e0-8b61-ffc0c3825031,DISK], DatanodeInfoWithStorage[127.0.0.1:35725,DS-451de074-3ef9-466b-ab6e-65eea6fc0bb0,DISK], DatanodeInfoWithStorage[127.0.0.1:40111,DS-72b350e0-c5be-4d3f-b2ee-d94726ee1657,DISK], DatanodeInfoWithStorage[127.0.0.1:42963,DS-ba224a49-db87-48f9-8b84-b116cd449d7d,DISK], DatanodeInfoWithStorage[127.0.0.1:42106,DS-f78dc944-6f7e-4b50-88e3-0b7c821629c5,DISK], DatanodeInfoWithStorage[127.0.0.1:46722,DS-f122f3d2-634b-4b84-88e9-384525d3c2bb,DISK]]; indices=[1, 2, 4, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1376327125-172.17.0.10-1599301384890:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40155,DS-c36ddaf3-2a21-46e0-8b61-ffc0c3825031,DISK], DatanodeInfoWithStorage[127.0.0.1:35725,DS-451de074-3ef9-466b-ab6e-65eea6fc0bb0,DISK], DatanodeInfoWithStorage[127.0.0.1:40111,DS-72b350e0-c5be-4d3f-b2ee-d94726ee1657,DISK], DatanodeInfoWithStorage[127.0.0.1:42963,DS-ba224a49-db87-48f9-8b84-b116cd449d7d,DISK], DatanodeInfoWithStorage[127.0.0.1:42106,DS-f78dc944-6f7e-4b50-88e3-0b7c821629c5,DISK], DatanodeInfoWithStorage[127.0.0.1:46722,DS-f122f3d2-634b-4b84-88e9-384525d3c2bb,DISK]]; indices=[1, 2, 4, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1376327125-172.17.0.10-1599301384890:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40155,DS-c36ddaf3-2a21-46e0-8b61-ffc0c3825031,DISK], DatanodeInfoWithStorage[127.0.0.1:35725,DS-451de074-3ef9-466b-ab6e-65eea6fc0bb0,DISK], DatanodeInfoWithStorage[127.0.0.1:40111,DS-72b350e0-c5be-4d3f-b2ee-d94726ee1657,DISK], DatanodeInfoWithStorage[127.0.0.1:42963,DS-ba224a49-db87-48f9-8b84-b116cd449d7d,DISK], DatanodeInfoWithStorage[127.0.0.1:42106,DS-f78dc944-6f7e-4b50-88e3-0b7c821629c5,DISK], DatanodeInfoWithStorage[127.0.0.1:46722,DS-f122f3d2-634b-4b84-88e9-384525d3c2bb,DISK]]; indices=[1, 2, 4, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1376327125-172.17.0.10-1599301384890:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40155,DS-c36ddaf3-2a21-46e0-8b61-ffc0c3825031,DISK], DatanodeInfoWithStorage[127.0.0.1:35725,DS-451de074-3ef9-466b-ab6e-65eea6fc0bb0,DISK], DatanodeInfoWithStorage[127.0.0.1:40111,DS-72b350e0-c5be-4d3f-b2ee-d94726ee1657,DISK], DatanodeInfoWithStorage[127.0.0.1:42963,DS-ba224a49-db87-48f9-8b84-b116cd449d7d,DISK], DatanodeInfoWithStorage[127.0.0.1:42106,DS-f78dc944-6f7e-4b50-88e3-0b7c821629c5,DISK], DatanodeInfoWithStorage[127.0.0.1:46722,DS-f122f3d2-634b-4b84-88e9-384525d3c2bb,DISK]]; indices=[1, 2, 4, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1182516847-172.17.0.10-1599301547649:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35928,DS-408bdf32-d3cd-413f-b7b8-0b92e0bd3c69,DISK], DatanodeInfoWithStorage[127.0.0.1:38859,DS-3c006d17-23b8-4d45-be99-faeb84b9ee52,DISK], DatanodeInfoWithStorage[127.0.0.1:35006,DS-4f18c462-4614-411e-b76c-a30f762bce84,DISK], DatanodeInfoWithStorage[127.0.0.1:46062,DS-5adeed82-8c3b-49df-85cd-83cdfcc15a7f,DISK], DatanodeInfoWithStorage[127.0.0.1:33977,DS-c5dfa021-51c3-4926-9722-3abf81a05cd3,DISK], DatanodeInfoWithStorage[127.0.0.1:46243,DS-06f8455d-cff5-49a2-bffe-0a5666b92656,DISK]]; indices=[0, 2, 3, 5, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1182516847-172.17.0.10-1599301547649:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35928,DS-408bdf32-d3cd-413f-b7b8-0b92e0bd3c69,DISK], DatanodeInfoWithStorage[127.0.0.1:38859,DS-3c006d17-23b8-4d45-be99-faeb84b9ee52,DISK], DatanodeInfoWithStorage[127.0.0.1:35006,DS-4f18c462-4614-411e-b76c-a30f762bce84,DISK], DatanodeInfoWithStorage[127.0.0.1:46062,DS-5adeed82-8c3b-49df-85cd-83cdfcc15a7f,DISK], DatanodeInfoWithStorage[127.0.0.1:33977,DS-c5dfa021-51c3-4926-9722-3abf81a05cd3,DISK], DatanodeInfoWithStorage[127.0.0.1:46243,DS-06f8455d-cff5-49a2-bffe-0a5666b92656,DISK]]; indices=[0, 2, 3, 5, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1182516847-172.17.0.10-1599301547649:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35928,DS-408bdf32-d3cd-413f-b7b8-0b92e0bd3c69,DISK], DatanodeInfoWithStorage[127.0.0.1:38859,DS-3c006d17-23b8-4d45-be99-faeb84b9ee52,DISK], DatanodeInfoWithStorage[127.0.0.1:35006,DS-4f18c462-4614-411e-b76c-a30f762bce84,DISK], DatanodeInfoWithStorage[127.0.0.1:46062,DS-5adeed82-8c3b-49df-85cd-83cdfcc15a7f,DISK], DatanodeInfoWithStorage[127.0.0.1:33977,DS-c5dfa021-51c3-4926-9722-3abf81a05cd3,DISK], DatanodeInfoWithStorage[127.0.0.1:46243,DS-06f8455d-cff5-49a2-bffe-0a5666b92656,DISK]]; indices=[0, 2, 3, 5, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1182516847-172.17.0.10-1599301547649:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35928,DS-408bdf32-d3cd-413f-b7b8-0b92e0bd3c69,DISK], DatanodeInfoWithStorage[127.0.0.1:38859,DS-3c006d17-23b8-4d45-be99-faeb84b9ee52,DISK], DatanodeInfoWithStorage[127.0.0.1:35006,DS-4f18c462-4614-411e-b76c-a30f762bce84,DISK], DatanodeInfoWithStorage[127.0.0.1:46062,DS-5adeed82-8c3b-49df-85cd-83cdfcc15a7f,DISK], DatanodeInfoWithStorage[127.0.0.1:33977,DS-c5dfa021-51c3-4926-9722-3abf81a05cd3,DISK], DatanodeInfoWithStorage[127.0.0.1:46243,DS-06f8455d-cff5-49a2-bffe-0a5666b92656,DISK]]; indices=[0, 2, 3, 5, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1610559352-172.17.0.10-1599303871143:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35860,DS-f65446c4-de30-4dd1-a77b-675351aa5397,DISK], DatanodeInfoWithStorage[127.0.0.1:37621,DS-a14f7eb6-2384-4dff-bb01-73676bb05cdc,DISK], DatanodeInfoWithStorage[127.0.0.1:39810,DS-72d1d5f3-48bf-41e3-aaeb-6f59a65dc29e,DISK], DatanodeInfoWithStorage[127.0.0.1:40761,DS-d3488d42-e6c5-42b5-a92d-bf7379ce1134,DISK], DatanodeInfoWithStorage[127.0.0.1:40341,DS-9513c687-9fff-4263-b706-bc4d82fa3b10,DISK], DatanodeInfoWithStorage[127.0.0.1:43287,DS-028715bc-6a84-4513-bb89-19a8777df674,DISK]]; indices=[0, 1, 2, 3, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1610559352-172.17.0.10-1599303871143:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35860,DS-f65446c4-de30-4dd1-a77b-675351aa5397,DISK], DatanodeInfoWithStorage[127.0.0.1:37621,DS-a14f7eb6-2384-4dff-bb01-73676bb05cdc,DISK], DatanodeInfoWithStorage[127.0.0.1:39810,DS-72d1d5f3-48bf-41e3-aaeb-6f59a65dc29e,DISK], DatanodeInfoWithStorage[127.0.0.1:40761,DS-d3488d42-e6c5-42b5-a92d-bf7379ce1134,DISK], DatanodeInfoWithStorage[127.0.0.1:40341,DS-9513c687-9fff-4263-b706-bc4d82fa3b10,DISK], DatanodeInfoWithStorage[127.0.0.1:43287,DS-028715bc-6a84-4513-bb89-19a8777df674,DISK]]; indices=[0, 1, 2, 3, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1610559352-172.17.0.10-1599303871143:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35860,DS-f65446c4-de30-4dd1-a77b-675351aa5397,DISK], DatanodeInfoWithStorage[127.0.0.1:37621,DS-a14f7eb6-2384-4dff-bb01-73676bb05cdc,DISK], DatanodeInfoWithStorage[127.0.0.1:39810,DS-72d1d5f3-48bf-41e3-aaeb-6f59a65dc29e,DISK], DatanodeInfoWithStorage[127.0.0.1:40761,DS-d3488d42-e6c5-42b5-a92d-bf7379ce1134,DISK], DatanodeInfoWithStorage[127.0.0.1:40341,DS-9513c687-9fff-4263-b706-bc4d82fa3b10,DISK], DatanodeInfoWithStorage[127.0.0.1:43287,DS-028715bc-6a84-4513-bb89-19a8777df674,DISK]]; indices=[0, 1, 2, 3, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1610559352-172.17.0.10-1599303871143:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35860,DS-f65446c4-de30-4dd1-a77b-675351aa5397,DISK], DatanodeInfoWithStorage[127.0.0.1:37621,DS-a14f7eb6-2384-4dff-bb01-73676bb05cdc,DISK], DatanodeInfoWithStorage[127.0.0.1:39810,DS-72d1d5f3-48bf-41e3-aaeb-6f59a65dc29e,DISK], DatanodeInfoWithStorage[127.0.0.1:40761,DS-d3488d42-e6c5-42b5-a92d-bf7379ce1134,DISK], DatanodeInfoWithStorage[127.0.0.1:40341,DS-9513c687-9fff-4263-b706-bc4d82fa3b10,DISK], DatanodeInfoWithStorage[127.0.0.1:43287,DS-028715bc-6a84-4513-bb89-19a8777df674,DISK]]; indices=[0, 1, 2, 3, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:378)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=1, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-495300964-172.17.0.10-1599303901894:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45788,DS-fc73996f-a82a-4a9d-96c6-6d0b3145abdb,DISK], DatanodeInfoWithStorage[127.0.0.1:45370,DS-f0c67707-2f4e-4ef8-accd-b22c4730f9bd,DISK], DatanodeInfoWithStorage[127.0.0.1:33342,DS-82e14a5d-67b2-4875-a22a-55a3c66d3cd5,DISK], DatanodeInfoWithStorage[127.0.0.1:45507,DS-fb5f3ba4-bd31-4591-8719-d0d69d38b623,DISK], DatanodeInfoWithStorage[127.0.0.1:37014,DS-960a34c6-6dec-4ed2-b274-95abd169cc00,DISK], DatanodeInfoWithStorage[127.0.0.1:44245,DS-6fa88748-d524-41ef-a385-eec6426f8ed0,DISK]]; indices=[0, 2, 4, 5, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-495300964-172.17.0.10-1599303901894:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45788,DS-fc73996f-a82a-4a9d-96c6-6d0b3145abdb,DISK], DatanodeInfoWithStorage[127.0.0.1:45370,DS-f0c67707-2f4e-4ef8-accd-b22c4730f9bd,DISK], DatanodeInfoWithStorage[127.0.0.1:33342,DS-82e14a5d-67b2-4875-a22a-55a3c66d3cd5,DISK], DatanodeInfoWithStorage[127.0.0.1:45507,DS-fb5f3ba4-bd31-4591-8719-d0d69d38b623,DISK], DatanodeInfoWithStorage[127.0.0.1:37014,DS-960a34c6-6dec-4ed2-b274-95abd169cc00,DISK], DatanodeInfoWithStorage[127.0.0.1:44245,DS-6fa88748-d524-41ef-a385-eec6426f8ed0,DISK]]; indices=[0, 2, 4, 5, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=1, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-495300964-172.17.0.10-1599303901894:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45788,DS-fc73996f-a82a-4a9d-96c6-6d0b3145abdb,DISK], DatanodeInfoWithStorage[127.0.0.1:45370,DS-f0c67707-2f4e-4ef8-accd-b22c4730f9bd,DISK], DatanodeInfoWithStorage[127.0.0.1:33342,DS-82e14a5d-67b2-4875-a22a-55a3c66d3cd5,DISK], DatanodeInfoWithStorage[127.0.0.1:45507,DS-fb5f3ba4-bd31-4591-8719-d0d69d38b623,DISK], DatanodeInfoWithStorage[127.0.0.1:37014,DS-960a34c6-6dec-4ed2-b274-95abd169cc00,DISK], DatanodeInfoWithStorage[127.0.0.1:44245,DS-6fa88748-d524-41ef-a385-eec6426f8ed0,DISK]]; indices=[0, 2, 4, 5, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-495300964-172.17.0.10-1599303901894:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45788,DS-fc73996f-a82a-4a9d-96c6-6d0b3145abdb,DISK], DatanodeInfoWithStorage[127.0.0.1:45370,DS-f0c67707-2f4e-4ef8-accd-b22c4730f9bd,DISK], DatanodeInfoWithStorage[127.0.0.1:33342,DS-82e14a5d-67b2-4875-a22a-55a3c66d3cd5,DISK], DatanodeInfoWithStorage[127.0.0.1:45507,DS-fb5f3ba4-bd31-4591-8719-d0d69d38b623,DISK], DatanodeInfoWithStorage[127.0.0.1:37014,DS-960a34c6-6dec-4ed2-b274-95abd169cc00,DISK], DatanodeInfoWithStorage[127.0.0.1:44245,DS-6fa88748-d524-41ef-a385-eec6426f8ed0,DISK]]; indices=[0, 2, 4, 5, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[1]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1446916990-172.17.0.10-1599304060901:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37596,DS-e06463f2-f084-4288-bba2-84f39133646c,DISK], DatanodeInfoWithStorage[127.0.0.1:33787,DS-01bbf234-a4e2-4879-a475-cee4b2ffa9f0,DISK], DatanodeInfoWithStorage[127.0.0.1:33565,DS-afa4b5fb-e097-4804-a9c2-afac3c1474e9,DISK], DatanodeInfoWithStorage[127.0.0.1:37286,DS-41b2d69f-55c0-44a2-9ae9-6e48a1016136,DISK], DatanodeInfoWithStorage[127.0.0.1:35081,DS-c93f49ab-6d33-4145-a063-ecb05ee3252f,DISK], DatanodeInfoWithStorage[127.0.0.1:34711,DS-ad5d0081-ab15-4257-a2b0-c29ded222b45,DISK]]; indices=[0, 1, 2, 3, 5, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1446916990-172.17.0.10-1599304060901:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37596,DS-e06463f2-f084-4288-bba2-84f39133646c,DISK], DatanodeInfoWithStorage[127.0.0.1:33787,DS-01bbf234-a4e2-4879-a475-cee4b2ffa9f0,DISK], DatanodeInfoWithStorage[127.0.0.1:33565,DS-afa4b5fb-e097-4804-a9c2-afac3c1474e9,DISK], DatanodeInfoWithStorage[127.0.0.1:37286,DS-41b2d69f-55c0-44a2-9ae9-6e48a1016136,DISK], DatanodeInfoWithStorage[127.0.0.1:35081,DS-c93f49ab-6d33-4145-a063-ecb05ee3252f,DISK], DatanodeInfoWithStorage[127.0.0.1:34711,DS-ad5d0081-ab15-4257-a2b0-c29ded222b45,DISK]]; indices=[0, 1, 2, 3, 5, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1446916990-172.17.0.10-1599304060901:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37596,DS-e06463f2-f084-4288-bba2-84f39133646c,DISK], DatanodeInfoWithStorage[127.0.0.1:33787,DS-01bbf234-a4e2-4879-a475-cee4b2ffa9f0,DISK], DatanodeInfoWithStorage[127.0.0.1:33565,DS-afa4b5fb-e097-4804-a9c2-afac3c1474e9,DISK], DatanodeInfoWithStorage[127.0.0.1:37286,DS-41b2d69f-55c0-44a2-9ae9-6e48a1016136,DISK], DatanodeInfoWithStorage[127.0.0.1:35081,DS-c93f49ab-6d33-4145-a063-ecb05ee3252f,DISK], DatanodeInfoWithStorage[127.0.0.1:34711,DS-ad5d0081-ab15-4257-a2b0-c29ded222b45,DISK]]; indices=[0, 1, 2, 3, 5, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1446916990-172.17.0.10-1599304060901:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37596,DS-e06463f2-f084-4288-bba2-84f39133646c,DISK], DatanodeInfoWithStorage[127.0.0.1:33787,DS-01bbf234-a4e2-4879-a475-cee4b2ffa9f0,DISK], DatanodeInfoWithStorage[127.0.0.1:33565,DS-afa4b5fb-e097-4804-a9c2-afac3c1474e9,DISK], DatanodeInfoWithStorage[127.0.0.1:37286,DS-41b2d69f-55c0-44a2-9ae9-6e48a1016136,DISK], DatanodeInfoWithStorage[127.0.0.1:35081,DS-c93f49ab-6d33-4145-a063-ecb05ee3252f,DISK], DatanodeInfoWithStorage[127.0.0.1:34711,DS-ad5d0081-ab15-4257-a2b0-c29ded222b45,DISK]]; indices=[0, 1, 2, 3, 5, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 14 out of 50
v1v1v2v2 failed with probability 12 out of 50
result: might be true error
Total execution time in seconds : 8987

reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1984410797-172.17.0.4-1599359336647:blk_-9223372036854775792_1002; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39322,DS-0ee32604-f34e-4c70-b0b8-6372a6bce6c9,DISK], DatanodeInfoWithStorage[127.0.0.1:36747,DS-1f4044b0-3e97-4cdf-9d8b-b46a35f8118f,DISK], DatanodeInfoWithStorage[127.0.0.1:41124,DS-910be717-4d85-494c-8b9f-c985770c3274,DISK], DatanodeInfoWithStorage[127.0.0.1:44637,DS-bd4afe29-f6ba-4d2a-8656-46a235aee6f1,DISK], DatanodeInfoWithStorage[127.0.0.1:44527,DS-5ea69fb1-3833-4524-9447-84a7f0e59167,DISK], DatanodeInfoWithStorage[127.0.0.1:35315,DS-06b84a9f-e365-40ff-addd-6cd20b9cc6b8,DISK]]; indices=[0, 1, 2, 3, 4, 5]}];  lastLocatedBlock=LocatedStripedBlock{BP-1984410797-172.17.0.4-1599359336647:blk_-9223372036854775792_1002; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39322,DS-0ee32604-f34e-4c70-b0b8-6372a6bce6c9,DISK], DatanodeInfoWithStorage[127.0.0.1:36747,DS-1f4044b0-3e97-4cdf-9d8b-b46a35f8118f,DISK], DatanodeInfoWithStorage[127.0.0.1:41124,DS-910be717-4d85-494c-8b9f-c985770c3274,DISK], DatanodeInfoWithStorage[127.0.0.1:44637,DS-bd4afe29-f6ba-4d2a-8656-46a235aee6f1,DISK], DatanodeInfoWithStorage[127.0.0.1:44527,DS-5ea69fb1-3833-4524-9447-84a7f0e59167,DISK], DatanodeInfoWithStorage[127.0.0.1:35315,DS-06b84a9f-e365-40ff-addd-6cd20b9cc6b8,DISK]]; indices=[0, 1, 2, 3, 4, 5]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1984410797-172.17.0.4-1599359336647:blk_-9223372036854775792_1002; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39322,DS-0ee32604-f34e-4c70-b0b8-6372a6bce6c9,DISK], DatanodeInfoWithStorage[127.0.0.1:36747,DS-1f4044b0-3e97-4cdf-9d8b-b46a35f8118f,DISK], DatanodeInfoWithStorage[127.0.0.1:41124,DS-910be717-4d85-494c-8b9f-c985770c3274,DISK], DatanodeInfoWithStorage[127.0.0.1:44637,DS-bd4afe29-f6ba-4d2a-8656-46a235aee6f1,DISK], DatanodeInfoWithStorage[127.0.0.1:44527,DS-5ea69fb1-3833-4524-9447-84a7f0e59167,DISK], DatanodeInfoWithStorage[127.0.0.1:35315,DS-06b84a9f-e365-40ff-addd-6cd20b9cc6b8,DISK]]; indices=[0, 1, 2, 3, 4, 5]}];  lastLocatedBlock=LocatedStripedBlock{BP-1984410797-172.17.0.4-1599359336647:blk_-9223372036854775792_1002; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39322,DS-0ee32604-f34e-4c70-b0b8-6372a6bce6c9,DISK], DatanodeInfoWithStorage[127.0.0.1:36747,DS-1f4044b0-3e97-4cdf-9d8b-b46a35f8118f,DISK], DatanodeInfoWithStorage[127.0.0.1:41124,DS-910be717-4d85-494c-8b9f-c985770c3274,DISK], DatanodeInfoWithStorage[127.0.0.1:44637,DS-bd4afe29-f6ba-4d2a-8656-46a235aee6f1,DISK], DatanodeInfoWithStorage[127.0.0.1:44527,DS-5ea69fb1-3833-4524-9447-84a7f0e59167,DISK], DatanodeInfoWithStorage[127.0.0.1:35315,DS-06b84a9f-e365-40ff-addd-6cd20b9cc6b8,DISK]]; indices=[0, 1, 2, 3, 4, 5]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-192059919-172.17.0.4-1599360562236:blk_-9223372036854775792_1002; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34536,DS-c71d9bd9-f5a8-4922-b8fd-679d1771e30b,DISK], DatanodeInfoWithStorage[127.0.0.1:38020,DS-23b0cdb8-c66b-4948-8c36-187e76c55753,DISK], DatanodeInfoWithStorage[127.0.0.1:40392,DS-944a680a-e274-4c0c-b94c-cbae239adef8,DISK], DatanodeInfoWithStorage[127.0.0.1:42368,DS-388f1e03-b135-4471-aeb0-38709df95daf,DISK], DatanodeInfoWithStorage[127.0.0.1:46876,DS-66889a92-b879-49e6-948e-416462aa011f,DISK], DatanodeInfoWithStorage[127.0.0.1:46252,DS-4c00c8b4-b1de-4283-8d23-389900b45849,DISK], DatanodeInfoWithStorage[127.0.0.1:33158,DS-49296d8a-6754-413d-8b10-2bf85525c4fb,DISK], DatanodeInfoWithStorage[127.0.0.1:41117,DS-84ea3719-6899-4238-9887-679a8a8cb8b2,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-192059919-172.17.0.4-1599360562236:blk_-9223372036854775792_1002; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34536,DS-c71d9bd9-f5a8-4922-b8fd-679d1771e30b,DISK], DatanodeInfoWithStorage[127.0.0.1:38020,DS-23b0cdb8-c66b-4948-8c36-187e76c55753,DISK], DatanodeInfoWithStorage[127.0.0.1:40392,DS-944a680a-e274-4c0c-b94c-cbae239adef8,DISK], DatanodeInfoWithStorage[127.0.0.1:42368,DS-388f1e03-b135-4471-aeb0-38709df95daf,DISK], DatanodeInfoWithStorage[127.0.0.1:46876,DS-66889a92-b879-49e6-948e-416462aa011f,DISK], DatanodeInfoWithStorage[127.0.0.1:46252,DS-4c00c8b4-b1de-4283-8d23-389900b45849,DISK], DatanodeInfoWithStorage[127.0.0.1:33158,DS-49296d8a-6754-413d-8b10-2bf85525c4fb,DISK], DatanodeInfoWithStorage[127.0.0.1:41117,DS-84ea3719-6899-4238-9887-679a8a8cb8b2,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-192059919-172.17.0.4-1599360562236:blk_-9223372036854775792_1002; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34536,DS-c71d9bd9-f5a8-4922-b8fd-679d1771e30b,DISK], DatanodeInfoWithStorage[127.0.0.1:38020,DS-23b0cdb8-c66b-4948-8c36-187e76c55753,DISK], DatanodeInfoWithStorage[127.0.0.1:40392,DS-944a680a-e274-4c0c-b94c-cbae239adef8,DISK], DatanodeInfoWithStorage[127.0.0.1:42368,DS-388f1e03-b135-4471-aeb0-38709df95daf,DISK], DatanodeInfoWithStorage[127.0.0.1:46876,DS-66889a92-b879-49e6-948e-416462aa011f,DISK], DatanodeInfoWithStorage[127.0.0.1:46252,DS-4c00c8b4-b1de-4283-8d23-389900b45849,DISK], DatanodeInfoWithStorage[127.0.0.1:33158,DS-49296d8a-6754-413d-8b10-2bf85525c4fb,DISK], DatanodeInfoWithStorage[127.0.0.1:41117,DS-84ea3719-6899-4238-9887-679a8a8cb8b2,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-192059919-172.17.0.4-1599360562236:blk_-9223372036854775792_1002; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34536,DS-c71d9bd9-f5a8-4922-b8fd-679d1771e30b,DISK], DatanodeInfoWithStorage[127.0.0.1:38020,DS-23b0cdb8-c66b-4948-8c36-187e76c55753,DISK], DatanodeInfoWithStorage[127.0.0.1:40392,DS-944a680a-e274-4c0c-b94c-cbae239adef8,DISK], DatanodeInfoWithStorage[127.0.0.1:42368,DS-388f1e03-b135-4471-aeb0-38709df95daf,DISK], DatanodeInfoWithStorage[127.0.0.1:46876,DS-66889a92-b879-49e6-948e-416462aa011f,DISK], DatanodeInfoWithStorage[127.0.0.1:46252,DS-4c00c8b4-b1de-4283-8d23-389900b45849,DISK], DatanodeInfoWithStorage[127.0.0.1:33158,DS-49296d8a-6754-413d-8b10-2bf85525c4fb,DISK], DatanodeInfoWithStorage[127.0.0.1:41117,DS-84ea3719-6899-4238-9887-679a8a8cb8b2,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-972679884-172.17.0.4-1599360589523:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37546,DS-e0b43ba0-10cd-405c-bd0b-3e16da88301d,DISK], DatanodeInfoWithStorage[127.0.0.1:39457,DS-e4a4bce3-fb60-4fb0-b033-ccdda7f07779,DISK], DatanodeInfoWithStorage[127.0.0.1:37449,DS-a77b9c30-504e-4ccb-8bb1-335011706aa9,DISK], DatanodeInfoWithStorage[127.0.0.1:42815,DS-6afb8a5c-c757-41d2-ae49-e45a52af55a8,DISK], DatanodeInfoWithStorage[127.0.0.1:43642,DS-b9b20ccd-400d-45cb-98cf-29bade68a2ff,DISK], DatanodeInfoWithStorage[127.0.0.1:33147,DS-dcf13875-ef8e-4cec-9e6a-06f84969b9dd,DISK], DatanodeInfoWithStorage[127.0.0.1:34978,DS-12d5c793-a2af-4ec5-8c48-5417681306a2,DISK]]; indices=[0, 2, 3, 4, 5, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-972679884-172.17.0.4-1599360589523:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37546,DS-e0b43ba0-10cd-405c-bd0b-3e16da88301d,DISK], DatanodeInfoWithStorage[127.0.0.1:39457,DS-e4a4bce3-fb60-4fb0-b033-ccdda7f07779,DISK], DatanodeInfoWithStorage[127.0.0.1:37449,DS-a77b9c30-504e-4ccb-8bb1-335011706aa9,DISK], DatanodeInfoWithStorage[127.0.0.1:42815,DS-6afb8a5c-c757-41d2-ae49-e45a52af55a8,DISK], DatanodeInfoWithStorage[127.0.0.1:43642,DS-b9b20ccd-400d-45cb-98cf-29bade68a2ff,DISK], DatanodeInfoWithStorage[127.0.0.1:33147,DS-dcf13875-ef8e-4cec-9e6a-06f84969b9dd,DISK], DatanodeInfoWithStorage[127.0.0.1:34978,DS-12d5c793-a2af-4ec5-8c48-5417681306a2,DISK]]; indices=[0, 2, 3, 4, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-972679884-172.17.0.4-1599360589523:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37546,DS-e0b43ba0-10cd-405c-bd0b-3e16da88301d,DISK], DatanodeInfoWithStorage[127.0.0.1:39457,DS-e4a4bce3-fb60-4fb0-b033-ccdda7f07779,DISK], DatanodeInfoWithStorage[127.0.0.1:37449,DS-a77b9c30-504e-4ccb-8bb1-335011706aa9,DISK], DatanodeInfoWithStorage[127.0.0.1:42815,DS-6afb8a5c-c757-41d2-ae49-e45a52af55a8,DISK], DatanodeInfoWithStorage[127.0.0.1:43642,DS-b9b20ccd-400d-45cb-98cf-29bade68a2ff,DISK], DatanodeInfoWithStorage[127.0.0.1:33147,DS-dcf13875-ef8e-4cec-9e6a-06f84969b9dd,DISK], DatanodeInfoWithStorage[127.0.0.1:34978,DS-12d5c793-a2af-4ec5-8c48-5417681306a2,DISK]]; indices=[0, 2, 3, 4, 5, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-972679884-172.17.0.4-1599360589523:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37546,DS-e0b43ba0-10cd-405c-bd0b-3e16da88301d,DISK], DatanodeInfoWithStorage[127.0.0.1:39457,DS-e4a4bce3-fb60-4fb0-b033-ccdda7f07779,DISK], DatanodeInfoWithStorage[127.0.0.1:37449,DS-a77b9c30-504e-4ccb-8bb1-335011706aa9,DISK], DatanodeInfoWithStorage[127.0.0.1:42815,DS-6afb8a5c-c757-41d2-ae49-e45a52af55a8,DISK], DatanodeInfoWithStorage[127.0.0.1:43642,DS-b9b20ccd-400d-45cb-98cf-29bade68a2ff,DISK], DatanodeInfoWithStorage[127.0.0.1:33147,DS-dcf13875-ef8e-4cec-9e6a-06f84969b9dd,DISK], DatanodeInfoWithStorage[127.0.0.1:34978,DS-12d5c793-a2af-4ec5-8c48-5417681306a2,DISK]]; indices=[0, 2, 3, 4, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1657039940-172.17.0.4-1599360896084:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35551,DS-423df50b-4c7b-4fd8-b8aa-c3b9b8d67407,DISK], DatanodeInfoWithStorage[127.0.0.1:39400,DS-a992919b-b902-442f-8bec-84a19aaf885c,DISK], DatanodeInfoWithStorage[127.0.0.1:37831,DS-3a7d2737-628f-4668-964b-91bd8aae7be1,DISK], DatanodeInfoWithStorage[127.0.0.1:33248,DS-481b649c-7c27-462d-a375-26a64e478345,DISK], DatanodeInfoWithStorage[127.0.0.1:40373,DS-c0be9a49-e18a-4462-8be2-aee3df8735e4,DISK], DatanodeInfoWithStorage[127.0.0.1:45597,DS-6a14d21a-e10b-4157-b2f9-0ff83e65da95,DISK], DatanodeInfoWithStorage[127.0.0.1:34344,DS-f8273f5b-9a7a-4716-9b17-47c672284038,DISK], DatanodeInfoWithStorage[127.0.0.1:40001,DS-fef46ae2-fb87-4d51-9256-b938d5deea6c,DISK]]; indices=[0, 1, 2, 3, 4, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1657039940-172.17.0.4-1599360896084:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35551,DS-423df50b-4c7b-4fd8-b8aa-c3b9b8d67407,DISK], DatanodeInfoWithStorage[127.0.0.1:39400,DS-a992919b-b902-442f-8bec-84a19aaf885c,DISK], DatanodeInfoWithStorage[127.0.0.1:37831,DS-3a7d2737-628f-4668-964b-91bd8aae7be1,DISK], DatanodeInfoWithStorage[127.0.0.1:33248,DS-481b649c-7c27-462d-a375-26a64e478345,DISK], DatanodeInfoWithStorage[127.0.0.1:40373,DS-c0be9a49-e18a-4462-8be2-aee3df8735e4,DISK], DatanodeInfoWithStorage[127.0.0.1:45597,DS-6a14d21a-e10b-4157-b2f9-0ff83e65da95,DISK], DatanodeInfoWithStorage[127.0.0.1:34344,DS-f8273f5b-9a7a-4716-9b17-47c672284038,DISK], DatanodeInfoWithStorage[127.0.0.1:40001,DS-fef46ae2-fb87-4d51-9256-b938d5deea6c,DISK]]; indices=[0, 1, 2, 3, 4, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1657039940-172.17.0.4-1599360896084:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35551,DS-423df50b-4c7b-4fd8-b8aa-c3b9b8d67407,DISK], DatanodeInfoWithStorage[127.0.0.1:39400,DS-a992919b-b902-442f-8bec-84a19aaf885c,DISK], DatanodeInfoWithStorage[127.0.0.1:37831,DS-3a7d2737-628f-4668-964b-91bd8aae7be1,DISK], DatanodeInfoWithStorage[127.0.0.1:33248,DS-481b649c-7c27-462d-a375-26a64e478345,DISK], DatanodeInfoWithStorage[127.0.0.1:40373,DS-c0be9a49-e18a-4462-8be2-aee3df8735e4,DISK], DatanodeInfoWithStorage[127.0.0.1:45597,DS-6a14d21a-e10b-4157-b2f9-0ff83e65da95,DISK], DatanodeInfoWithStorage[127.0.0.1:34344,DS-f8273f5b-9a7a-4716-9b17-47c672284038,DISK], DatanodeInfoWithStorage[127.0.0.1:40001,DS-fef46ae2-fb87-4d51-9256-b938d5deea6c,DISK]]; indices=[0, 1, 2, 3, 4, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1657039940-172.17.0.4-1599360896084:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35551,DS-423df50b-4c7b-4fd8-b8aa-c3b9b8d67407,DISK], DatanodeInfoWithStorage[127.0.0.1:39400,DS-a992919b-b902-442f-8bec-84a19aaf885c,DISK], DatanodeInfoWithStorage[127.0.0.1:37831,DS-3a7d2737-628f-4668-964b-91bd8aae7be1,DISK], DatanodeInfoWithStorage[127.0.0.1:33248,DS-481b649c-7c27-462d-a375-26a64e478345,DISK], DatanodeInfoWithStorage[127.0.0.1:40373,DS-c0be9a49-e18a-4462-8be2-aee3df8735e4,DISK], DatanodeInfoWithStorage[127.0.0.1:45597,DS-6a14d21a-e10b-4157-b2f9-0ff83e65da95,DISK], DatanodeInfoWithStorage[127.0.0.1:34344,DS-f8273f5b-9a7a-4716-9b17-47c672284038,DISK], DatanodeInfoWithStorage[127.0.0.1:40001,DS-fef46ae2-fb87-4d51-9256-b938d5deea6c,DISK]]; indices=[0, 1, 2, 3, 4, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1455711471-172.17.0.4-1599361343643:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39955,DS-b4e5ad37-b20b-4d49-a63f-d43dc70cbcf2,DISK], DatanodeInfoWithStorage[127.0.0.1:36598,DS-b5eb8f9b-bf8c-40a8-a2b8-aee42289ac02,DISK], DatanodeInfoWithStorage[127.0.0.1:33022,DS-b49ecd7a-1225-4680-817f-efd8a10d5535,DISK], DatanodeInfoWithStorage[127.0.0.1:34852,DS-c87d6468-eb8e-4aa0-aa94-f499bfba651d,DISK], DatanodeInfoWithStorage[127.0.0.1:42434,DS-130fc6e3-fa7c-4a09-9eed-c920775cd9f2,DISK], DatanodeInfoWithStorage[127.0.0.1:37728,DS-e46f7b93-3bea-473d-941b-8f4b324de6e2,DISK]]; indices=[0, 2, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1455711471-172.17.0.4-1599361343643:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39955,DS-b4e5ad37-b20b-4d49-a63f-d43dc70cbcf2,DISK], DatanodeInfoWithStorage[127.0.0.1:36598,DS-b5eb8f9b-bf8c-40a8-a2b8-aee42289ac02,DISK], DatanodeInfoWithStorage[127.0.0.1:33022,DS-b49ecd7a-1225-4680-817f-efd8a10d5535,DISK], DatanodeInfoWithStorage[127.0.0.1:34852,DS-c87d6468-eb8e-4aa0-aa94-f499bfba651d,DISK], DatanodeInfoWithStorage[127.0.0.1:42434,DS-130fc6e3-fa7c-4a09-9eed-c920775cd9f2,DISK], DatanodeInfoWithStorage[127.0.0.1:37728,DS-e46f7b93-3bea-473d-941b-8f4b324de6e2,DISK]]; indices=[0, 2, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1455711471-172.17.0.4-1599361343643:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39955,DS-b4e5ad37-b20b-4d49-a63f-d43dc70cbcf2,DISK], DatanodeInfoWithStorage[127.0.0.1:36598,DS-b5eb8f9b-bf8c-40a8-a2b8-aee42289ac02,DISK], DatanodeInfoWithStorage[127.0.0.1:33022,DS-b49ecd7a-1225-4680-817f-efd8a10d5535,DISK], DatanodeInfoWithStorage[127.0.0.1:34852,DS-c87d6468-eb8e-4aa0-aa94-f499bfba651d,DISK], DatanodeInfoWithStorage[127.0.0.1:42434,DS-130fc6e3-fa7c-4a09-9eed-c920775cd9f2,DISK], DatanodeInfoWithStorage[127.0.0.1:37728,DS-e46f7b93-3bea-473d-941b-8f4b324de6e2,DISK]]; indices=[0, 2, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1455711471-172.17.0.4-1599361343643:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39955,DS-b4e5ad37-b20b-4d49-a63f-d43dc70cbcf2,DISK], DatanodeInfoWithStorage[127.0.0.1:36598,DS-b5eb8f9b-bf8c-40a8-a2b8-aee42289ac02,DISK], DatanodeInfoWithStorage[127.0.0.1:33022,DS-b49ecd7a-1225-4680-817f-efd8a10d5535,DISK], DatanodeInfoWithStorage[127.0.0.1:34852,DS-c87d6468-eb8e-4aa0-aa94-f499bfba651d,DISK], DatanodeInfoWithStorage[127.0.0.1:42434,DS-130fc6e3-fa7c-4a09-9eed-c920775cd9f2,DISK], DatanodeInfoWithStorage[127.0.0.1:37728,DS-e46f7b93-3bea-473d-941b-8f4b324de6e2,DISK]]; indices=[0, 2, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-396448058-172.17.0.4-1599362303897:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42722,DS-c8772dd1-052c-4399-83bc-b0196233e2d9,DISK], DatanodeInfoWithStorage[127.0.0.1:42972,DS-48e1130f-999a-493e-b024-66992e94be9b,DISK], DatanodeInfoWithStorage[127.0.0.1:34444,DS-604d7e46-50ad-4abf-8864-820850249309,DISK], DatanodeInfoWithStorage[127.0.0.1:38471,DS-9893087f-f793-4854-aa94-ef8da96e4316,DISK], DatanodeInfoWithStorage[127.0.0.1:44129,DS-f95db16f-da7e-40fb-b93c-d4645d1ee1bb,DISK], DatanodeInfoWithStorage[127.0.0.1:34271,DS-e17bf7be-9291-4c78-836d-f363f192594b,DISK], DatanodeInfoWithStorage[127.0.0.1:35837,DS-c028bb0d-495e-423b-b04d-b2dfde00e50b,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6]}];  lastLocatedBlock=LocatedStripedBlock{BP-396448058-172.17.0.4-1599362303897:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42722,DS-c8772dd1-052c-4399-83bc-b0196233e2d9,DISK], DatanodeInfoWithStorage[127.0.0.1:42972,DS-48e1130f-999a-493e-b024-66992e94be9b,DISK], DatanodeInfoWithStorage[127.0.0.1:34444,DS-604d7e46-50ad-4abf-8864-820850249309,DISK], DatanodeInfoWithStorage[127.0.0.1:38471,DS-9893087f-f793-4854-aa94-ef8da96e4316,DISK], DatanodeInfoWithStorage[127.0.0.1:44129,DS-f95db16f-da7e-40fb-b93c-d4645d1ee1bb,DISK], DatanodeInfoWithStorage[127.0.0.1:34271,DS-e17bf7be-9291-4c78-836d-f363f192594b,DISK], DatanodeInfoWithStorage[127.0.0.1:35837,DS-c028bb0d-495e-423b-b04d-b2dfde00e50b,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-396448058-172.17.0.4-1599362303897:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42722,DS-c8772dd1-052c-4399-83bc-b0196233e2d9,DISK], DatanodeInfoWithStorage[127.0.0.1:42972,DS-48e1130f-999a-493e-b024-66992e94be9b,DISK], DatanodeInfoWithStorage[127.0.0.1:34444,DS-604d7e46-50ad-4abf-8864-820850249309,DISK], DatanodeInfoWithStorage[127.0.0.1:38471,DS-9893087f-f793-4854-aa94-ef8da96e4316,DISK], DatanodeInfoWithStorage[127.0.0.1:44129,DS-f95db16f-da7e-40fb-b93c-d4645d1ee1bb,DISK], DatanodeInfoWithStorage[127.0.0.1:34271,DS-e17bf7be-9291-4c78-836d-f363f192594b,DISK], DatanodeInfoWithStorage[127.0.0.1:35837,DS-c028bb0d-495e-423b-b04d-b2dfde00e50b,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6]}];  lastLocatedBlock=LocatedStripedBlock{BP-396448058-172.17.0.4-1599362303897:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42722,DS-c8772dd1-052c-4399-83bc-b0196233e2d9,DISK], DatanodeInfoWithStorage[127.0.0.1:42972,DS-48e1130f-999a-493e-b024-66992e94be9b,DISK], DatanodeInfoWithStorage[127.0.0.1:34444,DS-604d7e46-50ad-4abf-8864-820850249309,DISK], DatanodeInfoWithStorage[127.0.0.1:38471,DS-9893087f-f793-4854-aa94-ef8da96e4316,DISK], DatanodeInfoWithStorage[127.0.0.1:44129,DS-f95db16f-da7e-40fb-b93c-d4645d1ee1bb,DISK], DatanodeInfoWithStorage[127.0.0.1:34271,DS-e17bf7be-9291-4c78-836d-f363f192594b,DISK], DatanodeInfoWithStorage[127.0.0.1:35837,DS-c028bb0d-495e-423b-b04d-b2dfde00e50b,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-8203362-172.17.0.4-1599363388882:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36024,DS-c409c087-3b71-4ff2-9be2-4a51f3aedf97,DISK], DatanodeInfoWithStorage[127.0.0.1:45007,DS-c120feaf-5cfe-4eee-b92b-8c18882b0294,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-4bb41f1e-c704-489c-b7e0-8ad3fe8a8b01,DISK], DatanodeInfoWithStorage[127.0.0.1:38006,DS-6146e571-b1b6-454b-99c5-306a71213935,DISK], DatanodeInfoWithStorage[127.0.0.1:43464,DS-45787bb1-bd4f-438b-8b86-86de972aa39d,DISK], DatanodeInfoWithStorage[127.0.0.1:35052,DS-2419358d-517c-406d-a8cc-ffedb2744ea8,DISK]]; indices=[0, 1, 3, 4, 5, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-8203362-172.17.0.4-1599363388882:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36024,DS-c409c087-3b71-4ff2-9be2-4a51f3aedf97,DISK], DatanodeInfoWithStorage[127.0.0.1:45007,DS-c120feaf-5cfe-4eee-b92b-8c18882b0294,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-4bb41f1e-c704-489c-b7e0-8ad3fe8a8b01,DISK], DatanodeInfoWithStorage[127.0.0.1:38006,DS-6146e571-b1b6-454b-99c5-306a71213935,DISK], DatanodeInfoWithStorage[127.0.0.1:43464,DS-45787bb1-bd4f-438b-8b86-86de972aa39d,DISK], DatanodeInfoWithStorage[127.0.0.1:35052,DS-2419358d-517c-406d-a8cc-ffedb2744ea8,DISK]]; indices=[0, 1, 3, 4, 5, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-8203362-172.17.0.4-1599363388882:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36024,DS-c409c087-3b71-4ff2-9be2-4a51f3aedf97,DISK], DatanodeInfoWithStorage[127.0.0.1:45007,DS-c120feaf-5cfe-4eee-b92b-8c18882b0294,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-4bb41f1e-c704-489c-b7e0-8ad3fe8a8b01,DISK], DatanodeInfoWithStorage[127.0.0.1:38006,DS-6146e571-b1b6-454b-99c5-306a71213935,DISK], DatanodeInfoWithStorage[127.0.0.1:43464,DS-45787bb1-bd4f-438b-8b86-86de972aa39d,DISK], DatanodeInfoWithStorage[127.0.0.1:35052,DS-2419358d-517c-406d-a8cc-ffedb2744ea8,DISK]]; indices=[0, 1, 3, 4, 5, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-8203362-172.17.0.4-1599363388882:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36024,DS-c409c087-3b71-4ff2-9be2-4a51f3aedf97,DISK], DatanodeInfoWithStorage[127.0.0.1:45007,DS-c120feaf-5cfe-4eee-b92b-8c18882b0294,DISK], DatanodeInfoWithStorage[127.0.0.1:44715,DS-4bb41f1e-c704-489c-b7e0-8ad3fe8a8b01,DISK], DatanodeInfoWithStorage[127.0.0.1:38006,DS-6146e571-b1b6-454b-99c5-306a71213935,DISK], DatanodeInfoWithStorage[127.0.0.1:43464,DS-45787bb1-bd4f-438b-8b86-86de972aa39d,DISK], DatanodeInfoWithStorage[127.0.0.1:35052,DS-2419358d-517c-406d-a8cc-ffedb2744ea8,DISK]]; indices=[0, 1, 3, 4, 5, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1950623852-172.17.0.4-1599366367222:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40173,DS-95bac782-1b0d-4804-9f60-1559f39280c4,DISK], DatanodeInfoWithStorage[127.0.0.1:42507,DS-f9b44061-ab20-46db-8f05-e6d3f690be3d,DISK], DatanodeInfoWithStorage[127.0.0.1:42383,DS-49ea1ba3-c2f3-44b8-b35e-7c99a9cd5855,DISK], DatanodeInfoWithStorage[127.0.0.1:44891,DS-026e8e38-9995-4d17-baa1-a6633cca9be6,DISK], DatanodeInfoWithStorage[127.0.0.1:46155,DS-2cd3e661-a662-4f32-a44b-1742b3c14ee9,DISK], DatanodeInfoWithStorage[127.0.0.1:40178,DS-5ad24e79-1606-433a-8843-4a55570f433a,DISK]]; indices=[0, 1, 4, 5, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1950623852-172.17.0.4-1599366367222:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40173,DS-95bac782-1b0d-4804-9f60-1559f39280c4,DISK], DatanodeInfoWithStorage[127.0.0.1:42507,DS-f9b44061-ab20-46db-8f05-e6d3f690be3d,DISK], DatanodeInfoWithStorage[127.0.0.1:42383,DS-49ea1ba3-c2f3-44b8-b35e-7c99a9cd5855,DISK], DatanodeInfoWithStorage[127.0.0.1:44891,DS-026e8e38-9995-4d17-baa1-a6633cca9be6,DISK], DatanodeInfoWithStorage[127.0.0.1:46155,DS-2cd3e661-a662-4f32-a44b-1742b3c14ee9,DISK], DatanodeInfoWithStorage[127.0.0.1:40178,DS-5ad24e79-1606-433a-8843-4a55570f433a,DISK]]; indices=[0, 1, 4, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1950623852-172.17.0.4-1599366367222:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40173,DS-95bac782-1b0d-4804-9f60-1559f39280c4,DISK], DatanodeInfoWithStorage[127.0.0.1:42507,DS-f9b44061-ab20-46db-8f05-e6d3f690be3d,DISK], DatanodeInfoWithStorage[127.0.0.1:42383,DS-49ea1ba3-c2f3-44b8-b35e-7c99a9cd5855,DISK], DatanodeInfoWithStorage[127.0.0.1:44891,DS-026e8e38-9995-4d17-baa1-a6633cca9be6,DISK], DatanodeInfoWithStorage[127.0.0.1:46155,DS-2cd3e661-a662-4f32-a44b-1742b3c14ee9,DISK], DatanodeInfoWithStorage[127.0.0.1:40178,DS-5ad24e79-1606-433a-8843-4a55570f433a,DISK]]; indices=[0, 1, 4, 5, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1950623852-172.17.0.4-1599366367222:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40173,DS-95bac782-1b0d-4804-9f60-1559f39280c4,DISK], DatanodeInfoWithStorage[127.0.0.1:42507,DS-f9b44061-ab20-46db-8f05-e6d3f690be3d,DISK], DatanodeInfoWithStorage[127.0.0.1:42383,DS-49ea1ba3-c2f3-44b8-b35e-7c99a9cd5855,DISK], DatanodeInfoWithStorage[127.0.0.1:44891,DS-026e8e38-9995-4d17-baa1-a6633cca9be6,DISK], DatanodeInfoWithStorage[127.0.0.1:46155,DS-2cd3e661-a662-4f32-a44b-1742b3c14ee9,DISK], DatanodeInfoWithStorage[127.0.0.1:40178,DS-5ad24e79-1606-433a-8843-4a55570f433a,DISK]]; indices=[0, 1, 4, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-634160073-172.17.0.4-1599366899822:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41414,DS-78fd6b1b-b652-44e0-944d-f5f8c478e08f,DISK], DatanodeInfoWithStorage[127.0.0.1:45782,DS-fdbc6a44-a3be-4683-a109-00eca721fdf5,DISK], DatanodeInfoWithStorage[127.0.0.1:39130,DS-84613b93-d2f8-477b-8bd7-f96b232b0236,DISK], DatanodeInfoWithStorage[127.0.0.1:41770,DS-8eb45bc8-6a2a-44dd-aff6-d3bfd55b8800,DISK], DatanodeInfoWithStorage[127.0.0.1:38326,DS-ea41e37e-9e9b-435f-a8ff-fc06c7a223f5,DISK], DatanodeInfoWithStorage[127.0.0.1:43921,DS-20563235-b5eb-4ae5-8920-5cc7ec25db0a,DISK], DatanodeInfoWithStorage[127.0.0.1:33437,DS-9b146884-8332-4444-b84e-f4f929695b10,DISK]]; indices=[1, 2, 3, 4, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-634160073-172.17.0.4-1599366899822:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41414,DS-78fd6b1b-b652-44e0-944d-f5f8c478e08f,DISK], DatanodeInfoWithStorage[127.0.0.1:45782,DS-fdbc6a44-a3be-4683-a109-00eca721fdf5,DISK], DatanodeInfoWithStorage[127.0.0.1:39130,DS-84613b93-d2f8-477b-8bd7-f96b232b0236,DISK], DatanodeInfoWithStorage[127.0.0.1:41770,DS-8eb45bc8-6a2a-44dd-aff6-d3bfd55b8800,DISK], DatanodeInfoWithStorage[127.0.0.1:38326,DS-ea41e37e-9e9b-435f-a8ff-fc06c7a223f5,DISK], DatanodeInfoWithStorage[127.0.0.1:43921,DS-20563235-b5eb-4ae5-8920-5cc7ec25db0a,DISK], DatanodeInfoWithStorage[127.0.0.1:33437,DS-9b146884-8332-4444-b84e-f4f929695b10,DISK]]; indices=[1, 2, 3, 4, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-634160073-172.17.0.4-1599366899822:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41414,DS-78fd6b1b-b652-44e0-944d-f5f8c478e08f,DISK], DatanodeInfoWithStorage[127.0.0.1:45782,DS-fdbc6a44-a3be-4683-a109-00eca721fdf5,DISK], DatanodeInfoWithStorage[127.0.0.1:39130,DS-84613b93-d2f8-477b-8bd7-f96b232b0236,DISK], DatanodeInfoWithStorage[127.0.0.1:41770,DS-8eb45bc8-6a2a-44dd-aff6-d3bfd55b8800,DISK], DatanodeInfoWithStorage[127.0.0.1:38326,DS-ea41e37e-9e9b-435f-a8ff-fc06c7a223f5,DISK], DatanodeInfoWithStorage[127.0.0.1:43921,DS-20563235-b5eb-4ae5-8920-5cc7ec25db0a,DISK], DatanodeInfoWithStorage[127.0.0.1:33437,DS-9b146884-8332-4444-b84e-f4f929695b10,DISK]]; indices=[1, 2, 3, 4, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-634160073-172.17.0.4-1599366899822:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41414,DS-78fd6b1b-b652-44e0-944d-f5f8c478e08f,DISK], DatanodeInfoWithStorage[127.0.0.1:45782,DS-fdbc6a44-a3be-4683-a109-00eca721fdf5,DISK], DatanodeInfoWithStorage[127.0.0.1:39130,DS-84613b93-d2f8-477b-8bd7-f96b232b0236,DISK], DatanodeInfoWithStorage[127.0.0.1:41770,DS-8eb45bc8-6a2a-44dd-aff6-d3bfd55b8800,DISK], DatanodeInfoWithStorage[127.0.0.1:38326,DS-ea41e37e-9e9b-435f-a8ff-fc06c7a223f5,DISK], DatanodeInfoWithStorage[127.0.0.1:43921,DS-20563235-b5eb-4ae5-8920-5cc7ec25db0a,DISK], DatanodeInfoWithStorage[127.0.0.1:33437,DS-9b146884-8332-4444-b84e-f4f929695b10,DISK]]; indices=[1, 2, 3, 4, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1906683980-172.17.0.4-1599367855707:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34072,DS-e6093625-5bf4-4b8e-a818-57d71f390e76,DISK], DatanodeInfoWithStorage[127.0.0.1:38268,DS-a57e86eb-00b2-4b16-a690-ca362c43ecd2,DISK], DatanodeInfoWithStorage[127.0.0.1:40887,DS-3db096f8-017c-4537-aaf7-a6bd9e61f927,DISK], DatanodeInfoWithStorage[127.0.0.1:38282,DS-096d5a15-2e90-45c9-92a1-cf95f5f596eb,DISK], DatanodeInfoWithStorage[127.0.0.1:41166,DS-8374c4ba-85ba-42de-ade2-67b30b330ec1,DISK], DatanodeInfoWithStorage[127.0.0.1:40358,DS-c8eec6ee-4685-446e-966e-e34e4dac3943,DISK], DatanodeInfoWithStorage[127.0.0.1:37837,DS-89f83f87-427f-4ec8-83bc-613b97ce1e46,DISK]]; indices=[0, 1, 2, 3, 4, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1906683980-172.17.0.4-1599367855707:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34072,DS-e6093625-5bf4-4b8e-a818-57d71f390e76,DISK], DatanodeInfoWithStorage[127.0.0.1:38268,DS-a57e86eb-00b2-4b16-a690-ca362c43ecd2,DISK], DatanodeInfoWithStorage[127.0.0.1:40887,DS-3db096f8-017c-4537-aaf7-a6bd9e61f927,DISK], DatanodeInfoWithStorage[127.0.0.1:38282,DS-096d5a15-2e90-45c9-92a1-cf95f5f596eb,DISK], DatanodeInfoWithStorage[127.0.0.1:41166,DS-8374c4ba-85ba-42de-ade2-67b30b330ec1,DISK], DatanodeInfoWithStorage[127.0.0.1:40358,DS-c8eec6ee-4685-446e-966e-e34e4dac3943,DISK], DatanodeInfoWithStorage[127.0.0.1:37837,DS-89f83f87-427f-4ec8-83bc-613b97ce1e46,DISK]]; indices=[0, 1, 2, 3, 4, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1906683980-172.17.0.4-1599367855707:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34072,DS-e6093625-5bf4-4b8e-a818-57d71f390e76,DISK], DatanodeInfoWithStorage[127.0.0.1:38268,DS-a57e86eb-00b2-4b16-a690-ca362c43ecd2,DISK], DatanodeInfoWithStorage[127.0.0.1:40887,DS-3db096f8-017c-4537-aaf7-a6bd9e61f927,DISK], DatanodeInfoWithStorage[127.0.0.1:38282,DS-096d5a15-2e90-45c9-92a1-cf95f5f596eb,DISK], DatanodeInfoWithStorage[127.0.0.1:41166,DS-8374c4ba-85ba-42de-ade2-67b30b330ec1,DISK], DatanodeInfoWithStorage[127.0.0.1:40358,DS-c8eec6ee-4685-446e-966e-e34e4dac3943,DISK], DatanodeInfoWithStorage[127.0.0.1:37837,DS-89f83f87-427f-4ec8-83bc-613b97ce1e46,DISK]]; indices=[0, 1, 2, 3, 4, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1906683980-172.17.0.4-1599367855707:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34072,DS-e6093625-5bf4-4b8e-a818-57d71f390e76,DISK], DatanodeInfoWithStorage[127.0.0.1:38268,DS-a57e86eb-00b2-4b16-a690-ca362c43ecd2,DISK], DatanodeInfoWithStorage[127.0.0.1:40887,DS-3db096f8-017c-4537-aaf7-a6bd9e61f927,DISK], DatanodeInfoWithStorage[127.0.0.1:38282,DS-096d5a15-2e90-45c9-92a1-cf95f5f596eb,DISK], DatanodeInfoWithStorage[127.0.0.1:41166,DS-8374c4ba-85ba-42de-ade2-67b30b330ec1,DISK], DatanodeInfoWithStorage[127.0.0.1:40358,DS-c8eec6ee-4685-446e-966e-e34e4dac3943,DISK], DatanodeInfoWithStorage[127.0.0.1:37837,DS-89f83f87-427f-4ec8-83bc-613b97ce1e46,DISK]]; indices=[0, 1, 2, 3, 4, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 8 out of 50
v1v1v2v2 failed with probability 12 out of 50
result: false positive !!!
Total execution time in seconds : 9179

reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-9310453-172.17.0.20-1599302589937:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43188,DS-d1a2df61-a777-45b0-98fa-0ec6949f0f5c,DISK], DatanodeInfoWithStorage[127.0.0.1:39509,DS-c917813c-88db-4b4f-b291-56493bcf7a7e,DISK], DatanodeInfoWithStorage[127.0.0.1:46525,DS-c039052a-05f1-4ba8-b447-3e52d450391a,DISK], DatanodeInfoWithStorage[127.0.0.1:33479,DS-e7a8632d-66dd-40ee-aa3b-99c7ec52c6f9,DISK], DatanodeInfoWithStorage[127.0.0.1:37991,DS-0f8bfcaa-3c81-4672-af11-4cc92a7e2cc9,DISK], DatanodeInfoWithStorage[127.0.0.1:43973,DS-9c828e20-6936-4a71-96a0-00a92f72b8ab,DISK], DatanodeInfoWithStorage[127.0.0.1:43863,DS-29a68d05-33f1-4e4f-93e3-a876df17ed48,DISK], DatanodeInfoWithStorage[127.0.0.1:41451,DS-c568828a-f4f1-4c2c-b4f7-4ba84cd04f94,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-9310453-172.17.0.20-1599302589937:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43188,DS-d1a2df61-a777-45b0-98fa-0ec6949f0f5c,DISK], DatanodeInfoWithStorage[127.0.0.1:39509,DS-c917813c-88db-4b4f-b291-56493bcf7a7e,DISK], DatanodeInfoWithStorage[127.0.0.1:46525,DS-c039052a-05f1-4ba8-b447-3e52d450391a,DISK], DatanodeInfoWithStorage[127.0.0.1:33479,DS-e7a8632d-66dd-40ee-aa3b-99c7ec52c6f9,DISK], DatanodeInfoWithStorage[127.0.0.1:37991,DS-0f8bfcaa-3c81-4672-af11-4cc92a7e2cc9,DISK], DatanodeInfoWithStorage[127.0.0.1:43973,DS-9c828e20-6936-4a71-96a0-00a92f72b8ab,DISK], DatanodeInfoWithStorage[127.0.0.1:43863,DS-29a68d05-33f1-4e4f-93e3-a876df17ed48,DISK], DatanodeInfoWithStorage[127.0.0.1:41451,DS-c568828a-f4f1-4c2c-b4f7-4ba84cd04f94,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-324202901-172.17.0.20-1599302636345:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42666,DS-e30c3026-ca21-4597-8b09-2edb57ac490c,DISK], DatanodeInfoWithStorage[127.0.0.1:41564,DS-c688be45-7d88-4883-9df1-981e5898490d,DISK], DatanodeInfoWithStorage[127.0.0.1:40126,DS-d22e15a7-75b4-44ff-8e4d-9fa1b51cb5ab,DISK], DatanodeInfoWithStorage[127.0.0.1:40061,DS-446d3914-9184-4131-afe1-1690fece4b03,DISK], DatanodeInfoWithStorage[127.0.0.1:44044,DS-fb1dbb8e-f6c5-449f-a5ed-4dab9f3b1642,DISK], DatanodeInfoWithStorage[127.0.0.1:45298,DS-7a790aa1-1c94-417c-81f7-ecb34f481350,DISK], DatanodeInfoWithStorage[127.0.0.1:39732,DS-4f999048-7e06-40a2-a4c3-edba9ba13f49,DISK], DatanodeInfoWithStorage[127.0.0.1:36679,DS-f64ab855-5fa1-470c-a5eb-c31928b2d614,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-324202901-172.17.0.20-1599302636345:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42666,DS-e30c3026-ca21-4597-8b09-2edb57ac490c,DISK], DatanodeInfoWithStorage[127.0.0.1:41564,DS-c688be45-7d88-4883-9df1-981e5898490d,DISK], DatanodeInfoWithStorage[127.0.0.1:40126,DS-d22e15a7-75b4-44ff-8e4d-9fa1b51cb5ab,DISK], DatanodeInfoWithStorage[127.0.0.1:40061,DS-446d3914-9184-4131-afe1-1690fece4b03,DISK], DatanodeInfoWithStorage[127.0.0.1:44044,DS-fb1dbb8e-f6c5-449f-a5ed-4dab9f3b1642,DISK], DatanodeInfoWithStorage[127.0.0.1:45298,DS-7a790aa1-1c94-417c-81f7-ecb34f481350,DISK], DatanodeInfoWithStorage[127.0.0.1:39732,DS-4f999048-7e06-40a2-a4c3-edba9ba13f49,DISK], DatanodeInfoWithStorage[127.0.0.1:36679,DS-f64ab855-5fa1-470c-a5eb-c31928b2d614,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-326432236-172.17.0.20-1599302937198:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45427,DS-34acf6c7-e060-41cf-b673-0df666747702,DISK], DatanodeInfoWithStorage[127.0.0.1:33396,DS-9443bf65-b8de-4c48-927e-9eb5bcce0ff6,DISK], DatanodeInfoWithStorage[127.0.0.1:36387,DS-abafa500-b6ba-42d3-8d8f-d6f9b3288ef5,DISK], DatanodeInfoWithStorage[127.0.0.1:37860,DS-68c1f349-7b7e-46d2-95db-9024d0a72ed4,DISK], DatanodeInfoWithStorage[127.0.0.1:34560,DS-fcdf70df-a0b6-41ac-b448-b71405368b33,DISK], DatanodeInfoWithStorage[127.0.0.1:33220,DS-ea6e8f60-65d2-4f4e-9a98-7fc5a17c2ddf,DISK], DatanodeInfoWithStorage[127.0.0.1:36425,DS-cd6be38c-b239-4cd0-85a1-6a4de1916b0d,DISK], DatanodeInfoWithStorage[127.0.0.1:43229,DS-7dbff5aa-bcb1-4ce1-bf08-750c1560f528,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-326432236-172.17.0.20-1599302937198:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45427,DS-34acf6c7-e060-41cf-b673-0df666747702,DISK], DatanodeInfoWithStorage[127.0.0.1:33396,DS-9443bf65-b8de-4c48-927e-9eb5bcce0ff6,DISK], DatanodeInfoWithStorage[127.0.0.1:36387,DS-abafa500-b6ba-42d3-8d8f-d6f9b3288ef5,DISK], DatanodeInfoWithStorage[127.0.0.1:37860,DS-68c1f349-7b7e-46d2-95db-9024d0a72ed4,DISK], DatanodeInfoWithStorage[127.0.0.1:34560,DS-fcdf70df-a0b6-41ac-b448-b71405368b33,DISK], DatanodeInfoWithStorage[127.0.0.1:33220,DS-ea6e8f60-65d2-4f4e-9a98-7fc5a17c2ddf,DISK], DatanodeInfoWithStorage[127.0.0.1:36425,DS-cd6be38c-b239-4cd0-85a1-6a4de1916b0d,DISK], DatanodeInfoWithStorage[127.0.0.1:43229,DS-7dbff5aa-bcb1-4ce1-bf08-750c1560f528,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: File /striped/stripedFileChecksum3 could only be written to 4 of the 6 required nodes for RS-6-3-1024k. There are 4 datanode(s) running and 4 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2226)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

stackTrace: org.apache.hadoop.ipc.RemoteException(java.io.IOException): File /striped/stripedFileChecksum3 could only be written to 4 of the 6 required nodes for RS-6-3-1024k. There are 4 datanode(s) running and 4 node(s) are excluded in this operation.
	at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget4NewBlock(BlockManager.java:2226)
	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.chooseTargetForNewBlock(FSDirWriteFileOp.java:294)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2789)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1545)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy26.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy27.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.allocateNewBlock(DFSStripedOutputStream.java:480)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:526)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:164)
	at org.apache.hadoop.fs.FSOutputSummer.flushBuffer(FSOutputSummer.java:145)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1178)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
	at org.apache.hadoop.hdfs.TestFileChecksum.prepareTestFiles(TestFileChecksum.java:602)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:532)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1150064338-172.17.0.20-1599303155938:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33018,DS-f2393c7f-fb89-45e4-ae67-90cc0b0e886d,DISK], DatanodeInfoWithStorage[127.0.0.1:43540,DS-b67e0c61-8e45-4379-801f-5b9895631cb6,DISK], DatanodeInfoWithStorage[127.0.0.1:35156,DS-e30edb84-310d-41da-9dc2-679c70c341f9,DISK], DatanodeInfoWithStorage[127.0.0.1:43679,DS-bb7be225-a402-4200-a690-0913fef3e153,DISK], DatanodeInfoWithStorage[127.0.0.1:35350,DS-95ab7328-d596-48e9-84e9-8a8cfd04a832,DISK], DatanodeInfoWithStorage[127.0.0.1:37250,DS-3b36e00c-3033-4b63-8534-47733704d0e5,DISK], DatanodeInfoWithStorage[127.0.0.1:34202,DS-b745cb27-5835-4452-aa7c-5b034132a35c,DISK], DatanodeInfoWithStorage[127.0.0.1:44549,DS-13aad941-f5cd-4078-907a-a3a557195b2e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1150064338-172.17.0.20-1599303155938:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33018,DS-f2393c7f-fb89-45e4-ae67-90cc0b0e886d,DISK], DatanodeInfoWithStorage[127.0.0.1:43540,DS-b67e0c61-8e45-4379-801f-5b9895631cb6,DISK], DatanodeInfoWithStorage[127.0.0.1:35156,DS-e30edb84-310d-41da-9dc2-679c70c341f9,DISK], DatanodeInfoWithStorage[127.0.0.1:43679,DS-bb7be225-a402-4200-a690-0913fef3e153,DISK], DatanodeInfoWithStorage[127.0.0.1:35350,DS-95ab7328-d596-48e9-84e9-8a8cfd04a832,DISK], DatanodeInfoWithStorage[127.0.0.1:37250,DS-3b36e00c-3033-4b63-8534-47733704d0e5,DISK], DatanodeInfoWithStorage[127.0.0.1:34202,DS-b745cb27-5835-4452-aa7c-5b034132a35c,DISK], DatanodeInfoWithStorage[127.0.0.1:44549,DS-13aad941-f5cd-4078-907a-a3a557195b2e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1824323395-172.17.0.20-1599303272481:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40634,DS-a0a743ed-748a-4782-81ad-4df393e46c93,DISK], DatanodeInfoWithStorage[127.0.0.1:44499,DS-2151c362-5f00-45bb-bc0f-ad3d369052ee,DISK], DatanodeInfoWithStorage[127.0.0.1:42753,DS-e515b81e-f9d9-405f-8f51-8a2faec19c67,DISK], DatanodeInfoWithStorage[127.0.0.1:45368,DS-c9748e0c-2598-470f-aa63-1028ac18b7ac,DISK], DatanodeInfoWithStorage[127.0.0.1:35564,DS-1bdcca32-7e08-46ed-adb9-3299167e2d92,DISK], DatanodeInfoWithStorage[127.0.0.1:36787,DS-6f0fcd93-8a2e-4c48-865c-0101ccd7d5e2,DISK], DatanodeInfoWithStorage[127.0.0.1:45223,DS-706e93fa-f6c7-4dcf-bd5b-ff23555f8144,DISK], DatanodeInfoWithStorage[127.0.0.1:43855,DS-eba3344c-a2ef-41bc-9418-5dde525b3bf6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1824323395-172.17.0.20-1599303272481:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40634,DS-a0a743ed-748a-4782-81ad-4df393e46c93,DISK], DatanodeInfoWithStorage[127.0.0.1:44499,DS-2151c362-5f00-45bb-bc0f-ad3d369052ee,DISK], DatanodeInfoWithStorage[127.0.0.1:42753,DS-e515b81e-f9d9-405f-8f51-8a2faec19c67,DISK], DatanodeInfoWithStorage[127.0.0.1:45368,DS-c9748e0c-2598-470f-aa63-1028ac18b7ac,DISK], DatanodeInfoWithStorage[127.0.0.1:35564,DS-1bdcca32-7e08-46ed-adb9-3299167e2d92,DISK], DatanodeInfoWithStorage[127.0.0.1:36787,DS-6f0fcd93-8a2e-4c48-865c-0101ccd7d5e2,DISK], DatanodeInfoWithStorage[127.0.0.1:45223,DS-706e93fa-f6c7-4dcf-bd5b-ff23555f8144,DISK], DatanodeInfoWithStorage[127.0.0.1:43855,DS-eba3344c-a2ef-41bc-9418-5dde525b3bf6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1443754217-172.17.0.20-1599303314323:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41641,DS-4fc308d0-647f-4ef8-9a3f-7eaccfcb2800,DISK], DatanodeInfoWithStorage[127.0.0.1:35347,DS-04321ccd-9fa6-42b2-a523-9f4f5b927ee1,DISK], DatanodeInfoWithStorage[127.0.0.1:43107,DS-ed34025d-2cda-415d-9446-db8311c292c1,DISK], DatanodeInfoWithStorage[127.0.0.1:38582,DS-25f28307-fba0-40f9-8b0c-1e0bb9ea1e52,DISK], DatanodeInfoWithStorage[127.0.0.1:45642,DS-b30b61a5-acdd-43a6-bfca-ece9864838ac,DISK], DatanodeInfoWithStorage[127.0.0.1:43207,DS-495daa11-932d-4537-b8ef-3fe99b33dcfe,DISK], DatanodeInfoWithStorage[127.0.0.1:41433,DS-a9dbd699-f223-425e-9156-da262454067b,DISK], DatanodeInfoWithStorage[127.0.0.1:39472,DS-fc4b8a9b-69a1-4bcf-b8e4-534cea93d059,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1443754217-172.17.0.20-1599303314323:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41641,DS-4fc308d0-647f-4ef8-9a3f-7eaccfcb2800,DISK], DatanodeInfoWithStorage[127.0.0.1:35347,DS-04321ccd-9fa6-42b2-a523-9f4f5b927ee1,DISK], DatanodeInfoWithStorage[127.0.0.1:43107,DS-ed34025d-2cda-415d-9446-db8311c292c1,DISK], DatanodeInfoWithStorage[127.0.0.1:38582,DS-25f28307-fba0-40f9-8b0c-1e0bb9ea1e52,DISK], DatanodeInfoWithStorage[127.0.0.1:45642,DS-b30b61a5-acdd-43a6-bfca-ece9864838ac,DISK], DatanodeInfoWithStorage[127.0.0.1:43207,DS-495daa11-932d-4537-b8ef-3fe99b33dcfe,DISK], DatanodeInfoWithStorage[127.0.0.1:41433,DS-a9dbd699-f223-425e-9156-da262454067b,DISK], DatanodeInfoWithStorage[127.0.0.1:39472,DS-fc4b8a9b-69a1-4bcf-b8e4-534cea93d059,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1377428207-172.17.0.20-1599303501458:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45099,DS-c6ba0762-3574-4210-9208-5f7d658531ee,DISK], DatanodeInfoWithStorage[127.0.0.1:46011,DS-5315304c-6c29-4560-b510-a0442d2fe5fa,DISK], DatanodeInfoWithStorage[127.0.0.1:46434,DS-86e77d4d-88ef-455c-ab37-942b284bb090,DISK], DatanodeInfoWithStorage[127.0.0.1:34206,DS-7d9606be-49e2-4122-b44f-039f8d65f589,DISK], DatanodeInfoWithStorage[127.0.0.1:35941,DS-1a78e6b1-140a-4afa-b001-4b7647e90ca3,DISK], DatanodeInfoWithStorage[127.0.0.1:43738,DS-15f89dc1-71a5-4b4d-90e3-838e4c90568d,DISK], DatanodeInfoWithStorage[127.0.0.1:38832,DS-dc32f8fb-fca6-4d4b-8f56-4f20ed18a5ae,DISK], DatanodeInfoWithStorage[127.0.0.1:45562,DS-9da0623a-b1a7-4359-8378-90fd5e2ae004,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1377428207-172.17.0.20-1599303501458:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45099,DS-c6ba0762-3574-4210-9208-5f7d658531ee,DISK], DatanodeInfoWithStorage[127.0.0.1:46011,DS-5315304c-6c29-4560-b510-a0442d2fe5fa,DISK], DatanodeInfoWithStorage[127.0.0.1:46434,DS-86e77d4d-88ef-455c-ab37-942b284bb090,DISK], DatanodeInfoWithStorage[127.0.0.1:34206,DS-7d9606be-49e2-4122-b44f-039f8d65f589,DISK], DatanodeInfoWithStorage[127.0.0.1:35941,DS-1a78e6b1-140a-4afa-b001-4b7647e90ca3,DISK], DatanodeInfoWithStorage[127.0.0.1:43738,DS-15f89dc1-71a5-4b4d-90e3-838e4c90568d,DISK], DatanodeInfoWithStorage[127.0.0.1:38832,DS-dc32f8fb-fca6-4d4b-8f56-4f20ed18a5ae,DISK], DatanodeInfoWithStorage[127.0.0.1:45562,DS-9da0623a-b1a7-4359-8378-90fd5e2ae004,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-47409829-172.17.0.20-1599303589601:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37361,DS-2469459b-5807-4c47-aae2-79432e64348d,DISK], DatanodeInfoWithStorage[127.0.0.1:34252,DS-67db12da-0c85-4547-a775-32691ec3291d,DISK], DatanodeInfoWithStorage[127.0.0.1:33995,DS-1e59aa98-e9e7-46cf-9446-ecd808c27009,DISK], DatanodeInfoWithStorage[127.0.0.1:45545,DS-132ba0a8-f19a-49e4-8dd9-4625ebee8fb7,DISK], DatanodeInfoWithStorage[127.0.0.1:34161,DS-24827d75-ce5d-4904-80f4-c857cea0e062,DISK], DatanodeInfoWithStorage[127.0.0.1:38000,DS-c0dbb958-e68e-4348-9b64-9989f43d4b34,DISK], DatanodeInfoWithStorage[127.0.0.1:45870,DS-baa1c8e2-c816-48a3-a71d-b11e45808741,DISK], DatanodeInfoWithStorage[127.0.0.1:43279,DS-27e60766-44bf-4a65-88e7-addb8644a2dc,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-47409829-172.17.0.20-1599303589601:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37361,DS-2469459b-5807-4c47-aae2-79432e64348d,DISK], DatanodeInfoWithStorage[127.0.0.1:34252,DS-67db12da-0c85-4547-a775-32691ec3291d,DISK], DatanodeInfoWithStorage[127.0.0.1:33995,DS-1e59aa98-e9e7-46cf-9446-ecd808c27009,DISK], DatanodeInfoWithStorage[127.0.0.1:45545,DS-132ba0a8-f19a-49e4-8dd9-4625ebee8fb7,DISK], DatanodeInfoWithStorage[127.0.0.1:34161,DS-24827d75-ce5d-4904-80f4-c857cea0e062,DISK], DatanodeInfoWithStorage[127.0.0.1:38000,DS-c0dbb958-e68e-4348-9b64-9989f43d4b34,DISK], DatanodeInfoWithStorage[127.0.0.1:45870,DS-baa1c8e2-c816-48a3-a71d-b11e45808741,DISK], DatanodeInfoWithStorage[127.0.0.1:43279,DS-27e60766-44bf-4a65-88e7-addb8644a2dc,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1532493579-172.17.0.20-1599303717825:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40240,DS-edfb55bc-bb51-4215-8ed7-bce4a4283035,DISK], DatanodeInfoWithStorage[127.0.0.1:33583,DS-5e75fdd5-1dff-418f-a0a5-4189ce79773c,DISK], DatanodeInfoWithStorage[127.0.0.1:43242,DS-cbe2ebed-4619-4692-a071-cd91c18a5744,DISK], DatanodeInfoWithStorage[127.0.0.1:39251,DS-1a7ca9f0-39c1-4781-9e37-325c1bb33abe,DISK], DatanodeInfoWithStorage[127.0.0.1:45891,DS-b408de6e-43b8-475a-bfbc-5b71c0c331b3,DISK], DatanodeInfoWithStorage[127.0.0.1:44523,DS-726d898d-1e09-4ee3-abce-7f18c5f6327e,DISK], DatanodeInfoWithStorage[127.0.0.1:33637,DS-954e4922-bc6f-495d-ab0b-506aa04fd825,DISK], DatanodeInfoWithStorage[127.0.0.1:44322,DS-c346c313-877f-4cd4-bbec-d830df90360c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1532493579-172.17.0.20-1599303717825:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40240,DS-edfb55bc-bb51-4215-8ed7-bce4a4283035,DISK], DatanodeInfoWithStorage[127.0.0.1:33583,DS-5e75fdd5-1dff-418f-a0a5-4189ce79773c,DISK], DatanodeInfoWithStorage[127.0.0.1:43242,DS-cbe2ebed-4619-4692-a071-cd91c18a5744,DISK], DatanodeInfoWithStorage[127.0.0.1:39251,DS-1a7ca9f0-39c1-4781-9e37-325c1bb33abe,DISK], DatanodeInfoWithStorage[127.0.0.1:45891,DS-b408de6e-43b8-475a-bfbc-5b71c0c331b3,DISK], DatanodeInfoWithStorage[127.0.0.1:44523,DS-726d898d-1e09-4ee3-abce-7f18c5f6327e,DISK], DatanodeInfoWithStorage[127.0.0.1:33637,DS-954e4922-bc6f-495d-ab0b-506aa04fd825,DISK], DatanodeInfoWithStorage[127.0.0.1:44322,DS-c346c313-877f-4cd4-bbec-d830df90360c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-808754949-172.17.0.20-1599304297005:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35514,DS-57f62723-a187-43ac-96f5-f9c90e8d5f78,DISK], DatanodeInfoWithStorage[127.0.0.1:34384,DS-bb350204-cca3-422d-b216-0c83d325fd80,DISK], DatanodeInfoWithStorage[127.0.0.1:42848,DS-f84245d2-e23a-4893-942b-d25117e0ec5b,DISK], DatanodeInfoWithStorage[127.0.0.1:36307,DS-b847f3ad-e957-4529-b1ab-3421ae589ee9,DISK], DatanodeInfoWithStorage[127.0.0.1:44221,DS-dd462170-74a2-46c7-83a6-372a0f345068,DISK], DatanodeInfoWithStorage[127.0.0.1:41528,DS-9acdb773-bccc-4701-8d6f-0ef9239ef3d9,DISK], DatanodeInfoWithStorage[127.0.0.1:33606,DS-a0e9fd5d-afb9-481c-8ca6-213e35b56af1,DISK], DatanodeInfoWithStorage[127.0.0.1:41552,DS-ca1b9b57-e88a-4f09-a932-052bdf5c8b8b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-808754949-172.17.0.20-1599304297005:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35514,DS-57f62723-a187-43ac-96f5-f9c90e8d5f78,DISK], DatanodeInfoWithStorage[127.0.0.1:34384,DS-bb350204-cca3-422d-b216-0c83d325fd80,DISK], DatanodeInfoWithStorage[127.0.0.1:42848,DS-f84245d2-e23a-4893-942b-d25117e0ec5b,DISK], DatanodeInfoWithStorage[127.0.0.1:36307,DS-b847f3ad-e957-4529-b1ab-3421ae589ee9,DISK], DatanodeInfoWithStorage[127.0.0.1:44221,DS-dd462170-74a2-46c7-83a6-372a0f345068,DISK], DatanodeInfoWithStorage[127.0.0.1:41528,DS-9acdb773-bccc-4701-8d6f-0ef9239ef3d9,DISK], DatanodeInfoWithStorage[127.0.0.1:33606,DS-a0e9fd5d-afb9-481c-8ca6-213e35b56af1,DISK], DatanodeInfoWithStorage[127.0.0.1:41552,DS-ca1b9b57-e88a-4f09-a932-052bdf5c8b8b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-28604438-172.17.0.20-1599304718142:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34521,DS-26501bcf-a814-4499-b359-a05e224e6402,DISK], DatanodeInfoWithStorage[127.0.0.1:40894,DS-8e7cf663-7a86-4ffb-a9a0-479dd429595c,DISK], DatanodeInfoWithStorage[127.0.0.1:38331,DS-3d7cf60c-ec13-4657-a55b-927a5f97261c,DISK], DatanodeInfoWithStorage[127.0.0.1:40388,DS-3e1a30f5-dbe7-4ff9-aab5-190850ce5b5b,DISK], DatanodeInfoWithStorage[127.0.0.1:44516,DS-4ff198d2-ebef-4eee-b693-e84c46b8d62c,DISK], DatanodeInfoWithStorage[127.0.0.1:41936,DS-d2a464fa-9cb6-4fdc-9940-a98c72e7de71,DISK], DatanodeInfoWithStorage[127.0.0.1:36617,DS-7ef4fe74-1517-4755-8f7c-80ba57931663,DISK], DatanodeInfoWithStorage[127.0.0.1:36846,DS-6188a201-300c-4afa-91fe-d9129ff67a2a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-28604438-172.17.0.20-1599304718142:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34521,DS-26501bcf-a814-4499-b359-a05e224e6402,DISK], DatanodeInfoWithStorage[127.0.0.1:40894,DS-8e7cf663-7a86-4ffb-a9a0-479dd429595c,DISK], DatanodeInfoWithStorage[127.0.0.1:38331,DS-3d7cf60c-ec13-4657-a55b-927a5f97261c,DISK], DatanodeInfoWithStorage[127.0.0.1:40388,DS-3e1a30f5-dbe7-4ff9-aab5-190850ce5b5b,DISK], DatanodeInfoWithStorage[127.0.0.1:44516,DS-4ff198d2-ebef-4eee-b693-e84c46b8d62c,DISK], DatanodeInfoWithStorage[127.0.0.1:41936,DS-d2a464fa-9cb6-4fdc-9940-a98c72e7de71,DISK], DatanodeInfoWithStorage[127.0.0.1:36617,DS-7ef4fe74-1517-4755-8f7c-80ba57931663,DISK], DatanodeInfoWithStorage[127.0.0.1:36846,DS-6188a201-300c-4afa-91fe-d9129ff67a2a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1755704826-172.17.0.20-1599304921702:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35526,DS-f66a3280-df86-4cae-b0c8-1455cdeb9a03,DISK], DatanodeInfoWithStorage[127.0.0.1:39199,DS-2e8b6a04-4ff0-4901-a00e-f8b9b308fa8a,DISK], DatanodeInfoWithStorage[127.0.0.1:34747,DS-7445e4aa-c208-4e2f-8cda-d9b6fcfbda14,DISK], DatanodeInfoWithStorage[127.0.0.1:38515,DS-a1cdbf4d-7681-4144-94ca-ed783e102995,DISK], DatanodeInfoWithStorage[127.0.0.1:43169,DS-3be2d53b-c94b-4d4d-970a-9b27bb3d721d,DISK], DatanodeInfoWithStorage[127.0.0.1:42642,DS-db858a72-ca82-4e8c-af53-12bcf9baf133,DISK], DatanodeInfoWithStorage[127.0.0.1:40379,DS-f2c9ddaf-1a63-4d40-a31d-82bfaff71e40,DISK], DatanodeInfoWithStorage[127.0.0.1:37187,DS-85e915e5-720e-4e84-a9c8-620c461e5a6e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1755704826-172.17.0.20-1599304921702:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35526,DS-f66a3280-df86-4cae-b0c8-1455cdeb9a03,DISK], DatanodeInfoWithStorage[127.0.0.1:39199,DS-2e8b6a04-4ff0-4901-a00e-f8b9b308fa8a,DISK], DatanodeInfoWithStorage[127.0.0.1:34747,DS-7445e4aa-c208-4e2f-8cda-d9b6fcfbda14,DISK], DatanodeInfoWithStorage[127.0.0.1:38515,DS-a1cdbf4d-7681-4144-94ca-ed783e102995,DISK], DatanodeInfoWithStorage[127.0.0.1:43169,DS-3be2d53b-c94b-4d4d-970a-9b27bb3d721d,DISK], DatanodeInfoWithStorage[127.0.0.1:42642,DS-db858a72-ca82-4e8c-af53-12bcf9baf133,DISK], DatanodeInfoWithStorage[127.0.0.1:40379,DS-f2c9ddaf-1a63-4d40-a31d-82bfaff71e40,DISK], DatanodeInfoWithStorage[127.0.0.1:37187,DS-85e915e5-720e-4e84-a9c8-620c461e5a6e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1197473432-172.17.0.20-1599305276324:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33258,DS-af8ffc17-c6f4-44de-a8ab-d79c776cbbdd,DISK], DatanodeInfoWithStorage[127.0.0.1:44077,DS-cb76317a-2320-4a26-bfed-dec1e30772f3,DISK], DatanodeInfoWithStorage[127.0.0.1:39777,DS-9a2ddb64-1027-41f2-8ef3-61fef07f9103,DISK], DatanodeInfoWithStorage[127.0.0.1:38868,DS-13922b60-de64-417a-a2b8-5bb159ae3e5b,DISK], DatanodeInfoWithStorage[127.0.0.1:42563,DS-5daa56bd-a7c6-464a-9ad2-77f5b8d30b4c,DISK], DatanodeInfoWithStorage[127.0.0.1:45923,DS-080b6a8b-1be8-4c93-8e35-8bf1617a8d2e,DISK], DatanodeInfoWithStorage[127.0.0.1:35104,DS-8b2eb552-27f0-4007-b6aa-fa82c32f57c1,DISK], DatanodeInfoWithStorage[127.0.0.1:40878,DS-38714f44-d769-42a2-8cb6-a16f3e233585,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1197473432-172.17.0.20-1599305276324:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33258,DS-af8ffc17-c6f4-44de-a8ab-d79c776cbbdd,DISK], DatanodeInfoWithStorage[127.0.0.1:44077,DS-cb76317a-2320-4a26-bfed-dec1e30772f3,DISK], DatanodeInfoWithStorage[127.0.0.1:39777,DS-9a2ddb64-1027-41f2-8ef3-61fef07f9103,DISK], DatanodeInfoWithStorage[127.0.0.1:38868,DS-13922b60-de64-417a-a2b8-5bb159ae3e5b,DISK], DatanodeInfoWithStorage[127.0.0.1:42563,DS-5daa56bd-a7c6-464a-9ad2-77f5b8d30b4c,DISK], DatanodeInfoWithStorage[127.0.0.1:45923,DS-080b6a8b-1be8-4c93-8e35-8bf1617a8d2e,DISK], DatanodeInfoWithStorage[127.0.0.1:35104,DS-8b2eb552-27f0-4007-b6aa-fa82c32f57c1,DISK], DatanodeInfoWithStorage[127.0.0.1:40878,DS-38714f44-d769-42a2-8cb6-a16f3e233585,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-330052068-172.17.0.20-1599305353969:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33001,DS-0e38e58e-97ac-4e4c-8888-0f537e953047,DISK], DatanodeInfoWithStorage[127.0.0.1:39047,DS-c9a0f951-72f4-4970-8008-fd0308e07f0c,DISK], DatanodeInfoWithStorage[127.0.0.1:34351,DS-f4b8693d-fb70-4c58-b7b7-eee5b93e0e00,DISK], DatanodeInfoWithStorage[127.0.0.1:42928,DS-aec21bf3-7894-47d1-b2ba-5c4363e071c8,DISK], DatanodeInfoWithStorage[127.0.0.1:40142,DS-0d4a8f32-b15a-486b-bc74-ccbf0c25ad1c,DISK], DatanodeInfoWithStorage[127.0.0.1:34342,DS-7520446e-2915-4541-803e-31a364efb8c1,DISK], DatanodeInfoWithStorage[127.0.0.1:39329,DS-6454ca79-5239-4bd9-8a83-79b85cf18db7,DISK], DatanodeInfoWithStorage[127.0.0.1:46799,DS-0f1299a5-0a3a-45ce-97da-3370b4807d15,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-330052068-172.17.0.20-1599305353969:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33001,DS-0e38e58e-97ac-4e4c-8888-0f537e953047,DISK], DatanodeInfoWithStorage[127.0.0.1:39047,DS-c9a0f951-72f4-4970-8008-fd0308e07f0c,DISK], DatanodeInfoWithStorage[127.0.0.1:34351,DS-f4b8693d-fb70-4c58-b7b7-eee5b93e0e00,DISK], DatanodeInfoWithStorage[127.0.0.1:42928,DS-aec21bf3-7894-47d1-b2ba-5c4363e071c8,DISK], DatanodeInfoWithStorage[127.0.0.1:40142,DS-0d4a8f32-b15a-486b-bc74-ccbf0c25ad1c,DISK], DatanodeInfoWithStorage[127.0.0.1:34342,DS-7520446e-2915-4541-803e-31a364efb8c1,DISK], DatanodeInfoWithStorage[127.0.0.1:39329,DS-6454ca79-5239-4bd9-8a83-79b85cf18db7,DISK], DatanodeInfoWithStorage[127.0.0.1:46799,DS-0f1299a5-0a3a-45ce-97da-3370b4807d15,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1163696135-172.17.0.20-1599305537647:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35923,DS-5c6eea95-65b0-4f5b-ac91-613fde0ffe5c,DISK], DatanodeInfoWithStorage[127.0.0.1:36875,DS-699f18b2-ce3f-4f87-af9e-d873c03820b2,DISK], DatanodeInfoWithStorage[127.0.0.1:35028,DS-99f4f00b-5dfa-4da3-a7f3-419380224500,DISK], DatanodeInfoWithStorage[127.0.0.1:37311,DS-0ac5f2be-0a8b-48e1-a809-094575dbe2a3,DISK], DatanodeInfoWithStorage[127.0.0.1:40631,DS-ab6b6ba5-1756-4c09-aeb7-3b9a967f74a1,DISK], DatanodeInfoWithStorage[127.0.0.1:44944,DS-426334fa-7021-48cd-b043-bf90f1321112,DISK], DatanodeInfoWithStorage[127.0.0.1:36547,DS-cfdda77a-e293-4f41-bb2a-3b9cd6d5009d,DISK], DatanodeInfoWithStorage[127.0.0.1:45167,DS-e838a171-e602-440b-8f34-2e73dcd660e7,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1163696135-172.17.0.20-1599305537647:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35923,DS-5c6eea95-65b0-4f5b-ac91-613fde0ffe5c,DISK], DatanodeInfoWithStorage[127.0.0.1:36875,DS-699f18b2-ce3f-4f87-af9e-d873c03820b2,DISK], DatanodeInfoWithStorage[127.0.0.1:35028,DS-99f4f00b-5dfa-4da3-a7f3-419380224500,DISK], DatanodeInfoWithStorage[127.0.0.1:37311,DS-0ac5f2be-0a8b-48e1-a809-094575dbe2a3,DISK], DatanodeInfoWithStorage[127.0.0.1:40631,DS-ab6b6ba5-1756-4c09-aeb7-3b9a967f74a1,DISK], DatanodeInfoWithStorage[127.0.0.1:44944,DS-426334fa-7021-48cd-b043-bf90f1321112,DISK], DatanodeInfoWithStorage[127.0.0.1:36547,DS-cfdda77a-e293-4f41-bb2a-3b9cd6d5009d,DISK], DatanodeInfoWithStorage[127.0.0.1:45167,DS-e838a171-e602-440b-8f34-2e73dcd660e7,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-680352136-172.17.0.20-1599305587841:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41229,DS-3ccb8ca0-5d10-401e-8ea9-6c7970feb303,DISK], DatanodeInfoWithStorage[127.0.0.1:35205,DS-8a5fdba3-a046-413c-96e1-555f60b803e6,DISK], DatanodeInfoWithStorage[127.0.0.1:36280,DS-ba061fee-c10a-4179-a4d1-f052009327e0,DISK], DatanodeInfoWithStorage[127.0.0.1:46612,DS-225b8e47-f518-403a-b772-89146d3e72f8,DISK], DatanodeInfoWithStorage[127.0.0.1:32937,DS-f52a0e28-d979-457b-b9e7-6140ca84d56a,DISK], DatanodeInfoWithStorage[127.0.0.1:41957,DS-0b81985a-5cc0-4ef3-8feb-42ddffe887ae,DISK], DatanodeInfoWithStorage[127.0.0.1:45631,DS-1e793a21-b88b-4c5a-ba01-fbcf7b3c011c,DISK], DatanodeInfoWithStorage[127.0.0.1:43987,DS-09811c3c-0bf7-4b46-8b99-f4bb369418b2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-680352136-172.17.0.20-1599305587841:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41229,DS-3ccb8ca0-5d10-401e-8ea9-6c7970feb303,DISK], DatanodeInfoWithStorage[127.0.0.1:35205,DS-8a5fdba3-a046-413c-96e1-555f60b803e6,DISK], DatanodeInfoWithStorage[127.0.0.1:36280,DS-ba061fee-c10a-4179-a4d1-f052009327e0,DISK], DatanodeInfoWithStorage[127.0.0.1:46612,DS-225b8e47-f518-403a-b772-89146d3e72f8,DISK], DatanodeInfoWithStorage[127.0.0.1:32937,DS-f52a0e28-d979-457b-b9e7-6140ca84d56a,DISK], DatanodeInfoWithStorage[127.0.0.1:41957,DS-0b81985a-5cc0-4ef3-8feb-42ddffe887ae,DISK], DatanodeInfoWithStorage[127.0.0.1:45631,DS-1e793a21-b88b-4c5a-ba01-fbcf7b3c011c,DISK], DatanodeInfoWithStorage[127.0.0.1:43987,DS-09811c3c-0bf7-4b46-8b99-f4bb369418b2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-344404669-172.17.0.20-1599306007852:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37587,DS-0b15df98-6e6c-4ec2-8de4-d6b37c8ca262,DISK], DatanodeInfoWithStorage[127.0.0.1:34029,DS-dbf9f995-ef3a-4128-9b85-8368108422d7,DISK], DatanodeInfoWithStorage[127.0.0.1:46420,DS-24a155fd-0aeb-47ef-870c-a492b9035c7e,DISK], DatanodeInfoWithStorage[127.0.0.1:44795,DS-c698a79f-a2f9-458e-ab98-e969080a4dc0,DISK], DatanodeInfoWithStorage[127.0.0.1:37846,DS-9b0c091a-10d7-4af4-baf9-b4bf8e07b79c,DISK], DatanodeInfoWithStorage[127.0.0.1:39912,DS-b4fa24b1-5307-41c1-be7d-c5f4fe0c6312,DISK], DatanodeInfoWithStorage[127.0.0.1:33100,DS-75ed10c8-9dbb-4872-9b5f-aeb44aee77ab,DISK], DatanodeInfoWithStorage[127.0.0.1:36667,DS-fc4200ac-a77d-4932-a98a-d3189925fcf1,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-344404669-172.17.0.20-1599306007852:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37587,DS-0b15df98-6e6c-4ec2-8de4-d6b37c8ca262,DISK], DatanodeInfoWithStorage[127.0.0.1:34029,DS-dbf9f995-ef3a-4128-9b85-8368108422d7,DISK], DatanodeInfoWithStorage[127.0.0.1:46420,DS-24a155fd-0aeb-47ef-870c-a492b9035c7e,DISK], DatanodeInfoWithStorage[127.0.0.1:44795,DS-c698a79f-a2f9-458e-ab98-e969080a4dc0,DISK], DatanodeInfoWithStorage[127.0.0.1:37846,DS-9b0c091a-10d7-4af4-baf9-b4bf8e07b79c,DISK], DatanodeInfoWithStorage[127.0.0.1:39912,DS-b4fa24b1-5307-41c1-be7d-c5f4fe0c6312,DISK], DatanodeInfoWithStorage[127.0.0.1:33100,DS-75ed10c8-9dbb-4872-9b5f-aeb44aee77ab,DISK], DatanodeInfoWithStorage[127.0.0.1:36667,DS-fc4200ac-a77d-4932-a98a-d3189925fcf1,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-716559681-172.17.0.20-1599306207000:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44122,DS-ce0d31df-5de3-41c4-a276-2913bab87b14,DISK], DatanodeInfoWithStorage[127.0.0.1:39388,DS-4b846a00-f67b-46cf-b378-be725af77b68,DISK], DatanodeInfoWithStorage[127.0.0.1:37597,DS-4d127d9e-c165-4c61-a845-c1a11a515de7,DISK], DatanodeInfoWithStorage[127.0.0.1:33616,DS-17adf0f1-66f7-42d1-ab7c-6c585ff3f41c,DISK], DatanodeInfoWithStorage[127.0.0.1:34542,DS-f1f11555-4b81-487e-9258-9ab5470d9bcc,DISK], DatanodeInfoWithStorage[127.0.0.1:46332,DS-938f449b-c29f-4958-a44f-6ab5397174ab,DISK], DatanodeInfoWithStorage[127.0.0.1:46388,DS-157021a6-d55c-4b9b-a416-7b69d2364164,DISK], DatanodeInfoWithStorage[127.0.0.1:45961,DS-0cb1c8c6-4e8c-43e9-8c82-9bc2b3e9f774,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-716559681-172.17.0.20-1599306207000:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44122,DS-ce0d31df-5de3-41c4-a276-2913bab87b14,DISK], DatanodeInfoWithStorage[127.0.0.1:39388,DS-4b846a00-f67b-46cf-b378-be725af77b68,DISK], DatanodeInfoWithStorage[127.0.0.1:37597,DS-4d127d9e-c165-4c61-a845-c1a11a515de7,DISK], DatanodeInfoWithStorage[127.0.0.1:33616,DS-17adf0f1-66f7-42d1-ab7c-6c585ff3f41c,DISK], DatanodeInfoWithStorage[127.0.0.1:34542,DS-f1f11555-4b81-487e-9258-9ab5470d9bcc,DISK], DatanodeInfoWithStorage[127.0.0.1:46332,DS-938f449b-c29f-4958-a44f-6ab5397174ab,DISK], DatanodeInfoWithStorage[127.0.0.1:46388,DS-157021a6-d55c-4b9b-a416-7b69d2364164,DISK], DatanodeInfoWithStorage[127.0.0.1:45961,DS-0cb1c8c6-4e8c-43e9-8c82-9bc2b3e9f774,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1483420137-172.17.0.20-1599306431379:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35347,DS-97e8dcbe-3c51-4228-855e-f67673b7553b,DISK], DatanodeInfoWithStorage[127.0.0.1:38653,DS-8b0e06c0-ae77-462e-8a78-a27fa10ce335,DISK], DatanodeInfoWithStorage[127.0.0.1:41463,DS-68a27158-12f4-4bb7-917f-40318022d655,DISK], DatanodeInfoWithStorage[127.0.0.1:45968,DS-96475057-02bc-489c-a456-62ebfb78d43f,DISK], DatanodeInfoWithStorage[127.0.0.1:37712,DS-c69ed775-6531-4173-97f6-5270f9db86be,DISK], DatanodeInfoWithStorage[127.0.0.1:39532,DS-23ed53c7-12cf-4da6-a087-86fe0c29e88f,DISK], DatanodeInfoWithStorage[127.0.0.1:40348,DS-02cb2139-24e2-4a8f-bfb9-776ce2aabb99,DISK], DatanodeInfoWithStorage[127.0.0.1:45153,DS-41f3301b-2d33-4428-8800-39bb377e1400,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1483420137-172.17.0.20-1599306431379:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35347,DS-97e8dcbe-3c51-4228-855e-f67673b7553b,DISK], DatanodeInfoWithStorage[127.0.0.1:38653,DS-8b0e06c0-ae77-462e-8a78-a27fa10ce335,DISK], DatanodeInfoWithStorage[127.0.0.1:41463,DS-68a27158-12f4-4bb7-917f-40318022d655,DISK], DatanodeInfoWithStorage[127.0.0.1:45968,DS-96475057-02bc-489c-a456-62ebfb78d43f,DISK], DatanodeInfoWithStorage[127.0.0.1:37712,DS-c69ed775-6531-4173-97f6-5270f9db86be,DISK], DatanodeInfoWithStorage[127.0.0.1:39532,DS-23ed53c7-12cf-4da6-a087-86fe0c29e88f,DISK], DatanodeInfoWithStorage[127.0.0.1:40348,DS-02cb2139-24e2-4a8f-bfb9-776ce2aabb99,DISK], DatanodeInfoWithStorage[127.0.0.1:45153,DS-41f3301b-2d33-4428-8800-39bb377e1400,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1648707283-172.17.0.20-1599306541777:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38917,DS-81e67471-5825-4cbe-8916-0a977e4d9e7e,DISK], DatanodeInfoWithStorage[127.0.0.1:37853,DS-c3eea722-1a87-4dc6-a846-281fbdadeee2,DISK], DatanodeInfoWithStorage[127.0.0.1:44384,DS-b4c06a18-fe82-4346-9a07-6e548aef53c4,DISK], DatanodeInfoWithStorage[127.0.0.1:38704,DS-877addab-5fc1-4aeb-bcd3-e54adf78a557,DISK], DatanodeInfoWithStorage[127.0.0.1:41939,DS-d5b48fcf-3762-4e32-aab7-4cd0cfa69627,DISK], DatanodeInfoWithStorage[127.0.0.1:40166,DS-c7afefe4-56c9-47a6-8e5c-ef34a2d31095,DISK], DatanodeInfoWithStorage[127.0.0.1:41246,DS-72d145d0-6e4b-463c-9814-d1f182b0ffd7,DISK], DatanodeInfoWithStorage[127.0.0.1:33819,DS-7c0db7af-2c9d-4fbe-8071-bff811e50688,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1648707283-172.17.0.20-1599306541777:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38917,DS-81e67471-5825-4cbe-8916-0a977e4d9e7e,DISK], DatanodeInfoWithStorage[127.0.0.1:37853,DS-c3eea722-1a87-4dc6-a846-281fbdadeee2,DISK], DatanodeInfoWithStorage[127.0.0.1:44384,DS-b4c06a18-fe82-4346-9a07-6e548aef53c4,DISK], DatanodeInfoWithStorage[127.0.0.1:38704,DS-877addab-5fc1-4aeb-bcd3-e54adf78a557,DISK], DatanodeInfoWithStorage[127.0.0.1:41939,DS-d5b48fcf-3762-4e32-aab7-4cd0cfa69627,DISK], DatanodeInfoWithStorage[127.0.0.1:40166,DS-c7afefe4-56c9-47a6-8e5c-ef34a2d31095,DISK], DatanodeInfoWithStorage[127.0.0.1:41246,DS-72d145d0-6e4b-463c-9814-d1f182b0ffd7,DISK], DatanodeInfoWithStorage[127.0.0.1:33819,DS-7c0db7af-2c9d-4fbe-8071-bff811e50688,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1033849849-172.17.0.20-1599306658418:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44550,DS-ceaa20ac-c2dc-41fb-8e19-879fe971ae2f,DISK], DatanodeInfoWithStorage[127.0.0.1:39374,DS-6abc364b-5ebb-4903-bf8e-f32345ec9e38,DISK], DatanodeInfoWithStorage[127.0.0.1:46544,DS-f0975393-a144-4d3a-a3cb-20bae1743cc4,DISK], DatanodeInfoWithStorage[127.0.0.1:34260,DS-ee2db06f-2822-4bb2-b541-5891dfc5ec05,DISK], DatanodeInfoWithStorage[127.0.0.1:45294,DS-15090bc6-859b-45cb-aff8-127437f50fda,DISK], DatanodeInfoWithStorage[127.0.0.1:42498,DS-100b7212-d6dc-46d2-97af-53264a5cfe3a,DISK], DatanodeInfoWithStorage[127.0.0.1:46834,DS-1fcb6f8c-b5da-4dc6-9150-9a5efb5c124e,DISK], DatanodeInfoWithStorage[127.0.0.1:33681,DS-b75d6e47-c8bd-4ecf-bf7e-9ae8237a93d4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1033849849-172.17.0.20-1599306658418:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44550,DS-ceaa20ac-c2dc-41fb-8e19-879fe971ae2f,DISK], DatanodeInfoWithStorage[127.0.0.1:39374,DS-6abc364b-5ebb-4903-bf8e-f32345ec9e38,DISK], DatanodeInfoWithStorage[127.0.0.1:46544,DS-f0975393-a144-4d3a-a3cb-20bae1743cc4,DISK], DatanodeInfoWithStorage[127.0.0.1:34260,DS-ee2db06f-2822-4bb2-b541-5891dfc5ec05,DISK], DatanodeInfoWithStorage[127.0.0.1:45294,DS-15090bc6-859b-45cb-aff8-127437f50fda,DISK], DatanodeInfoWithStorage[127.0.0.1:42498,DS-100b7212-d6dc-46d2-97af-53264a5cfe3a,DISK], DatanodeInfoWithStorage[127.0.0.1:46834,DS-1fcb6f8c-b5da-4dc6-9150-9a5efb5c124e,DISK], DatanodeInfoWithStorage[127.0.0.1:33681,DS-b75d6e47-c8bd-4ecf-bf7e-9ae8237a93d4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-11319453-172.17.0.20-1599307002355:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38898,DS-ba117049-5b8c-442e-b686-e9d8ead8656c,DISK], DatanodeInfoWithStorage[127.0.0.1:36930,DS-70e2d461-0648-4c12-a16f-c042d63c8308,DISK], DatanodeInfoWithStorage[127.0.0.1:38119,DS-0ef6bb5b-3e1b-4e44-8f6c-6934a353249a,DISK], DatanodeInfoWithStorage[127.0.0.1:45361,DS-c5ec89ef-951c-41e3-a22c-8d9930553b33,DISK], DatanodeInfoWithStorage[127.0.0.1:42019,DS-1e3b658a-83f9-4ec7-97a6-b013b17116f7,DISK], DatanodeInfoWithStorage[127.0.0.1:38086,DS-f36606a7-ef6e-46dc-aa8a-6cc9cf612de2,DISK], DatanodeInfoWithStorage[127.0.0.1:43357,DS-4c072c85-4e55-4ab3-8d9d-f54561b7c17f,DISK], DatanodeInfoWithStorage[127.0.0.1:42661,DS-d8562a1e-7e87-4f24-82c1-c28a7e213901,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-11319453-172.17.0.20-1599307002355:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38898,DS-ba117049-5b8c-442e-b686-e9d8ead8656c,DISK], DatanodeInfoWithStorage[127.0.0.1:36930,DS-70e2d461-0648-4c12-a16f-c042d63c8308,DISK], DatanodeInfoWithStorage[127.0.0.1:38119,DS-0ef6bb5b-3e1b-4e44-8f6c-6934a353249a,DISK], DatanodeInfoWithStorage[127.0.0.1:45361,DS-c5ec89ef-951c-41e3-a22c-8d9930553b33,DISK], DatanodeInfoWithStorage[127.0.0.1:42019,DS-1e3b658a-83f9-4ec7-97a6-b013b17116f7,DISK], DatanodeInfoWithStorage[127.0.0.1:38086,DS-f36606a7-ef6e-46dc-aa8a-6cc9cf612de2,DISK], DatanodeInfoWithStorage[127.0.0.1:43357,DS-4c072c85-4e55-4ab3-8d9d-f54561b7c17f,DISK], DatanodeInfoWithStorage[127.0.0.1:42661,DS-d8562a1e-7e87-4f24-82c1-c28a7e213901,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-468777294-172.17.0.20-1599308048464:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43949,DS-e85c355a-4915-4fec-b296-6635f5e702ee,DISK], DatanodeInfoWithStorage[127.0.0.1:35048,DS-ca4ebc40-e843-4e8c-9e2b-b00866f82ee3,DISK], DatanodeInfoWithStorage[127.0.0.1:38392,DS-a43ce64b-5094-4706-85e7-1ba394eb93d0,DISK], DatanodeInfoWithStorage[127.0.0.1:35554,DS-4fc8aeb2-6f52-4c10-a714-dcca9273594c,DISK], DatanodeInfoWithStorage[127.0.0.1:39337,DS-80a66ab8-4b0a-4132-ad5a-dc78afaf4c64,DISK], DatanodeInfoWithStorage[127.0.0.1:41265,DS-948d22b1-6d64-49f7-9916-e9fc31aa67ab,DISK], DatanodeInfoWithStorage[127.0.0.1:43627,DS-553fedad-2209-4548-89d3-87469ea0b8eb,DISK], DatanodeInfoWithStorage[127.0.0.1:46731,DS-c4fd34f2-f259-4c5d-bf86-00f8d7391e5d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-468777294-172.17.0.20-1599308048464:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43949,DS-e85c355a-4915-4fec-b296-6635f5e702ee,DISK], DatanodeInfoWithStorage[127.0.0.1:35048,DS-ca4ebc40-e843-4e8c-9e2b-b00866f82ee3,DISK], DatanodeInfoWithStorage[127.0.0.1:38392,DS-a43ce64b-5094-4706-85e7-1ba394eb93d0,DISK], DatanodeInfoWithStorage[127.0.0.1:35554,DS-4fc8aeb2-6f52-4c10-a714-dcca9273594c,DISK], DatanodeInfoWithStorage[127.0.0.1:39337,DS-80a66ab8-4b0a-4132-ad5a-dc78afaf4c64,DISK], DatanodeInfoWithStorage[127.0.0.1:41265,DS-948d22b1-6d64-49f7-9916-e9fc31aa67ab,DISK], DatanodeInfoWithStorage[127.0.0.1:43627,DS-553fedad-2209-4548-89d3-87469ea0b8eb,DISK], DatanodeInfoWithStorage[127.0.0.1:46731,DS-c4fd34f2-f259-4c5d-bf86-00f8d7391e5d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 6 out of 50
v1v1v2v2 failed with probability 17 out of 50
result: false positive !!!
Total execution time in seconds : 5878

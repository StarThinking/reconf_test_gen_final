reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 120000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppendRequiringBlockTransfer[1]
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 120000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppendRequiringBlockTransfer[1]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46252,DS-37ac8818-65e1-43a6-9ef1-1972a66b6df9,DISK], DatanodeInfoWithStorage[127.0.0.1:44189,DS-0fd14ed3-fc6a-4bf6-914a-baa00d3fe6fb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46252,DS-37ac8818-65e1-43a6-9ef1-1972a66b6df9,DISK], DatanodeInfoWithStorage[127.0.0.1:44189,DS-0fd14ed3-fc6a-4bf6-914a-baa00d3fe6fb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46252,DS-37ac8818-65e1-43a6-9ef1-1972a66b6df9,DISK], DatanodeInfoWithStorage[127.0.0.1:44189,DS-0fd14ed3-fc6a-4bf6-914a-baa00d3fe6fb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46252,DS-37ac8818-65e1-43a6-9ef1-1972a66b6df9,DISK], DatanodeInfoWithStorage[127.0.0.1:44189,DS-0fd14ed3-fc6a-4bf6-914a-baa00d3fe6fb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 120000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppendRequiringBlockTransfer[1]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43611,DS-09a2cc54-a9c9-48cc-a7dd-8c04a2e043b2,DISK], DatanodeInfoWithStorage[127.0.0.1:46846,DS-343a3c2a-3615-4b15-8418-9594cd28a784,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43611,DS-09a2cc54-a9c9-48cc-a7dd-8c04a2e043b2,DISK], DatanodeInfoWithStorage[127.0.0.1:46846,DS-343a3c2a-3615-4b15-8418-9594cd28a784,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43611,DS-09a2cc54-a9c9-48cc-a7dd-8c04a2e043b2,DISK], DatanodeInfoWithStorage[127.0.0.1:46846,DS-343a3c2a-3615-4b15-8418-9594cd28a784,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43611,DS-09a2cc54-a9c9-48cc-a7dd-8c04a2e043b2,DISK], DatanodeInfoWithStorage[127.0.0.1:46846,DS-343a3c2a-3615-4b15-8418-9594cd28a784,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 120000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppendRequiringBlockTransfer[1]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38270,DS-45d9c80d-e278-43bc-9a86-1ae923a97a82,DISK], DatanodeInfoWithStorage[127.0.0.1:38132,DS-220adc8b-894f-4733-8c82-f8e75bb2fba8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38270,DS-45d9c80d-e278-43bc-9a86-1ae923a97a82,DISK], DatanodeInfoWithStorage[127.0.0.1:38132,DS-220adc8b-894f-4733-8c82-f8e75bb2fba8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38270,DS-45d9c80d-e278-43bc-9a86-1ae923a97a82,DISK], DatanodeInfoWithStorage[127.0.0.1:38132,DS-220adc8b-894f-4733-8c82-f8e75bb2fba8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38270,DS-45d9c80d-e278-43bc-9a86-1ae923a97a82,DISK], DatanodeInfoWithStorage[127.0.0.1:38132,DS-220adc8b-894f-4733-8c82-f8e75bb2fba8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 120000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppendRequiringBlockTransfer[1]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46201,DS-0d9cf623-d70c-4ae1-be37-413f6600101c,DISK], DatanodeInfoWithStorage[127.0.0.1:35189,DS-02d60197-55c3-498c-8640-7a86b797bba7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46201,DS-0d9cf623-d70c-4ae1-be37-413f6600101c,DISK], DatanodeInfoWithStorage[127.0.0.1:35189,DS-02d60197-55c3-498c-8640-7a86b797bba7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46201,DS-0d9cf623-d70c-4ae1-be37-413f6600101c,DISK], DatanodeInfoWithStorage[127.0.0.1:35189,DS-02d60197-55c3-498c-8640-7a86b797bba7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46201,DS-0d9cf623-d70c-4ae1-be37-413f6600101c,DISK], DatanodeInfoWithStorage[127.0.0.1:35189,DS-02d60197-55c3-498c-8640-7a86b797bba7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 120000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppendRequiringBlockTransfer[1]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36236,DS-1a8c4942-a490-42a4-97f7-ba4dad9ce3fd,DISK], DatanodeInfoWithStorage[127.0.0.1:37944,DS-0162a262-ad90-43ab-9428-9df1819daf9e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37944,DS-0162a262-ad90-43ab-9428-9df1819daf9e,DISK], DatanodeInfoWithStorage[127.0.0.1:36236,DS-1a8c4942-a490-42a4-97f7-ba4dad9ce3fd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36236,DS-1a8c4942-a490-42a4-97f7-ba4dad9ce3fd,DISK], DatanodeInfoWithStorage[127.0.0.1:37944,DS-0162a262-ad90-43ab-9428-9df1819daf9e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37944,DS-0162a262-ad90-43ab-9428-9df1819daf9e,DISK], DatanodeInfoWithStorage[127.0.0.1:36236,DS-1a8c4942-a490-42a4-97f7-ba4dad9ce3fd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 120000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppendRequiringBlockTransfer[1]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42189,DS-83567ba0-a9dd-4218-b1e0-6f9269ba396f,DISK], DatanodeInfoWithStorage[127.0.0.1:34817,DS-20503d2a-a240-4cdc-bd97-86de8c393e98,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42189,DS-83567ba0-a9dd-4218-b1e0-6f9269ba396f,DISK], DatanodeInfoWithStorage[127.0.0.1:34817,DS-20503d2a-a240-4cdc-bd97-86de8c393e98,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42189,DS-83567ba0-a9dd-4218-b1e0-6f9269ba396f,DISK], DatanodeInfoWithStorage[127.0.0.1:34817,DS-20503d2a-a240-4cdc-bd97-86de8c393e98,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42189,DS-83567ba0-a9dd-4218-b1e0-6f9269ba396f,DISK], DatanodeInfoWithStorage[127.0.0.1:34817,DS-20503d2a-a240-4cdc-bd97-86de8c393e98,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 3 out of 50
v1v1v2v2 failed with probability 3 out of 50
result: false positive !!!
Total execution time in seconds : 3804

reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42280,DS-b8f61350-b70e-43c5-8f33-f44cffcb997a,DISK], DatanodeInfoWithStorage[127.0.0.1:38587,DS-4ede2294-fda4-4363-a07b-35897d2fcf12,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42280,DS-b8f61350-b70e-43c5-8f33-f44cffcb997a,DISK], DatanodeInfoWithStorage[127.0.0.1:38587,DS-4ede2294-fda4-4363-a07b-35897d2fcf12,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42280,DS-b8f61350-b70e-43c5-8f33-f44cffcb997a,DISK], DatanodeInfoWithStorage[127.0.0.1:38587,DS-4ede2294-fda4-4363-a07b-35897d2fcf12,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42280,DS-b8f61350-b70e-43c5-8f33-f44cffcb997a,DISK], DatanodeInfoWithStorage[127.0.0.1:38587,DS-4ede2294-fda4-4363-a07b-35897d2fcf12,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44957,DS-beec6969-4611-4044-b55f-1173dd0b0668,DISK], DatanodeInfoWithStorage[127.0.0.1:45513,DS-85522a1d-18c8-4b96-b8aa-1c0e5bb4022d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45513,DS-85522a1d-18c8-4b96-b8aa-1c0e5bb4022d,DISK], DatanodeInfoWithStorage[127.0.0.1:44957,DS-beec6969-4611-4044-b55f-1173dd0b0668,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44957,DS-beec6969-4611-4044-b55f-1173dd0b0668,DISK], DatanodeInfoWithStorage[127.0.0.1:45513,DS-85522a1d-18c8-4b96-b8aa-1c0e5bb4022d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45513,DS-85522a1d-18c8-4b96-b8aa-1c0e5bb4022d,DISK], DatanodeInfoWithStorage[127.0.0.1:44957,DS-beec6969-4611-4044-b55f-1173dd0b0668,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38289,DS-b4c8ef71-e4e0-4d1a-a220-6b5b27d1bb8f,DISK], DatanodeInfoWithStorage[127.0.0.1:43238,DS-bc3af8c6-bc7d-46c9-87aa-218309057b22,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43238,DS-bc3af8c6-bc7d-46c9-87aa-218309057b22,DISK], DatanodeInfoWithStorage[127.0.0.1:38289,DS-b4c8ef71-e4e0-4d1a-a220-6b5b27d1bb8f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38289,DS-b4c8ef71-e4e0-4d1a-a220-6b5b27d1bb8f,DISK], DatanodeInfoWithStorage[127.0.0.1:43238,DS-bc3af8c6-bc7d-46c9-87aa-218309057b22,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43238,DS-bc3af8c6-bc7d-46c9-87aa-218309057b22,DISK], DatanodeInfoWithStorage[127.0.0.1:38289,DS-b4c8ef71-e4e0-4d1a-a220-6b5b27d1bb8f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42488,DS-4b180d5d-4358-4ea2-97e3-ab29d281ee7c,DISK], DatanodeInfoWithStorage[127.0.0.1:36154,DS-7b946533-037a-4070-a27d-7c934bc2226c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42488,DS-4b180d5d-4358-4ea2-97e3-ab29d281ee7c,DISK], DatanodeInfoWithStorage[127.0.0.1:36154,DS-7b946533-037a-4070-a27d-7c934bc2226c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42488,DS-4b180d5d-4358-4ea2-97e3-ab29d281ee7c,DISK], DatanodeInfoWithStorage[127.0.0.1:36154,DS-7b946533-037a-4070-a27d-7c934bc2226c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42488,DS-4b180d5d-4358-4ea2-97e3-ab29d281ee7c,DISK], DatanodeInfoWithStorage[127.0.0.1:36154,DS-7b946533-037a-4070-a27d-7c934bc2226c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36894,DS-900ed125-b9e4-4e4e-8861-11ec1e24b845,DISK], DatanodeInfoWithStorage[127.0.0.1:39018,DS-d170dc20-6bbc-448f-a800-c39ad76333e2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36894,DS-900ed125-b9e4-4e4e-8861-11ec1e24b845,DISK], DatanodeInfoWithStorage[127.0.0.1:39018,DS-d170dc20-6bbc-448f-a800-c39ad76333e2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36894,DS-900ed125-b9e4-4e4e-8861-11ec1e24b845,DISK], DatanodeInfoWithStorage[127.0.0.1:39018,DS-d170dc20-6bbc-448f-a800-c39ad76333e2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36894,DS-900ed125-b9e4-4e4e-8861-11ec1e24b845,DISK], DatanodeInfoWithStorage[127.0.0.1:39018,DS-d170dc20-6bbc-448f-a800-c39ad76333e2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36864,DS-4d05efbd-495f-45ea-bc61-3a07bb061c4d,DISK], DatanodeInfoWithStorage[127.0.0.1:44729,DS-eca63d4a-9ea2-48e7-8e1b-5cfb5be67f13,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36864,DS-4d05efbd-495f-45ea-bc61-3a07bb061c4d,DISK], DatanodeInfoWithStorage[127.0.0.1:44729,DS-eca63d4a-9ea2-48e7-8e1b-5cfb5be67f13,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36864,DS-4d05efbd-495f-45ea-bc61-3a07bb061c4d,DISK], DatanodeInfoWithStorage[127.0.0.1:44729,DS-eca63d4a-9ea2-48e7-8e1b-5cfb5be67f13,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36864,DS-4d05efbd-495f-45ea-bc61-3a07bb061c4d,DISK], DatanodeInfoWithStorage[127.0.0.1:44729,DS-eca63d4a-9ea2-48e7-8e1b-5cfb5be67f13,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37545,DS-28c83a99-1bf1-40cf-a885-b55ab54b693c,DISK], DatanodeInfoWithStorage[127.0.0.1:45282,DS-b70ad3f5-3852-46d1-ac5b-92593a9c5bea,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37545,DS-28c83a99-1bf1-40cf-a885-b55ab54b693c,DISK], DatanodeInfoWithStorage[127.0.0.1:45282,DS-b70ad3f5-3852-46d1-ac5b-92593a9c5bea,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37545,DS-28c83a99-1bf1-40cf-a885-b55ab54b693c,DISK], DatanodeInfoWithStorage[127.0.0.1:45282,DS-b70ad3f5-3852-46d1-ac5b-92593a9c5bea,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37545,DS-28c83a99-1bf1-40cf-a885-b55ab54b693c,DISK], DatanodeInfoWithStorage[127.0.0.1:45282,DS-b70ad3f5-3852-46d1-ac5b-92593a9c5bea,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43116,DS-35d9881d-9018-4f15-8dee-7ea2c714dfe2,DISK], DatanodeInfoWithStorage[127.0.0.1:46524,DS-11da44cd-b380-452f-b853-752f844a075a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43116,DS-35d9881d-9018-4f15-8dee-7ea2c714dfe2,DISK], DatanodeInfoWithStorage[127.0.0.1:46524,DS-11da44cd-b380-452f-b853-752f844a075a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43116,DS-35d9881d-9018-4f15-8dee-7ea2c714dfe2,DISK], DatanodeInfoWithStorage[127.0.0.1:46524,DS-11da44cd-b380-452f-b853-752f844a075a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43116,DS-35d9881d-9018-4f15-8dee-7ea2c714dfe2,DISK], DatanodeInfoWithStorage[127.0.0.1:46524,DS-11da44cd-b380-452f-b853-752f844a075a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38647,DS-c6a40a77-ca15-4d94-b51b-5e34dab8016b,DISK], DatanodeInfoWithStorage[127.0.0.1:34650,DS-0cf68059-b487-4199-a855-84c47743be3c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38647,DS-c6a40a77-ca15-4d94-b51b-5e34dab8016b,DISK], DatanodeInfoWithStorage[127.0.0.1:34650,DS-0cf68059-b487-4199-a855-84c47743be3c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38647,DS-c6a40a77-ca15-4d94-b51b-5e34dab8016b,DISK], DatanodeInfoWithStorage[127.0.0.1:34650,DS-0cf68059-b487-4199-a855-84c47743be3c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38647,DS-c6a40a77-ca15-4d94-b51b-5e34dab8016b,DISK], DatanodeInfoWithStorage[127.0.0.1:34650,DS-0cf68059-b487-4199-a855-84c47743be3c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43959,DS-5a0dae97-8473-4b01-adf8-deb483475d25,DISK], DatanodeInfoWithStorage[127.0.0.1:33979,DS-b7116abd-c31a-497e-a78e-67fef36728c1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43959,DS-5a0dae97-8473-4b01-adf8-deb483475d25,DISK], DatanodeInfoWithStorage[127.0.0.1:33979,DS-b7116abd-c31a-497e-a78e-67fef36728c1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43959,DS-5a0dae97-8473-4b01-adf8-deb483475d25,DISK], DatanodeInfoWithStorage[127.0.0.1:33979,DS-b7116abd-c31a-497e-a78e-67fef36728c1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43959,DS-5a0dae97-8473-4b01-adf8-deb483475d25,DISK], DatanodeInfoWithStorage[127.0.0.1:33979,DS-b7116abd-c31a-497e-a78e-67fef36728c1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38919,DS-decc3fe2-b175-459b-85d6-7299bb89fc31,DISK], DatanodeInfoWithStorage[127.0.0.1:36188,DS-067b4642-4a40-42d5-9141-bb50414afb93,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38919,DS-decc3fe2-b175-459b-85d6-7299bb89fc31,DISK], DatanodeInfoWithStorage[127.0.0.1:36188,DS-067b4642-4a40-42d5-9141-bb50414afb93,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38919,DS-decc3fe2-b175-459b-85d6-7299bb89fc31,DISK], DatanodeInfoWithStorage[127.0.0.1:36188,DS-067b4642-4a40-42d5-9141-bb50414afb93,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38919,DS-decc3fe2-b175-459b-85d6-7299bb89fc31,DISK], DatanodeInfoWithStorage[127.0.0.1:36188,DS-067b4642-4a40-42d5-9141-bb50414afb93,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39797,DS-ca2eff2e-0943-457b-8f86-74490960866a,DISK], DatanodeInfoWithStorage[127.0.0.1:44664,DS-d10d430b-bde8-42fc-bc7f-46d738781d8e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44664,DS-d10d430b-bde8-42fc-bc7f-46d738781d8e,DISK], DatanodeInfoWithStorage[127.0.0.1:39797,DS-ca2eff2e-0943-457b-8f86-74490960866a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39797,DS-ca2eff2e-0943-457b-8f86-74490960866a,DISK], DatanodeInfoWithStorage[127.0.0.1:44664,DS-d10d430b-bde8-42fc-bc7f-46d738781d8e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44664,DS-d10d430b-bde8-42fc-bc7f-46d738781d8e,DISK], DatanodeInfoWithStorage[127.0.0.1:39797,DS-ca2eff2e-0943-457b-8f86-74490960866a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40454,DS-e1b7ea57-f9b1-4e45-82c3-7b71fec2a07b,DISK], DatanodeInfoWithStorage[127.0.0.1:36571,DS-29799ce0-a631-4955-9735-c371b3a96b14,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40454,DS-e1b7ea57-f9b1-4e45-82c3-7b71fec2a07b,DISK], DatanodeInfoWithStorage[127.0.0.1:36571,DS-29799ce0-a631-4955-9735-c371b3a96b14,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40454,DS-e1b7ea57-f9b1-4e45-82c3-7b71fec2a07b,DISK], DatanodeInfoWithStorage[127.0.0.1:36571,DS-29799ce0-a631-4955-9735-c371b3a96b14,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40454,DS-e1b7ea57-f9b1-4e45-82c3-7b71fec2a07b,DISK], DatanodeInfoWithStorage[127.0.0.1:36571,DS-29799ce0-a631-4955-9735-c371b3a96b14,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41222,DS-f5e40cff-1fce-4263-ba36-73a3bfd83873,DISK], DatanodeInfoWithStorage[127.0.0.1:42420,DS-4f20cd8f-3b90-4895-a290-511589e0787e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41222,DS-f5e40cff-1fce-4263-ba36-73a3bfd83873,DISK], DatanodeInfoWithStorage[127.0.0.1:42420,DS-4f20cd8f-3b90-4895-a290-511589e0787e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41222,DS-f5e40cff-1fce-4263-ba36-73a3bfd83873,DISK], DatanodeInfoWithStorage[127.0.0.1:42420,DS-4f20cd8f-3b90-4895-a290-511589e0787e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41222,DS-f5e40cff-1fce-4263-ba36-73a3bfd83873,DISK], DatanodeInfoWithStorage[127.0.0.1:42420,DS-4f20cd8f-3b90-4895-a290-511589e0787e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44265,DS-7f1e9873-bd18-44b2-8784-1b94f960051a,DISK], DatanodeInfoWithStorage[127.0.0.1:39595,DS-333e2516-de85-48b5-957f-00206d754cdb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44265,DS-7f1e9873-bd18-44b2-8784-1b94f960051a,DISK], DatanodeInfoWithStorage[127.0.0.1:39595,DS-333e2516-de85-48b5-957f-00206d754cdb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44265,DS-7f1e9873-bd18-44b2-8784-1b94f960051a,DISK], DatanodeInfoWithStorage[127.0.0.1:39595,DS-333e2516-de85-48b5-957f-00206d754cdb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44265,DS-7f1e9873-bd18-44b2-8784-1b94f960051a,DISK], DatanodeInfoWithStorage[127.0.0.1:39595,DS-333e2516-de85-48b5-957f-00206d754cdb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41850,DS-c5cadbe6-edba-49c7-ab46-58c1b3c44493,DISK], DatanodeInfoWithStorage[127.0.0.1:46055,DS-9493f748-c22c-47fc-bb65-0e9dd21a7a89,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46055,DS-9493f748-c22c-47fc-bb65-0e9dd21a7a89,DISK], DatanodeInfoWithStorage[127.0.0.1:41850,DS-c5cadbe6-edba-49c7-ab46-58c1b3c44493,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41850,DS-c5cadbe6-edba-49c7-ab46-58c1b3c44493,DISK], DatanodeInfoWithStorage[127.0.0.1:46055,DS-9493f748-c22c-47fc-bb65-0e9dd21a7a89,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46055,DS-9493f748-c22c-47fc-bb65-0e9dd21a7a89,DISK], DatanodeInfoWithStorage[127.0.0.1:41850,DS-c5cadbe6-edba-49c7-ab46-58c1b3c44493,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39064,DS-b9ed32e6-f732-4afc-a5d7-17aebf9abd1a,DISK], DatanodeInfoWithStorage[127.0.0.1:36047,DS-0c50e2e1-1bfe-4e39-a8b1-1a7f8be6bd89,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39064,DS-b9ed32e6-f732-4afc-a5d7-17aebf9abd1a,DISK], DatanodeInfoWithStorage[127.0.0.1:36047,DS-0c50e2e1-1bfe-4e39-a8b1-1a7f8be6bd89,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39064,DS-b9ed32e6-f732-4afc-a5d7-17aebf9abd1a,DISK], DatanodeInfoWithStorage[127.0.0.1:36047,DS-0c50e2e1-1bfe-4e39-a8b1-1a7f8be6bd89,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39064,DS-b9ed32e6-f732-4afc-a5d7-17aebf9abd1a,DISK], DatanodeInfoWithStorage[127.0.0.1:36047,DS-0c50e2e1-1bfe-4e39-a8b1-1a7f8be6bd89,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41168,DS-9ff54ad5-ed25-40c4-94ec-91c7f99066ba,DISK], DatanodeInfoWithStorage[127.0.0.1:33416,DS-19e32993-f796-4521-a995-59e529c98415,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41168,DS-9ff54ad5-ed25-40c4-94ec-91c7f99066ba,DISK], DatanodeInfoWithStorage[127.0.0.1:33416,DS-19e32993-f796-4521-a995-59e529c98415,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41168,DS-9ff54ad5-ed25-40c4-94ec-91c7f99066ba,DISK], DatanodeInfoWithStorage[127.0.0.1:33416,DS-19e32993-f796-4521-a995-59e529c98415,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41168,DS-9ff54ad5-ed25-40c4-94ec-91c7f99066ba,DISK], DatanodeInfoWithStorage[127.0.0.1:33416,DS-19e32993-f796-4521-a995-59e529c98415,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40254,DS-f8ba2fb9-2343-4f2c-ba68-2443638c046c,DISK], DatanodeInfoWithStorage[127.0.0.1:39434,DS-8086aae2-214c-4a16-8d30-d806976bb0f4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40254,DS-f8ba2fb9-2343-4f2c-ba68-2443638c046c,DISK], DatanodeInfoWithStorage[127.0.0.1:39434,DS-8086aae2-214c-4a16-8d30-d806976bb0f4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40254,DS-f8ba2fb9-2343-4f2c-ba68-2443638c046c,DISK], DatanodeInfoWithStorage[127.0.0.1:39434,DS-8086aae2-214c-4a16-8d30-d806976bb0f4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40254,DS-f8ba2fb9-2343-4f2c-ba68-2443638c046c,DISK], DatanodeInfoWithStorage[127.0.0.1:39434,DS-8086aae2-214c-4a16-8d30-d806976bb0f4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32992,DS-6767082e-1023-4ab8-886b-2a7b278c6757,DISK], DatanodeInfoWithStorage[127.0.0.1:32907,DS-6e028545-30af-42dd-8543-6d58da2f454a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32992,DS-6767082e-1023-4ab8-886b-2a7b278c6757,DISK], DatanodeInfoWithStorage[127.0.0.1:32907,DS-6e028545-30af-42dd-8543-6d58da2f454a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32992,DS-6767082e-1023-4ab8-886b-2a7b278c6757,DISK], DatanodeInfoWithStorage[127.0.0.1:32907,DS-6e028545-30af-42dd-8543-6d58da2f454a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32992,DS-6767082e-1023-4ab8-886b-2a7b278c6757,DISK], DatanodeInfoWithStorage[127.0.0.1:32907,DS-6e028545-30af-42dd-8543-6d58da2f454a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37180,DS-bf803290-356d-4f6b-913e-39acce8258f7,DISK], DatanodeInfoWithStorage[127.0.0.1:44472,DS-c8065225-2c97-4088-9043-04319767b4de,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44472,DS-c8065225-2c97-4088-9043-04319767b4de,DISK], DatanodeInfoWithStorage[127.0.0.1:37180,DS-bf803290-356d-4f6b-913e-39acce8258f7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37180,DS-bf803290-356d-4f6b-913e-39acce8258f7,DISK], DatanodeInfoWithStorage[127.0.0.1:44472,DS-c8065225-2c97-4088-9043-04319767b4de,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44472,DS-c8065225-2c97-4088-9043-04319767b4de,DISK], DatanodeInfoWithStorage[127.0.0.1:37180,DS-bf803290-356d-4f6b-913e-39acce8258f7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33180,DS-b0d6224d-7bae-46ed-92c8-0b8c846a8850,DISK], DatanodeInfoWithStorage[127.0.0.1:33725,DS-c69b1744-d042-4ff1-97a5-e5b787923804,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33725,DS-c69b1744-d042-4ff1-97a5-e5b787923804,DISK], DatanodeInfoWithStorage[127.0.0.1:33180,DS-b0d6224d-7bae-46ed-92c8-0b8c846a8850,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33180,DS-b0d6224d-7bae-46ed-92c8-0b8c846a8850,DISK], DatanodeInfoWithStorage[127.0.0.1:33725,DS-c69b1744-d042-4ff1-97a5-e5b787923804,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33725,DS-c69b1744-d042-4ff1-97a5-e5b787923804,DISK], DatanodeInfoWithStorage[127.0.0.1:33180,DS-b0d6224d-7bae-46ed-92c8-0b8c846a8850,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35088,DS-a5c63495-92b7-42da-b283-a6a95f5d8e37,DISK], DatanodeInfoWithStorage[127.0.0.1:42376,DS-fc057916-bbfb-4b97-a6cb-c1182cae588d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35088,DS-a5c63495-92b7-42da-b283-a6a95f5d8e37,DISK], DatanodeInfoWithStorage[127.0.0.1:42376,DS-fc057916-bbfb-4b97-a6cb-c1182cae588d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35088,DS-a5c63495-92b7-42da-b283-a6a95f5d8e37,DISK], DatanodeInfoWithStorage[127.0.0.1:42376,DS-fc057916-bbfb-4b97-a6cb-c1182cae588d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35088,DS-a5c63495-92b7-42da-b283-a6a95f5d8e37,DISK], DatanodeInfoWithStorage[127.0.0.1:42376,DS-fc057916-bbfb-4b97-a6cb-c1182cae588d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35169,DS-0a453861-928d-4e5a-a07b-841910b51755,DISK], DatanodeInfoWithStorage[127.0.0.1:34517,DS-9f789f0e-1cc0-46e4-a276-fe7d57ee7b45,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34517,DS-9f789f0e-1cc0-46e4-a276-fe7d57ee7b45,DISK], DatanodeInfoWithStorage[127.0.0.1:35169,DS-0a453861-928d-4e5a-a07b-841910b51755,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35169,DS-0a453861-928d-4e5a-a07b-841910b51755,DISK], DatanodeInfoWithStorage[127.0.0.1:34517,DS-9f789f0e-1cc0-46e4-a276-fe7d57ee7b45,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34517,DS-9f789f0e-1cc0-46e4-a276-fe7d57ee7b45,DISK], DatanodeInfoWithStorage[127.0.0.1:35169,DS-0a453861-928d-4e5a-a07b-841910b51755,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35427,DS-3afab67a-9195-4641-8072-aa0420db6767,DISK], DatanodeInfoWithStorage[127.0.0.1:43652,DS-d0e5ff53-1816-44bd-8e6e-31c5db010426,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35427,DS-3afab67a-9195-4641-8072-aa0420db6767,DISK], DatanodeInfoWithStorage[127.0.0.1:43652,DS-d0e5ff53-1816-44bd-8e6e-31c5db010426,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35427,DS-3afab67a-9195-4641-8072-aa0420db6767,DISK], DatanodeInfoWithStorage[127.0.0.1:43652,DS-d0e5ff53-1816-44bd-8e6e-31c5db010426,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35427,DS-3afab67a-9195-4641-8072-aa0420db6767,DISK], DatanodeInfoWithStorage[127.0.0.1:43652,DS-d0e5ff53-1816-44bd-8e6e-31c5db010426,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36026,DS-69f983a8-2d2e-4482-8371-867d1b192102,DISK], DatanodeInfoWithStorage[127.0.0.1:45444,DS-fc95910b-1a8c-4a72-8f60-9d4201f89e09,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36026,DS-69f983a8-2d2e-4482-8371-867d1b192102,DISK], DatanodeInfoWithStorage[127.0.0.1:45444,DS-fc95910b-1a8c-4a72-8f60-9d4201f89e09,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36026,DS-69f983a8-2d2e-4482-8371-867d1b192102,DISK], DatanodeInfoWithStorage[127.0.0.1:45444,DS-fc95910b-1a8c-4a72-8f60-9d4201f89e09,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36026,DS-69f983a8-2d2e-4482-8371-867d1b192102,DISK], DatanodeInfoWithStorage[127.0.0.1:45444,DS-fc95910b-1a8c-4a72-8f60-9d4201f89e09,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43939,DS-2268e22b-933b-43f9-abdf-ec4ea67770e6,DISK], DatanodeInfoWithStorage[127.0.0.1:36855,DS-b0e88d6b-d62a-40c2-b04a-5d22e91ab6c8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43939,DS-2268e22b-933b-43f9-abdf-ec4ea67770e6,DISK], DatanodeInfoWithStorage[127.0.0.1:36855,DS-b0e88d6b-d62a-40c2-b04a-5d22e91ab6c8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43939,DS-2268e22b-933b-43f9-abdf-ec4ea67770e6,DISK], DatanodeInfoWithStorage[127.0.0.1:36855,DS-b0e88d6b-d62a-40c2-b04a-5d22e91ab6c8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43939,DS-2268e22b-933b-43f9-abdf-ec4ea67770e6,DISK], DatanodeInfoWithStorage[127.0.0.1:36855,DS-b0e88d6b-d62a-40c2-b04a-5d22e91ab6c8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36106,DS-e57bbd70-9408-4c69-aef0-6c8c118ee5fb,DISK], DatanodeInfoWithStorage[127.0.0.1:42365,DS-9f0f70a9-4dcf-42a0-a3af-0761acb519aa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42365,DS-9f0f70a9-4dcf-42a0-a3af-0761acb519aa,DISK], DatanodeInfoWithStorage[127.0.0.1:36106,DS-e57bbd70-9408-4c69-aef0-6c8c118ee5fb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36106,DS-e57bbd70-9408-4c69-aef0-6c8c118ee5fb,DISK], DatanodeInfoWithStorage[127.0.0.1:42365,DS-9f0f70a9-4dcf-42a0-a3af-0761acb519aa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42365,DS-9f0f70a9-4dcf-42a0-a3af-0761acb519aa,DISK], DatanodeInfoWithStorage[127.0.0.1:36106,DS-e57bbd70-9408-4c69-aef0-6c8c118ee5fb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37010,DS-575fda91-e611-48aa-a4ee-99eace786717,DISK], DatanodeInfoWithStorage[127.0.0.1:42992,DS-c92da3b7-8177-43d3-8b86-3bfbcc6e5ea3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42992,DS-c92da3b7-8177-43d3-8b86-3bfbcc6e5ea3,DISK], DatanodeInfoWithStorage[127.0.0.1:37010,DS-575fda91-e611-48aa-a4ee-99eace786717,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37010,DS-575fda91-e611-48aa-a4ee-99eace786717,DISK], DatanodeInfoWithStorage[127.0.0.1:42992,DS-c92da3b7-8177-43d3-8b86-3bfbcc6e5ea3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42992,DS-c92da3b7-8177-43d3-8b86-3bfbcc6e5ea3,DISK], DatanodeInfoWithStorage[127.0.0.1:37010,DS-575fda91-e611-48aa-a4ee-99eace786717,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42063,DS-3b7a3d6b-1910-44d8-8aeb-99a25949f83e,DISK], DatanodeInfoWithStorage[127.0.0.1:38642,DS-ca85a357-094a-4807-85d9-fc8381f50226,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38642,DS-ca85a357-094a-4807-85d9-fc8381f50226,DISK], DatanodeInfoWithStorage[127.0.0.1:42063,DS-3b7a3d6b-1910-44d8-8aeb-99a25949f83e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42063,DS-3b7a3d6b-1910-44d8-8aeb-99a25949f83e,DISK], DatanodeInfoWithStorage[127.0.0.1:38642,DS-ca85a357-094a-4807-85d9-fc8381f50226,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38642,DS-ca85a357-094a-4807-85d9-fc8381f50226,DISK], DatanodeInfoWithStorage[127.0.0.1:42063,DS-3b7a3d6b-1910-44d8-8aeb-99a25949f83e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34549,DS-7cda3465-b212-4850-aacb-6598fb922b3b,DISK], DatanodeInfoWithStorage[127.0.0.1:46296,DS-e7b26665-36c0-476b-b298-17a489ee7974,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34549,DS-7cda3465-b212-4850-aacb-6598fb922b3b,DISK], DatanodeInfoWithStorage[127.0.0.1:46296,DS-e7b26665-36c0-476b-b298-17a489ee7974,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34549,DS-7cda3465-b212-4850-aacb-6598fb922b3b,DISK], DatanodeInfoWithStorage[127.0.0.1:46296,DS-e7b26665-36c0-476b-b298-17a489ee7974,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34549,DS-7cda3465-b212-4850-aacb-6598fb922b3b,DISK], DatanodeInfoWithStorage[127.0.0.1:46296,DS-e7b26665-36c0-476b-b298-17a489ee7974,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41101,DS-250a0650-5584-459a-84cc-01197a430d81,DISK], DatanodeInfoWithStorage[127.0.0.1:41351,DS-de73b804-8422-4241-9f3c-426a934e40bc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41101,DS-250a0650-5584-459a-84cc-01197a430d81,DISK], DatanodeInfoWithStorage[127.0.0.1:41351,DS-de73b804-8422-4241-9f3c-426a934e40bc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41101,DS-250a0650-5584-459a-84cc-01197a430d81,DISK], DatanodeInfoWithStorage[127.0.0.1:41351,DS-de73b804-8422-4241-9f3c-426a934e40bc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41101,DS-250a0650-5584-459a-84cc-01197a430d81,DISK], DatanodeInfoWithStorage[127.0.0.1:41351,DS-de73b804-8422-4241-9f3c-426a934e40bc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45765,DS-a3d4dcc2-14d8-4326-a2a8-f89ad99bfe5d,DISK], DatanodeInfoWithStorage[127.0.0.1:45177,DS-f28ae104-23e4-4ba0-b9a8-057b758b33ed,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45177,DS-f28ae104-23e4-4ba0-b9a8-057b758b33ed,DISK], DatanodeInfoWithStorage[127.0.0.1:45765,DS-a3d4dcc2-14d8-4326-a2a8-f89ad99bfe5d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45765,DS-a3d4dcc2-14d8-4326-a2a8-f89ad99bfe5d,DISK], DatanodeInfoWithStorage[127.0.0.1:45177,DS-f28ae104-23e4-4ba0-b9a8-057b758b33ed,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45177,DS-f28ae104-23e4-4ba0-b9a8-057b758b33ed,DISK], DatanodeInfoWithStorage[127.0.0.1:45765,DS-a3d4dcc2-14d8-4326-a2a8-f89ad99bfe5d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42814,DS-f9ed13ac-3a4d-4408-9ff0-cd40a04d49f7,DISK], DatanodeInfoWithStorage[127.0.0.1:33539,DS-439b3718-bd8d-4168-b0ad-f9eff9415d9a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42814,DS-f9ed13ac-3a4d-4408-9ff0-cd40a04d49f7,DISK], DatanodeInfoWithStorage[127.0.0.1:33539,DS-439b3718-bd8d-4168-b0ad-f9eff9415d9a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42814,DS-f9ed13ac-3a4d-4408-9ff0-cd40a04d49f7,DISK], DatanodeInfoWithStorage[127.0.0.1:33539,DS-439b3718-bd8d-4168-b0ad-f9eff9415d9a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42814,DS-f9ed13ac-3a4d-4408-9ff0-cd40a04d49f7,DISK], DatanodeInfoWithStorage[127.0.0.1:33539,DS-439b3718-bd8d-4168-b0ad-f9eff9415d9a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44104,DS-ddb43050-0d61-4c84-a31a-6a3c36ca94de,DISK], DatanodeInfoWithStorage[127.0.0.1:34741,DS-5a680601-5e64-49ac-b65c-e59e54c8841b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44104,DS-ddb43050-0d61-4c84-a31a-6a3c36ca94de,DISK], DatanodeInfoWithStorage[127.0.0.1:34741,DS-5a680601-5e64-49ac-b65c-e59e54c8841b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44104,DS-ddb43050-0d61-4c84-a31a-6a3c36ca94de,DISK], DatanodeInfoWithStorage[127.0.0.1:34741,DS-5a680601-5e64-49ac-b65c-e59e54c8841b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44104,DS-ddb43050-0d61-4c84-a31a-6a3c36ca94de,DISK], DatanodeInfoWithStorage[127.0.0.1:34741,DS-5a680601-5e64-49ac-b65c-e59e54c8841b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45758,DS-7766c061-e6ab-4c06-b598-4e93b9e3feb2,DISK], DatanodeInfoWithStorage[127.0.0.1:38172,DS-1768404a-a911-4acd-a228-1d51f877fb40,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45758,DS-7766c061-e6ab-4c06-b598-4e93b9e3feb2,DISK], DatanodeInfoWithStorage[127.0.0.1:38172,DS-1768404a-a911-4acd-a228-1d51f877fb40,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45758,DS-7766c061-e6ab-4c06-b598-4e93b9e3feb2,DISK], DatanodeInfoWithStorage[127.0.0.1:38172,DS-1768404a-a911-4acd-a228-1d51f877fb40,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45758,DS-7766c061-e6ab-4c06-b598-4e93b9e3feb2,DISK], DatanodeInfoWithStorage[127.0.0.1:38172,DS-1768404a-a911-4acd-a228-1d51f877fb40,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34139,DS-736d86aa-8d64-46fb-a883-3333f32cab38,DISK], DatanodeInfoWithStorage[127.0.0.1:43320,DS-f83bf0fa-b812-47d1-8be3-f37d4fead6ea,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34139,DS-736d86aa-8d64-46fb-a883-3333f32cab38,DISK], DatanodeInfoWithStorage[127.0.0.1:43320,DS-f83bf0fa-b812-47d1-8be3-f37d4fead6ea,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34139,DS-736d86aa-8d64-46fb-a883-3333f32cab38,DISK], DatanodeInfoWithStorage[127.0.0.1:43320,DS-f83bf0fa-b812-47d1-8be3-f37d4fead6ea,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34139,DS-736d86aa-8d64-46fb-a883-3333f32cab38,DISK], DatanodeInfoWithStorage[127.0.0.1:43320,DS-f83bf0fa-b812-47d1-8be3-f37d4fead6ea,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38167,DS-35828d17-ab6a-4367-af48-7f83d1c0ea2c,DISK], DatanodeInfoWithStorage[127.0.0.1:43485,DS-5063e0a2-5d49-41d0-affd-fcb4b94fd041,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43485,DS-5063e0a2-5d49-41d0-affd-fcb4b94fd041,DISK], DatanodeInfoWithStorage[127.0.0.1:38167,DS-35828d17-ab6a-4367-af48-7f83d1c0ea2c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38167,DS-35828d17-ab6a-4367-af48-7f83d1c0ea2c,DISK], DatanodeInfoWithStorage[127.0.0.1:43485,DS-5063e0a2-5d49-41d0-affd-fcb4b94fd041,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43485,DS-5063e0a2-5d49-41d0-affd-fcb4b94fd041,DISK], DatanodeInfoWithStorage[127.0.0.1:38167,DS-35828d17-ab6a-4367-af48-7f83d1c0ea2c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32810,DS-635ae7ad-a76c-4b42-8b9a-1c2ed8287591,DISK], DatanodeInfoWithStorage[127.0.0.1:39312,DS-98a258a1-a423-4ce8-ab77-ac8a63be2d8c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32810,DS-635ae7ad-a76c-4b42-8b9a-1c2ed8287591,DISK], DatanodeInfoWithStorage[127.0.0.1:39312,DS-98a258a1-a423-4ce8-ab77-ac8a63be2d8c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32810,DS-635ae7ad-a76c-4b42-8b9a-1c2ed8287591,DISK], DatanodeInfoWithStorage[127.0.0.1:39312,DS-98a258a1-a423-4ce8-ab77-ac8a63be2d8c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32810,DS-635ae7ad-a76c-4b42-8b9a-1c2ed8287591,DISK], DatanodeInfoWithStorage[127.0.0.1:39312,DS-98a258a1-a423-4ce8-ab77-ac8a63be2d8c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34092,DS-ea3ddf09-6dd7-41cb-a9c3-6edff64eda5a,DISK], DatanodeInfoWithStorage[127.0.0.1:43215,DS-dd3cc187-95cc-4f4d-8ed9-7f1ad9fe3d18,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43215,DS-dd3cc187-95cc-4f4d-8ed9-7f1ad9fe3d18,DISK], DatanodeInfoWithStorage[127.0.0.1:34092,DS-ea3ddf09-6dd7-41cb-a9c3-6edff64eda5a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34092,DS-ea3ddf09-6dd7-41cb-a9c3-6edff64eda5a,DISK], DatanodeInfoWithStorage[127.0.0.1:43215,DS-dd3cc187-95cc-4f4d-8ed9-7f1ad9fe3d18,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43215,DS-dd3cc187-95cc-4f4d-8ed9-7f1ad9fe3d18,DISK], DatanodeInfoWithStorage[127.0.0.1:34092,DS-ea3ddf09-6dd7-41cb-a9c3-6edff64eda5a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46800,DS-391b4fb4-16d0-462e-ae54-2e15c619c8f0,DISK], DatanodeInfoWithStorage[127.0.0.1:38152,DS-88d55deb-5b3f-4207-9457-3226116b8e7c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38152,DS-88d55deb-5b3f-4207-9457-3226116b8e7c,DISK], DatanodeInfoWithStorage[127.0.0.1:46800,DS-391b4fb4-16d0-462e-ae54-2e15c619c8f0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46800,DS-391b4fb4-16d0-462e-ae54-2e15c619c8f0,DISK], DatanodeInfoWithStorage[127.0.0.1:38152,DS-88d55deb-5b3f-4207-9457-3226116b8e7c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38152,DS-88d55deb-5b3f-4207-9457-3226116b8e7c,DISK], DatanodeInfoWithStorage[127.0.0.1:46800,DS-391b4fb4-16d0-462e-ae54-2e15c619c8f0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40130,DS-ecb98e57-ed2f-4cfd-9ac6-2e5f1c9f4684,DISK], DatanodeInfoWithStorage[127.0.0.1:35756,DS-0471c4bf-0121-4b6f-a019-502286f20483,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35756,DS-0471c4bf-0121-4b6f-a019-502286f20483,DISK], DatanodeInfoWithStorage[127.0.0.1:40130,DS-ecb98e57-ed2f-4cfd-9ac6-2e5f1c9f4684,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40130,DS-ecb98e57-ed2f-4cfd-9ac6-2e5f1c9f4684,DISK], DatanodeInfoWithStorage[127.0.0.1:35756,DS-0471c4bf-0121-4b6f-a019-502286f20483,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35756,DS-0471c4bf-0121-4b6f-a019-502286f20483,DISK], DatanodeInfoWithStorage[127.0.0.1:40130,DS-ecb98e57-ed2f-4cfd-9ac6-2e5f1c9f4684,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41170,DS-a9cb1aa1-6cae-4f21-be64-663a5189be57,DISK], DatanodeInfoWithStorage[127.0.0.1:40953,DS-f20c7fa7-c045-483e-8af4-d81fe57cc261,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40953,DS-f20c7fa7-c045-483e-8af4-d81fe57cc261,DISK], DatanodeInfoWithStorage[127.0.0.1:41170,DS-a9cb1aa1-6cae-4f21-be64-663a5189be57,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41170,DS-a9cb1aa1-6cae-4f21-be64-663a5189be57,DISK], DatanodeInfoWithStorage[127.0.0.1:40953,DS-f20c7fa7-c045-483e-8af4-d81fe57cc261,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40953,DS-f20c7fa7-c045-483e-8af4-d81fe57cc261,DISK], DatanodeInfoWithStorage[127.0.0.1:41170,DS-a9cb1aa1-6cae-4f21-be64-663a5189be57,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39026,DS-c9f84e2f-ba39-4e03-9550-22a0fac82981,DISK], DatanodeInfoWithStorage[127.0.0.1:39955,DS-b7c85c8b-38f3-4c83-80f3-7ade51ccf6dd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39026,DS-c9f84e2f-ba39-4e03-9550-22a0fac82981,DISK], DatanodeInfoWithStorage[127.0.0.1:39955,DS-b7c85c8b-38f3-4c83-80f3-7ade51ccf6dd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39026,DS-c9f84e2f-ba39-4e03-9550-22a0fac82981,DISK], DatanodeInfoWithStorage[127.0.0.1:39955,DS-b7c85c8b-38f3-4c83-80f3-7ade51ccf6dd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39026,DS-c9f84e2f-ba39-4e03-9550-22a0fac82981,DISK], DatanodeInfoWithStorage[127.0.0.1:39955,DS-b7c85c8b-38f3-4c83-80f3-7ade51ccf6dd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41289,DS-5c79d975-e7e5-4839-9636-c2d0c84b774a,DISK], DatanodeInfoWithStorage[127.0.0.1:36721,DS-c5f45dfc-d365-44cf-8ab9-f2a03112337f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41289,DS-5c79d975-e7e5-4839-9636-c2d0c84b774a,DISK], DatanodeInfoWithStorage[127.0.0.1:36721,DS-c5f45dfc-d365-44cf-8ab9-f2a03112337f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41289,DS-5c79d975-e7e5-4839-9636-c2d0c84b774a,DISK], DatanodeInfoWithStorage[127.0.0.1:36721,DS-c5f45dfc-d365-44cf-8ab9-f2a03112337f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41289,DS-5c79d975-e7e5-4839-9636-c2d0c84b774a,DISK], DatanodeInfoWithStorage[127.0.0.1:36721,DS-c5f45dfc-d365-44cf-8ab9-f2a03112337f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33304,DS-f2510956-5f42-4488-89b5-b6911786dc6c,DISK], DatanodeInfoWithStorage[127.0.0.1:41843,DS-14b3d743-554b-4562-98ac-fb0a443b7836,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41843,DS-14b3d743-554b-4562-98ac-fb0a443b7836,DISK], DatanodeInfoWithStorage[127.0.0.1:33304,DS-f2510956-5f42-4488-89b5-b6911786dc6c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33304,DS-f2510956-5f42-4488-89b5-b6911786dc6c,DISK], DatanodeInfoWithStorage[127.0.0.1:41843,DS-14b3d743-554b-4562-98ac-fb0a443b7836,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41843,DS-14b3d743-554b-4562-98ac-fb0a443b7836,DISK], DatanodeInfoWithStorage[127.0.0.1:33304,DS-f2510956-5f42-4488-89b5-b6911786dc6c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40803,DS-ffd4b5e4-24b9-4b00-932e-bfb2e31f4ee6,DISK], DatanodeInfoWithStorage[127.0.0.1:40613,DS-4e8812eb-778f-446a-be19-c984fd6ca515,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40803,DS-ffd4b5e4-24b9-4b00-932e-bfb2e31f4ee6,DISK], DatanodeInfoWithStorage[127.0.0.1:40613,DS-4e8812eb-778f-446a-be19-c984fd6ca515,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40803,DS-ffd4b5e4-24b9-4b00-932e-bfb2e31f4ee6,DISK], DatanodeInfoWithStorage[127.0.0.1:40613,DS-4e8812eb-778f-446a-be19-c984fd6ca515,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40803,DS-ffd4b5e4-24b9-4b00-932e-bfb2e31f4ee6,DISK], DatanodeInfoWithStorage[127.0.0.1:40613,DS-4e8812eb-778f-446a-be19-c984fd6ca515,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39598,DS-d7656eed-d157-40de-b617-345c00e39084,DISK], DatanodeInfoWithStorage[127.0.0.1:40980,DS-82109c03-d3b2-4550-b45b-7f48e2eaaa84,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39598,DS-d7656eed-d157-40de-b617-345c00e39084,DISK], DatanodeInfoWithStorage[127.0.0.1:40980,DS-82109c03-d3b2-4550-b45b-7f48e2eaaa84,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39598,DS-d7656eed-d157-40de-b617-345c00e39084,DISK], DatanodeInfoWithStorage[127.0.0.1:40980,DS-82109c03-d3b2-4550-b45b-7f48e2eaaa84,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39598,DS-d7656eed-d157-40de-b617-345c00e39084,DISK], DatanodeInfoWithStorage[127.0.0.1:40980,DS-82109c03-d3b2-4550-b45b-7f48e2eaaa84,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38250,DS-2e42ea98-469f-49cd-bf7c-2868ea0857c8,DISK], DatanodeInfoWithStorage[127.0.0.1:39982,DS-a4369433-179d-40ac-beea-e512c97af3d0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39982,DS-a4369433-179d-40ac-beea-e512c97af3d0,DISK], DatanodeInfoWithStorage[127.0.0.1:38250,DS-2e42ea98-469f-49cd-bf7c-2868ea0857c8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38250,DS-2e42ea98-469f-49cd-bf7c-2868ea0857c8,DISK], DatanodeInfoWithStorage[127.0.0.1:39982,DS-a4369433-179d-40ac-beea-e512c97af3d0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39982,DS-a4369433-179d-40ac-beea-e512c97af3d0,DISK], DatanodeInfoWithStorage[127.0.0.1:38250,DS-2e42ea98-469f-49cd-bf7c-2868ea0857c8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37539,DS-7379a96d-cc65-4c11-b173-d2fabc620f1c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37539,DS-7379a96d-cc65-4c11-b173-d2fabc620f1c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37539,DS-7379a96d-cc65-4c11-b173-d2fabc620f1c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37539,DS-7379a96d-cc65-4c11-b173-d2fabc620f1c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43952,DS-f8e56b87-6338-4be8-b359-ebe9dd8e19b8,DISK], DatanodeInfoWithStorage[127.0.0.1:36311,DS-89a6cb3d-748e-4509-9a97-26987237e96e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36311,DS-89a6cb3d-748e-4509-9a97-26987237e96e,DISK], DatanodeInfoWithStorage[127.0.0.1:43952,DS-f8e56b87-6338-4be8-b359-ebe9dd8e19b8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43952,DS-f8e56b87-6338-4be8-b359-ebe9dd8e19b8,DISK], DatanodeInfoWithStorage[127.0.0.1:36311,DS-89a6cb3d-748e-4509-9a97-26987237e96e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36311,DS-89a6cb3d-748e-4509-9a97-26987237e96e,DISK], DatanodeInfoWithStorage[127.0.0.1:43952,DS-f8e56b87-6338-4be8-b359-ebe9dd8e19b8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42301,DS-013ed4cb-a144-4b92-9f38-dc3fb3fff6f1,DISK], DatanodeInfoWithStorage[127.0.0.1:33790,DS-4da4358d-0d50-4bc8-b796-1a0968d04289,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42301,DS-013ed4cb-a144-4b92-9f38-dc3fb3fff6f1,DISK], DatanodeInfoWithStorage[127.0.0.1:33790,DS-4da4358d-0d50-4bc8-b796-1a0968d04289,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42301,DS-013ed4cb-a144-4b92-9f38-dc3fb3fff6f1,DISK], DatanodeInfoWithStorage[127.0.0.1:33790,DS-4da4358d-0d50-4bc8-b796-1a0968d04289,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42301,DS-013ed4cb-a144-4b92-9f38-dc3fb3fff6f1,DISK], DatanodeInfoWithStorage[127.0.0.1:33790,DS-4da4358d-0d50-4bc8-b796-1a0968d04289,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33088,DS-f9aca7c7-678b-4f90-bbb5-9443899d9443,DISK], DatanodeInfoWithStorage[127.0.0.1:44698,DS-15c2bf14-28b8-42a9-8442-baeb6bc19304,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33088,DS-f9aca7c7-678b-4f90-bbb5-9443899d9443,DISK], DatanodeInfoWithStorage[127.0.0.1:44698,DS-15c2bf14-28b8-42a9-8442-baeb6bc19304,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33088,DS-f9aca7c7-678b-4f90-bbb5-9443899d9443,DISK], DatanodeInfoWithStorage[127.0.0.1:44698,DS-15c2bf14-28b8-42a9-8442-baeb6bc19304,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33088,DS-f9aca7c7-678b-4f90-bbb5-9443899d9443,DISK], DatanodeInfoWithStorage[127.0.0.1:44698,DS-15c2bf14-28b8-42a9-8442-baeb6bc19304,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.encrypt.data.transfer
component: hdfs:DataNode
v1: true
v2: false
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestEncryptedTransfer#testEncryptedAppend[0]
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41499,DS-1e889b68-a408-49fb-87be-fd32853c4d62,DISK], DatanodeInfoWithStorage[127.0.0.1:41138,DS-a9909bb4-999c-4c60-80a1-98f74b0abc4e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41138,DS-a9909bb4-999c-4c60-80a1-98f74b0abc4e,DISK], DatanodeInfoWithStorage[127.0.0.1:41499,DS-1e889b68-a408-49fb-87be-fd32853c4d62,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41499,DS-1e889b68-a408-49fb-87be-fd32853c4d62,DISK], DatanodeInfoWithStorage[127.0.0.1:41138,DS-a9909bb4-999c-4c60-80a1-98f74b0abc4e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41138,DS-a9909bb4-999c-4c60-80a1-98f74b0abc4e,DISK], DatanodeInfoWithStorage[127.0.0.1:41499,DS-1e889b68-a408-49fb-87be-fd32853c4d62,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 50 out of 50
v1v1v2v2 failed with probability 4 out of 50
result: might be true error
Total execution time in seconds : 3743

reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34007,DS-b5db8a1a-7652-4416-a1d5-a07bf3529fb5,DISK], DatanodeInfoWithStorage[127.0.0.1:35944,DS-a4bf136e-99aa-42b9-80a5-f1542a9bc949,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34007,DS-b5db8a1a-7652-4416-a1d5-a07bf3529fb5,DISK], DatanodeInfoWithStorage[127.0.0.1:35944,DS-a4bf136e-99aa-42b9-80a5-f1542a9bc949,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34007,DS-b5db8a1a-7652-4416-a1d5-a07bf3529fb5,DISK], DatanodeInfoWithStorage[127.0.0.1:35944,DS-a4bf136e-99aa-42b9-80a5-f1542a9bc949,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34007,DS-b5db8a1a-7652-4416-a1d5-a07bf3529fb5,DISK], DatanodeInfoWithStorage[127.0.0.1:35944,DS-a4bf136e-99aa-42b9-80a5-f1542a9bc949,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33680,DS-b149d373-fbf7-4e65-b203-30f49bed3129,DISK], DatanodeInfoWithStorage[127.0.0.1:43336,DS-11b8ebfe-06f7-4901-bd0d-8d59bd7c094c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43336,DS-11b8ebfe-06f7-4901-bd0d-8d59bd7c094c,DISK], DatanodeInfoWithStorage[127.0.0.1:33680,DS-b149d373-fbf7-4e65-b203-30f49bed3129,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33680,DS-b149d373-fbf7-4e65-b203-30f49bed3129,DISK], DatanodeInfoWithStorage[127.0.0.1:43336,DS-11b8ebfe-06f7-4901-bd0d-8d59bd7c094c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43336,DS-11b8ebfe-06f7-4901-bd0d-8d59bd7c094c,DISK], DatanodeInfoWithStorage[127.0.0.1:33680,DS-b149d373-fbf7-4e65-b203-30f49bed3129,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46192,DS-cf8f0d1c-31bf-4677-b9d4-81d39d60fb3f,DISK], DatanodeInfoWithStorage[127.0.0.1:40171,DS-ab33b724-653b-4d92-8eb9-144b41533b0e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46192,DS-cf8f0d1c-31bf-4677-b9d4-81d39d60fb3f,DISK], DatanodeInfoWithStorage[127.0.0.1:40171,DS-ab33b724-653b-4d92-8eb9-144b41533b0e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46192,DS-cf8f0d1c-31bf-4677-b9d4-81d39d60fb3f,DISK], DatanodeInfoWithStorage[127.0.0.1:40171,DS-ab33b724-653b-4d92-8eb9-144b41533b0e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46192,DS-cf8f0d1c-31bf-4677-b9d4-81d39d60fb3f,DISK], DatanodeInfoWithStorage[127.0.0.1:40171,DS-ab33b724-653b-4d92-8eb9-144b41533b0e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44542,DS-34591daa-e8dd-4ec8-a47b-2a437ac31cff,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44542,DS-34591daa-e8dd-4ec8-a47b-2a437ac31cff,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44542,DS-34591daa-e8dd-4ec8-a47b-2a437ac31cff,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44542,DS-34591daa-e8dd-4ec8-a47b-2a437ac31cff,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37417,DS-c11996b6-d0e2-4ff9-a211-af30fb2e120a,DISK], DatanodeInfoWithStorage[127.0.0.1:40030,DS-3d4dcbef-6f3b-45f0-aa4a-e653b57a25ac,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37417,DS-c11996b6-d0e2-4ff9-a211-af30fb2e120a,DISK], DatanodeInfoWithStorage[127.0.0.1:40030,DS-3d4dcbef-6f3b-45f0-aa4a-e653b57a25ac,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37417,DS-c11996b6-d0e2-4ff9-a211-af30fb2e120a,DISK], DatanodeInfoWithStorage[127.0.0.1:40030,DS-3d4dcbef-6f3b-45f0-aa4a-e653b57a25ac,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37417,DS-c11996b6-d0e2-4ff9-a211-af30fb2e120a,DISK], DatanodeInfoWithStorage[127.0.0.1:40030,DS-3d4dcbef-6f3b-45f0-aa4a-e653b57a25ac,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41079,DS-45c8a32b-8a58-4870-a88e-55bbcd00e471,DISK], DatanodeInfoWithStorage[127.0.0.1:39222,DS-65d2720e-c1d8-44ed-a4bf-922bc330e903,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41079,DS-45c8a32b-8a58-4870-a88e-55bbcd00e471,DISK], DatanodeInfoWithStorage[127.0.0.1:39222,DS-65d2720e-c1d8-44ed-a4bf-922bc330e903,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41079,DS-45c8a32b-8a58-4870-a88e-55bbcd00e471,DISK], DatanodeInfoWithStorage[127.0.0.1:39222,DS-65d2720e-c1d8-44ed-a4bf-922bc330e903,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41079,DS-45c8a32b-8a58-4870-a88e-55bbcd00e471,DISK], DatanodeInfoWithStorage[127.0.0.1:39222,DS-65d2720e-c1d8-44ed-a4bf-922bc330e903,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35663,DS-7b8fcc37-48f0-4882-ace4-6a60d5d72784,DISK], DatanodeInfoWithStorage[127.0.0.1:45802,DS-d80201db-bc3e-4e8a-925e-9f44233342f4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35663,DS-7b8fcc37-48f0-4882-ace4-6a60d5d72784,DISK], DatanodeInfoWithStorage[127.0.0.1:45802,DS-d80201db-bc3e-4e8a-925e-9f44233342f4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35663,DS-7b8fcc37-48f0-4882-ace4-6a60d5d72784,DISK], DatanodeInfoWithStorage[127.0.0.1:45802,DS-d80201db-bc3e-4e8a-925e-9f44233342f4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35663,DS-7b8fcc37-48f0-4882-ace4-6a60d5d72784,DISK], DatanodeInfoWithStorage[127.0.0.1:45802,DS-d80201db-bc3e-4e8a-925e-9f44233342f4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41974,DS-cc9f38db-00eb-4429-8e52-f5572f0fcc64,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41974,DS-cc9f38db-00eb-4429-8e52-f5572f0fcc64,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41974,DS-cc9f38db-00eb-4429-8e52-f5572f0fcc64,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41974,DS-cc9f38db-00eb-4429-8e52-f5572f0fcc64,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34757,DS-3313316e-5012-4eae-84fd-0143dfc98fb5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34757,DS-3313316e-5012-4eae-84fd-0143dfc98fb5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34757,DS-3313316e-5012-4eae-84fd-0143dfc98fb5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34757,DS-3313316e-5012-4eae-84fd-0143dfc98fb5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41783,DS-3d030b28-80a3-4478-9e0b-090103a1fefe,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41783,DS-3d030b28-80a3-4478-9e0b-090103a1fefe,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41783,DS-3d030b28-80a3-4478-9e0b-090103a1fefe,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41783,DS-3d030b28-80a3-4478-9e0b-090103a1fefe,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 630

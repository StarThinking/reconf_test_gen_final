reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -3
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43088,DS-4b1c61ef-9f98-4927-b88e-a69690b11fdc,DISK], DatanodeInfoWithStorage[127.0.0.1:35741,DS-5f0362e5-6bbc-4d90-aa95-9a806f0c892a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43088,DS-4b1c61ef-9f98-4927-b88e-a69690b11fdc,DISK], DatanodeInfoWithStorage[127.0.0.1:35741,DS-5f0362e5-6bbc-4d90-aa95-9a806f0c892a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43088,DS-4b1c61ef-9f98-4927-b88e-a69690b11fdc,DISK], DatanodeInfoWithStorage[127.0.0.1:35741,DS-5f0362e5-6bbc-4d90-aa95-9a806f0c892a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43088,DS-4b1c61ef-9f98-4927-b88e-a69690b11fdc,DISK], DatanodeInfoWithStorage[127.0.0.1:35741,DS-5f0362e5-6bbc-4d90-aa95-9a806f0c892a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38664,DS-3819c283-fc97-4afd-9df8-4b33024ea31e,DISK], DatanodeInfoWithStorage[127.0.0.1:34602,DS-3d6e312a-c654-4cf0-a722-30fba0162287,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34602,DS-3d6e312a-c654-4cf0-a722-30fba0162287,DISK], DatanodeInfoWithStorage[127.0.0.1:38664,DS-3819c283-fc97-4afd-9df8-4b33024ea31e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38664,DS-3819c283-fc97-4afd-9df8-4b33024ea31e,DISK], DatanodeInfoWithStorage[127.0.0.1:34602,DS-3d6e312a-c654-4cf0-a722-30fba0162287,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34602,DS-3d6e312a-c654-4cf0-a722-30fba0162287,DISK], DatanodeInfoWithStorage[127.0.0.1:38664,DS-3819c283-fc97-4afd-9df8-4b33024ea31e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38294,DS-4c0b84d7-e905-404a-8e3b-174b13138726,DISK], DatanodeInfoWithStorage[127.0.0.1:36957,DS-c3dc5d55-0e8b-4389-a0ad-4d08630fd266,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38294,DS-4c0b84d7-e905-404a-8e3b-174b13138726,DISK], DatanodeInfoWithStorage[127.0.0.1:36957,DS-c3dc5d55-0e8b-4389-a0ad-4d08630fd266,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38294,DS-4c0b84d7-e905-404a-8e3b-174b13138726,DISK], DatanodeInfoWithStorage[127.0.0.1:36957,DS-c3dc5d55-0e8b-4389-a0ad-4d08630fd266,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38294,DS-4c0b84d7-e905-404a-8e3b-174b13138726,DISK], DatanodeInfoWithStorage[127.0.0.1:36957,DS-c3dc5d55-0e8b-4389-a0ad-4d08630fd266,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38947,DS-e428faa4-d76e-45ad-9f4a-1e340faf23ce,DISK], DatanodeInfoWithStorage[127.0.0.1:41280,DS-f00482cb-1102-4e29-9eca-97643c51598a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38947,DS-e428faa4-d76e-45ad-9f4a-1e340faf23ce,DISK], DatanodeInfoWithStorage[127.0.0.1:41280,DS-f00482cb-1102-4e29-9eca-97643c51598a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38947,DS-e428faa4-d76e-45ad-9f4a-1e340faf23ce,DISK], DatanodeInfoWithStorage[127.0.0.1:41280,DS-f00482cb-1102-4e29-9eca-97643c51598a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38947,DS-e428faa4-d76e-45ad-9f4a-1e340faf23ce,DISK], DatanodeInfoWithStorage[127.0.0.1:41280,DS-f00482cb-1102-4e29-9eca-97643c51598a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43610,DS-8db169ff-ebd4-4814-8765-8fdfcbe93c62,DISK], DatanodeInfoWithStorage[127.0.0.1:39826,DS-09fbc74e-58ac-4700-af76-88145af86391,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43610,DS-8db169ff-ebd4-4814-8765-8fdfcbe93c62,DISK], DatanodeInfoWithStorage[127.0.0.1:39826,DS-09fbc74e-58ac-4700-af76-88145af86391,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43610,DS-8db169ff-ebd4-4814-8765-8fdfcbe93c62,DISK], DatanodeInfoWithStorage[127.0.0.1:39826,DS-09fbc74e-58ac-4700-af76-88145af86391,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43610,DS-8db169ff-ebd4-4814-8765-8fdfcbe93c62,DISK], DatanodeInfoWithStorage[127.0.0.1:39826,DS-09fbc74e-58ac-4700-af76-88145af86391,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39979,DS-18624eb8-1391-4fd0-9da8-5778597da87d,DISK], DatanodeInfoWithStorage[127.0.0.1:39970,DS-d12a0296-b4c3-490d-8104-e4c5b3d55ef3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39970,DS-d12a0296-b4c3-490d-8104-e4c5b3d55ef3,DISK], DatanodeInfoWithStorage[127.0.0.1:39979,DS-18624eb8-1391-4fd0-9da8-5778597da87d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39979,DS-18624eb8-1391-4fd0-9da8-5778597da87d,DISK], DatanodeInfoWithStorage[127.0.0.1:39970,DS-d12a0296-b4c3-490d-8104-e4c5b3d55ef3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39970,DS-d12a0296-b4c3-490d-8104-e4c5b3d55ef3,DISK], DatanodeInfoWithStorage[127.0.0.1:39979,DS-18624eb8-1391-4fd0-9da8-5778597da87d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35662,DS-a083d35f-dec6-46a3-b70c-a538b15be196,DISK], DatanodeInfoWithStorage[127.0.0.1:46821,DS-202f8a39-daee-4592-b3d5-e050ac440a7a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35662,DS-a083d35f-dec6-46a3-b70c-a538b15be196,DISK], DatanodeInfoWithStorage[127.0.0.1:46821,DS-202f8a39-daee-4592-b3d5-e050ac440a7a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35662,DS-a083d35f-dec6-46a3-b70c-a538b15be196,DISK], DatanodeInfoWithStorage[127.0.0.1:46821,DS-202f8a39-daee-4592-b3d5-e050ac440a7a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35662,DS-a083d35f-dec6-46a3-b70c-a538b15be196,DISK], DatanodeInfoWithStorage[127.0.0.1:46821,DS-202f8a39-daee-4592-b3d5-e050ac440a7a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39587,DS-e3986171-7475-4966-a5f2-208623e87285,DISK], DatanodeInfoWithStorage[127.0.0.1:44339,DS-c2a6bd57-449b-479b-9b9d-07d370229628,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39587,DS-e3986171-7475-4966-a5f2-208623e87285,DISK], DatanodeInfoWithStorage[127.0.0.1:44339,DS-c2a6bd57-449b-479b-9b9d-07d370229628,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39587,DS-e3986171-7475-4966-a5f2-208623e87285,DISK], DatanodeInfoWithStorage[127.0.0.1:44339,DS-c2a6bd57-449b-479b-9b9d-07d370229628,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39587,DS-e3986171-7475-4966-a5f2-208623e87285,DISK], DatanodeInfoWithStorage[127.0.0.1:44339,DS-c2a6bd57-449b-479b-9b9d-07d370229628,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42552,DS-b4ed7035-62ea-4831-ac90-f5426a8cf7f1,DISK], DatanodeInfoWithStorage[127.0.0.1:39617,DS-b48b46ce-33f5-4f4c-a579-2f68f748f75e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42552,DS-b4ed7035-62ea-4831-ac90-f5426a8cf7f1,DISK], DatanodeInfoWithStorage[127.0.0.1:39617,DS-b48b46ce-33f5-4f4c-a579-2f68f748f75e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42552,DS-b4ed7035-62ea-4831-ac90-f5426a8cf7f1,DISK], DatanodeInfoWithStorage[127.0.0.1:39617,DS-b48b46ce-33f5-4f4c-a579-2f68f748f75e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42552,DS-b4ed7035-62ea-4831-ac90-f5426a8cf7f1,DISK], DatanodeInfoWithStorage[127.0.0.1:39617,DS-b48b46ce-33f5-4f4c-a579-2f68f748f75e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34415,DS-fec4e242-ac1d-439a-8adc-e492537709db,DISK], DatanodeInfoWithStorage[127.0.0.1:32882,DS-03795197-efc1-4258-a74b-18eb46a84eab,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32882,DS-03795197-efc1-4258-a74b-18eb46a84eab,DISK], DatanodeInfoWithStorage[127.0.0.1:34415,DS-fec4e242-ac1d-439a-8adc-e492537709db,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34415,DS-fec4e242-ac1d-439a-8adc-e492537709db,DISK], DatanodeInfoWithStorage[127.0.0.1:32882,DS-03795197-efc1-4258-a74b-18eb46a84eab,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32882,DS-03795197-efc1-4258-a74b-18eb46a84eab,DISK], DatanodeInfoWithStorage[127.0.0.1:34415,DS-fec4e242-ac1d-439a-8adc-e492537709db,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 637

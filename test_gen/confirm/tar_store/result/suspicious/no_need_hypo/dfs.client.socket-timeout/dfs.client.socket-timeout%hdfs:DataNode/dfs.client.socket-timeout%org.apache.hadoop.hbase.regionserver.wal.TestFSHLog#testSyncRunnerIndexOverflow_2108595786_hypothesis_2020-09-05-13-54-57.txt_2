reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncRunnerIndexOverflow
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncRunnerIndexOverflow
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37926,DS-ccf5015c-b3ec-43fa-a90b-cc95385c256c,DISK], DatanodeInfoWithStorage[127.0.0.1:46480,DS-1406030e-b7d3-471e-ab04-a3b81ba077da,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46480,DS-1406030e-b7d3-471e-ab04-a3b81ba077da,DISK], DatanodeInfoWithStorage[127.0.0.1:37926,DS-ccf5015c-b3ec-43fa-a90b-cc95385c256c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37926,DS-ccf5015c-b3ec-43fa-a90b-cc95385c256c,DISK], DatanodeInfoWithStorage[127.0.0.1:46480,DS-1406030e-b7d3-471e-ab04-a3b81ba077da,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46480,DS-1406030e-b7d3-471e-ab04-a3b81ba077da,DISK], DatanodeInfoWithStorage[127.0.0.1:37926,DS-ccf5015c-b3ec-43fa-a90b-cc95385c256c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncRunnerIndexOverflow
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41519,DS-f18573c0-30b4-4b40-9960-ad3311f75971,DISK], DatanodeInfoWithStorage[127.0.0.1:33722,DS-f1ae84a7-19dc-45cd-803b-f6ae3f351b81,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41519,DS-f18573c0-30b4-4b40-9960-ad3311f75971,DISK], DatanodeInfoWithStorage[127.0.0.1:33722,DS-f1ae84a7-19dc-45cd-803b-f6ae3f351b81,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41519,DS-f18573c0-30b4-4b40-9960-ad3311f75971,DISK], DatanodeInfoWithStorage[127.0.0.1:33722,DS-f1ae84a7-19dc-45cd-803b-f6ae3f351b81,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41519,DS-f18573c0-30b4-4b40-9960-ad3311f75971,DISK], DatanodeInfoWithStorage[127.0.0.1:33722,DS-f1ae84a7-19dc-45cd-803b-f6ae3f351b81,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncRunnerIndexOverflow
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43291,DS-0636cd2e-0b92-4ea7-a80f-3540219eb7ca,DISK], DatanodeInfoWithStorage[127.0.0.1:38154,DS-633ed5ec-af49-403e-ae3d-4033a76217ab,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43291,DS-0636cd2e-0b92-4ea7-a80f-3540219eb7ca,DISK], DatanodeInfoWithStorage[127.0.0.1:38154,DS-633ed5ec-af49-403e-ae3d-4033a76217ab,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43291,DS-0636cd2e-0b92-4ea7-a80f-3540219eb7ca,DISK], DatanodeInfoWithStorage[127.0.0.1:38154,DS-633ed5ec-af49-403e-ae3d-4033a76217ab,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43291,DS-0636cd2e-0b92-4ea7-a80f-3540219eb7ca,DISK], DatanodeInfoWithStorage[127.0.0.1:38154,DS-633ed5ec-af49-403e-ae3d-4033a76217ab,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncRunnerIndexOverflow
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38871,DS-f063cc4a-6419-4535-beb1-28ef3f257d82,DISK], DatanodeInfoWithStorage[127.0.0.1:45463,DS-cadcc1fc-3849-4ee0-9819-df170160572b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38871,DS-f063cc4a-6419-4535-beb1-28ef3f257d82,DISK], DatanodeInfoWithStorage[127.0.0.1:45463,DS-cadcc1fc-3849-4ee0-9819-df170160572b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38871,DS-f063cc4a-6419-4535-beb1-28ef3f257d82,DISK], DatanodeInfoWithStorage[127.0.0.1:45463,DS-cadcc1fc-3849-4ee0-9819-df170160572b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38871,DS-f063cc4a-6419-4535-beb1-28ef3f257d82,DISK], DatanodeInfoWithStorage[127.0.0.1:45463,DS-cadcc1fc-3849-4ee0-9819-df170160572b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncRunnerIndexOverflow
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39878,DS-87fd0f31-3c7a-4074-a90d-485815f13158,DISK], DatanodeInfoWithStorage[127.0.0.1:41756,DS-9cd5d0d9-0128-4d5c-a03a-6e7bf6e6640f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39878,DS-87fd0f31-3c7a-4074-a90d-485815f13158,DISK], DatanodeInfoWithStorage[127.0.0.1:41756,DS-9cd5d0d9-0128-4d5c-a03a-6e7bf6e6640f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39878,DS-87fd0f31-3c7a-4074-a90d-485815f13158,DISK], DatanodeInfoWithStorage[127.0.0.1:41756,DS-9cd5d0d9-0128-4d5c-a03a-6e7bf6e6640f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39878,DS-87fd0f31-3c7a-4074-a90d-485815f13158,DISK], DatanodeInfoWithStorage[127.0.0.1:41756,DS-9cd5d0d9-0128-4d5c-a03a-6e7bf6e6640f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncRunnerIndexOverflow
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40913,DS-f1872fb0-2a3d-477d-8dc2-e7e7532e5bfe,DISK], DatanodeInfoWithStorage[127.0.0.1:37268,DS-5b90af19-a5c6-4dbc-88eb-62a929c56c05,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40913,DS-f1872fb0-2a3d-477d-8dc2-e7e7532e5bfe,DISK], DatanodeInfoWithStorage[127.0.0.1:37268,DS-5b90af19-a5c6-4dbc-88eb-62a929c56c05,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40913,DS-f1872fb0-2a3d-477d-8dc2-e7e7532e5bfe,DISK], DatanodeInfoWithStorage[127.0.0.1:37268,DS-5b90af19-a5c6-4dbc-88eb-62a929c56c05,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40913,DS-f1872fb0-2a3d-477d-8dc2-e7e7532e5bfe,DISK], DatanodeInfoWithStorage[127.0.0.1:37268,DS-5b90af19-a5c6-4dbc-88eb-62a929c56c05,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncRunnerIndexOverflow
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46556,DS-b03762b6-58ad-47cf-a720-364311dbea8e,DISK], DatanodeInfoWithStorage[127.0.0.1:42473,DS-8e6d89d8-d905-43a8-b0c1-88adbff82e6c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42473,DS-8e6d89d8-d905-43a8-b0c1-88adbff82e6c,DISK], DatanodeInfoWithStorage[127.0.0.1:46556,DS-b03762b6-58ad-47cf-a720-364311dbea8e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46556,DS-b03762b6-58ad-47cf-a720-364311dbea8e,DISK], DatanodeInfoWithStorage[127.0.0.1:42473,DS-8e6d89d8-d905-43a8-b0c1-88adbff82e6c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42473,DS-8e6d89d8-d905-43a8-b0c1-88adbff82e6c,DISK], DatanodeInfoWithStorage[127.0.0.1:46556,DS-b03762b6-58ad-47cf-a720-364311dbea8e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncRunnerIndexOverflow
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44989,DS-7076d960-d587-4c90-9bec-2b035f8159bc,DISK], DatanodeInfoWithStorage[127.0.0.1:41110,DS-700dd59d-6f55-4bdb-9a72-b053337b0541,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44989,DS-7076d960-d587-4c90-9bec-2b035f8159bc,DISK], DatanodeInfoWithStorage[127.0.0.1:41110,DS-700dd59d-6f55-4bdb-9a72-b053337b0541,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44989,DS-7076d960-d587-4c90-9bec-2b035f8159bc,DISK], DatanodeInfoWithStorage[127.0.0.1:41110,DS-700dd59d-6f55-4bdb-9a72-b053337b0541,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44989,DS-7076d960-d587-4c90-9bec-2b035f8159bc,DISK], DatanodeInfoWithStorage[127.0.0.1:41110,DS-700dd59d-6f55-4bdb-9a72-b053337b0541,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncRunnerIndexOverflow
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38657,DS-063400d4-5224-4388-b7fd-2e4062eb42b6,DISK], DatanodeInfoWithStorage[127.0.0.1:40324,DS-1f6aebf3-8f39-4e24-9d73-f77370301bee,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38657,DS-063400d4-5224-4388-b7fd-2e4062eb42b6,DISK], DatanodeInfoWithStorage[127.0.0.1:40324,DS-1f6aebf3-8f39-4e24-9d73-f77370301bee,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38657,DS-063400d4-5224-4388-b7fd-2e4062eb42b6,DISK], DatanodeInfoWithStorage[127.0.0.1:40324,DS-1f6aebf3-8f39-4e24-9d73-f77370301bee,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38657,DS-063400d4-5224-4388-b7fd-2e4062eb42b6,DISK], DatanodeInfoWithStorage[127.0.0.1:40324,DS-1f6aebf3-8f39-4e24-9d73-f77370301bee,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncRunnerIndexOverflow
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40626,DS-dfc3e2de-9b96-453e-9e99-fdef59253a82,DISK], DatanodeInfoWithStorage[127.0.0.1:44903,DS-e67be674-5833-4d1b-8972-6938c6d5c392,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44903,DS-e67be674-5833-4d1b-8972-6938c6d5c392,DISK], DatanodeInfoWithStorage[127.0.0.1:40626,DS-dfc3e2de-9b96-453e-9e99-fdef59253a82,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40626,DS-dfc3e2de-9b96-453e-9e99-fdef59253a82,DISK], DatanodeInfoWithStorage[127.0.0.1:44903,DS-e67be674-5833-4d1b-8972-6938c6d5c392,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44903,DS-e67be674-5833-4d1b-8972-6938c6d5c392,DISK], DatanodeInfoWithStorage[127.0.0.1:40626,DS-dfc3e2de-9b96-453e-9e99-fdef59253a82,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 1311

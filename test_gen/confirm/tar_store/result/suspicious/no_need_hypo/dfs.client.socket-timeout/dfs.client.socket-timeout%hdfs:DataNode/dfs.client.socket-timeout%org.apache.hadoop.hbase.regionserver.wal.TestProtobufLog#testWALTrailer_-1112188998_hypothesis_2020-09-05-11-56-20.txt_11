reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestProtobufLog#testWALTrailer
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestProtobufLog#testWALTrailer
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44527,DS-f33d9afb-5cff-4067-bc28-eedd06c3f0d6,DISK], DatanodeInfoWithStorage[127.0.0.1:34508,DS-ccaea4e6-f508-4831-9af0-9d735bf35ba2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34508,DS-ccaea4e6-f508-4831-9af0-9d735bf35ba2,DISK], DatanodeInfoWithStorage[127.0.0.1:44527,DS-f33d9afb-5cff-4067-bc28-eedd06c3f0d6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44527,DS-f33d9afb-5cff-4067-bc28-eedd06c3f0d6,DISK], DatanodeInfoWithStorage[127.0.0.1:34508,DS-ccaea4e6-f508-4831-9af0-9d735bf35ba2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34508,DS-ccaea4e6-f508-4831-9af0-9d735bf35ba2,DISK], DatanodeInfoWithStorage[127.0.0.1:44527,DS-f33d9afb-5cff-4067-bc28-eedd06c3f0d6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestProtobufLog#testWALTrailer
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40792,DS-92c44ef8-ec46-47ec-a341-86a6428c9b4a,DISK], DatanodeInfoWithStorage[127.0.0.1:46043,DS-8c07c91b-0bad-482a-8963-ef577bd9c0bf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40792,DS-92c44ef8-ec46-47ec-a341-86a6428c9b4a,DISK], DatanodeInfoWithStorage[127.0.0.1:46043,DS-8c07c91b-0bad-482a-8963-ef577bd9c0bf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40792,DS-92c44ef8-ec46-47ec-a341-86a6428c9b4a,DISK], DatanodeInfoWithStorage[127.0.0.1:46043,DS-8c07c91b-0bad-482a-8963-ef577bd9c0bf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40792,DS-92c44ef8-ec46-47ec-a341-86a6428c9b4a,DISK], DatanodeInfoWithStorage[127.0.0.1:46043,DS-8c07c91b-0bad-482a-8963-ef577bd9c0bf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestProtobufLog#testWALTrailer
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45316,DS-8e99a87e-f797-4b46-9f23-4cfcaf38b417,DISK], DatanodeInfoWithStorage[127.0.0.1:37167,DS-d4ad862b-6b1d-420e-b2e1-b6f8dfae38a6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45316,DS-8e99a87e-f797-4b46-9f23-4cfcaf38b417,DISK], DatanodeInfoWithStorage[127.0.0.1:37167,DS-d4ad862b-6b1d-420e-b2e1-b6f8dfae38a6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45316,DS-8e99a87e-f797-4b46-9f23-4cfcaf38b417,DISK], DatanodeInfoWithStorage[127.0.0.1:37167,DS-d4ad862b-6b1d-420e-b2e1-b6f8dfae38a6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45316,DS-8e99a87e-f797-4b46-9f23-4cfcaf38b417,DISK], DatanodeInfoWithStorage[127.0.0.1:37167,DS-d4ad862b-6b1d-420e-b2e1-b6f8dfae38a6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestProtobufLog#testWALTrailer
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42576,DS-121e542f-eb1d-4200-a53e-bdcfc85f186a,DISK], DatanodeInfoWithStorage[127.0.0.1:41331,DS-24a8d27d-81d1-46e1-bbbb-01a73a8b55e5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42576,DS-121e542f-eb1d-4200-a53e-bdcfc85f186a,DISK], DatanodeInfoWithStorage[127.0.0.1:41331,DS-24a8d27d-81d1-46e1-bbbb-01a73a8b55e5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42576,DS-121e542f-eb1d-4200-a53e-bdcfc85f186a,DISK], DatanodeInfoWithStorage[127.0.0.1:41331,DS-24a8d27d-81d1-46e1-bbbb-01a73a8b55e5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42576,DS-121e542f-eb1d-4200-a53e-bdcfc85f186a,DISK], DatanodeInfoWithStorage[127.0.0.1:41331,DS-24a8d27d-81d1-46e1-bbbb-01a73a8b55e5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestProtobufLog#testWALTrailer
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36610,DS-0e26f369-63b6-4765-8c19-68fd52e63936,DISK], DatanodeInfoWithStorage[127.0.0.1:36699,DS-9bbf8f69-8b45-42ac-a103-ae22d3b080cb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36699,DS-9bbf8f69-8b45-42ac-a103-ae22d3b080cb,DISK], DatanodeInfoWithStorage[127.0.0.1:36610,DS-0e26f369-63b6-4765-8c19-68fd52e63936,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36610,DS-0e26f369-63b6-4765-8c19-68fd52e63936,DISK], DatanodeInfoWithStorage[127.0.0.1:36699,DS-9bbf8f69-8b45-42ac-a103-ae22d3b080cb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36699,DS-9bbf8f69-8b45-42ac-a103-ae22d3b080cb,DISK], DatanodeInfoWithStorage[127.0.0.1:36610,DS-0e26f369-63b6-4765-8c19-68fd52e63936,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestProtobufLog#testWALTrailer
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34817,DS-207b2ee5-4ab2-4556-b87b-5727d2b53d31,DISK], DatanodeInfoWithStorage[127.0.0.1:43613,DS-f75e8a0e-dd1d-468d-978f-8acf5607f8c3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34817,DS-207b2ee5-4ab2-4556-b87b-5727d2b53d31,DISK], DatanodeInfoWithStorage[127.0.0.1:43613,DS-f75e8a0e-dd1d-468d-978f-8acf5607f8c3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34817,DS-207b2ee5-4ab2-4556-b87b-5727d2b53d31,DISK], DatanodeInfoWithStorage[127.0.0.1:43613,DS-f75e8a0e-dd1d-468d-978f-8acf5607f8c3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34817,DS-207b2ee5-4ab2-4556-b87b-5727d2b53d31,DISK], DatanodeInfoWithStorage[127.0.0.1:43613,DS-f75e8a0e-dd1d-468d-978f-8acf5607f8c3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestProtobufLog#testWALTrailer
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40061,DS-67a52b6f-3694-40d1-802d-031f1905c5c6,DISK], DatanodeInfoWithStorage[127.0.0.1:39372,DS-add5dcfa-70e2-4d4d-a1ba-8f5baaa8fcef,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39372,DS-add5dcfa-70e2-4d4d-a1ba-8f5baaa8fcef,DISK], DatanodeInfoWithStorage[127.0.0.1:40061,DS-67a52b6f-3694-40d1-802d-031f1905c5c6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40061,DS-67a52b6f-3694-40d1-802d-031f1905c5c6,DISK], DatanodeInfoWithStorage[127.0.0.1:39372,DS-add5dcfa-70e2-4d4d-a1ba-8f5baaa8fcef,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39372,DS-add5dcfa-70e2-4d4d-a1ba-8f5baaa8fcef,DISK], DatanodeInfoWithStorage[127.0.0.1:40061,DS-67a52b6f-3694-40d1-802d-031f1905c5c6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestProtobufLog#testWALTrailer
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37728,DS-5eff76dd-e860-4881-8e25-1623f6e126f3,DISK], DatanodeInfoWithStorage[127.0.0.1:35105,DS-8d631bfe-00ec-4f39-80e8-24c359534da0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37728,DS-5eff76dd-e860-4881-8e25-1623f6e126f3,DISK], DatanodeInfoWithStorage[127.0.0.1:35105,DS-8d631bfe-00ec-4f39-80e8-24c359534da0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37728,DS-5eff76dd-e860-4881-8e25-1623f6e126f3,DISK], DatanodeInfoWithStorage[127.0.0.1:35105,DS-8d631bfe-00ec-4f39-80e8-24c359534da0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37728,DS-5eff76dd-e860-4881-8e25-1623f6e126f3,DISK], DatanodeInfoWithStorage[127.0.0.1:35105,DS-8d631bfe-00ec-4f39-80e8-24c359534da0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestProtobufLog#testWALTrailer
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46656,DS-f73ccc66-f597-48b0-992c-a504ed2bf726,DISK], DatanodeInfoWithStorage[127.0.0.1:45711,DS-7588819c-830c-4dd9-bd01-23bb94ddb758,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46656,DS-f73ccc66-f597-48b0-992c-a504ed2bf726,DISK], DatanodeInfoWithStorage[127.0.0.1:45711,DS-7588819c-830c-4dd9-bd01-23bb94ddb758,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46656,DS-f73ccc66-f597-48b0-992c-a504ed2bf726,DISK], DatanodeInfoWithStorage[127.0.0.1:45711,DS-7588819c-830c-4dd9-bd01-23bb94ddb758,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46656,DS-f73ccc66-f597-48b0-992c-a504ed2bf726,DISK], DatanodeInfoWithStorage[127.0.0.1:45711,DS-7588819c-830c-4dd9-bd01-23bb94ddb758,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestProtobufLog#testWALTrailer
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36536,DS-77a0dd7a-dd25-4296-9ebc-d821a6008bd0,DISK], DatanodeInfoWithStorage[127.0.0.1:33409,DS-b5211389-0588-4870-b0e5-b7099431d8e9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36536,DS-77a0dd7a-dd25-4296-9ebc-d821a6008bd0,DISK], DatanodeInfoWithStorage[127.0.0.1:33409,DS-b5211389-0588-4870-b0e5-b7099431d8e9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36536,DS-77a0dd7a-dd25-4296-9ebc-d821a6008bd0,DISK], DatanodeInfoWithStorage[127.0.0.1:33409,DS-b5211389-0588-4870-b0e5-b7099431d8e9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36536,DS-77a0dd7a-dd25-4296-9ebc-d821a6008bd0,DISK], DatanodeInfoWithStorage[127.0.0.1:33409,DS-b5211389-0588-4870-b0e5-b7099431d8e9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 1208

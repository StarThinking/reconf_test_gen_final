reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALCoprocessorLoaded
reconfPoint: -3
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALCoprocessorLoaded
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44868,DS-753c8ff2-4251-4fe3-b531-f68ec40d3279,DISK], DatanodeInfoWithStorage[127.0.0.1:45788,DS-7e61c9c2-805f-4191-ac9c-d07573369032,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45788,DS-7e61c9c2-805f-4191-ac9c-d07573369032,DISK], DatanodeInfoWithStorage[127.0.0.1:44868,DS-753c8ff2-4251-4fe3-b531-f68ec40d3279,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44868,DS-753c8ff2-4251-4fe3-b531-f68ec40d3279,DISK], DatanodeInfoWithStorage[127.0.0.1:45788,DS-7e61c9c2-805f-4191-ac9c-d07573369032,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45788,DS-7e61c9c2-805f-4191-ac9c-d07573369032,DISK], DatanodeInfoWithStorage[127.0.0.1:44868,DS-753c8ff2-4251-4fe3-b531-f68ec40d3279,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALCoprocessorLoaded
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42577,DS-becf3469-6779-4392-a9d9-f333ea9e0304,DISK], DatanodeInfoWithStorage[127.0.0.1:34610,DS-34f30ca1-8518-406e-aaa8-77319d138169,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34610,DS-34f30ca1-8518-406e-aaa8-77319d138169,DISK], DatanodeInfoWithStorage[127.0.0.1:42577,DS-becf3469-6779-4392-a9d9-f333ea9e0304,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42577,DS-becf3469-6779-4392-a9d9-f333ea9e0304,DISK], DatanodeInfoWithStorage[127.0.0.1:34610,DS-34f30ca1-8518-406e-aaa8-77319d138169,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34610,DS-34f30ca1-8518-406e-aaa8-77319d138169,DISK], DatanodeInfoWithStorage[127.0.0.1:42577,DS-becf3469-6779-4392-a9d9-f333ea9e0304,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALCoprocessorLoaded
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42776,DS-efcc7633-2842-4670-83c9-f0d2c50c8454,DISK], DatanodeInfoWithStorage[127.0.0.1:33898,DS-b8d625bf-18bd-42ad-b51a-9a7bfe795129,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42776,DS-efcc7633-2842-4670-83c9-f0d2c50c8454,DISK], DatanodeInfoWithStorage[127.0.0.1:33898,DS-b8d625bf-18bd-42ad-b51a-9a7bfe795129,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42776,DS-efcc7633-2842-4670-83c9-f0d2c50c8454,DISK], DatanodeInfoWithStorage[127.0.0.1:33898,DS-b8d625bf-18bd-42ad-b51a-9a7bfe795129,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42776,DS-efcc7633-2842-4670-83c9-f0d2c50c8454,DISK], DatanodeInfoWithStorage[127.0.0.1:33898,DS-b8d625bf-18bd-42ad-b51a-9a7bfe795129,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALCoprocessorLoaded
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37696,DS-45d1947e-89de-4838-9332-3dbde63a8205,DISK], DatanodeInfoWithStorage[127.0.0.1:33280,DS-badab2ce-4628-473b-9d68-e0dbdaf006dc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37696,DS-45d1947e-89de-4838-9332-3dbde63a8205,DISK], DatanodeInfoWithStorage[127.0.0.1:33280,DS-badab2ce-4628-473b-9d68-e0dbdaf006dc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37696,DS-45d1947e-89de-4838-9332-3dbde63a8205,DISK], DatanodeInfoWithStorage[127.0.0.1:33280,DS-badab2ce-4628-473b-9d68-e0dbdaf006dc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37696,DS-45d1947e-89de-4838-9332-3dbde63a8205,DISK], DatanodeInfoWithStorage[127.0.0.1:33280,DS-badab2ce-4628-473b-9d68-e0dbdaf006dc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALCoprocessorLoaded
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41562,DS-a3b3dd48-1328-468c-b753-2dc07055dffb,DISK], DatanodeInfoWithStorage[127.0.0.1:39286,DS-5a9f8b4c-0b26-4012-82ce-5bcc585f3619,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41562,DS-a3b3dd48-1328-468c-b753-2dc07055dffb,DISK], DatanodeInfoWithStorage[127.0.0.1:39286,DS-5a9f8b4c-0b26-4012-82ce-5bcc585f3619,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41562,DS-a3b3dd48-1328-468c-b753-2dc07055dffb,DISK], DatanodeInfoWithStorage[127.0.0.1:39286,DS-5a9f8b4c-0b26-4012-82ce-5bcc585f3619,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41562,DS-a3b3dd48-1328-468c-b753-2dc07055dffb,DISK], DatanodeInfoWithStorage[127.0.0.1:39286,DS-5a9f8b4c-0b26-4012-82ce-5bcc585f3619,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALCoprocessorLoaded
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45381,DS-217627e6-d775-4f4d-88f7-9a139dbf58c7,DISK], DatanodeInfoWithStorage[127.0.0.1:39855,DS-0c3f4fa3-932f-4322-a6b4-ce1dd29c1ab8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39855,DS-0c3f4fa3-932f-4322-a6b4-ce1dd29c1ab8,DISK], DatanodeInfoWithStorage[127.0.0.1:45381,DS-217627e6-d775-4f4d-88f7-9a139dbf58c7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45381,DS-217627e6-d775-4f4d-88f7-9a139dbf58c7,DISK], DatanodeInfoWithStorage[127.0.0.1:39855,DS-0c3f4fa3-932f-4322-a6b4-ce1dd29c1ab8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39855,DS-0c3f4fa3-932f-4322-a6b4-ce1dd29c1ab8,DISK], DatanodeInfoWithStorage[127.0.0.1:45381,DS-217627e6-d775-4f4d-88f7-9a139dbf58c7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALCoprocessorLoaded
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36454,DS-239b529a-45f2-4820-96c9-9935e42f2ae8,DISK], DatanodeInfoWithStorage[127.0.0.1:44686,DS-73e39deb-23de-43bd-8cf6-f22b9865ca4a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36454,DS-239b529a-45f2-4820-96c9-9935e42f2ae8,DISK], DatanodeInfoWithStorage[127.0.0.1:44686,DS-73e39deb-23de-43bd-8cf6-f22b9865ca4a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36454,DS-239b529a-45f2-4820-96c9-9935e42f2ae8,DISK], DatanodeInfoWithStorage[127.0.0.1:44686,DS-73e39deb-23de-43bd-8cf6-f22b9865ca4a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36454,DS-239b529a-45f2-4820-96c9-9935e42f2ae8,DISK], DatanodeInfoWithStorage[127.0.0.1:44686,DS-73e39deb-23de-43bd-8cf6-f22b9865ca4a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALCoprocessorLoaded
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37589,DS-13f3aa77-4c17-49ac-9c19-fde99a77457d,DISK], DatanodeInfoWithStorage[127.0.0.1:37039,DS-ba4262b6-2e55-4968-a928-da83d160310c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37589,DS-13f3aa77-4c17-49ac-9c19-fde99a77457d,DISK], DatanodeInfoWithStorage[127.0.0.1:37039,DS-ba4262b6-2e55-4968-a928-da83d160310c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37589,DS-13f3aa77-4c17-49ac-9c19-fde99a77457d,DISK], DatanodeInfoWithStorage[127.0.0.1:37039,DS-ba4262b6-2e55-4968-a928-da83d160310c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37589,DS-13f3aa77-4c17-49ac-9c19-fde99a77457d,DISK], DatanodeInfoWithStorage[127.0.0.1:37039,DS-ba4262b6-2e55-4968-a928-da83d160310c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALCoprocessorLoaded
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43549,DS-c6eb81d0-91ab-45f2-8679-c8144c8c9e62,DISK], DatanodeInfoWithStorage[127.0.0.1:40510,DS-48fe435e-86c1-428c-9ca4-7687c9451c9c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40510,DS-48fe435e-86c1-428c-9ca4-7687c9451c9c,DISK], DatanodeInfoWithStorage[127.0.0.1:43549,DS-c6eb81d0-91ab-45f2-8679-c8144c8c9e62,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43549,DS-c6eb81d0-91ab-45f2-8679-c8144c8c9e62,DISK], DatanodeInfoWithStorage[127.0.0.1:40510,DS-48fe435e-86c1-428c-9ca4-7687c9451c9c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40510,DS-48fe435e-86c1-428c-9ca4-7687c9451c9c,DISK], DatanodeInfoWithStorage[127.0.0.1:43549,DS-c6eb81d0-91ab-45f2-8679-c8144c8c9e62,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALCoprocessorLoaded
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34781,DS-1ff24f6a-8b51-44da-bbc1-651b9c7f1379,DISK], DatanodeInfoWithStorage[127.0.0.1:35044,DS-96f0b9d1-a9c7-4ea7-9095-cb5c7983508e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35044,DS-96f0b9d1-a9c7-4ea7-9095-cb5c7983508e,DISK], DatanodeInfoWithStorage[127.0.0.1:34781,DS-1ff24f6a-8b51-44da-bbc1-651b9c7f1379,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34781,DS-1ff24f6a-8b51-44da-bbc1-651b9c7f1379,DISK], DatanodeInfoWithStorage[127.0.0.1:35044,DS-96f0b9d1-a9c7-4ea7-9095-cb5c7983508e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35044,DS-96f0b9d1-a9c7-4ea7-9095-cb5c7983508e,DISK], DatanodeInfoWithStorage[127.0.0.1:34781,DS-1ff24f6a-8b51-44da-bbc1-651b9c7f1379,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 1398

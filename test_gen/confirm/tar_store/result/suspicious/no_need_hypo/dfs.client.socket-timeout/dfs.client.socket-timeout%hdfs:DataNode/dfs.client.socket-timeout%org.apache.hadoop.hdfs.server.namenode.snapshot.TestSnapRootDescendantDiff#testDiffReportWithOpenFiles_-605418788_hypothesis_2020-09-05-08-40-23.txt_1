reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapRootDescendantDiff#testDiffReportWithOpenFiles
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapRootDescendantDiff#testDiffReportWithOpenFiles
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44733,DS-b527b940-575e-4193-8799-6c437a58b665,DISK], DatanodeInfoWithStorage[127.0.0.1:35651,DS-7ce2249b-1836-4e7a-96d4-a1335086f66d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44733,DS-b527b940-575e-4193-8799-6c437a58b665,DISK], DatanodeInfoWithStorage[127.0.0.1:35651,DS-7ce2249b-1836-4e7a-96d4-a1335086f66d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44733,DS-b527b940-575e-4193-8799-6c437a58b665,DISK], DatanodeInfoWithStorage[127.0.0.1:35651,DS-7ce2249b-1836-4e7a-96d4-a1335086f66d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44733,DS-b527b940-575e-4193-8799-6c437a58b665,DISK], DatanodeInfoWithStorage[127.0.0.1:35651,DS-7ce2249b-1836-4e7a-96d4-a1335086f66d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapRootDescendantDiff#testDiffReportWithOpenFiles
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34037,DS-85d969f3-cb6c-4d2c-ae81-ef1e57440ac7,DISK], DatanodeInfoWithStorage[127.0.0.1:44961,DS-98a2293e-0afc-41d2-a502-2c7a73f78832,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34037,DS-85d969f3-cb6c-4d2c-ae81-ef1e57440ac7,DISK], DatanodeInfoWithStorage[127.0.0.1:44961,DS-98a2293e-0afc-41d2-a502-2c7a73f78832,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34037,DS-85d969f3-cb6c-4d2c-ae81-ef1e57440ac7,DISK], DatanodeInfoWithStorage[127.0.0.1:44961,DS-98a2293e-0afc-41d2-a502-2c7a73f78832,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34037,DS-85d969f3-cb6c-4d2c-ae81-ef1e57440ac7,DISK], DatanodeInfoWithStorage[127.0.0.1:44961,DS-98a2293e-0afc-41d2-a502-2c7a73f78832,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapRootDescendantDiff#testDiffReportWithOpenFiles
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34667,DS-6efe6f49-730e-43bc-bbaa-c23de1141dc8,DISK], DatanodeInfoWithStorage[127.0.0.1:32952,DS-d8e9d132-7905-498b-9d66-0924d5067df0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34667,DS-6efe6f49-730e-43bc-bbaa-c23de1141dc8,DISK], DatanodeInfoWithStorage[127.0.0.1:32952,DS-d8e9d132-7905-498b-9d66-0924d5067df0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34667,DS-6efe6f49-730e-43bc-bbaa-c23de1141dc8,DISK], DatanodeInfoWithStorage[127.0.0.1:32952,DS-d8e9d132-7905-498b-9d66-0924d5067df0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34667,DS-6efe6f49-730e-43bc-bbaa-c23de1141dc8,DISK], DatanodeInfoWithStorage[127.0.0.1:32952,DS-d8e9d132-7905-498b-9d66-0924d5067df0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapRootDescendantDiff#testDiffReportWithOpenFiles
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46612,DS-b4215810-9ff8-46c4-a390-9647f3100b8e,DISK], DatanodeInfoWithStorage[127.0.0.1:42533,DS-8d868d75-f2c3-482a-8354-6b6cebf70ad8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46612,DS-b4215810-9ff8-46c4-a390-9647f3100b8e,DISK], DatanodeInfoWithStorage[127.0.0.1:42533,DS-8d868d75-f2c3-482a-8354-6b6cebf70ad8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46612,DS-b4215810-9ff8-46c4-a390-9647f3100b8e,DISK], DatanodeInfoWithStorage[127.0.0.1:42533,DS-8d868d75-f2c3-482a-8354-6b6cebf70ad8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46612,DS-b4215810-9ff8-46c4-a390-9647f3100b8e,DISK], DatanodeInfoWithStorage[127.0.0.1:42533,DS-8d868d75-f2c3-482a-8354-6b6cebf70ad8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapRootDescendantDiff#testDiffReportWithOpenFiles
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37169,DS-5947fdd4-4fdf-40fb-bbbf-070986a1433c,DISK], DatanodeInfoWithStorage[127.0.0.1:41163,DS-062eeded-efea-463d-976a-e8ccb654064c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37169,DS-5947fdd4-4fdf-40fb-bbbf-070986a1433c,DISK], DatanodeInfoWithStorage[127.0.0.1:41163,DS-062eeded-efea-463d-976a-e8ccb654064c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37169,DS-5947fdd4-4fdf-40fb-bbbf-070986a1433c,DISK], DatanodeInfoWithStorage[127.0.0.1:41163,DS-062eeded-efea-463d-976a-e8ccb654064c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37169,DS-5947fdd4-4fdf-40fb-bbbf-070986a1433c,DISK], DatanodeInfoWithStorage[127.0.0.1:41163,DS-062eeded-efea-463d-976a-e8ccb654064c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapRootDescendantDiff#testDiffReportWithOpenFiles
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46214,DS-ba358554-acf3-485b-b37b-083977993435,DISK], DatanodeInfoWithStorage[127.0.0.1:32994,DS-8dd1b028-ec13-4aa6-8911-92a453d062bd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46214,DS-ba358554-acf3-485b-b37b-083977993435,DISK], DatanodeInfoWithStorage[127.0.0.1:32994,DS-8dd1b028-ec13-4aa6-8911-92a453d062bd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46214,DS-ba358554-acf3-485b-b37b-083977993435,DISK], DatanodeInfoWithStorage[127.0.0.1:32994,DS-8dd1b028-ec13-4aa6-8911-92a453d062bd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46214,DS-ba358554-acf3-485b-b37b-083977993435,DISK], DatanodeInfoWithStorage[127.0.0.1:32994,DS-8dd1b028-ec13-4aa6-8911-92a453d062bd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapRootDescendantDiff#testDiffReportWithOpenFiles
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36102,DS-ada3b1ca-aec9-433e-9476-7e7358617993,DISK], DatanodeInfoWithStorage[127.0.0.1:38599,DS-75f12f58-1631-47ea-af29-3309676343dc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38599,DS-75f12f58-1631-47ea-af29-3309676343dc,DISK], DatanodeInfoWithStorage[127.0.0.1:36102,DS-ada3b1ca-aec9-433e-9476-7e7358617993,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36102,DS-ada3b1ca-aec9-433e-9476-7e7358617993,DISK], DatanodeInfoWithStorage[127.0.0.1:38599,DS-75f12f58-1631-47ea-af29-3309676343dc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38599,DS-75f12f58-1631-47ea-af29-3309676343dc,DISK], DatanodeInfoWithStorage[127.0.0.1:36102,DS-ada3b1ca-aec9-433e-9476-7e7358617993,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapRootDescendantDiff#testDiffReportWithOpenFiles
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39789,DS-1dc3c420-229b-41f2-b57c-3ebac8d75f07,DISK], DatanodeInfoWithStorage[127.0.0.1:46058,DS-77ddf373-fe0a-4b89-b470-04d0b369ef27,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39789,DS-1dc3c420-229b-41f2-b57c-3ebac8d75f07,DISK], DatanodeInfoWithStorage[127.0.0.1:46058,DS-77ddf373-fe0a-4b89-b470-04d0b369ef27,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39789,DS-1dc3c420-229b-41f2-b57c-3ebac8d75f07,DISK], DatanodeInfoWithStorage[127.0.0.1:46058,DS-77ddf373-fe0a-4b89-b470-04d0b369ef27,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39789,DS-1dc3c420-229b-41f2-b57c-3ebac8d75f07,DISK], DatanodeInfoWithStorage[127.0.0.1:46058,DS-77ddf373-fe0a-4b89-b470-04d0b369ef27,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapRootDescendantDiff#testDiffReportWithOpenFiles
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40055,DS-87b564dc-6208-45eb-ac2d-d204e174cec0,DISK], DatanodeInfoWithStorage[127.0.0.1:37723,DS-85f2403e-74f5-4c20-baf0-fbda5c347117,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37723,DS-85f2403e-74f5-4c20-baf0-fbda5c347117,DISK], DatanodeInfoWithStorage[127.0.0.1:40055,DS-87b564dc-6208-45eb-ac2d-d204e174cec0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40055,DS-87b564dc-6208-45eb-ac2d-d204e174cec0,DISK], DatanodeInfoWithStorage[127.0.0.1:37723,DS-85f2403e-74f5-4c20-baf0-fbda5c347117,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37723,DS-85f2403e-74f5-4c20-baf0-fbda5c347117,DISK], DatanodeInfoWithStorage[127.0.0.1:40055,DS-87b564dc-6208-45eb-ac2d-d204e174cec0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapRootDescendantDiff#testDiffReportWithOpenFiles
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37513,DS-9884354f-fd7a-4b7f-903b-a9c92544c3c0,DISK], DatanodeInfoWithStorage[127.0.0.1:40512,DS-0bd56e06-a6bd-4600-917e-64ffe5ea5585,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37513,DS-9884354f-fd7a-4b7f-903b-a9c92544c3c0,DISK], DatanodeInfoWithStorage[127.0.0.1:40512,DS-0bd56e06-a6bd-4600-917e-64ffe5ea5585,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37513,DS-9884354f-fd7a-4b7f-903b-a9c92544c3c0,DISK], DatanodeInfoWithStorage[127.0.0.1:40512,DS-0bd56e06-a6bd-4600-917e-64ffe5ea5585,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37513,DS-9884354f-fd7a-4b7f-903b-a9c92544c3c0,DISK], DatanodeInfoWithStorage[127.0.0.1:40512,DS-0bd56e06-a6bd-4600-917e-64ffe5ea5585,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 850

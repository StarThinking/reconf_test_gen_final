reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35331,DS-d3260907-2cea-4e95-b9bf-2b0a2915e0c2,DISK], DatanodeInfoWithStorage[127.0.0.1:37077,DS-3cd8aefc-9559-4840-b761-6fd1221dd2b4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37077,DS-3cd8aefc-9559-4840-b761-6fd1221dd2b4,DISK], DatanodeInfoWithStorage[127.0.0.1:35331,DS-d3260907-2cea-4e95-b9bf-2b0a2915e0c2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35331,DS-d3260907-2cea-4e95-b9bf-2b0a2915e0c2,DISK], DatanodeInfoWithStorage[127.0.0.1:37077,DS-3cd8aefc-9559-4840-b761-6fd1221dd2b4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37077,DS-3cd8aefc-9559-4840-b761-6fd1221dd2b4,DISK], DatanodeInfoWithStorage[127.0.0.1:35331,DS-d3260907-2cea-4e95-b9bf-2b0a2915e0c2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44799,DS-4b064e4d-6a5f-4d4f-9a8e-615ea5a72c2a,DISK], DatanodeInfoWithStorage[127.0.0.1:34609,DS-6299f51f-08cd-423e-956f-4de5f8603da9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34609,DS-6299f51f-08cd-423e-956f-4de5f8603da9,DISK], DatanodeInfoWithStorage[127.0.0.1:44799,DS-4b064e4d-6a5f-4d4f-9a8e-615ea5a72c2a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44799,DS-4b064e4d-6a5f-4d4f-9a8e-615ea5a72c2a,DISK], DatanodeInfoWithStorage[127.0.0.1:34609,DS-6299f51f-08cd-423e-956f-4de5f8603da9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34609,DS-6299f51f-08cd-423e-956f-4de5f8603da9,DISK], DatanodeInfoWithStorage[127.0.0.1:44799,DS-4b064e4d-6a5f-4d4f-9a8e-615ea5a72c2a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35735,DS-3d573ea9-6458-4b19-a043-dbffe212790e,DISK], DatanodeInfoWithStorage[127.0.0.1:34416,DS-331b5d75-38e0-46cd-af4d-611f0fc09bde,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35735,DS-3d573ea9-6458-4b19-a043-dbffe212790e,DISK], DatanodeInfoWithStorage[127.0.0.1:34416,DS-331b5d75-38e0-46cd-af4d-611f0fc09bde,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35735,DS-3d573ea9-6458-4b19-a043-dbffe212790e,DISK], DatanodeInfoWithStorage[127.0.0.1:34416,DS-331b5d75-38e0-46cd-af4d-611f0fc09bde,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35735,DS-3d573ea9-6458-4b19-a043-dbffe212790e,DISK], DatanodeInfoWithStorage[127.0.0.1:34416,DS-331b5d75-38e0-46cd-af4d-611f0fc09bde,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38939,DS-102bc2a3-b0d7-43f2-af67-78bd0d5f0461,DISK], DatanodeInfoWithStorage[127.0.0.1:38135,DS-49af5a79-1a73-4915-81b1-ef8b44465354,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38135,DS-49af5a79-1a73-4915-81b1-ef8b44465354,DISK], DatanodeInfoWithStorage[127.0.0.1:38939,DS-102bc2a3-b0d7-43f2-af67-78bd0d5f0461,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38939,DS-102bc2a3-b0d7-43f2-af67-78bd0d5f0461,DISK], DatanodeInfoWithStorage[127.0.0.1:38135,DS-49af5a79-1a73-4915-81b1-ef8b44465354,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38135,DS-49af5a79-1a73-4915-81b1-ef8b44465354,DISK], DatanodeInfoWithStorage[127.0.0.1:38939,DS-102bc2a3-b0d7-43f2-af67-78bd0d5f0461,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38753,DS-bac8201e-f7eb-4ab6-99ed-4a40bec28cfd,DISK], DatanodeInfoWithStorage[127.0.0.1:42684,DS-7109cf13-e4c0-4ef3-9091-638094294788,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38753,DS-bac8201e-f7eb-4ab6-99ed-4a40bec28cfd,DISK], DatanodeInfoWithStorage[127.0.0.1:42684,DS-7109cf13-e4c0-4ef3-9091-638094294788,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38753,DS-bac8201e-f7eb-4ab6-99ed-4a40bec28cfd,DISK], DatanodeInfoWithStorage[127.0.0.1:42684,DS-7109cf13-e4c0-4ef3-9091-638094294788,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38753,DS-bac8201e-f7eb-4ab6-99ed-4a40bec28cfd,DISK], DatanodeInfoWithStorage[127.0.0.1:42684,DS-7109cf13-e4c0-4ef3-9091-638094294788,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40970,DS-76035001-907f-451b-92fa-1d07358da945,DISK], DatanodeInfoWithStorage[127.0.0.1:36486,DS-2c7a6c09-cfc4-49d9-8414-9a3a1b32e000,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36486,DS-2c7a6c09-cfc4-49d9-8414-9a3a1b32e000,DISK], DatanodeInfoWithStorage[127.0.0.1:40970,DS-76035001-907f-451b-92fa-1d07358da945,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40970,DS-76035001-907f-451b-92fa-1d07358da945,DISK], DatanodeInfoWithStorage[127.0.0.1:36486,DS-2c7a6c09-cfc4-49d9-8414-9a3a1b32e000,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36486,DS-2c7a6c09-cfc4-49d9-8414-9a3a1b32e000,DISK], DatanodeInfoWithStorage[127.0.0.1:40970,DS-76035001-907f-451b-92fa-1d07358da945,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44219,DS-ba0d42fd-6aeb-4e51-9936-02e275978040,DISK], DatanodeInfoWithStorage[127.0.0.1:38835,DS-57fa3f58-2788-4e8d-8a69-d7e626be159f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44219,DS-ba0d42fd-6aeb-4e51-9936-02e275978040,DISK], DatanodeInfoWithStorage[127.0.0.1:38835,DS-57fa3f58-2788-4e8d-8a69-d7e626be159f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44219,DS-ba0d42fd-6aeb-4e51-9936-02e275978040,DISK], DatanodeInfoWithStorage[127.0.0.1:38835,DS-57fa3f58-2788-4e8d-8a69-d7e626be159f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44219,DS-ba0d42fd-6aeb-4e51-9936-02e275978040,DISK], DatanodeInfoWithStorage[127.0.0.1:38835,DS-57fa3f58-2788-4e8d-8a69-d7e626be159f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42940,DS-0acb2df5-22e5-4ea3-a161-71fe27636b18,DISK], DatanodeInfoWithStorage[127.0.0.1:33212,DS-8367b3a9-0ee1-4345-8535-1a824e49b2da,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42940,DS-0acb2df5-22e5-4ea3-a161-71fe27636b18,DISK], DatanodeInfoWithStorage[127.0.0.1:33212,DS-8367b3a9-0ee1-4345-8535-1a824e49b2da,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42940,DS-0acb2df5-22e5-4ea3-a161-71fe27636b18,DISK], DatanodeInfoWithStorage[127.0.0.1:33212,DS-8367b3a9-0ee1-4345-8535-1a824e49b2da,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42940,DS-0acb2df5-22e5-4ea3-a161-71fe27636b18,DISK], DatanodeInfoWithStorage[127.0.0.1:33212,DS-8367b3a9-0ee1-4345-8535-1a824e49b2da,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46783,DS-804532e4-9469-4a5e-97f6-9e2c1be029ee,DISK], DatanodeInfoWithStorage[127.0.0.1:38060,DS-b084625f-ed0d-411f-8d96-4841948d32d3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38060,DS-b084625f-ed0d-411f-8d96-4841948d32d3,DISK], DatanodeInfoWithStorage[127.0.0.1:46783,DS-804532e4-9469-4a5e-97f6-9e2c1be029ee,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46783,DS-804532e4-9469-4a5e-97f6-9e2c1be029ee,DISK], DatanodeInfoWithStorage[127.0.0.1:38060,DS-b084625f-ed0d-411f-8d96-4841948d32d3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38060,DS-b084625f-ed0d-411f-8d96-4841948d32d3,DISK], DatanodeInfoWithStorage[127.0.0.1:46783,DS-804532e4-9469-4a5e-97f6-9e2c1be029ee,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestSnapshotDiffReport#testDiffReportWithOpenFiles
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32993,DS-32f77d16-7e40-404f-97f0-1cc109775f10,DISK], DatanodeInfoWithStorage[127.0.0.1:43152,DS-a9ebeb57-4d1f-452a-ab33-2120bd6b6546,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43152,DS-a9ebeb57-4d1f-452a-ab33-2120bd6b6546,DISK], DatanodeInfoWithStorage[127.0.0.1:32993,DS-32f77d16-7e40-404f-97f0-1cc109775f10,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32993,DS-32f77d16-7e40-404f-97f0-1cc109775f10,DISK], DatanodeInfoWithStorage[127.0.0.1:43152,DS-a9ebeb57-4d1f-452a-ab33-2120bd6b6546,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43152,DS-a9ebeb57-4d1f-452a-ab33-2120bd6b6546,DISK], DatanodeInfoWithStorage[127.0.0.1:32993,DS-32f77d16-7e40-404f-97f0-1cc109775f10,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 11
v1v1v2v2 failed with probability 0 out of 11
result: might be true error
Total execution time in seconds : 991

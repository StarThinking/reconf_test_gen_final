reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testCompleteFileAfterCrashFailover
reconfPoint: -3
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testCompleteFileAfterCrashFailover
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46575,DS-4012b849-76b8-43c7-ae53-e3f5a0ee9bf2,DISK], DatanodeInfoWithStorage[127.0.0.1:42711,DS-52d4501c-a0a0-44fc-b798-9507a78a3b63,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46575,DS-4012b849-76b8-43c7-ae53-e3f5a0ee9bf2,DISK], DatanodeInfoWithStorage[127.0.0.1:42711,DS-52d4501c-a0a0-44fc-b798-9507a78a3b63,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46575,DS-4012b849-76b8-43c7-ae53-e3f5a0ee9bf2,DISK], DatanodeInfoWithStorage[127.0.0.1:42711,DS-52d4501c-a0a0-44fc-b798-9507a78a3b63,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46575,DS-4012b849-76b8-43c7-ae53-e3f5a0ee9bf2,DISK], DatanodeInfoWithStorage[127.0.0.1:42711,DS-52d4501c-a0a0-44fc-b798-9507a78a3b63,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testCompleteFileAfterCrashFailover
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37588,DS-deef3a0a-2d9d-442e-b082-d848c9a4a8bc,DISK], DatanodeInfoWithStorage[127.0.0.1:34991,DS-a48b92a6-9418-40de-aec9-c4c81fce8306,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37588,DS-deef3a0a-2d9d-442e-b082-d848c9a4a8bc,DISK], DatanodeInfoWithStorage[127.0.0.1:34991,DS-a48b92a6-9418-40de-aec9-c4c81fce8306,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37588,DS-deef3a0a-2d9d-442e-b082-d848c9a4a8bc,DISK], DatanodeInfoWithStorage[127.0.0.1:34991,DS-a48b92a6-9418-40de-aec9-c4c81fce8306,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37588,DS-deef3a0a-2d9d-442e-b082-d848c9a4a8bc,DISK], DatanodeInfoWithStorage[127.0.0.1:34991,DS-a48b92a6-9418-40de-aec9-c4c81fce8306,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testCompleteFileAfterCrashFailover
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44321,DS-a233fe50-f59f-4577-852d-acade8322998,DISK], DatanodeInfoWithStorage[127.0.0.1:40137,DS-f252cde0-444c-4547-b828-44bda9ab892f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44321,DS-a233fe50-f59f-4577-852d-acade8322998,DISK], DatanodeInfoWithStorage[127.0.0.1:40137,DS-f252cde0-444c-4547-b828-44bda9ab892f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44321,DS-a233fe50-f59f-4577-852d-acade8322998,DISK], DatanodeInfoWithStorage[127.0.0.1:40137,DS-f252cde0-444c-4547-b828-44bda9ab892f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44321,DS-a233fe50-f59f-4577-852d-acade8322998,DISK], DatanodeInfoWithStorage[127.0.0.1:40137,DS-f252cde0-444c-4547-b828-44bda9ab892f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testCompleteFileAfterCrashFailover
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38808,DS-00ed9878-c3d2-440a-9ba7-0ae8768406a3,DISK], DatanodeInfoWithStorage[127.0.0.1:40502,DS-8e28303b-b101-4af6-9c0e-f64e4a8e148f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38808,DS-00ed9878-c3d2-440a-9ba7-0ae8768406a3,DISK], DatanodeInfoWithStorage[127.0.0.1:40502,DS-8e28303b-b101-4af6-9c0e-f64e4a8e148f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38808,DS-00ed9878-c3d2-440a-9ba7-0ae8768406a3,DISK], DatanodeInfoWithStorage[127.0.0.1:40502,DS-8e28303b-b101-4af6-9c0e-f64e4a8e148f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38808,DS-00ed9878-c3d2-440a-9ba7-0ae8768406a3,DISK], DatanodeInfoWithStorage[127.0.0.1:40502,DS-8e28303b-b101-4af6-9c0e-f64e4a8e148f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testCompleteFileAfterCrashFailover
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38111,DS-f3086761-a8ae-4e12-ab4a-39f335e134e2,DISK], DatanodeInfoWithStorage[127.0.0.1:45411,DS-1a0e5fbf-d5ac-4eaf-896b-3787280ecbba,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38111,DS-f3086761-a8ae-4e12-ab4a-39f335e134e2,DISK], DatanodeInfoWithStorage[127.0.0.1:45411,DS-1a0e5fbf-d5ac-4eaf-896b-3787280ecbba,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38111,DS-f3086761-a8ae-4e12-ab4a-39f335e134e2,DISK], DatanodeInfoWithStorage[127.0.0.1:45411,DS-1a0e5fbf-d5ac-4eaf-896b-3787280ecbba,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38111,DS-f3086761-a8ae-4e12-ab4a-39f335e134e2,DISK], DatanodeInfoWithStorage[127.0.0.1:45411,DS-1a0e5fbf-d5ac-4eaf-896b-3787280ecbba,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testCompleteFileAfterCrashFailover
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36093,DS-f300de86-4b3b-45d9-a300-642f87ad1956,DISK], DatanodeInfoWithStorage[127.0.0.1:40120,DS-3707e23f-a228-4a18-b048-b60d03ca545a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40120,DS-3707e23f-a228-4a18-b048-b60d03ca545a,DISK], DatanodeInfoWithStorage[127.0.0.1:36093,DS-f300de86-4b3b-45d9-a300-642f87ad1956,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36093,DS-f300de86-4b3b-45d9-a300-642f87ad1956,DISK], DatanodeInfoWithStorage[127.0.0.1:40120,DS-3707e23f-a228-4a18-b048-b60d03ca545a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40120,DS-3707e23f-a228-4a18-b048-b60d03ca545a,DISK], DatanodeInfoWithStorage[127.0.0.1:36093,DS-f300de86-4b3b-45d9-a300-642f87ad1956,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testCompleteFileAfterCrashFailover
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43223,DS-f6ad7394-7c96-42df-88a8-2b3ed3c1266a,DISK], DatanodeInfoWithStorage[127.0.0.1:40170,DS-1f0bd0f1-12a3-492f-99a1-d208b10b0215,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40170,DS-1f0bd0f1-12a3-492f-99a1-d208b10b0215,DISK], DatanodeInfoWithStorage[127.0.0.1:43223,DS-f6ad7394-7c96-42df-88a8-2b3ed3c1266a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43223,DS-f6ad7394-7c96-42df-88a8-2b3ed3c1266a,DISK], DatanodeInfoWithStorage[127.0.0.1:40170,DS-1f0bd0f1-12a3-492f-99a1-d208b10b0215,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40170,DS-1f0bd0f1-12a3-492f-99a1-d208b10b0215,DISK], DatanodeInfoWithStorage[127.0.0.1:43223,DS-f6ad7394-7c96-42df-88a8-2b3ed3c1266a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testCompleteFileAfterCrashFailover
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35524,DS-51e74b06-7476-4199-a93e-b27a4b92caa1,DISK], DatanodeInfoWithStorage[127.0.0.1:41817,DS-e6f143a2-e6eb-454d-b7c1-acf3bca7a2ce,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35524,DS-51e74b06-7476-4199-a93e-b27a4b92caa1,DISK], DatanodeInfoWithStorage[127.0.0.1:41817,DS-e6f143a2-e6eb-454d-b7c1-acf3bca7a2ce,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35524,DS-51e74b06-7476-4199-a93e-b27a4b92caa1,DISK], DatanodeInfoWithStorage[127.0.0.1:41817,DS-e6f143a2-e6eb-454d-b7c1-acf3bca7a2ce,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35524,DS-51e74b06-7476-4199-a93e-b27a4b92caa1,DISK], DatanodeInfoWithStorage[127.0.0.1:41817,DS-e6f143a2-e6eb-454d-b7c1-acf3bca7a2ce,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testCompleteFileAfterCrashFailover
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40698,DS-5f8cc41d-2042-48d8-a7e0-8024b36626f4,DISK], DatanodeInfoWithStorage[127.0.0.1:44019,DS-9ea3af81-e913-4246-8487-a62978c9d01d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44019,DS-9ea3af81-e913-4246-8487-a62978c9d01d,DISK], DatanodeInfoWithStorage[127.0.0.1:40698,DS-5f8cc41d-2042-48d8-a7e0-8024b36626f4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40698,DS-5f8cc41d-2042-48d8-a7e0-8024b36626f4,DISK], DatanodeInfoWithStorage[127.0.0.1:44019,DS-9ea3af81-e913-4246-8487-a62978c9d01d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44019,DS-9ea3af81-e913-4246-8487-a62978c9d01d,DISK], DatanodeInfoWithStorage[127.0.0.1:40698,DS-5f8cc41d-2042-48d8-a7e0-8024b36626f4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testCompleteFileAfterCrashFailover
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37305,DS-d28ffe7f-c27c-4be7-842d-6f0030a67943,DISK], DatanodeInfoWithStorage[127.0.0.1:34069,DS-6d6b86f8-2f5a-48b2-8769-c81b48654618,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37305,DS-d28ffe7f-c27c-4be7-842d-6f0030a67943,DISK], DatanodeInfoWithStorage[127.0.0.1:34069,DS-6d6b86f8-2f5a-48b2-8769-c81b48654618,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37305,DS-d28ffe7f-c27c-4be7-842d-6f0030a67943,DISK], DatanodeInfoWithStorage[127.0.0.1:34069,DS-6d6b86f8-2f5a-48b2-8769-c81b48654618,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37305,DS-d28ffe7f-c27c-4be7-842d-6f0030a67943,DISK], DatanodeInfoWithStorage[127.0.0.1:34069,DS-6d6b86f8-2f5a-48b2-8769-c81b48654618,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 896

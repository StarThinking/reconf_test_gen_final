reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1117558590-172.17.0.17-1599329262743:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37021,DS-46c1442c-bf27-4990-a665-3145c43ba0ce,DISK], DatanodeInfoWithStorage[127.0.0.1:38246,DS-0f32ea5c-1d43-41ac-975f-6112458faf71,DISK], DatanodeInfoWithStorage[127.0.0.1:33536,DS-c5017ab7-aa9d-4c85-b879-791161af4cfb,DISK], DatanodeInfoWithStorage[127.0.0.1:33583,DS-06380012-36e4-4b08-b140-64f49aa46d7f,DISK], DatanodeInfoWithStorage[127.0.0.1:33017,DS-9f0e1ca8-f613-4da4-a180-621bce725ac4,DISK], DatanodeInfoWithStorage[127.0.0.1:40333,DS-130cd601-9b10-4f1d-a944-14f0b64f6c75,DISK], DatanodeInfoWithStorage[127.0.0.1:34240,DS-d53fbb31-ebd3-40bf-a099-f403a22be724,DISK], DatanodeInfoWithStorage[127.0.0.1:36505,DS-f39f4a06-7a8b-4a27-bf2e-e8cf09fa428a,DISK]]; indices=[0, 1, 2, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-1117558590-172.17.0.17-1599329262743:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:34240,DS-6cd492b3-1ad8-4dc5-a570-b87e375f4014,DISK], DatanodeInfoWithStorage[127.0.0.1:33536,DS-e674e457-eea6-40cb-90d8-44057f24bd95,DISK], DatanodeInfoWithStorage[127.0.0.1:37021,DS-73bdad2d-bda3-4c3a-826c-b14471e80288,DISK]]; indices=[0, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1117558590-172.17.0.17-1599329262743:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:34240,DS-6cd492b3-1ad8-4dc5-a570-b87e375f4014,DISK], DatanodeInfoWithStorage[127.0.0.1:33536,DS-e674e457-eea6-40cb-90d8-44057f24bd95,DISK], DatanodeInfoWithStorage[127.0.0.1:37021,DS-73bdad2d-bda3-4c3a-826c-b14471e80288,DISK]]; indices=[0, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1117558590-172.17.0.17-1599329262743:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37021,DS-46c1442c-bf27-4990-a665-3145c43ba0ce,DISK], DatanodeInfoWithStorage[127.0.0.1:38246,DS-0f32ea5c-1d43-41ac-975f-6112458faf71,DISK], DatanodeInfoWithStorage[127.0.0.1:33536,DS-c5017ab7-aa9d-4c85-b879-791161af4cfb,DISK], DatanodeInfoWithStorage[127.0.0.1:33583,DS-06380012-36e4-4b08-b140-64f49aa46d7f,DISK], DatanodeInfoWithStorage[127.0.0.1:33017,DS-9f0e1ca8-f613-4da4-a180-621bce725ac4,DISK], DatanodeInfoWithStorage[127.0.0.1:40333,DS-130cd601-9b10-4f1d-a944-14f0b64f6c75,DISK], DatanodeInfoWithStorage[127.0.0.1:34240,DS-d53fbb31-ebd3-40bf-a099-f403a22be724,DISK], DatanodeInfoWithStorage[127.0.0.1:36505,DS-f39f4a06-7a8b-4a27-bf2e-e8cf09fa428a,DISK]]; indices=[0, 1, 2, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-1117558590-172.17.0.17-1599329262743:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:34240,DS-6cd492b3-1ad8-4dc5-a570-b87e375f4014,DISK], DatanodeInfoWithStorage[127.0.0.1:33536,DS-e674e457-eea6-40cb-90d8-44057f24bd95,DISK], DatanodeInfoWithStorage[127.0.0.1:37021,DS-73bdad2d-bda3-4c3a-826c-b14471e80288,DISK]]; indices=[0, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1117558590-172.17.0.17-1599329262743:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:34240,DS-6cd492b3-1ad8-4dc5-a570-b87e375f4014,DISK], DatanodeInfoWithStorage[127.0.0.1:33536,DS-e674e457-eea6-40cb-90d8-44057f24bd95,DISK], DatanodeInfoWithStorage[127.0.0.1:37021,DS-73bdad2d-bda3-4c3a-826c-b14471e80288,DISK]]; indices=[0, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-896623978-172.17.0.17-1599330222038:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39459,DS-f07ab109-9838-4e4e-8ef3-95a3bfb181cc,DISK], DatanodeInfoWithStorage[127.0.0.1:38078,DS-96112d69-3664-433f-8bef-bfedff28498c,DISK], DatanodeInfoWithStorage[127.0.0.1:44263,DS-e447ee79-7568-4e09-9843-f73c0763994a,DISK], DatanodeInfoWithStorage[127.0.0.1:40838,DS-274b0afd-661f-4d3f-99bd-4557acd0d419,DISK], DatanodeInfoWithStorage[127.0.0.1:37195,DS-a4cb2e05-dc41-4a8d-93c6-30058f205888,DISK], DatanodeInfoWithStorage[127.0.0.1:39571,DS-d512e206-790b-460d-99a0-649a3cbf9661,DISK], DatanodeInfoWithStorage[127.0.0.1:36396,DS-894712a1-2684-408a-949a-7dde357f3248,DISK], DatanodeInfoWithStorage[127.0.0.1:37353,DS-26ae907f-d229-4c2a-a151-1bdebb2fad86,DISK], DatanodeInfoWithStorage[127.0.0.1:46117,DS-ef6d4514-a4ac-401b-88b8-708c2ac73b84,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-896623978-172.17.0.17-1599330222038:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:46117,DS-5d62177f-d0eb-4be0-ac27-85174f046c3d,DISK]]; indices=[0]}];  lastLocatedBlock=LocatedStripedBlock{BP-896623978-172.17.0.17-1599330222038:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:46117,DS-5d62177f-d0eb-4be0-ac27-85174f046c3d,DISK]]; indices=[0]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-896623978-172.17.0.17-1599330222038:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39459,DS-f07ab109-9838-4e4e-8ef3-95a3bfb181cc,DISK], DatanodeInfoWithStorage[127.0.0.1:38078,DS-96112d69-3664-433f-8bef-bfedff28498c,DISK], DatanodeInfoWithStorage[127.0.0.1:44263,DS-e447ee79-7568-4e09-9843-f73c0763994a,DISK], DatanodeInfoWithStorage[127.0.0.1:40838,DS-274b0afd-661f-4d3f-99bd-4557acd0d419,DISK], DatanodeInfoWithStorage[127.0.0.1:37195,DS-a4cb2e05-dc41-4a8d-93c6-30058f205888,DISK], DatanodeInfoWithStorage[127.0.0.1:39571,DS-d512e206-790b-460d-99a0-649a3cbf9661,DISK], DatanodeInfoWithStorage[127.0.0.1:36396,DS-894712a1-2684-408a-949a-7dde357f3248,DISK], DatanodeInfoWithStorage[127.0.0.1:37353,DS-26ae907f-d229-4c2a-a151-1bdebb2fad86,DISK], DatanodeInfoWithStorage[127.0.0.1:46117,DS-ef6d4514-a4ac-401b-88b8-708c2ac73b84,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-896623978-172.17.0.17-1599330222038:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:46117,DS-5d62177f-d0eb-4be0-ac27-85174f046c3d,DISK]]; indices=[0]}];  lastLocatedBlock=LocatedStripedBlock{BP-896623978-172.17.0.17-1599330222038:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:46117,DS-5d62177f-d0eb-4be0-ac27-85174f046c3d,DISK]]; indices=[0]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:378)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: Invalid buffer found, not allowing null
stackTrace: org.apache.hadoop.HadoopIllegalArgumentException: Invalid buffer found, not allowing null
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.checkOutputBuffers(ByteBufferDecodingState.java:132)
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.<init>(ByteBufferDecodingState.java:48)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:86)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:170)
	at org.apache.hadoop.hdfs.StripeReader.decodeAndFillBuffer(StripeReader.java:433)
	at org.apache.hadoop.hdfs.StatefulStripeReader.decode(StatefulStripeReader.java:94)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:390)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readOneStripe(DFSStripedInputStream.java:326)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readWithStrategy(DFSStripedInputStream.java:419)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyStatefulRead(StripedFileTestUtil.java:126)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:141)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: Invalid buffer found, not allowing null
stackTrace: org.apache.hadoop.HadoopIllegalArgumentException: Invalid buffer found, not allowing null
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.checkOutputBuffers(ByteBufferDecodingState.java:132)
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.<init>(ByteBufferDecodingState.java:48)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:86)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:170)
	at org.apache.hadoop.hdfs.StripeReader.decodeAndFillBuffer(StripeReader.java:433)
	at org.apache.hadoop.hdfs.StatefulStripeReader.decode(StatefulStripeReader.java:94)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:390)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readOneStripe(DFSStripedInputStream.java:326)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readWithStrategy(DFSStripedInputStream.java:419)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyStatefulRead(StripedFileTestUtil.java:126)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:141)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1056139006-172.17.0.17-1599330988458:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43105,DS-a04c29de-0fb2-429b-8336-173d30be558f,DISK], DatanodeInfoWithStorage[127.0.0.1:46188,DS-341fe77d-7f02-4f19-b406-1a1003ff027c,DISK], DatanodeInfoWithStorage[127.0.0.1:36251,DS-8cc528d8-3b62-4369-b878-dd58a0628182,DISK], DatanodeInfoWithStorage[127.0.0.1:36628,DS-84130443-21d4-4606-8a34-ac7108b92bd1,DISK], DatanodeInfoWithStorage[127.0.0.1:42784,DS-d663b5fb-327c-448a-8759-e9c30d1232f7,DISK], DatanodeInfoWithStorage[127.0.0.1:33790,DS-17d90680-94a0-427b-96b1-5656c5fca400,DISK], DatanodeInfoWithStorage[127.0.0.1:44709,DS-9180a459-a321-457b-b827-8ab13005a227,DISK], DatanodeInfoWithStorage[127.0.0.1:36726,DS-93a5be8e-14bb-4f6a-a7e1-91938a9afaf7,DISK], DatanodeInfoWithStorage[127.0.0.1:36505,DS-3d0ad423-d0d0-4a83-b436-73f961cabf9d,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-1056139006-172.17.0.17-1599330988458:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43105,DS-a57ec675-05ec-44e6-b948-ad430946973a,DISK]]; indices=[6]}];  lastLocatedBlock=LocatedStripedBlock{BP-1056139006-172.17.0.17-1599330988458:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43105,DS-a57ec675-05ec-44e6-b948-ad430946973a,DISK]]; indices=[6]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1056139006-172.17.0.17-1599330988458:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43105,DS-a04c29de-0fb2-429b-8336-173d30be558f,DISK], DatanodeInfoWithStorage[127.0.0.1:46188,DS-341fe77d-7f02-4f19-b406-1a1003ff027c,DISK], DatanodeInfoWithStorage[127.0.0.1:36251,DS-8cc528d8-3b62-4369-b878-dd58a0628182,DISK], DatanodeInfoWithStorage[127.0.0.1:36628,DS-84130443-21d4-4606-8a34-ac7108b92bd1,DISK], DatanodeInfoWithStorage[127.0.0.1:42784,DS-d663b5fb-327c-448a-8759-e9c30d1232f7,DISK], DatanodeInfoWithStorage[127.0.0.1:33790,DS-17d90680-94a0-427b-96b1-5656c5fca400,DISK], DatanodeInfoWithStorage[127.0.0.1:44709,DS-9180a459-a321-457b-b827-8ab13005a227,DISK], DatanodeInfoWithStorage[127.0.0.1:36726,DS-93a5be8e-14bb-4f6a-a7e1-91938a9afaf7,DISK], DatanodeInfoWithStorage[127.0.0.1:36505,DS-3d0ad423-d0d0-4a83-b436-73f961cabf9d,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-1056139006-172.17.0.17-1599330988458:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43105,DS-a57ec675-05ec-44e6-b948-ad430946973a,DISK]]; indices=[6]}];  lastLocatedBlock=LocatedStripedBlock{BP-1056139006-172.17.0.17-1599330988458:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43105,DS-a57ec675-05ec-44e6-b948-ad430946973a,DISK]]; indices=[6]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:378)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: Invalid buffer found, not allowing null
stackTrace: org.apache.hadoop.HadoopIllegalArgumentException: Invalid buffer found, not allowing null
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.checkOutputBuffers(ByteBufferDecodingState.java:132)
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.<init>(ByteBufferDecodingState.java:48)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:86)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:170)
	at org.apache.hadoop.hdfs.StripeReader.decodeAndFillBuffer(StripeReader.java:433)
	at org.apache.hadoop.hdfs.StatefulStripeReader.decode(StatefulStripeReader.java:94)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:390)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readOneStripe(DFSStripedInputStream.java:326)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readWithStrategy(DFSStripedInputStream.java:419)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyStatefulRead(StripedFileTestUtil.java:126)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:141)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-196645429-172.17.0.17-1599332046104:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46429,DS-27937f2f-5b4a-42e3-bb46-686b098ce67c,DISK], DatanodeInfoWithStorage[127.0.0.1:37212,DS-61046bac-fd4c-44fe-81bf-5c69b8bed324,DISK], DatanodeInfoWithStorage[127.0.0.1:43017,DS-a3f7f78b-9341-456f-b898-c6095cfd7726,DISK], DatanodeInfoWithStorage[127.0.0.1:42177,DS-93c30233-c911-4f24-8c26-6276cca94531,DISK], DatanodeInfoWithStorage[127.0.0.1:41440,DS-1556521d-11e4-4da2-bd8d-c78d079fc8db,DISK], DatanodeInfoWithStorage[127.0.0.1:42149,DS-c51818d9-ef37-4f8c-8416-fac95f6cdbd7,DISK], DatanodeInfoWithStorage[127.0.0.1:42014,DS-69e24384-23ad-41c8-b809-2676d66a0464,DISK], DatanodeInfoWithStorage[127.0.0.1:40098,DS-ea817902-abc9-4144-b024-96f474bdd482,DISK]]; indices=[0, 1, 2, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-196645429-172.17.0.17-1599332046104:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43017,DS-0d2d2437-aac0-4a97-ad37-4a21ccece3d6,DISK], DatanodeInfoWithStorage[127.0.0.1:42014,DS-ae9a45c1-eb7c-44ea-bd67-0a62cb0b6f36,DISK]]; indices=[0, 6]}];  lastLocatedBlock=LocatedStripedBlock{BP-196645429-172.17.0.17-1599332046104:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43017,DS-0d2d2437-aac0-4a97-ad37-4a21ccece3d6,DISK], DatanodeInfoWithStorage[127.0.0.1:42014,DS-ae9a45c1-eb7c-44ea-bd67-0a62cb0b6f36,DISK]]; indices=[0, 6]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-196645429-172.17.0.17-1599332046104:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46429,DS-27937f2f-5b4a-42e3-bb46-686b098ce67c,DISK], DatanodeInfoWithStorage[127.0.0.1:37212,DS-61046bac-fd4c-44fe-81bf-5c69b8bed324,DISK], DatanodeInfoWithStorage[127.0.0.1:43017,DS-a3f7f78b-9341-456f-b898-c6095cfd7726,DISK], DatanodeInfoWithStorage[127.0.0.1:42177,DS-93c30233-c911-4f24-8c26-6276cca94531,DISK], DatanodeInfoWithStorage[127.0.0.1:41440,DS-1556521d-11e4-4da2-bd8d-c78d079fc8db,DISK], DatanodeInfoWithStorage[127.0.0.1:42149,DS-c51818d9-ef37-4f8c-8416-fac95f6cdbd7,DISK], DatanodeInfoWithStorage[127.0.0.1:42014,DS-69e24384-23ad-41c8-b809-2676d66a0464,DISK], DatanodeInfoWithStorage[127.0.0.1:40098,DS-ea817902-abc9-4144-b024-96f474bdd482,DISK]]; indices=[0, 1, 2, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-196645429-172.17.0.17-1599332046104:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43017,DS-0d2d2437-aac0-4a97-ad37-4a21ccece3d6,DISK], DatanodeInfoWithStorage[127.0.0.1:42014,DS-ae9a45c1-eb7c-44ea-bd67-0a62cb0b6f36,DISK]]; indices=[0, 6]}];  lastLocatedBlock=LocatedStripedBlock{BP-196645429-172.17.0.17-1599332046104:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43017,DS-0d2d2437-aac0-4a97-ad37-4a21ccece3d6,DISK], DatanodeInfoWithStorage[127.0.0.1:42014,DS-ae9a45c1-eb7c-44ea-bd67-0a62cb0b6f36,DISK]]; indices=[0, 6]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:378)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-835867880-172.17.0.17-1599333256075:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39700,DS-74ab7320-6dfc-4e9b-9591-0a4ce1568ff3,DISK], DatanodeInfoWithStorage[127.0.0.1:45230,DS-fa73a96c-dc19-404a-b0c0-6cbe6f5d9c59,DISK], DatanodeInfoWithStorage[127.0.0.1:43925,DS-754a4f6c-3a4e-4a26-8679-e222e383a980,DISK], DatanodeInfoWithStorage[127.0.0.1:36715,DS-5e934c1c-18d8-4f0c-9b1c-7cc1b45e23d1,DISK], DatanodeInfoWithStorage[127.0.0.1:32804,DS-c89a53fa-1807-450b-ba78-d042f7894c4c,DISK], DatanodeInfoWithStorage[127.0.0.1:35868,DS-67faa6b7-1013-49d0-857d-4f6676301563,DISK], DatanodeInfoWithStorage[127.0.0.1:35684,DS-8c28bc35-8251-4ad3-a008-4503f8fa40dc,DISK], DatanodeInfoWithStorage[127.0.0.1:36808,DS-e5c27113-a459-4cc6-a963-ee0f9e4bb6b7,DISK], DatanodeInfoWithStorage[127.0.0.1:41509,DS-3bdf8c20-9af7-43e6-9547-92bf2b188a86,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-835867880-172.17.0.17-1599333256075:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:45230,DS-e603ab9d-0c1e-4434-be8b-669aeaa77717,DISK], DatanodeInfoWithStorage[127.0.0.1:32804,DS-79afa7ca-4258-4d15-af8b-7ac4398bdfcf,DISK]]; indices=[7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-835867880-172.17.0.17-1599333256075:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:45230,DS-e603ab9d-0c1e-4434-be8b-669aeaa77717,DISK], DatanodeInfoWithStorage[127.0.0.1:32804,DS-79afa7ca-4258-4d15-af8b-7ac4398bdfcf,DISK]]; indices=[7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-835867880-172.17.0.17-1599333256075:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39700,DS-74ab7320-6dfc-4e9b-9591-0a4ce1568ff3,DISK], DatanodeInfoWithStorage[127.0.0.1:45230,DS-fa73a96c-dc19-404a-b0c0-6cbe6f5d9c59,DISK], DatanodeInfoWithStorage[127.0.0.1:43925,DS-754a4f6c-3a4e-4a26-8679-e222e383a980,DISK], DatanodeInfoWithStorage[127.0.0.1:36715,DS-5e934c1c-18d8-4f0c-9b1c-7cc1b45e23d1,DISK], DatanodeInfoWithStorage[127.0.0.1:32804,DS-c89a53fa-1807-450b-ba78-d042f7894c4c,DISK], DatanodeInfoWithStorage[127.0.0.1:35868,DS-67faa6b7-1013-49d0-857d-4f6676301563,DISK], DatanodeInfoWithStorage[127.0.0.1:35684,DS-8c28bc35-8251-4ad3-a008-4503f8fa40dc,DISK], DatanodeInfoWithStorage[127.0.0.1:36808,DS-e5c27113-a459-4cc6-a963-ee0f9e4bb6b7,DISK], DatanodeInfoWithStorage[127.0.0.1:41509,DS-3bdf8c20-9af7-43e6-9547-92bf2b188a86,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-835867880-172.17.0.17-1599333256075:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:45230,DS-e603ab9d-0c1e-4434-be8b-669aeaa77717,DISK], DatanodeInfoWithStorage[127.0.0.1:32804,DS-79afa7ca-4258-4d15-af8b-7ac4398bdfcf,DISK]]; indices=[7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-835867880-172.17.0.17-1599333256075:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:45230,DS-e603ab9d-0c1e-4434-be8b-669aeaa77717,DISK], DatanodeInfoWithStorage[127.0.0.1:32804,DS-79afa7ca-4258-4d15-af8b-7ac4398bdfcf,DISK]]; indices=[7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-2113953567-172.17.0.17-1599333409500:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33632,DS-9bd1ada7-7f1b-4b60-9ac6-671e5003640f,DISK], DatanodeInfoWithStorage[127.0.0.1:34729,DS-6caf4347-f95e-4ce0-9333-2c0dd02c1d69,DISK], DatanodeInfoWithStorage[127.0.0.1:35690,DS-e5d97d7e-9476-4ac9-811c-2d78141ec6ad,DISK], DatanodeInfoWithStorage[127.0.0.1:34698,DS-6f7abd1f-0b70-4007-b209-538ac55eca33,DISK], DatanodeInfoWithStorage[127.0.0.1:43271,DS-efd76903-df3b-435c-97f3-19ba779c04bc,DISK], DatanodeInfoWithStorage[127.0.0.1:45308,DS-4180bf36-f8e9-4026-a671-f2ddc9690601,DISK], DatanodeInfoWithStorage[127.0.0.1:41254,DS-b96281fd-4061-48bf-8dc2-e615c5f5cebf,DISK], DatanodeInfoWithStorage[127.0.0.1:37837,DS-b2c6c020-af28-4351-b42b-913865a3c2dc,DISK], DatanodeInfoWithStorage[127.0.0.1:35267,DS-13296e67-2d33-4361-93ed-3d17fe8625bc,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-2113953567-172.17.0.17-1599333409500:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43271,DS-2570331f-01bb-42a3-88da-e88d3473ca0d,DISK]]; indices=[8]}];  lastLocatedBlock=LocatedStripedBlock{BP-2113953567-172.17.0.17-1599333409500:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43271,DS-2570331f-01bb-42a3-88da-e88d3473ca0d,DISK]]; indices=[8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-2113953567-172.17.0.17-1599333409500:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33632,DS-9bd1ada7-7f1b-4b60-9ac6-671e5003640f,DISK], DatanodeInfoWithStorage[127.0.0.1:34729,DS-6caf4347-f95e-4ce0-9333-2c0dd02c1d69,DISK], DatanodeInfoWithStorage[127.0.0.1:35690,DS-e5d97d7e-9476-4ac9-811c-2d78141ec6ad,DISK], DatanodeInfoWithStorage[127.0.0.1:34698,DS-6f7abd1f-0b70-4007-b209-538ac55eca33,DISK], DatanodeInfoWithStorage[127.0.0.1:43271,DS-efd76903-df3b-435c-97f3-19ba779c04bc,DISK], DatanodeInfoWithStorage[127.0.0.1:45308,DS-4180bf36-f8e9-4026-a671-f2ddc9690601,DISK], DatanodeInfoWithStorage[127.0.0.1:41254,DS-b96281fd-4061-48bf-8dc2-e615c5f5cebf,DISK], DatanodeInfoWithStorage[127.0.0.1:37837,DS-b2c6c020-af28-4351-b42b-913865a3c2dc,DISK], DatanodeInfoWithStorage[127.0.0.1:35267,DS-13296e67-2d33-4361-93ed-3d17fe8625bc,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-2113953567-172.17.0.17-1599333409500:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43271,DS-2570331f-01bb-42a3-88da-e88d3473ca0d,DISK]]; indices=[8]}];  lastLocatedBlock=LocatedStripedBlock{BP-2113953567-172.17.0.17-1599333409500:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43271,DS-2570331f-01bb-42a3-88da-e88d3473ca0d,DISK]]; indices=[8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1216207431-172.17.0.17-1599334167369:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43689,DS-9c16975f-8b42-433a-aa35-a83e81928075,DISK], DatanodeInfoWithStorage[127.0.0.1:42255,DS-bed70987-3e76-45ce-8deb-57ca7d77f78b,DISK], DatanodeInfoWithStorage[127.0.0.1:44229,DS-953da768-9995-4de3-bd1b-7b8ff139a9db,DISK], DatanodeInfoWithStorage[127.0.0.1:45021,DS-152abce5-a522-4f89-aaab-77724d56c71a,DISK], DatanodeInfoWithStorage[127.0.0.1:42415,DS-6f2c5c30-6bef-41f1-8ead-964eda4114c6,DISK], DatanodeInfoWithStorage[127.0.0.1:41413,DS-04132c22-f95a-4186-b728-f59442d1190e,DISK], DatanodeInfoWithStorage[127.0.0.1:45953,DS-fd24bc76-0279-4b5b-b9da-f48167616c3e,DISK], DatanodeInfoWithStorage[127.0.0.1:40471,DS-6c9d0085-a73a-4925-b754-c2da960acd35,DISK], DatanodeInfoWithStorage[127.0.0.1:46675,DS-e931bcaf-605e-4e14-8547-49849ad1da14,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-1216207431-172.17.0.17-1599334167369:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43689,DS-7d41cccb-10ce-4981-a459-6d259fd7ac60,DISK]]; indices=[8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1216207431-172.17.0.17-1599334167369:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43689,DS-7d41cccb-10ce-4981-a459-6d259fd7ac60,DISK]]; indices=[8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1216207431-172.17.0.17-1599334167369:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43689,DS-9c16975f-8b42-433a-aa35-a83e81928075,DISK], DatanodeInfoWithStorage[127.0.0.1:42255,DS-bed70987-3e76-45ce-8deb-57ca7d77f78b,DISK], DatanodeInfoWithStorage[127.0.0.1:44229,DS-953da768-9995-4de3-bd1b-7b8ff139a9db,DISK], DatanodeInfoWithStorage[127.0.0.1:45021,DS-152abce5-a522-4f89-aaab-77724d56c71a,DISK], DatanodeInfoWithStorage[127.0.0.1:42415,DS-6f2c5c30-6bef-41f1-8ead-964eda4114c6,DISK], DatanodeInfoWithStorage[127.0.0.1:41413,DS-04132c22-f95a-4186-b728-f59442d1190e,DISK], DatanodeInfoWithStorage[127.0.0.1:45953,DS-fd24bc76-0279-4b5b-b9da-f48167616c3e,DISK], DatanodeInfoWithStorage[127.0.0.1:40471,DS-6c9d0085-a73a-4925-b754-c2da960acd35,DISK], DatanodeInfoWithStorage[127.0.0.1:46675,DS-e931bcaf-605e-4e14-8547-49849ad1da14,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-1216207431-172.17.0.17-1599334167369:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43689,DS-7d41cccb-10ce-4981-a459-6d259fd7ac60,DISK]]; indices=[8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1216207431-172.17.0.17-1599334167369:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43689,DS-7d41cccb-10ce-4981-a459-6d259fd7ac60,DISK]]; indices=[8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-813046481-172.17.0.17-1599334769295:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38046,DS-d2952cd4-c508-4603-884a-04326b7e8b8b,DISK], DatanodeInfoWithStorage[127.0.0.1:35915,DS-ce6fcfd8-dc2f-4eca-83e2-471972a6fdd8,DISK], DatanodeInfoWithStorage[127.0.0.1:38133,DS-52b3d8af-3725-4c81-a264-866648f83c31,DISK], DatanodeInfoWithStorage[127.0.0.1:37220,DS-3c769401-6bbd-4e42-9c27-0028508d56e3,DISK], DatanodeInfoWithStorage[127.0.0.1:37374,DS-8922bdff-e0a1-4b77-b117-ec271eff4ff1,DISK], DatanodeInfoWithStorage[127.0.0.1:39295,DS-b38b4bed-de34-4066-b3d2-1778882fe7cd,DISK], DatanodeInfoWithStorage[127.0.0.1:42878,DS-0233410e-0dec-4e09-9767-798cf6f317b0,DISK], DatanodeInfoWithStorage[127.0.0.1:35111,DS-30c346f5-5ed0-42fb-9865-4679d652f029,DISK]]; indices=[0, 1, 2, 3, 4, 5, 7, 8]}, LocatedStripedBlock{BP-813046481-172.17.0.17-1599334769295:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:38046,DS-dcb5505e-5366-468d-a051-b87765ccfe5f,DISK]]; indices=[7]}];  lastLocatedBlock=LocatedStripedBlock{BP-813046481-172.17.0.17-1599334769295:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:38046,DS-dcb5505e-5366-468d-a051-b87765ccfe5f,DISK]]; indices=[7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-813046481-172.17.0.17-1599334769295:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38046,DS-d2952cd4-c508-4603-884a-04326b7e8b8b,DISK], DatanodeInfoWithStorage[127.0.0.1:35915,DS-ce6fcfd8-dc2f-4eca-83e2-471972a6fdd8,DISK], DatanodeInfoWithStorage[127.0.0.1:38133,DS-52b3d8af-3725-4c81-a264-866648f83c31,DISK], DatanodeInfoWithStorage[127.0.0.1:37220,DS-3c769401-6bbd-4e42-9c27-0028508d56e3,DISK], DatanodeInfoWithStorage[127.0.0.1:37374,DS-8922bdff-e0a1-4b77-b117-ec271eff4ff1,DISK], DatanodeInfoWithStorage[127.0.0.1:39295,DS-b38b4bed-de34-4066-b3d2-1778882fe7cd,DISK], DatanodeInfoWithStorage[127.0.0.1:42878,DS-0233410e-0dec-4e09-9767-798cf6f317b0,DISK], DatanodeInfoWithStorage[127.0.0.1:35111,DS-30c346f5-5ed0-42fb-9865-4679d652f029,DISK]]; indices=[0, 1, 2, 3, 4, 5, 7, 8]}, LocatedStripedBlock{BP-813046481-172.17.0.17-1599334769295:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:38046,DS-dcb5505e-5366-468d-a051-b87765ccfe5f,DISK]]; indices=[7]}];  lastLocatedBlock=LocatedStripedBlock{BP-813046481-172.17.0.17-1599334769295:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:38046,DS-dcb5505e-5366-468d-a051-b87765ccfe5f,DISK]]; indices=[7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:378)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: Invalid buffer found, not allowing null
stackTrace: org.apache.hadoop.HadoopIllegalArgumentException: Invalid buffer found, not allowing null
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.checkOutputBuffers(ByteBufferDecodingState.java:132)
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.<init>(ByteBufferDecodingState.java:48)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:86)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:170)
	at org.apache.hadoop.hdfs.StripeReader.decodeAndFillBuffer(StripeReader.java:433)
	at org.apache.hadoop.hdfs.StatefulStripeReader.decode(StatefulStripeReader.java:94)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:390)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readOneStripe(DFSStripedInputStream.java:326)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readWithStrategy(DFSStripedInputStream.java:419)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyStatefulRead(StripedFileTestUtil.java:126)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:141)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1935290055-172.17.0.17-1599336859547:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46064,DS-e6449fe8-2d88-44f3-bc11-e99895cc54e2,DISK], DatanodeInfoWithStorage[127.0.0.1:41129,DS-f5ee9542-4642-4108-a61f-d73c021fd993,DISK], DatanodeInfoWithStorage[127.0.0.1:33891,DS-fef44735-bbae-4ec7-aa25-21f0eadd0615,DISK], DatanodeInfoWithStorage[127.0.0.1:44025,DS-d81fd64c-e0c4-492a-9bb5-7eacbdb1c222,DISK], DatanodeInfoWithStorage[127.0.0.1:43841,DS-b8445394-51fb-4c19-b676-462f4d87f07c,DISK], DatanodeInfoWithStorage[127.0.0.1:46136,DS-d6ff8401-712b-4c1d-b15e-54245aa6471d,DISK], DatanodeInfoWithStorage[127.0.0.1:39117,DS-7bac6d04-15f3-4bbe-8e3e-1a4d0124d66a,DISK], DatanodeInfoWithStorage[127.0.0.1:39351,DS-7d483815-7cd1-4d7c-ace0-889b153a8699,DISK]]; indices=[0, 1, 2, 3, 4, 5, 7, 8]}, LocatedStripedBlock{BP-1935290055-172.17.0.17-1599336859547:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43841,DS-fc305127-fa5c-4b90-8cac-6cec8952749d,DISK], DatanodeInfoWithStorage[127.0.0.1:39351,DS-59b4c522-ab01-47fb-96fd-453ad5081b2f,DISK]]; indices=[6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1935290055-172.17.0.17-1599336859547:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43841,DS-fc305127-fa5c-4b90-8cac-6cec8952749d,DISK], DatanodeInfoWithStorage[127.0.0.1:39351,DS-59b4c522-ab01-47fb-96fd-453ad5081b2f,DISK]]; indices=[6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1935290055-172.17.0.17-1599336859547:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46064,DS-e6449fe8-2d88-44f3-bc11-e99895cc54e2,DISK], DatanodeInfoWithStorage[127.0.0.1:41129,DS-f5ee9542-4642-4108-a61f-d73c021fd993,DISK], DatanodeInfoWithStorage[127.0.0.1:33891,DS-fef44735-bbae-4ec7-aa25-21f0eadd0615,DISK], DatanodeInfoWithStorage[127.0.0.1:44025,DS-d81fd64c-e0c4-492a-9bb5-7eacbdb1c222,DISK], DatanodeInfoWithStorage[127.0.0.1:43841,DS-b8445394-51fb-4c19-b676-462f4d87f07c,DISK], DatanodeInfoWithStorage[127.0.0.1:46136,DS-d6ff8401-712b-4c1d-b15e-54245aa6471d,DISK], DatanodeInfoWithStorage[127.0.0.1:39117,DS-7bac6d04-15f3-4bbe-8e3e-1a4d0124d66a,DISK], DatanodeInfoWithStorage[127.0.0.1:39351,DS-7d483815-7cd1-4d7c-ace0-889b153a8699,DISK]]; indices=[0, 1, 2, 3, 4, 5, 7, 8]}, LocatedStripedBlock{BP-1935290055-172.17.0.17-1599336859547:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43841,DS-fc305127-fa5c-4b90-8cac-6cec8952749d,DISK], DatanodeInfoWithStorage[127.0.0.1:39351,DS-59b4c522-ab01-47fb-96fd-453ad5081b2f,DISK]]; indices=[6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1935290055-172.17.0.17-1599336859547:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:43841,DS-fc305127-fa5c-4b90-8cac-6cec8952749d,DISK], DatanodeInfoWithStorage[127.0.0.1:39351,DS-59b4c522-ab01-47fb-96fd-453ad5081b2f,DISK]]; indices=[6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1116493331-172.17.0.17-1599337014017:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43600,DS-a52065c6-b984-459d-91ab-33b801095ff2,DISK], DatanodeInfoWithStorage[127.0.0.1:43150,DS-0ec1b62a-d3cd-4182-b840-66d1dcd7af60,DISK], DatanodeInfoWithStorage[127.0.0.1:42343,DS-f7c3bd83-d6a0-46e7-9921-a53b159932f7,DISK], DatanodeInfoWithStorage[127.0.0.1:46384,DS-5272fa4f-3932-41fc-95d8-73dbffe7357e,DISK], DatanodeInfoWithStorage[127.0.0.1:46301,DS-d9d06605-1f42-47cf-bdaf-e87c8f244a69,DISK], DatanodeInfoWithStorage[127.0.0.1:39472,DS-c3b87c32-aa45-4ae5-9c99-2d50fc6680f9,DISK]]; indices=[1, 2, 4, 5, 6, 7]}, LocatedStripedBlock{BP-1116493331-172.17.0.17-1599337014017:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:46301,DS-47446446-3f4b-41b6-a057-198d9ad512ff,DISK], DatanodeInfoWithStorage[127.0.0.1:43600,DS-373a6ed5-be63-4a82-9583-4b873e2b517d,DISK]]; indices=[6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1116493331-172.17.0.17-1599337014017:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:46301,DS-47446446-3f4b-41b6-a057-198d9ad512ff,DISK], DatanodeInfoWithStorage[127.0.0.1:43600,DS-373a6ed5-be63-4a82-9583-4b873e2b517d,DISK]]; indices=[6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1116493331-172.17.0.17-1599337014017:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43600,DS-a52065c6-b984-459d-91ab-33b801095ff2,DISK], DatanodeInfoWithStorage[127.0.0.1:43150,DS-0ec1b62a-d3cd-4182-b840-66d1dcd7af60,DISK], DatanodeInfoWithStorage[127.0.0.1:42343,DS-f7c3bd83-d6a0-46e7-9921-a53b159932f7,DISK], DatanodeInfoWithStorage[127.0.0.1:46384,DS-5272fa4f-3932-41fc-95d8-73dbffe7357e,DISK], DatanodeInfoWithStorage[127.0.0.1:46301,DS-d9d06605-1f42-47cf-bdaf-e87c8f244a69,DISK], DatanodeInfoWithStorage[127.0.0.1:39472,DS-c3b87c32-aa45-4ae5-9c99-2d50fc6680f9,DISK]]; indices=[1, 2, 4, 5, 6, 7]}, LocatedStripedBlock{BP-1116493331-172.17.0.17-1599337014017:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:46301,DS-47446446-3f4b-41b6-a057-198d9ad512ff,DISK], DatanodeInfoWithStorage[127.0.0.1:43600,DS-373a6ed5-be63-4a82-9583-4b873e2b517d,DISK]]; indices=[6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1116493331-172.17.0.17-1599337014017:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:46301,DS-47446446-3f4b-41b6-a057-198d9ad512ff,DISK], DatanodeInfoWithStorage[127.0.0.1:43600,DS-373a6ed5-be63-4a82-9583-4b873e2b517d,DISK]]; indices=[6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:378)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1376779546-172.17.0.17-1599337170738:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38677,DS-904b5572-4c3a-4c92-8a6b-fea36d4e9bde,DISK], DatanodeInfoWithStorage[127.0.0.1:34918,DS-93b7955c-a10b-4aee-ab44-06b71585ecd5,DISK], DatanodeInfoWithStorage[127.0.0.1:40941,DS-cc0b29de-bda0-4a2d-984c-0edade5c1437,DISK], DatanodeInfoWithStorage[127.0.0.1:37379,DS-750604b1-01f0-4059-bf8d-486fcd033bc9,DISK], DatanodeInfoWithStorage[127.0.0.1:38384,DS-4464a7aa-858a-4449-8688-96e67eebd924,DISK], DatanodeInfoWithStorage[127.0.0.1:45044,DS-bb82fd3d-6a14-4a7a-8747-70e3adc5a123,DISK]]; indices=[1, 3, 5, 6, 7, 8]}, LocatedStripedBlock{BP-1376779546-172.17.0.17-1599337170738:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:38384,DS-d59eb49d-d7f3-41ad-83a7-46014a4282ce,DISK]]; indices=[8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1376779546-172.17.0.17-1599337170738:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:38384,DS-d59eb49d-d7f3-41ad-83a7-46014a4282ce,DISK]]; indices=[8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1376779546-172.17.0.17-1599337170738:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38677,DS-904b5572-4c3a-4c92-8a6b-fea36d4e9bde,DISK], DatanodeInfoWithStorage[127.0.0.1:34918,DS-93b7955c-a10b-4aee-ab44-06b71585ecd5,DISK], DatanodeInfoWithStorage[127.0.0.1:40941,DS-cc0b29de-bda0-4a2d-984c-0edade5c1437,DISK], DatanodeInfoWithStorage[127.0.0.1:37379,DS-750604b1-01f0-4059-bf8d-486fcd033bc9,DISK], DatanodeInfoWithStorage[127.0.0.1:38384,DS-4464a7aa-858a-4449-8688-96e67eebd924,DISK], DatanodeInfoWithStorage[127.0.0.1:45044,DS-bb82fd3d-6a14-4a7a-8747-70e3adc5a123,DISK]]; indices=[1, 3, 5, 6, 7, 8]}, LocatedStripedBlock{BP-1376779546-172.17.0.17-1599337170738:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:38384,DS-d59eb49d-d7f3-41ad-83a7-46014a4282ce,DISK]]; indices=[8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1376779546-172.17.0.17-1599337170738:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:38384,DS-d59eb49d-d7f3-41ad-83a7-46014a4282ce,DISK]]; indices=[8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-898252345-172.17.0.17-1599337328758:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45942,DS-0f4a5f0a-190f-4d02-9ca4-00fb4ff97296,DISK], DatanodeInfoWithStorage[127.0.0.1:38798,DS-a28ac299-dbbf-4130-9aa0-e8d0ca6b6c83,DISK], DatanodeInfoWithStorage[127.0.0.1:33722,DS-2f21fa6c-cd80-4d91-824c-9667c1b92128,DISK], DatanodeInfoWithStorage[127.0.0.1:36573,DS-5bb15778-97db-485f-8019-1fa59e36b125,DISK], DatanodeInfoWithStorage[127.0.0.1:36742,DS-256b236a-8e5b-4ff8-94b6-723e44cc3d80,DISK], DatanodeInfoWithStorage[127.0.0.1:43940,DS-704ab11e-034f-4260-9c49-d46f6707f1c3,DISK], DatanodeInfoWithStorage[127.0.0.1:36696,DS-3e4ae6e9-0960-4dce-ae21-9348f5792ef3,DISK], DatanodeInfoWithStorage[127.0.0.1:34782,DS-3a82eb8a-b18c-4e25-837d-3a209b11b576,DISK], DatanodeInfoWithStorage[127.0.0.1:36577,DS-db97760b-ed85-4b7f-a3a3-95d052458567,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-898252345-172.17.0.17-1599337328758:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:34782,DS-413a1e2a-8f47-4fe4-961e-c767d458da3f,DISK], DatanodeInfoWithStorage[127.0.0.1:36573,DS-a50a58d5-b284-4f55-9da4-a932bb9ed463,DISK]]; indices=[7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-898252345-172.17.0.17-1599337328758:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:34782,DS-413a1e2a-8f47-4fe4-961e-c767d458da3f,DISK], DatanodeInfoWithStorage[127.0.0.1:36573,DS-a50a58d5-b284-4f55-9da4-a932bb9ed463,DISK]]; indices=[7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=123, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165947;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-898252345-172.17.0.17-1599337328758:blk_-9223372036854775792_1001; getBlockSize()=25165824; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45942,DS-0f4a5f0a-190f-4d02-9ca4-00fb4ff97296,DISK], DatanodeInfoWithStorage[127.0.0.1:38798,DS-a28ac299-dbbf-4130-9aa0-e8d0ca6b6c83,DISK], DatanodeInfoWithStorage[127.0.0.1:33722,DS-2f21fa6c-cd80-4d91-824c-9667c1b92128,DISK], DatanodeInfoWithStorage[127.0.0.1:36573,DS-5bb15778-97db-485f-8019-1fa59e36b125,DISK], DatanodeInfoWithStorage[127.0.0.1:36742,DS-256b236a-8e5b-4ff8-94b6-723e44cc3d80,DISK], DatanodeInfoWithStorage[127.0.0.1:43940,DS-704ab11e-034f-4260-9c49-d46f6707f1c3,DISK], DatanodeInfoWithStorage[127.0.0.1:36696,DS-3e4ae6e9-0960-4dce-ae21-9348f5792ef3,DISK], DatanodeInfoWithStorage[127.0.0.1:34782,DS-3a82eb8a-b18c-4e25-837d-3a209b11b576,DISK], DatanodeInfoWithStorage[127.0.0.1:36577,DS-db97760b-ed85-4b7f-a3a3-95d052458567,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}, LocatedStripedBlock{BP-898252345-172.17.0.17-1599337328758:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:34782,DS-413a1e2a-8f47-4fe4-961e-c767d458da3f,DISK], DatanodeInfoWithStorage[127.0.0.1:36573,DS-a50a58d5-b284-4f55-9da4-a932bb9ed463,DISK]]; indices=[7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-898252345-172.17.0.17-1599337328758:blk_-9223372036854775776_1002; getBlockSize()=123; corrupt=false; offset=25165824; locs=[DatanodeInfoWithStorage[127.0.0.1:34782,DS-413a1e2a-8f47-4fe4-961e-c767d458da3f,DISK], DatanodeInfoWithStorage[127.0.0.1:36573,DS-a50a58d5-b284-4f55-9da4-a932bb9ed463,DISK]]; indices=[7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[8]
reconfPoint: -1
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodes(MiniDFSCluster.java:2227)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:260)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


v1v2 failed with probability 16 out of 50
v1v1v2v2 failed with probability 12 out of 50
result: might be true error
Total execution time in seconds : 8661

reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1783925762-172.17.0.14-1599290922701:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45512,DS-b07e646c-c44b-4416-84b7-64b38043b001,DISK], DatanodeInfoWithStorage[127.0.0.1:40730,DS-ae2d2ba6-068e-431c-84a2-f1e5866294dd,DISK], DatanodeInfoWithStorage[127.0.0.1:35495,DS-6cd828af-d219-4067-8de8-bd7449ae935e,DISK], DatanodeInfoWithStorage[127.0.0.1:38450,DS-d70fcda7-d205-4089-9e85-63d4eab128d9,DISK], DatanodeInfoWithStorage[127.0.0.1:41687,DS-f6af66b6-a9b6-49ac-9b00-320969b97f08,DISK], DatanodeInfoWithStorage[127.0.0.1:44962,DS-45945b87-a0ea-42ef-89a0-ea69e0c9f77a,DISK], DatanodeInfoWithStorage[127.0.0.1:36427,DS-fe880567-6aac-4918-8f4f-c022b0c5b79d,DISK], DatanodeInfoWithStorage[127.0.0.1:34491,DS-a2f0dddc-0791-44dc-ad50-402e7a56e5e1,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1783925762-172.17.0.14-1599290922701:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45512,DS-b07e646c-c44b-4416-84b7-64b38043b001,DISK], DatanodeInfoWithStorage[127.0.0.1:40730,DS-ae2d2ba6-068e-431c-84a2-f1e5866294dd,DISK], DatanodeInfoWithStorage[127.0.0.1:35495,DS-6cd828af-d219-4067-8de8-bd7449ae935e,DISK], DatanodeInfoWithStorage[127.0.0.1:38450,DS-d70fcda7-d205-4089-9e85-63d4eab128d9,DISK], DatanodeInfoWithStorage[127.0.0.1:41687,DS-f6af66b6-a9b6-49ac-9b00-320969b97f08,DISK], DatanodeInfoWithStorage[127.0.0.1:44962,DS-45945b87-a0ea-42ef-89a0-ea69e0c9f77a,DISK], DatanodeInfoWithStorage[127.0.0.1:36427,DS-fe880567-6aac-4918-8f4f-c022b0c5b79d,DISK], DatanodeInfoWithStorage[127.0.0.1:34491,DS-a2f0dddc-0791-44dc-ad50-402e7a56e5e1,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1783925762-172.17.0.14-1599290922701:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45512,DS-b07e646c-c44b-4416-84b7-64b38043b001,DISK], DatanodeInfoWithStorage[127.0.0.1:40730,DS-ae2d2ba6-068e-431c-84a2-f1e5866294dd,DISK], DatanodeInfoWithStorage[127.0.0.1:35495,DS-6cd828af-d219-4067-8de8-bd7449ae935e,DISK], DatanodeInfoWithStorage[127.0.0.1:38450,DS-d70fcda7-d205-4089-9e85-63d4eab128d9,DISK], DatanodeInfoWithStorage[127.0.0.1:41687,DS-f6af66b6-a9b6-49ac-9b00-320969b97f08,DISK], DatanodeInfoWithStorage[127.0.0.1:44962,DS-45945b87-a0ea-42ef-89a0-ea69e0c9f77a,DISK], DatanodeInfoWithStorage[127.0.0.1:36427,DS-fe880567-6aac-4918-8f4f-c022b0c5b79d,DISK], DatanodeInfoWithStorage[127.0.0.1:34491,DS-a2f0dddc-0791-44dc-ad50-402e7a56e5e1,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1783925762-172.17.0.14-1599290922701:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45512,DS-b07e646c-c44b-4416-84b7-64b38043b001,DISK], DatanodeInfoWithStorage[127.0.0.1:40730,DS-ae2d2ba6-068e-431c-84a2-f1e5866294dd,DISK], DatanodeInfoWithStorage[127.0.0.1:35495,DS-6cd828af-d219-4067-8de8-bd7449ae935e,DISK], DatanodeInfoWithStorage[127.0.0.1:38450,DS-d70fcda7-d205-4089-9e85-63d4eab128d9,DISK], DatanodeInfoWithStorage[127.0.0.1:41687,DS-f6af66b6-a9b6-49ac-9b00-320969b97f08,DISK], DatanodeInfoWithStorage[127.0.0.1:44962,DS-45945b87-a0ea-42ef-89a0-ea69e0c9f77a,DISK], DatanodeInfoWithStorage[127.0.0.1:36427,DS-fe880567-6aac-4918-8f4f-c022b0c5b79d,DISK], DatanodeInfoWithStorage[127.0.0.1:34491,DS-a2f0dddc-0791-44dc-ad50-402e7a56e5e1,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1109365562-172.17.0.14-1599291014085:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38908,DS-e798c437-a80d-4764-b1fc-bb6b749acd84,DISK], DatanodeInfoWithStorage[127.0.0.1:33435,DS-dfb63914-a5c3-4710-92e0-906411c6e9e6,DISK], DatanodeInfoWithStorage[127.0.0.1:44281,DS-de9811f0-a83c-4d84-bf40-117ba047b219,DISK], DatanodeInfoWithStorage[127.0.0.1:40844,DS-7395a115-4e68-4e94-8f81-4d96d5befc35,DISK], DatanodeInfoWithStorage[127.0.0.1:34688,DS-4183c348-12e2-4a6e-af41-599c90bd785f,DISK], DatanodeInfoWithStorage[127.0.0.1:37436,DS-0878b1b1-0749-44e0-aea4-c6f34db13b0c,DISK], DatanodeInfoWithStorage[127.0.0.1:34744,DS-de1be556-9ab0-4dde-9c33-c6290a7c3e23,DISK], DatanodeInfoWithStorage[127.0.0.1:37909,DS-0cad07f6-0559-42fc-8930-9b23316958cd,DISK], DatanodeInfoWithStorage[127.0.0.1:39933,DS-a2db9107-b54c-4208-8213-b9f945b16a73,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1109365562-172.17.0.14-1599291014085:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38908,DS-e798c437-a80d-4764-b1fc-bb6b749acd84,DISK], DatanodeInfoWithStorage[127.0.0.1:33435,DS-dfb63914-a5c3-4710-92e0-906411c6e9e6,DISK], DatanodeInfoWithStorage[127.0.0.1:44281,DS-de9811f0-a83c-4d84-bf40-117ba047b219,DISK], DatanodeInfoWithStorage[127.0.0.1:40844,DS-7395a115-4e68-4e94-8f81-4d96d5befc35,DISK], DatanodeInfoWithStorage[127.0.0.1:34688,DS-4183c348-12e2-4a6e-af41-599c90bd785f,DISK], DatanodeInfoWithStorage[127.0.0.1:37436,DS-0878b1b1-0749-44e0-aea4-c6f34db13b0c,DISK], DatanodeInfoWithStorage[127.0.0.1:37909,DS-0cad07f6-0559-42fc-8930-9b23316958cd,DISK], DatanodeInfoWithStorage[127.0.0.1:39933,DS-a2db9107-b54c-4208-8213-b9f945b16a73,DISK]]; indices=[0, 1, 2, 3, 4, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1109365562-172.17.0.14-1599291014085:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38908,DS-e798c437-a80d-4764-b1fc-bb6b749acd84,DISK], DatanodeInfoWithStorage[127.0.0.1:33435,DS-dfb63914-a5c3-4710-92e0-906411c6e9e6,DISK], DatanodeInfoWithStorage[127.0.0.1:44281,DS-de9811f0-a83c-4d84-bf40-117ba047b219,DISK], DatanodeInfoWithStorage[127.0.0.1:40844,DS-7395a115-4e68-4e94-8f81-4d96d5befc35,DISK], DatanodeInfoWithStorage[127.0.0.1:34688,DS-4183c348-12e2-4a6e-af41-599c90bd785f,DISK], DatanodeInfoWithStorage[127.0.0.1:37436,DS-0878b1b1-0749-44e0-aea4-c6f34db13b0c,DISK], DatanodeInfoWithStorage[127.0.0.1:34744,DS-de1be556-9ab0-4dde-9c33-c6290a7c3e23,DISK], DatanodeInfoWithStorage[127.0.0.1:37909,DS-0cad07f6-0559-42fc-8930-9b23316958cd,DISK], DatanodeInfoWithStorage[127.0.0.1:39933,DS-a2db9107-b54c-4208-8213-b9f945b16a73,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1109365562-172.17.0.14-1599291014085:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38908,DS-e798c437-a80d-4764-b1fc-bb6b749acd84,DISK], DatanodeInfoWithStorage[127.0.0.1:33435,DS-dfb63914-a5c3-4710-92e0-906411c6e9e6,DISK], DatanodeInfoWithStorage[127.0.0.1:44281,DS-de9811f0-a83c-4d84-bf40-117ba047b219,DISK], DatanodeInfoWithStorage[127.0.0.1:40844,DS-7395a115-4e68-4e94-8f81-4d96d5befc35,DISK], DatanodeInfoWithStorage[127.0.0.1:34688,DS-4183c348-12e2-4a6e-af41-599c90bd785f,DISK], DatanodeInfoWithStorage[127.0.0.1:37436,DS-0878b1b1-0749-44e0-aea4-c6f34db13b0c,DISK], DatanodeInfoWithStorage[127.0.0.1:37909,DS-0cad07f6-0559-42fc-8930-9b23316958cd,DISK], DatanodeInfoWithStorage[127.0.0.1:39933,DS-a2db9107-b54c-4208-8213-b9f945b16a73,DISK]]; indices=[0, 1, 2, 3, 4, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1148291678-172.17.0.14-1599292015359:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45540,DS-e79d19b0-da05-429e-8fe4-3a686165285c,DISK], DatanodeInfoWithStorage[127.0.0.1:41731,DS-23e9856b-3741-4ebd-9634-28f303095b70,DISK], DatanodeInfoWithStorage[127.0.0.1:33835,DS-95a7f06a-841b-45ee-8a96-61573a6533e2,DISK], DatanodeInfoWithStorage[127.0.0.1:43424,DS-6df52c74-7cc8-4dee-8a2c-8a4643853024,DISK], DatanodeInfoWithStorage[127.0.0.1:39850,DS-d556740c-d278-4da8-afa0-1eb23cfd5623,DISK], DatanodeInfoWithStorage[127.0.0.1:44885,DS-520cdb73-9555-46eb-b3e0-941ee385f948,DISK]]; indices=[0, 1, 2, 4, 5, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1148291678-172.17.0.14-1599292015359:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45540,DS-e79d19b0-da05-429e-8fe4-3a686165285c,DISK], DatanodeInfoWithStorage[127.0.0.1:41731,DS-23e9856b-3741-4ebd-9634-28f303095b70,DISK], DatanodeInfoWithStorage[127.0.0.1:33835,DS-95a7f06a-841b-45ee-8a96-61573a6533e2,DISK], DatanodeInfoWithStorage[127.0.0.1:43424,DS-6df52c74-7cc8-4dee-8a2c-8a4643853024,DISK], DatanodeInfoWithStorage[127.0.0.1:39850,DS-d556740c-d278-4da8-afa0-1eb23cfd5623,DISK], DatanodeInfoWithStorage[127.0.0.1:44885,DS-520cdb73-9555-46eb-b3e0-941ee385f948,DISK]]; indices=[0, 1, 2, 4, 5, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1148291678-172.17.0.14-1599292015359:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45540,DS-e79d19b0-da05-429e-8fe4-3a686165285c,DISK], DatanodeInfoWithStorage[127.0.0.1:41731,DS-23e9856b-3741-4ebd-9634-28f303095b70,DISK], DatanodeInfoWithStorage[127.0.0.1:33835,DS-95a7f06a-841b-45ee-8a96-61573a6533e2,DISK], DatanodeInfoWithStorage[127.0.0.1:43424,DS-6df52c74-7cc8-4dee-8a2c-8a4643853024,DISK], DatanodeInfoWithStorage[127.0.0.1:39850,DS-d556740c-d278-4da8-afa0-1eb23cfd5623,DISK], DatanodeInfoWithStorage[127.0.0.1:44885,DS-520cdb73-9555-46eb-b3e0-941ee385f948,DISK]]; indices=[0, 1, 2, 4, 5, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1148291678-172.17.0.14-1599292015359:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45540,DS-e79d19b0-da05-429e-8fe4-3a686165285c,DISK], DatanodeInfoWithStorage[127.0.0.1:41731,DS-23e9856b-3741-4ebd-9634-28f303095b70,DISK], DatanodeInfoWithStorage[127.0.0.1:33835,DS-95a7f06a-841b-45ee-8a96-61573a6533e2,DISK], DatanodeInfoWithStorage[127.0.0.1:43424,DS-6df52c74-7cc8-4dee-8a2c-8a4643853024,DISK], DatanodeInfoWithStorage[127.0.0.1:39850,DS-d556740c-d278-4da8-afa0-1eb23cfd5623,DISK], DatanodeInfoWithStorage[127.0.0.1:44885,DS-520cdb73-9555-46eb-b3e0-941ee385f948,DISK]]; indices=[0, 1, 2, 4, 5, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-636809440-172.17.0.14-1599292345554:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37979,DS-50357674-df77-4cfa-a8e0-fe47c2b1490b,DISK], DatanodeInfoWithStorage[127.0.0.1:33633,DS-932c8133-ace6-4736-9566-00558cda9592,DISK], DatanodeInfoWithStorage[127.0.0.1:34716,DS-0fbf7ed8-9366-4163-992b-b5a94f2da605,DISK], DatanodeInfoWithStorage[127.0.0.1:37968,DS-e90b84b0-f87d-4f2d-a3f9-c605e7839c3b,DISK], DatanodeInfoWithStorage[127.0.0.1:34145,DS-22974672-762f-447a-a13f-967464ae75ad,DISK], DatanodeInfoWithStorage[127.0.0.1:44147,DS-3701e646-678b-4042-b76c-4a7b5b9539d8,DISK]]; indices=[3, 4, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-636809440-172.17.0.14-1599292345554:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37979,DS-50357674-df77-4cfa-a8e0-fe47c2b1490b,DISK], DatanodeInfoWithStorage[127.0.0.1:33633,DS-932c8133-ace6-4736-9566-00558cda9592,DISK], DatanodeInfoWithStorage[127.0.0.1:34716,DS-0fbf7ed8-9366-4163-992b-b5a94f2da605,DISK], DatanodeInfoWithStorage[127.0.0.1:37968,DS-e90b84b0-f87d-4f2d-a3f9-c605e7839c3b,DISK], DatanodeInfoWithStorage[127.0.0.1:34145,DS-22974672-762f-447a-a13f-967464ae75ad,DISK], DatanodeInfoWithStorage[127.0.0.1:44147,DS-3701e646-678b-4042-b76c-4a7b5b9539d8,DISK]]; indices=[3, 4, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-636809440-172.17.0.14-1599292345554:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37979,DS-50357674-df77-4cfa-a8e0-fe47c2b1490b,DISK], DatanodeInfoWithStorage[127.0.0.1:33633,DS-932c8133-ace6-4736-9566-00558cda9592,DISK], DatanodeInfoWithStorage[127.0.0.1:34716,DS-0fbf7ed8-9366-4163-992b-b5a94f2da605,DISK], DatanodeInfoWithStorage[127.0.0.1:37968,DS-e90b84b0-f87d-4f2d-a3f9-c605e7839c3b,DISK], DatanodeInfoWithStorage[127.0.0.1:34145,DS-22974672-762f-447a-a13f-967464ae75ad,DISK], DatanodeInfoWithStorage[127.0.0.1:44147,DS-3701e646-678b-4042-b76c-4a7b5b9539d8,DISK]]; indices=[3, 4, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-636809440-172.17.0.14-1599292345554:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37979,DS-50357674-df77-4cfa-a8e0-fe47c2b1490b,DISK], DatanodeInfoWithStorage[127.0.0.1:33633,DS-932c8133-ace6-4736-9566-00558cda9592,DISK], DatanodeInfoWithStorage[127.0.0.1:34716,DS-0fbf7ed8-9366-4163-992b-b5a94f2da605,DISK], DatanodeInfoWithStorage[127.0.0.1:37968,DS-e90b84b0-f87d-4f2d-a3f9-c605e7839c3b,DISK], DatanodeInfoWithStorage[127.0.0.1:34145,DS-22974672-762f-447a-a13f-967464ae75ad,DISK], DatanodeInfoWithStorage[127.0.0.1:44147,DS-3701e646-678b-4042-b76c-4a7b5b9539d8,DISK]]; indices=[3, 4, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: Data streamers failed while creating new block streams: [#6: failed, blk_-9223372036854775786_1001, #7: failed, blk_-9223372036854775785_1001, #4: failed, blk_-9223372036854775788_1001, #5: failed, blk_-9223372036854775787_1001]. There are not enough healthy streamers.
stackTrace: java.io.IOException: Data streamers failed while creating new block streams: [#6: failed, blk_-9223372036854775786_1001, #7: failed, blk_-9223372036854775785_1001, #4: failed, blk_-9223372036854775788_1001, #5: failed, blk_-9223372036854775787_1001]. There are not enough healthy streamers.
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamerFailures(DFSStripedOutputStream.java:651)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:567)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:865)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:214)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
	Suppressed: java.io.IOException: Failed: the number of failed blocks = 6 > the number of parity blocks = 3
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamers(DFSStripedOutputStream.java:396)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamerFailures(DFSStripedOutputStream.java:624)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1188)
		at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
		at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
		at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
		at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
		... 11 more



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1083232267-172.17.0.14-1599293311184:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45188,DS-5ef9f7a9-106d-43ff-943e-4c866ddbbef4,DISK], DatanodeInfoWithStorage[127.0.0.1:32930,DS-4ce65241-3857-4277-88da-f88ecc165bc9,DISK], DatanodeInfoWithStorage[127.0.0.1:42245,DS-7aaa4ed0-7810-4313-b1b0-0cb0127f2921,DISK], DatanodeInfoWithStorage[127.0.0.1:42548,DS-c20ca3b6-9797-47fa-90c4-b295ddbe643c,DISK], DatanodeInfoWithStorage[127.0.0.1:36896,DS-53216455-ffe4-4f99-8bdb-fa2b2da3c878,DISK], DatanodeInfoWithStorage[127.0.0.1:42022,DS-aef7114f-96c3-44a9-9232-143b9f1b1c18,DISK]]; indices=[1, 2, 3, 4, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1083232267-172.17.0.14-1599293311184:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45188,DS-5ef9f7a9-106d-43ff-943e-4c866ddbbef4,DISK], DatanodeInfoWithStorage[127.0.0.1:32930,DS-4ce65241-3857-4277-88da-f88ecc165bc9,DISK], DatanodeInfoWithStorage[127.0.0.1:42245,DS-7aaa4ed0-7810-4313-b1b0-0cb0127f2921,DISK], DatanodeInfoWithStorage[127.0.0.1:42548,DS-c20ca3b6-9797-47fa-90c4-b295ddbe643c,DISK], DatanodeInfoWithStorage[127.0.0.1:36896,DS-53216455-ffe4-4f99-8bdb-fa2b2da3c878,DISK], DatanodeInfoWithStorage[127.0.0.1:42022,DS-aef7114f-96c3-44a9-9232-143b9f1b1c18,DISK]]; indices=[1, 2, 3, 4, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1083232267-172.17.0.14-1599293311184:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45188,DS-5ef9f7a9-106d-43ff-943e-4c866ddbbef4,DISK], DatanodeInfoWithStorage[127.0.0.1:32930,DS-4ce65241-3857-4277-88da-f88ecc165bc9,DISK], DatanodeInfoWithStorage[127.0.0.1:42245,DS-7aaa4ed0-7810-4313-b1b0-0cb0127f2921,DISK], DatanodeInfoWithStorage[127.0.0.1:42548,DS-c20ca3b6-9797-47fa-90c4-b295ddbe643c,DISK], DatanodeInfoWithStorage[127.0.0.1:36896,DS-53216455-ffe4-4f99-8bdb-fa2b2da3c878,DISK], DatanodeInfoWithStorage[127.0.0.1:42022,DS-aef7114f-96c3-44a9-9232-143b9f1b1c18,DISK]]; indices=[1, 2, 3, 4, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1083232267-172.17.0.14-1599293311184:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45188,DS-5ef9f7a9-106d-43ff-943e-4c866ddbbef4,DISK], DatanodeInfoWithStorage[127.0.0.1:32930,DS-4ce65241-3857-4277-88da-f88ecc165bc9,DISK], DatanodeInfoWithStorage[127.0.0.1:42245,DS-7aaa4ed0-7810-4313-b1b0-0cb0127f2921,DISK], DatanodeInfoWithStorage[127.0.0.1:42548,DS-c20ca3b6-9797-47fa-90c4-b295ddbe643c,DISK], DatanodeInfoWithStorage[127.0.0.1:36896,DS-53216455-ffe4-4f99-8bdb-fa2b2da3c878,DISK], DatanodeInfoWithStorage[127.0.0.1:42022,DS-aef7114f-96c3-44a9-9232-143b9f1b1c18,DISK]]; indices=[1, 2, 3, 4, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1359102114-172.17.0.14-1599293780634:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34193,DS-792f78e4-af79-4312-b56c-39a14d71ed46,DISK], DatanodeInfoWithStorage[127.0.0.1:36990,DS-81220d40-34d2-474b-8f60-1a4ee8a85dc7,DISK], DatanodeInfoWithStorage[127.0.0.1:45747,DS-486045e3-d9a1-4c6e-aaa6-99eb4bdf73a1,DISK], DatanodeInfoWithStorage[127.0.0.1:34165,DS-76860a62-ffde-476a-8d24-ba551b387dd4,DISK], DatanodeInfoWithStorage[127.0.0.1:44163,DS-3bb53d2b-86e3-4c50-8b22-209c890dc53a,DISK], DatanodeInfoWithStorage[127.0.0.1:36326,DS-6207ad59-8a61-4cce-84fc-b51619f7c73d,DISK]]; indices=[0, 1, 2, 5, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1359102114-172.17.0.14-1599293780634:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34193,DS-792f78e4-af79-4312-b56c-39a14d71ed46,DISK], DatanodeInfoWithStorage[127.0.0.1:36990,DS-81220d40-34d2-474b-8f60-1a4ee8a85dc7,DISK], DatanodeInfoWithStorage[127.0.0.1:45747,DS-486045e3-d9a1-4c6e-aaa6-99eb4bdf73a1,DISK], DatanodeInfoWithStorage[127.0.0.1:34165,DS-76860a62-ffde-476a-8d24-ba551b387dd4,DISK], DatanodeInfoWithStorage[127.0.0.1:44163,DS-3bb53d2b-86e3-4c50-8b22-209c890dc53a,DISK], DatanodeInfoWithStorage[127.0.0.1:36326,DS-6207ad59-8a61-4cce-84fc-b51619f7c73d,DISK]]; indices=[0, 1, 2, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1359102114-172.17.0.14-1599293780634:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34193,DS-792f78e4-af79-4312-b56c-39a14d71ed46,DISK], DatanodeInfoWithStorage[127.0.0.1:36990,DS-81220d40-34d2-474b-8f60-1a4ee8a85dc7,DISK], DatanodeInfoWithStorage[127.0.0.1:45747,DS-486045e3-d9a1-4c6e-aaa6-99eb4bdf73a1,DISK], DatanodeInfoWithStorage[127.0.0.1:34165,DS-76860a62-ffde-476a-8d24-ba551b387dd4,DISK], DatanodeInfoWithStorage[127.0.0.1:44163,DS-3bb53d2b-86e3-4c50-8b22-209c890dc53a,DISK], DatanodeInfoWithStorage[127.0.0.1:36326,DS-6207ad59-8a61-4cce-84fc-b51619f7c73d,DISK]]; indices=[0, 1, 2, 5, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1359102114-172.17.0.14-1599293780634:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34193,DS-792f78e4-af79-4312-b56c-39a14d71ed46,DISK], DatanodeInfoWithStorage[127.0.0.1:36990,DS-81220d40-34d2-474b-8f60-1a4ee8a85dc7,DISK], DatanodeInfoWithStorage[127.0.0.1:45747,DS-486045e3-d9a1-4c6e-aaa6-99eb4bdf73a1,DISK], DatanodeInfoWithStorage[127.0.0.1:34165,DS-76860a62-ffde-476a-8d24-ba551b387dd4,DISK], DatanodeInfoWithStorage[127.0.0.1:44163,DS-3bb53d2b-86e3-4c50-8b22-209c890dc53a,DISK], DatanodeInfoWithStorage[127.0.0.1:36326,DS-6207ad59-8a61-4cce-84fc-b51619f7c73d,DISK]]; indices=[0, 1, 2, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-222183773-172.17.0.14-1599293804617:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37320,DS-2212aaed-6353-4598-9fa3-81e3b8a6ff74,DISK], DatanodeInfoWithStorage[127.0.0.1:33668,DS-dec41609-def0-4a21-bbb4-2886d461d005,DISK], DatanodeInfoWithStorage[127.0.0.1:37043,DS-8d424c4e-1aeb-4cea-a03d-3200982de2bd,DISK], DatanodeInfoWithStorage[127.0.0.1:40579,DS-2517afef-075b-4f0a-aea5-49fcd715774d,DISK], DatanodeInfoWithStorage[127.0.0.1:37753,DS-a754e4eb-96f4-42cc-b7bd-a308d04cbf76,DISK], DatanodeInfoWithStorage[127.0.0.1:35368,DS-3d9570b7-6f62-409c-ac09-8fa18f03d923,DISK]]; indices=[0, 1, 2, 5, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-222183773-172.17.0.14-1599293804617:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37320,DS-2212aaed-6353-4598-9fa3-81e3b8a6ff74,DISK], DatanodeInfoWithStorage[127.0.0.1:33668,DS-dec41609-def0-4a21-bbb4-2886d461d005,DISK], DatanodeInfoWithStorage[127.0.0.1:37043,DS-8d424c4e-1aeb-4cea-a03d-3200982de2bd,DISK], DatanodeInfoWithStorage[127.0.0.1:40579,DS-2517afef-075b-4f0a-aea5-49fcd715774d,DISK], DatanodeInfoWithStorage[127.0.0.1:37753,DS-a754e4eb-96f4-42cc-b7bd-a308d04cbf76,DISK], DatanodeInfoWithStorage[127.0.0.1:35368,DS-3d9570b7-6f62-409c-ac09-8fa18f03d923,DISK]]; indices=[0, 1, 2, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-222183773-172.17.0.14-1599293804617:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37320,DS-2212aaed-6353-4598-9fa3-81e3b8a6ff74,DISK], DatanodeInfoWithStorage[127.0.0.1:33668,DS-dec41609-def0-4a21-bbb4-2886d461d005,DISK], DatanodeInfoWithStorage[127.0.0.1:37043,DS-8d424c4e-1aeb-4cea-a03d-3200982de2bd,DISK], DatanodeInfoWithStorage[127.0.0.1:40579,DS-2517afef-075b-4f0a-aea5-49fcd715774d,DISK], DatanodeInfoWithStorage[127.0.0.1:37753,DS-a754e4eb-96f4-42cc-b7bd-a308d04cbf76,DISK], DatanodeInfoWithStorage[127.0.0.1:35368,DS-3d9570b7-6f62-409c-ac09-8fa18f03d923,DISK]]; indices=[0, 1, 2, 5, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-222183773-172.17.0.14-1599293804617:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37320,DS-2212aaed-6353-4598-9fa3-81e3b8a6ff74,DISK], DatanodeInfoWithStorage[127.0.0.1:33668,DS-dec41609-def0-4a21-bbb4-2886d461d005,DISK], DatanodeInfoWithStorage[127.0.0.1:37043,DS-8d424c4e-1aeb-4cea-a03d-3200982de2bd,DISK], DatanodeInfoWithStorage[127.0.0.1:40579,DS-2517afef-075b-4f0a-aea5-49fcd715774d,DISK], DatanodeInfoWithStorage[127.0.0.1:37753,DS-a754e4eb-96f4-42cc-b7bd-a308d04cbf76,DISK], DatanodeInfoWithStorage[127.0.0.1:35368,DS-3d9570b7-6f62-409c-ac09-8fa18f03d923,DISK]]; indices=[0, 1, 2, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-2032044830-172.17.0.14-1599294588986:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40066,DS-f15e04f3-47a3-4047-9412-35977dafd502,DISK], DatanodeInfoWithStorage[127.0.0.1:43200,DS-214f2766-97e7-4e66-a54e-d600dd1a0d3a,DISK], DatanodeInfoWithStorage[127.0.0.1:33793,DS-8e4da988-2660-42c7-b4a8-d8e5c25d475d,DISK], DatanodeInfoWithStorage[127.0.0.1:36022,DS-dc4c870e-c370-49bf-8889-ba7ca7acf2f9,DISK], DatanodeInfoWithStorage[127.0.0.1:34883,DS-9924616d-1f35-4791-8021-8eb256b573a9,DISK], DatanodeInfoWithStorage[127.0.0.1:35203,DS-02457b81-4f93-4766-8980-f4f189298521,DISK]]; indices=[0, 4, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-2032044830-172.17.0.14-1599294588986:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40066,DS-f15e04f3-47a3-4047-9412-35977dafd502,DISK], DatanodeInfoWithStorage[127.0.0.1:43200,DS-214f2766-97e7-4e66-a54e-d600dd1a0d3a,DISK], DatanodeInfoWithStorage[127.0.0.1:33793,DS-8e4da988-2660-42c7-b4a8-d8e5c25d475d,DISK], DatanodeInfoWithStorage[127.0.0.1:36022,DS-dc4c870e-c370-49bf-8889-ba7ca7acf2f9,DISK], DatanodeInfoWithStorage[127.0.0.1:34883,DS-9924616d-1f35-4791-8021-8eb256b573a9,DISK], DatanodeInfoWithStorage[127.0.0.1:35203,DS-02457b81-4f93-4766-8980-f4f189298521,DISK]]; indices=[0, 4, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-2032044830-172.17.0.14-1599294588986:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40066,DS-f15e04f3-47a3-4047-9412-35977dafd502,DISK], DatanodeInfoWithStorage[127.0.0.1:43200,DS-214f2766-97e7-4e66-a54e-d600dd1a0d3a,DISK], DatanodeInfoWithStorage[127.0.0.1:33793,DS-8e4da988-2660-42c7-b4a8-d8e5c25d475d,DISK], DatanodeInfoWithStorage[127.0.0.1:36022,DS-dc4c870e-c370-49bf-8889-ba7ca7acf2f9,DISK], DatanodeInfoWithStorage[127.0.0.1:34883,DS-9924616d-1f35-4791-8021-8eb256b573a9,DISK], DatanodeInfoWithStorage[127.0.0.1:35203,DS-02457b81-4f93-4766-8980-f4f189298521,DISK]]; indices=[0, 4, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-2032044830-172.17.0.14-1599294588986:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40066,DS-f15e04f3-47a3-4047-9412-35977dafd502,DISK], DatanodeInfoWithStorage[127.0.0.1:43200,DS-214f2766-97e7-4e66-a54e-d600dd1a0d3a,DISK], DatanodeInfoWithStorage[127.0.0.1:33793,DS-8e4da988-2660-42c7-b4a8-d8e5c25d475d,DISK], DatanodeInfoWithStorage[127.0.0.1:36022,DS-dc4c870e-c370-49bf-8889-ba7ca7acf2f9,DISK], DatanodeInfoWithStorage[127.0.0.1:34883,DS-9924616d-1f35-4791-8021-8eb256b573a9,DISK], DatanodeInfoWithStorage[127.0.0.1:35203,DS-02457b81-4f93-4766-8980-f4f189298521,DISK]]; indices=[0, 4, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-58227851-172.17.0.14-1599294920257:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37079,DS-a0ec6671-c2f5-4c3d-be4b-42566c3beb00,DISK], DatanodeInfoWithStorage[127.0.0.1:46685,DS-6133d6da-4169-48e6-ad75-8317d1b1f122,DISK], DatanodeInfoWithStorage[127.0.0.1:34809,DS-794734bf-755f-41ba-ab16-d5d054a6beeb,DISK], DatanodeInfoWithStorage[127.0.0.1:46134,DS-eddff39e-74e4-4efa-bcdf-0f2bf578b092,DISK], DatanodeInfoWithStorage[127.0.0.1:42132,DS-49c0b86e-fd52-4246-a880-4671ff8b36ec,DISK], DatanodeInfoWithStorage[127.0.0.1:41887,DS-bbc9c2d0-7373-43b0-9b7a-6138795ef5ce,DISK], DatanodeInfoWithStorage[127.0.0.1:36167,DS-b79b5536-29f3-4024-a4a9-cde431145527,DISK]]; indices=[0, 1, 2, 3, 5, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-58227851-172.17.0.14-1599294920257:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37079,DS-a0ec6671-c2f5-4c3d-be4b-42566c3beb00,DISK], DatanodeInfoWithStorage[127.0.0.1:46685,DS-6133d6da-4169-48e6-ad75-8317d1b1f122,DISK], DatanodeInfoWithStorage[127.0.0.1:34809,DS-794734bf-755f-41ba-ab16-d5d054a6beeb,DISK], DatanodeInfoWithStorage[127.0.0.1:46134,DS-eddff39e-74e4-4efa-bcdf-0f2bf578b092,DISK], DatanodeInfoWithStorage[127.0.0.1:42132,DS-49c0b86e-fd52-4246-a880-4671ff8b36ec,DISK], DatanodeInfoWithStorage[127.0.0.1:41887,DS-bbc9c2d0-7373-43b0-9b7a-6138795ef5ce,DISK], DatanodeInfoWithStorage[127.0.0.1:36167,DS-b79b5536-29f3-4024-a4a9-cde431145527,DISK]]; indices=[0, 1, 2, 3, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-58227851-172.17.0.14-1599294920257:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37079,DS-a0ec6671-c2f5-4c3d-be4b-42566c3beb00,DISK], DatanodeInfoWithStorage[127.0.0.1:46685,DS-6133d6da-4169-48e6-ad75-8317d1b1f122,DISK], DatanodeInfoWithStorage[127.0.0.1:34809,DS-794734bf-755f-41ba-ab16-d5d054a6beeb,DISK], DatanodeInfoWithStorage[127.0.0.1:46134,DS-eddff39e-74e4-4efa-bcdf-0f2bf578b092,DISK], DatanodeInfoWithStorage[127.0.0.1:42132,DS-49c0b86e-fd52-4246-a880-4671ff8b36ec,DISK], DatanodeInfoWithStorage[127.0.0.1:41887,DS-bbc9c2d0-7373-43b0-9b7a-6138795ef5ce,DISK], DatanodeInfoWithStorage[127.0.0.1:36167,DS-b79b5536-29f3-4024-a4a9-cde431145527,DISK]]; indices=[0, 1, 2, 3, 5, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-58227851-172.17.0.14-1599294920257:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37079,DS-a0ec6671-c2f5-4c3d-be4b-42566c3beb00,DISK], DatanodeInfoWithStorage[127.0.0.1:46685,DS-6133d6da-4169-48e6-ad75-8317d1b1f122,DISK], DatanodeInfoWithStorage[127.0.0.1:34809,DS-794734bf-755f-41ba-ab16-d5d054a6beeb,DISK], DatanodeInfoWithStorage[127.0.0.1:46134,DS-eddff39e-74e4-4efa-bcdf-0f2bf578b092,DISK], DatanodeInfoWithStorage[127.0.0.1:42132,DS-49c0b86e-fd52-4246-a880-4671ff8b36ec,DISK], DatanodeInfoWithStorage[127.0.0.1:41887,DS-bbc9c2d0-7373-43b0-9b7a-6138795ef5ce,DISK], DatanodeInfoWithStorage[127.0.0.1:36167,DS-b79b5536-29f3-4024-a4a9-cde431145527,DISK]]; indices=[0, 1, 2, 3, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1285352358-172.17.0.14-1599295408496:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43148,DS-3cffcefe-a6c4-47fd-8d74-75cffceb5ef4,DISK], DatanodeInfoWithStorage[127.0.0.1:43372,DS-6a3d582a-c632-4c60-8038-2d7da244575e,DISK], DatanodeInfoWithStorage[127.0.0.1:45779,DS-e9718e1e-da48-4e57-b105-d34ad18e043b,DISK], DatanodeInfoWithStorage[127.0.0.1:36674,DS-77a7042f-53a5-4f15-aa1f-8060e4fcb184,DISK], DatanodeInfoWithStorage[127.0.0.1:36010,DS-98278e1b-d777-4b5a-b109-cec8d7048103,DISK], DatanodeInfoWithStorage[127.0.0.1:45278,DS-f55ddb7c-b041-433f-8133-e89870101cc4,DISK]]; indices=[0, 1, 2, 3, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1285352358-172.17.0.14-1599295408496:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43148,DS-3cffcefe-a6c4-47fd-8d74-75cffceb5ef4,DISK], DatanodeInfoWithStorage[127.0.0.1:43372,DS-6a3d582a-c632-4c60-8038-2d7da244575e,DISK], DatanodeInfoWithStorage[127.0.0.1:45779,DS-e9718e1e-da48-4e57-b105-d34ad18e043b,DISK], DatanodeInfoWithStorage[127.0.0.1:36674,DS-77a7042f-53a5-4f15-aa1f-8060e4fcb184,DISK], DatanodeInfoWithStorage[127.0.0.1:36010,DS-98278e1b-d777-4b5a-b109-cec8d7048103,DISK], DatanodeInfoWithStorage[127.0.0.1:45278,DS-f55ddb7c-b041-433f-8133-e89870101cc4,DISK]]; indices=[0, 1, 2, 3, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1285352358-172.17.0.14-1599295408496:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43148,DS-3cffcefe-a6c4-47fd-8d74-75cffceb5ef4,DISK], DatanodeInfoWithStorage[127.0.0.1:43372,DS-6a3d582a-c632-4c60-8038-2d7da244575e,DISK], DatanodeInfoWithStorage[127.0.0.1:45779,DS-e9718e1e-da48-4e57-b105-d34ad18e043b,DISK], DatanodeInfoWithStorage[127.0.0.1:36674,DS-77a7042f-53a5-4f15-aa1f-8060e4fcb184,DISK], DatanodeInfoWithStorage[127.0.0.1:36010,DS-98278e1b-d777-4b5a-b109-cec8d7048103,DISK], DatanodeInfoWithStorage[127.0.0.1:45278,DS-f55ddb7c-b041-433f-8133-e89870101cc4,DISK]]; indices=[0, 1, 2, 3, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1285352358-172.17.0.14-1599295408496:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43148,DS-3cffcefe-a6c4-47fd-8d74-75cffceb5ef4,DISK], DatanodeInfoWithStorage[127.0.0.1:43372,DS-6a3d582a-c632-4c60-8038-2d7da244575e,DISK], DatanodeInfoWithStorage[127.0.0.1:45779,DS-e9718e1e-da48-4e57-b105-d34ad18e043b,DISK], DatanodeInfoWithStorage[127.0.0.1:36674,DS-77a7042f-53a5-4f15-aa1f-8060e4fcb184,DISK], DatanodeInfoWithStorage[127.0.0.1:36010,DS-98278e1b-d777-4b5a-b109-cec8d7048103,DISK], DatanodeInfoWithStorage[127.0.0.1:45278,DS-f55ddb7c-b041-433f-8133-e89870101cc4,DISK]]; indices=[0, 1, 2, 3, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-2136061340-172.17.0.14-1599295820793:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35319,DS-36916836-6786-4784-884b-56cae8997b3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46166,DS-ec08dd5e-c41e-4a3f-9432-11dab82e4f7d,DISK], DatanodeInfoWithStorage[127.0.0.1:38304,DS-0739ef51-d48a-4b45-84e7-4ca593740436,DISK], DatanodeInfoWithStorage[127.0.0.1:36284,DS-426ad8fa-8126-4fca-b6a9-91efc84fde24,DISK], DatanodeInfoWithStorage[127.0.0.1:45274,DS-536ea672-1f11-4d69-ad85-124b8bc4fc85,DISK], DatanodeInfoWithStorage[127.0.0.1:40044,DS-ae2ee9b9-88c2-4ef3-96ed-57caba8bbb32,DISK]]; indices=[0, 2, 4, 5, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-2136061340-172.17.0.14-1599295820793:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35319,DS-36916836-6786-4784-884b-56cae8997b3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46166,DS-ec08dd5e-c41e-4a3f-9432-11dab82e4f7d,DISK], DatanodeInfoWithStorage[127.0.0.1:38304,DS-0739ef51-d48a-4b45-84e7-4ca593740436,DISK], DatanodeInfoWithStorage[127.0.0.1:36284,DS-426ad8fa-8126-4fca-b6a9-91efc84fde24,DISK], DatanodeInfoWithStorage[127.0.0.1:45274,DS-536ea672-1f11-4d69-ad85-124b8bc4fc85,DISK], DatanodeInfoWithStorage[127.0.0.1:40044,DS-ae2ee9b9-88c2-4ef3-96ed-57caba8bbb32,DISK]]; indices=[0, 2, 4, 5, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 5 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=5); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-2136061340-172.17.0.14-1599295820793:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35319,DS-36916836-6786-4784-884b-56cae8997b3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46166,DS-ec08dd5e-c41e-4a3f-9432-11dab82e4f7d,DISK], DatanodeInfoWithStorage[127.0.0.1:38304,DS-0739ef51-d48a-4b45-84e7-4ca593740436,DISK], DatanodeInfoWithStorage[127.0.0.1:36284,DS-426ad8fa-8126-4fca-b6a9-91efc84fde24,DISK], DatanodeInfoWithStorage[127.0.0.1:45274,DS-536ea672-1f11-4d69-ad85-124b8bc4fc85,DISK], DatanodeInfoWithStorage[127.0.0.1:40044,DS-ae2ee9b9-88c2-4ef3-96ed-57caba8bbb32,DISK]]; indices=[0, 2, 4, 5, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-2136061340-172.17.0.14-1599295820793:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35319,DS-36916836-6786-4784-884b-56cae8997b3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46166,DS-ec08dd5e-c41e-4a3f-9432-11dab82e4f7d,DISK], DatanodeInfoWithStorage[127.0.0.1:38304,DS-0739ef51-d48a-4b45-84e7-4ca593740436,DISK], DatanodeInfoWithStorage[127.0.0.1:36284,DS-426ad8fa-8126-4fca-b6a9-91efc84fde24,DISK], DatanodeInfoWithStorage[127.0.0.1:45274,DS-536ea672-1f11-4d69-ad85-124b8bc4fc85,DISK], DatanodeInfoWithStorage[127.0.0.1:40044,DS-ae2ee9b9-88c2-4ef3-96ed-57caba8bbb32,DISK]]; indices=[0, 2, 4, 5, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 6 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=6); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1569474313-172.17.0.14-1599296171396:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45786,DS-ea470f7e-dae5-4b1b-801a-2ce2ccab3504,DISK], DatanodeInfoWithStorage[127.0.0.1:38430,DS-3cc00595-1794-4150-9858-9a50954c8b71,DISK], DatanodeInfoWithStorage[127.0.0.1:36731,DS-8f285205-e64a-4f18-b354-7e4aea337e61,DISK], DatanodeInfoWithStorage[127.0.0.1:36475,DS-f82452aa-c9ab-45d4-8000-679f7848e073,DISK], DatanodeInfoWithStorage[127.0.0.1:37026,DS-c6a87411-b3ee-4f75-8de2-a2cb0102abe8,DISK], DatanodeInfoWithStorage[127.0.0.1:39094,DS-771f4cd0-d868-4fa1-98c4-8e276a4daa00,DISK]]; indices=[2, 3, 4, 5, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1569474313-172.17.0.14-1599296171396:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45786,DS-ea470f7e-dae5-4b1b-801a-2ce2ccab3504,DISK], DatanodeInfoWithStorage[127.0.0.1:38430,DS-3cc00595-1794-4150-9858-9a50954c8b71,DISK], DatanodeInfoWithStorage[127.0.0.1:36731,DS-8f285205-e64a-4f18-b354-7e4aea337e61,DISK], DatanodeInfoWithStorage[127.0.0.1:36475,DS-f82452aa-c9ab-45d4-8000-679f7848e073,DISK], DatanodeInfoWithStorage[127.0.0.1:37026,DS-c6a87411-b3ee-4f75-8de2-a2cb0102abe8,DISK], DatanodeInfoWithStorage[127.0.0.1:39094,DS-771f4cd0-d868-4fa1-98c4-8e276a4daa00,DISK]]; indices=[2, 3, 4, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 6 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=6); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1569474313-172.17.0.14-1599296171396:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45786,DS-ea470f7e-dae5-4b1b-801a-2ce2ccab3504,DISK], DatanodeInfoWithStorage[127.0.0.1:38430,DS-3cc00595-1794-4150-9858-9a50954c8b71,DISK], DatanodeInfoWithStorage[127.0.0.1:36731,DS-8f285205-e64a-4f18-b354-7e4aea337e61,DISK], DatanodeInfoWithStorage[127.0.0.1:36475,DS-f82452aa-c9ab-45d4-8000-679f7848e073,DISK], DatanodeInfoWithStorage[127.0.0.1:37026,DS-c6a87411-b3ee-4f75-8de2-a2cb0102abe8,DISK], DatanodeInfoWithStorage[127.0.0.1:39094,DS-771f4cd0-d868-4fa1-98c4-8e276a4daa00,DISK]]; indices=[2, 3, 4, 5, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1569474313-172.17.0.14-1599296171396:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45786,DS-ea470f7e-dae5-4b1b-801a-2ce2ccab3504,DISK], DatanodeInfoWithStorage[127.0.0.1:38430,DS-3cc00595-1794-4150-9858-9a50954c8b71,DISK], DatanodeInfoWithStorage[127.0.0.1:36731,DS-8f285205-e64a-4f18-b354-7e4aea337e61,DISK], DatanodeInfoWithStorage[127.0.0.1:36475,DS-f82452aa-c9ab-45d4-8000-679f7848e073,DISK], DatanodeInfoWithStorage[127.0.0.1:37026,DS-c6a87411-b3ee-4f75-8de2-a2cb0102abe8,DISK], DatanodeInfoWithStorage[127.0.0.1:39094,DS-771f4cd0-d868-4fa1-98c4-8e276a4daa00,DISK]]; indices=[2, 3, 4, 5, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-128889940-172.17.0.14-1599296533689:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37673,DS-6b4fb867-1169-434c-a795-47689942266d,DISK], DatanodeInfoWithStorage[127.0.0.1:35348,DS-c18a24fa-57cb-4f5e-96c8-a072116909a2,DISK], DatanodeInfoWithStorage[127.0.0.1:46288,DS-dd264f44-0f8b-4fa6-aa09-8429774eaea7,DISK], DatanodeInfoWithStorage[127.0.0.1:45875,DS-a87a1b5d-e7cc-4c11-854e-0645fdebc300,DISK], DatanodeInfoWithStorage[127.0.0.1:33075,DS-7aa21ca2-8db2-4cf7-800e-4aee3bad373e,DISK], DatanodeInfoWithStorage[127.0.0.1:42693,DS-28709707-97fe-4ee8-8a01-3f61f7a9fd4c,DISK]]; indices=[0, 1, 3, 4, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-128889940-172.17.0.14-1599296533689:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37673,DS-6b4fb867-1169-434c-a795-47689942266d,DISK], DatanodeInfoWithStorage[127.0.0.1:35348,DS-c18a24fa-57cb-4f5e-96c8-a072116909a2,DISK], DatanodeInfoWithStorage[127.0.0.1:46288,DS-dd264f44-0f8b-4fa6-aa09-8429774eaea7,DISK], DatanodeInfoWithStorage[127.0.0.1:45875,DS-a87a1b5d-e7cc-4c11-854e-0645fdebc300,DISK], DatanodeInfoWithStorage[127.0.0.1:33075,DS-7aa21ca2-8db2-4cf7-800e-4aee3bad373e,DISK], DatanodeInfoWithStorage[127.0.0.1:42693,DS-28709707-97fe-4ee8-8a01-3f61f7a9fd4c,DISK]]; indices=[0, 1, 3, 4, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-128889940-172.17.0.14-1599296533689:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37673,DS-6b4fb867-1169-434c-a795-47689942266d,DISK], DatanodeInfoWithStorage[127.0.0.1:35348,DS-c18a24fa-57cb-4f5e-96c8-a072116909a2,DISK], DatanodeInfoWithStorage[127.0.0.1:46288,DS-dd264f44-0f8b-4fa6-aa09-8429774eaea7,DISK], DatanodeInfoWithStorage[127.0.0.1:45875,DS-a87a1b5d-e7cc-4c11-854e-0645fdebc300,DISK], DatanodeInfoWithStorage[127.0.0.1:33075,DS-7aa21ca2-8db2-4cf7-800e-4aee3bad373e,DISK], DatanodeInfoWithStorage[127.0.0.1:42693,DS-28709707-97fe-4ee8-8a01-3f61f7a9fd4c,DISK]]; indices=[0, 1, 3, 4, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-128889940-172.17.0.14-1599296533689:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37673,DS-6b4fb867-1169-434c-a795-47689942266d,DISK], DatanodeInfoWithStorage[127.0.0.1:35348,DS-c18a24fa-57cb-4f5e-96c8-a072116909a2,DISK], DatanodeInfoWithStorage[127.0.0.1:46288,DS-dd264f44-0f8b-4fa6-aa09-8429774eaea7,DISK], DatanodeInfoWithStorage[127.0.0.1:45875,DS-a87a1b5d-e7cc-4c11-854e-0645fdebc300,DISK], DatanodeInfoWithStorage[127.0.0.1:33075,DS-7aa21ca2-8db2-4cf7-800e-4aee3bad373e,DISK], DatanodeInfoWithStorage[127.0.0.1:42693,DS-28709707-97fe-4ee8-8a01-3f61f7a9fd4c,DISK]]; indices=[0, 1, 3, 4, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: null
stackTrace: java.lang.IllegalArgumentException
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:127)
	at org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImplTestUtils.<init>(FsDatasetImplTestUtils.java:210)
	at org.apache.hadoop.hdfs.server.datanode.FsDatasetImplTestUtilsFactory.newInstance(FsDatasetImplTestUtilsFactory.java:30)
	at org.apache.hadoop.hdfs.MiniDFSCluster.getFsDatasetTestUtils(MiniDFSCluster.java:1983)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesHelper(MiniDFSCluster.java:2206)
	at org.apache.hadoop.hdfs.MiniDFSCluster.corruptBlockOnDataNodesByDeletingBlockFile(MiniDFSCluster.java:2238)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.corruptBlocks(ReadStripedFileWithDecodingHelper.java:256)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:217)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData#testReadCorruptedDataByDeleting[2]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1979504817-172.17.0.14-1599299959858:blk_-9223372036854775792_1002; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42699,DS-c392e690-17c3-4ba2-aafb-3cc8c1410f10,DISK], DatanodeInfoWithStorage[127.0.0.1:38440,DS-0e84e1b3-9ad3-4262-bc5f-8b347085c56b,DISK], DatanodeInfoWithStorage[127.0.0.1:37282,DS-2b0b45ca-f143-47b6-930f-8148b2dd8e4a,DISK], DatanodeInfoWithStorage[127.0.0.1:33875,DS-3bd9b08f-c3e3-4875-ad08-8f89e70ce3e4,DISK], DatanodeInfoWithStorage[127.0.0.1:35860,DS-b4ee7364-b54e-420d-871c-1cc6a6cdea46,DISK], DatanodeInfoWithStorage[127.0.0.1:39356,DS-ba3d487e-1744-4e7a-826e-01be996cabac,DISK]]; indices=[0, 2, 3, 4, 5, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1979504817-172.17.0.14-1599299959858:blk_-9223372036854775792_1002; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42699,DS-c392e690-17c3-4ba2-aafb-3cc8c1410f10,DISK], DatanodeInfoWithStorage[127.0.0.1:39877,DS-18331d13-e1eb-49f3-b58a-2d728b201b02,DISK], DatanodeInfoWithStorage[127.0.0.1:38440,DS-0e84e1b3-9ad3-4262-bc5f-8b347085c56b,DISK], DatanodeInfoWithStorage[127.0.0.1:37282,DS-2b0b45ca-f143-47b6-930f-8148b2dd8e4a,DISK], DatanodeInfoWithStorage[127.0.0.1:33875,DS-3bd9b08f-c3e3-4875-ad08-8f89e70ce3e4,DISK], DatanodeInfoWithStorage[127.0.0.1:35860,DS-b4ee7364-b54e-420d-871c-1cc6a6cdea46,DISK], DatanodeInfoWithStorage[127.0.0.1:35119,DS-8f01410f-7184-46e7-832a-07deedeab831,DISK], DatanodeInfoWithStorage[127.0.0.1:39356,DS-ba3d487e-1744-4e7a-826e-01be996cabac,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=0, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1979504817-172.17.0.14-1599299959858:blk_-9223372036854775792_1002; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42699,DS-c392e690-17c3-4ba2-aafb-3cc8c1410f10,DISK], DatanodeInfoWithStorage[127.0.0.1:38440,DS-0e84e1b3-9ad3-4262-bc5f-8b347085c56b,DISK], DatanodeInfoWithStorage[127.0.0.1:37282,DS-2b0b45ca-f143-47b6-930f-8148b2dd8e4a,DISK], DatanodeInfoWithStorage[127.0.0.1:33875,DS-3bd9b08f-c3e3-4875-ad08-8f89e70ce3e4,DISK], DatanodeInfoWithStorage[127.0.0.1:35860,DS-b4ee7364-b54e-420d-871c-1cc6a6cdea46,DISK], DatanodeInfoWithStorage[127.0.0.1:39356,DS-ba3d487e-1744-4e7a-826e-01be996cabac,DISK]]; indices=[0, 2, 3, 4, 5, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-1979504817-172.17.0.14-1599299959858:blk_-9223372036854775792_1002; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42699,DS-c392e690-17c3-4ba2-aafb-3cc8c1410f10,DISK], DatanodeInfoWithStorage[127.0.0.1:39877,DS-18331d13-e1eb-49f3-b58a-2d728b201b02,DISK], DatanodeInfoWithStorage[127.0.0.1:38440,DS-0e84e1b3-9ad3-4262-bc5f-8b347085c56b,DISK], DatanodeInfoWithStorage[127.0.0.1:37282,DS-2b0b45ca-f143-47b6-930f-8148b2dd8e4a,DISK], DatanodeInfoWithStorage[127.0.0.1:33875,DS-3bd9b08f-c3e3-4875-ad08-8f89e70ce3e4,DISK], DatanodeInfoWithStorage[127.0.0.1:35860,DS-b4ee7364-b54e-420d-871c-1cc6a6cdea46,DISK], DatanodeInfoWithStorage[127.0.0.1:35119,DS-8f01410f-7184-46e7-832a-07deedeab831,DISK], DatanodeInfoWithStorage[127.0.0.1:39356,DS-ba3d487e-1744-4e7a-826e-01be996cabac,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:341)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingDeletedData.testReadCorruptedDataByDeleting(TestReadStripedFileWithDecodingDeletedData.java:85)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 16 out of 50
v1v1v2v2 failed with probability 8 out of 50
result: might be true error
Total execution time in seconds : 9607

reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate#testDecreaseReplicationBeforeCommitting
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate#testDecreaseReplicationBeforeCommitting
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40117,DS-927fa665-07ea-4960-bbcd-f2bc38615db3,DISK], DatanodeInfoWithStorage[127.0.0.1:38423,DS-88af198b-9ac1-4697-af91-8b3781427a95,DISK], DatanodeInfoWithStorage[127.0.0.1:35487,DS-3ac08c79-36ed-4f3d-bde7-7aa9feceffe5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38423,DS-88af198b-9ac1-4697-af91-8b3781427a95,DISK], DatanodeInfoWithStorage[127.0.0.1:40117,DS-927fa665-07ea-4960-bbcd-f2bc38615db3,DISK], DatanodeInfoWithStorage[127.0.0.1:35487,DS-3ac08c79-36ed-4f3d-bde7-7aa9feceffe5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40117,DS-927fa665-07ea-4960-bbcd-f2bc38615db3,DISK], DatanodeInfoWithStorage[127.0.0.1:38423,DS-88af198b-9ac1-4697-af91-8b3781427a95,DISK], DatanodeInfoWithStorage[127.0.0.1:35487,DS-3ac08c79-36ed-4f3d-bde7-7aa9feceffe5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38423,DS-88af198b-9ac1-4697-af91-8b3781427a95,DISK], DatanodeInfoWithStorage[127.0.0.1:40117,DS-927fa665-07ea-4960-bbcd-f2bc38615db3,DISK], DatanodeInfoWithStorage[127.0.0.1:35487,DS-3ac08c79-36ed-4f3d-bde7-7aa9feceffe5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate#testDecreaseReplicationBeforeCommitting
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44036,DS-f05e89e0-2473-4011-a1bb-956a285d514b,DISK], DatanodeInfoWithStorage[127.0.0.1:37804,DS-4a02e245-36c3-4f17-b104-1ba717368ee6,DISK], DatanodeInfoWithStorage[127.0.0.1:45430,DS-b8a56160-01d3-4bdd-b8b8-dffcfd4a1f40,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45430,DS-b8a56160-01d3-4bdd-b8b8-dffcfd4a1f40,DISK], DatanodeInfoWithStorage[127.0.0.1:37804,DS-4a02e245-36c3-4f17-b104-1ba717368ee6,DISK], DatanodeInfoWithStorage[127.0.0.1:44036,DS-f05e89e0-2473-4011-a1bb-956a285d514b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44036,DS-f05e89e0-2473-4011-a1bb-956a285d514b,DISK], DatanodeInfoWithStorage[127.0.0.1:37804,DS-4a02e245-36c3-4f17-b104-1ba717368ee6,DISK], DatanodeInfoWithStorage[127.0.0.1:45430,DS-b8a56160-01d3-4bdd-b8b8-dffcfd4a1f40,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45430,DS-b8a56160-01d3-4bdd-b8b8-dffcfd4a1f40,DISK], DatanodeInfoWithStorage[127.0.0.1:37804,DS-4a02e245-36c3-4f17-b104-1ba717368ee6,DISK], DatanodeInfoWithStorage[127.0.0.1:44036,DS-f05e89e0-2473-4011-a1bb-956a285d514b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate#testDecreaseReplicationBeforeCommitting
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37500,DS-8ec641be-121e-4cd8-946d-ed3e0df5f015,DISK], DatanodeInfoWithStorage[127.0.0.1:34511,DS-43753bb3-c269-4016-8f61-d1510a1099b6,DISK], DatanodeInfoWithStorage[127.0.0.1:42209,DS-492dcca0-7332-415e-ba40-a749ee9c8bfc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42209,DS-492dcca0-7332-415e-ba40-a749ee9c8bfc,DISK], DatanodeInfoWithStorage[127.0.0.1:34511,DS-43753bb3-c269-4016-8f61-d1510a1099b6,DISK], DatanodeInfoWithStorage[127.0.0.1:37500,DS-8ec641be-121e-4cd8-946d-ed3e0df5f015,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37500,DS-8ec641be-121e-4cd8-946d-ed3e0df5f015,DISK], DatanodeInfoWithStorage[127.0.0.1:34511,DS-43753bb3-c269-4016-8f61-d1510a1099b6,DISK], DatanodeInfoWithStorage[127.0.0.1:42209,DS-492dcca0-7332-415e-ba40-a749ee9c8bfc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42209,DS-492dcca0-7332-415e-ba40-a749ee9c8bfc,DISK], DatanodeInfoWithStorage[127.0.0.1:34511,DS-43753bb3-c269-4016-8f61-d1510a1099b6,DISK], DatanodeInfoWithStorage[127.0.0.1:37500,DS-8ec641be-121e-4cd8-946d-ed3e0df5f015,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate#testDecreaseReplicationBeforeCommitting
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33517,DS-f47df03f-8917-4dd8-89f6-754076cfae01,DISK], DatanodeInfoWithStorage[127.0.0.1:38911,DS-ddd8a797-930d-4d8b-92cc-ed12c8253e84,DISK], DatanodeInfoWithStorage[127.0.0.1:35060,DS-4b7b7d67-d2fb-48f1-aea0-1e50753b8031,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33517,DS-f47df03f-8917-4dd8-89f6-754076cfae01,DISK], DatanodeInfoWithStorage[127.0.0.1:38911,DS-ddd8a797-930d-4d8b-92cc-ed12c8253e84,DISK], DatanodeInfoWithStorage[127.0.0.1:35060,DS-4b7b7d67-d2fb-48f1-aea0-1e50753b8031,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33517,DS-f47df03f-8917-4dd8-89f6-754076cfae01,DISK], DatanodeInfoWithStorage[127.0.0.1:38911,DS-ddd8a797-930d-4d8b-92cc-ed12c8253e84,DISK], DatanodeInfoWithStorage[127.0.0.1:35060,DS-4b7b7d67-d2fb-48f1-aea0-1e50753b8031,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33517,DS-f47df03f-8917-4dd8-89f6-754076cfae01,DISK], DatanodeInfoWithStorage[127.0.0.1:38911,DS-ddd8a797-930d-4d8b-92cc-ed12c8253e84,DISK], DatanodeInfoWithStorage[127.0.0.1:35060,DS-4b7b7d67-d2fb-48f1-aea0-1e50753b8031,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate#testDecreaseReplicationBeforeCommitting
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38848,DS-f827f4f1-1604-499b-a3c3-96ee1ca201ef,DISK], DatanodeInfoWithStorage[127.0.0.1:43699,DS-073307fe-4fad-4f86-aa42-d7696f3e55ec,DISK], DatanodeInfoWithStorage[127.0.0.1:44673,DS-e0828bf0-78b2-47b7-afff-bdf3c9779893,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43699,DS-073307fe-4fad-4f86-aa42-d7696f3e55ec,DISK], DatanodeInfoWithStorage[127.0.0.1:38848,DS-f827f4f1-1604-499b-a3c3-96ee1ca201ef,DISK], DatanodeInfoWithStorage[127.0.0.1:44673,DS-e0828bf0-78b2-47b7-afff-bdf3c9779893,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38848,DS-f827f4f1-1604-499b-a3c3-96ee1ca201ef,DISK], DatanodeInfoWithStorage[127.0.0.1:43699,DS-073307fe-4fad-4f86-aa42-d7696f3e55ec,DISK], DatanodeInfoWithStorage[127.0.0.1:44673,DS-e0828bf0-78b2-47b7-afff-bdf3c9779893,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43699,DS-073307fe-4fad-4f86-aa42-d7696f3e55ec,DISK], DatanodeInfoWithStorage[127.0.0.1:38848,DS-f827f4f1-1604-499b-a3c3-96ee1ca201ef,DISK], DatanodeInfoWithStorage[127.0.0.1:44673,DS-e0828bf0-78b2-47b7-afff-bdf3c9779893,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestDiskspaceQuotaUpdate#testDecreaseReplicationBeforeCommitting
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38336,DS-0090f3a3-0e34-4a5c-a722-38b4d7cf0f16,DISK], DatanodeInfoWithStorage[127.0.0.1:46821,DS-efa468f2-0a5d-400a-bc9f-54fb42c4de94,DISK], DatanodeInfoWithStorage[127.0.0.1:38826,DS-0f7328d1-87af-4497-a745-32f42835520f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46821,DS-efa468f2-0a5d-400a-bc9f-54fb42c4de94,DISK], DatanodeInfoWithStorage[127.0.0.1:38336,DS-0090f3a3-0e34-4a5c-a722-38b4d7cf0f16,DISK], DatanodeInfoWithStorage[127.0.0.1:38826,DS-0f7328d1-87af-4497-a745-32f42835520f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38336,DS-0090f3a3-0e34-4a5c-a722-38b4d7cf0f16,DISK], DatanodeInfoWithStorage[127.0.0.1:46821,DS-efa468f2-0a5d-400a-bc9f-54fb42c4de94,DISK], DatanodeInfoWithStorage[127.0.0.1:38826,DS-0f7328d1-87af-4497-a745-32f42835520f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46821,DS-efa468f2-0a5d-400a-bc9f-54fb42c4de94,DISK], DatanodeInfoWithStorage[127.0.0.1:38336,DS-0090f3a3-0e34-4a5c-a722-38b4d7cf0f16,DISK], DatanodeInfoWithStorage[127.0.0.1:38826,DS-0f7328d1-87af-4497-a745-32f42835520f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 6 out of 50
v1v1v2v2 failed with probability 0 out of 50
result: might be true error
Total execution time in seconds : 3065

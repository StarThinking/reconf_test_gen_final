reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-234362145-172.17.0.12-1599296795185:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33718,DS-550bb4df-c776-4f39-8c69-4ba76ec2a399,DISK], DatanodeInfoWithStorage[127.0.0.1:43263,DS-84b36cb1-4ae0-44a7-b679-a8380f8568f3,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-234362145-172.17.0.12-1599296795185:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33718,DS-550bb4df-c776-4f39-8c69-4ba76ec2a399,DISK], DatanodeInfoWithStorage[127.0.0.1:43263,DS-84b36cb1-4ae0-44a7-b679-a8380f8568f3,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1577592829-172.17.0.12-1599297145691:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39839,DS-232ace72-8e82-4c16-80c8-aa7b3ccf0441,DISK], DatanodeInfoWithStorage[127.0.0.1:33369,DS-5304d023-3796-47aa-a13f-3617046dbb84,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1577592829-172.17.0.12-1599297145691:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39839,DS-232ace72-8e82-4c16-80c8-aa7b3ccf0441,DISK], DatanodeInfoWithStorage[127.0.0.1:33369,DS-5304d023-3796-47aa-a13f-3617046dbb84,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-2024006372-172.17.0.12-1599297312605:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43085,DS-9aa0f2b9-a8f0-4fd5-ac63-29b6ddd44d84,DISK], DatanodeInfoWithStorage[127.0.0.1:41327,DS-05a39987-21fc-4881-8ddc-e2d1447c15f1,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-2024006372-172.17.0.12-1599297312605:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43085,DS-9aa0f2b9-a8f0-4fd5-ac63-29b6ddd44d84,DISK], DatanodeInfoWithStorage[127.0.0.1:41327,DS-05a39987-21fc-4881-8ddc-e2d1447c15f1,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-2020679373-172.17.0.12-1599297471544:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35114,DS-050afaa7-f1e3-47e8-95ee-47b715d2cca8,DISK], DatanodeInfoWithStorage[127.0.0.1:45655,DS-a556b4f0-ca58-46fb-a138-bd70078d353d,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-2020679373-172.17.0.12-1599297471544:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35114,DS-050afaa7-f1e3-47e8-95ee-47b715d2cca8,DISK], DatanodeInfoWithStorage[127.0.0.1:45655,DS-a556b4f0-ca58-46fb-a138-bd70078d353d,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1166187449-172.17.0.12-1599297789493:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41071,DS-098f6836-6e57-4b53-9168-9a378bd5e1ff,DISK], DatanodeInfoWithStorage[127.0.0.1:41374,DS-65f5287c-4e3e-4b5c-a1ba-363cb99ff553,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1166187449-172.17.0.12-1599297789493:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41071,DS-098f6836-6e57-4b53-9168-9a378bd5e1ff,DISK], DatanodeInfoWithStorage[127.0.0.1:41374,DS-65f5287c-4e3e-4b5c-a1ba-363cb99ff553,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1919683894-172.17.0.12-1599298033431:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38533,DS-8f5ead09-c236-4abf-a856-757dfe602097,DISK], DatanodeInfoWithStorage[127.0.0.1:39502,DS-6516a949-0057-4f67-9bfe-06d509cc3cc3,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1919683894-172.17.0.12-1599298033431:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38533,DS-8f5ead09-c236-4abf-a856-757dfe602097,DISK], DatanodeInfoWithStorage[127.0.0.1:39502,DS-6516a949-0057-4f67-9bfe-06d509cc3cc3,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1991079238-172.17.0.12-1599298093814:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38073,DS-b0bcae7e-622e-47c2-b107-ec5b45972b6b,DISK], DatanodeInfoWithStorage[127.0.0.1:36569,DS-e5745e29-7067-432d-b00d-b9b472e35bad,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1991079238-172.17.0.12-1599298093814:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38073,DS-b0bcae7e-622e-47c2-b107-ec5b45972b6b,DISK], DatanodeInfoWithStorage[127.0.0.1:36569,DS-e5745e29-7067-432d-b00d-b9b472e35bad,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1797644532-172.17.0.12-1599298417557:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41412,DS-ca1c3333-b5d7-4f6d-8359-dcd7891ef76b,DISK], DatanodeInfoWithStorage[127.0.0.1:46372,DS-a33105bb-8dea-4f1a-956a-00a8296c0ce3,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1797644532-172.17.0.12-1599298417557:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41412,DS-ca1c3333-b5d7-4f6d-8359-dcd7891ef76b,DISK], DatanodeInfoWithStorage[127.0.0.1:46372,DS-a33105bb-8dea-4f1a-956a-00a8296c0ce3,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-326252824-172.17.0.12-1599298835537:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41090,DS-85343c42-d49d-4eb1-8234-ac2cf4be5ca0,DISK], DatanodeInfoWithStorage[127.0.0.1:38234,DS-325dad58-2654-43d2-97d2-b7b7719bddc8,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-326252824-172.17.0.12-1599298835537:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41090,DS-85343c42-d49d-4eb1-8234-ac2cf4be5ca0,DISK], DatanodeInfoWithStorage[127.0.0.1:38234,DS-325dad58-2654-43d2-97d2-b7b7719bddc8,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-703272583-172.17.0.12-1599299166725:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37863,DS-984a552f-b618-4237-9ab1-b6c91a87a120,DISK], DatanodeInfoWithStorage[127.0.0.1:44616,DS-3ef86b49-4b96-4f23-9fab-ab4a9a781fcd,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-703272583-172.17.0.12-1599299166725:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37863,DS-984a552f-b618-4237-9ab1-b6c91a87a120,DISK], DatanodeInfoWithStorage[127.0.0.1:44616,DS-3ef86b49-4b96-4f23-9fab-ab4a9a781fcd,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-170576978-172.17.0.12-1599299382035:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46736,DS-3d096c9a-1b6e-4347-be24-fcc3c30c4614,DISK], DatanodeInfoWithStorage[127.0.0.1:34043,DS-d99038c2-0e09-4c0d-97df-d1e97d71901f,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-170576978-172.17.0.12-1599299382035:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46736,DS-3d096c9a-1b6e-4347-be24-fcc3c30c4614,DISK], DatanodeInfoWithStorage[127.0.0.1:34043,DS-d99038c2-0e09-4c0d-97df-d1e97d71901f,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-2065585894-172.17.0.12-1599299722763:blk_1073741825_1002; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46332,DS-0f0d34d0-f879-4e9a-9216-312e59e2a7d8,DISK], DatanodeInfoWithStorage[127.0.0.1:39674,DS-420e9466-1f6e-44cc-8528-47147c28baef,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-2065585894-172.17.0.12-1599299722763:blk_1073741825_1002; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46332,DS-0f0d34d0-f879-4e9a-9216-312e59e2a7d8,DISK], DatanodeInfoWithStorage[127.0.0.1:39674,DS-420e9466-1f6e-44cc-8528-47147c28baef,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-220525018-172.17.0.12-1599299833704:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36861,DS-0f8cdd48-1d4f-4895-af2c-61a58ed69348,DISK], DatanodeInfoWithStorage[127.0.0.1:44130,DS-3b67fbf6-3688-4a18-ae97-c0904cf2335f,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-220525018-172.17.0.12-1599299833704:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36861,DS-0f8cdd48-1d4f-4895-af2c-61a58ed69348,DISK], DatanodeInfoWithStorage[127.0.0.1:44130,DS-3b67fbf6-3688-4a18-ae97-c0904cf2335f,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1415672872-172.17.0.12-1599300033010:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37043,DS-bc754bbd-7fe6-4924-b3e8-45f2d07361d7,DISK], DatanodeInfoWithStorage[127.0.0.1:37826,DS-c3454628-ee36-478a-8594-8ed2318a59f8,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1415672872-172.17.0.12-1599300033010:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37043,DS-bc754bbd-7fe6-4924-b3e8-45f2d07361d7,DISK], DatanodeInfoWithStorage[127.0.0.1:37826,DS-c3454628-ee36-478a-8594-8ed2318a59f8,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-132181336-172.17.0.12-1599300134052:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38689,DS-953f5604-8aaf-4301-88bc-b973e5dce4ca,DISK], DatanodeInfoWithStorage[127.0.0.1:34066,DS-43f9593f-5913-4567-9efa-e66722817781,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-132181336-172.17.0.12-1599300134052:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38689,DS-953f5604-8aaf-4301-88bc-b973e5dce4ca,DISK], DatanodeInfoWithStorage[127.0.0.1:34066,DS-43f9593f-5913-4567-9efa-e66722817781,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1934508673-172.17.0.12-1599300197255:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36219,DS-366f74d4-17fa-4d3c-a50c-9cde4694f519,DISK], DatanodeInfoWithStorage[127.0.0.1:42266,DS-fcd6564e-5cfd-4683-9418-5cd39acb3728,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1934508673-172.17.0.12-1599300197255:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36219,DS-366f74d4-17fa-4d3c-a50c-9cde4694f519,DISK], DatanodeInfoWithStorage[127.0.0.1:42266,DS-fcd6564e-5cfd-4683-9418-5cd39acb3728,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-557242671-172.17.0.12-1599300499776:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36364,DS-07f07a67-ca7a-4149-8b45-6041c4c9e4f6,DISK], DatanodeInfoWithStorage[127.0.0.1:40343,DS-3a9e6178-0b7c-4528-910a-408d0d6742fe,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-557242671-172.17.0.12-1599300499776:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36364,DS-07f07a67-ca7a-4149-8b45-6041c4c9e4f6,DISK], DatanodeInfoWithStorage[127.0.0.1:40343,DS-3a9e6178-0b7c-4528-910a-408d0d6742fe,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1347231556-172.17.0.12-1599300951221:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44609,DS-3ff8f0e9-5de1-482a-bcdf-87dc96027965,DISK], DatanodeInfoWithStorage[127.0.0.1:37170,DS-6315bd5e-5e2e-4716-a424-0f63f4902cc1,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1347231556-172.17.0.12-1599300951221:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44609,DS-3ff8f0e9-5de1-482a-bcdf-87dc96027965,DISK], DatanodeInfoWithStorage[127.0.0.1:37170,DS-6315bd5e-5e2e-4716-a424-0f63f4902cc1,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1638892680-172.17.0.12-1599301057126:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37155,DS-396165db-c935-43ac-b314-5b49998de2ec,DISK], DatanodeInfoWithStorage[127.0.0.1:35729,DS-cf7256a4-4cfd-4c0e-ae74-00791df54e78,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1638892680-172.17.0.12-1599301057126:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37155,DS-396165db-c935-43ac-b314-5b49998de2ec,DISK], DatanodeInfoWithStorage[127.0.0.1:35729,DS-cf7256a4-4cfd-4c0e-ae74-00791df54e78,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1552036909-172.17.0.12-1599301120630:blk_1073741825_1002; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36790,DS-a9bc340c-87e1-457f-ab2e-9498432ae1cf,DISK], DatanodeInfoWithStorage[127.0.0.1:43984,DS-0eefc195-dd27-45dc-9065-3e1ce9e14622,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1552036909-172.17.0.12-1599301120630:blk_1073741825_1002; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36790,DS-a9bc340c-87e1-457f-ab2e-9498432ae1cf,DISK], DatanodeInfoWithStorage[127.0.0.1:43984,DS-0eefc195-dd27-45dc-9065-3e1ce9e14622,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-973439355-172.17.0.12-1599301231656:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36273,DS-49ee559d-4b47-4c43-b743-7e50acd8f0ec,DISK], DatanodeInfoWithStorage[127.0.0.1:33783,DS-0717dfdb-ad6c-4237-99cc-43ba4a9d6a31,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-973439355-172.17.0.12-1599301231656:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36273,DS-49ee559d-4b47-4c43-b743-7e50acd8f0ec,DISK], DatanodeInfoWithStorage[127.0.0.1:33783,DS-0717dfdb-ad6c-4237-99cc-43ba4a9d6a31,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1092396383-172.17.0.12-1599301389128:blk_1073741825_1002; getBlockSize()=4; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40124,DS-55a81a5f-b78a-4681-a779-3d88f2eae375,DISK], DatanodeInfoWithStorage[127.0.0.1:35727,DS-b7acd4a3-22e3-4cb9-b769-c6be2a85ed13,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1092396383-172.17.0.12-1599301389128:blk_1073741825_1002; getBlockSize()=4; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40124,DS-55a81a5f-b78a-4681-a779-3d88f2eae375,DISK], DatanodeInfoWithStorage[127.0.0.1:35727,DS-b7acd4a3-22e3-4cb9-b769-c6be2a85ed13,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-178465022-172.17.0.12-1599301443243:blk_1073741825_1002; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41690,DS-be993e4e-df81-45cb-ab97-e13c0bd39553,DISK], DatanodeInfoWithStorage[127.0.0.1:36300,DS-462c77c0-5326-4bfd-8b0a-1bc38f98151a,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-178465022-172.17.0.12-1599301443243:blk_1073741825_1002; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41690,DS-be993e4e-df81-45cb-ab97-e13c0bd39553,DISK], DatanodeInfoWithStorage[127.0.0.1:36300,DS-462c77c0-5326-4bfd-8b0a-1bc38f98151a,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1337398963-172.17.0.12-1599301712395:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44490,DS-7be82598-7787-4e00-b903-b040bb68b970,DISK], DatanodeInfoWithStorage[127.0.0.1:46501,DS-b30a788a-d566-4a97-9d32-9c5decd24119,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1337398963-172.17.0.12-1599301712395:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44490,DS-7be82598-7787-4e00-b903-b040bb68b970,DISK], DatanodeInfoWithStorage[127.0.0.1:46501,DS-b30a788a-d566-4a97-9d32-9c5decd24119,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-539248921-172.17.0.12-1599301773477:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40301,DS-0220dca4-3931-4c05-9083-1323bec1ba20,DISK], DatanodeInfoWithStorage[127.0.0.1:34186,DS-1e0e38ed-4145-4480-a3cb-881c55145aef,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-539248921-172.17.0.12-1599301773477:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40301,DS-0220dca4-3931-4c05-9083-1323bec1ba20,DISK], DatanodeInfoWithStorage[127.0.0.1:34186,DS-1e0e38ed-4145-4480-a3cb-881c55145aef,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-598554604-172.17.0.12-1599301941236:blk_1073741825_1002; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46455,DS-27fc0c1c-546f-42b7-bae0-99115db141fe,DISK], DatanodeInfoWithStorage[127.0.0.1:40968,DS-5d8063b5-0ddc-4b3f-9800-af4ac29467ae,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-598554604-172.17.0.12-1599301941236:blk_1073741825_1002; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46455,DS-27fc0c1c-546f-42b7-bae0-99115db141fe,DISK], DatanodeInfoWithStorage[127.0.0.1:40968,DS-5d8063b5-0ddc-4b3f-9800-af4ac29467ae,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-2008089232-172.17.0.12-1599302098540:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44959,DS-df049cc8-84f0-4ca5-8c17-c7953f64a3e5,DISK], DatanodeInfoWithStorage[127.0.0.1:40896,DS-277ba847-ac03-489b-bd5a-d67a52aa2630,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-2008089232-172.17.0.12-1599302098540:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44959,DS-df049cc8-84f0-4ca5-8c17-c7953f64a3e5,DISK], DatanodeInfoWithStorage[127.0.0.1:40896,DS-277ba847-ac03-489b-bd5a-d67a52aa2630,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-2041371504-172.17.0.12-1599302255153:blk_1073741825_1002; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42108,DS-7e4cb84c-615a-4ab3-a1f1-9c5dac2813d3,DISK], DatanodeInfoWithStorage[127.0.0.1:34130,DS-f65dc7b7-ae0e-4b86-8870-e4f4d8b13227,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-2041371504-172.17.0.12-1599302255153:blk_1073741825_1002; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42108,DS-7e4cb84c-615a-4ab3-a1f1-9c5dac2813d3,DISK], DatanodeInfoWithStorage[127.0.0.1:34130,DS-f65dc7b7-ae0e-4b86-8870-e4f4d8b13227,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1495285721-172.17.0.12-1599302552318:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42406,DS-48dd34e6-d968-4644-8839-a1b47a292350,DISK], DatanodeInfoWithStorage[127.0.0.1:41793,DS-6c44daab-c8fc-4a6f-92de-78357318d6d2,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1495285721-172.17.0.12-1599302552318:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42406,DS-48dd34e6-d968-4644-8839-a1b47a292350,DISK], DatanodeInfoWithStorage[127.0.0.1:41793,DS-6c44daab-c8fc-4a6f-92de-78357318d6d2,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-723313930-172.17.0.12-1599302665904:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46006,DS-9e10d37c-8eca-4fb6-a4f9-66d592ce25ad,DISK], DatanodeInfoWithStorage[127.0.0.1:43485,DS-325bc2d7-a93c-4bbe-a2a9-2f4e13b95372,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-723313930-172.17.0.12-1599302665904:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46006,DS-9e10d37c-8eca-4fb6-a4f9-66d592ce25ad,DISK], DatanodeInfoWithStorage[127.0.0.1:43485,DS-325bc2d7-a93c-4bbe-a2a9-2f4e13b95372,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1088977126-172.17.0.12-1599303028161:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43511,DS-9b1e31de-1edc-4713-9477-8914c2f7f63c,DISK], DatanodeInfoWithStorage[127.0.0.1:39088,DS-5b44b790-46d0-48dc-8a93-f78f6e142e07,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1088977126-172.17.0.12-1599303028161:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43511,DS-9b1e31de-1edc-4713-9477-8914c2f7f63c,DISK], DatanodeInfoWithStorage[127.0.0.1:39088,DS-5b44b790-46d0-48dc-8a93-f78f6e142e07,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-459111489-172.17.0.12-1599303194804:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35843,DS-b8a352c6-5961-4577-95de-08010c4b70e5,DISK], DatanodeInfoWithStorage[127.0.0.1:35174,DS-04acf03c-e157-4a71-99c4-e98d0e9dfb15,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-459111489-172.17.0.12-1599303194804:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35843,DS-b8a352c6-5961-4577-95de-08010c4b70e5,DISK], DatanodeInfoWithStorage[127.0.0.1:35174,DS-04acf03c-e157-4a71-99c4-e98d0e9dfb15,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-151418659-172.17.0.12-1599303311265:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41225,DS-48af0702-e7dc-415f-a921-805faf484f95,DISK], DatanodeInfoWithStorage[127.0.0.1:37267,DS-901ee2bd-038b-4b63-a68b-535217f08ec6,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-151418659-172.17.0.12-1599303311265:blk_1073741826_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41225,DS-48af0702-e7dc-415f-a921-805faf484f95,DISK], DatanodeInfoWithStorage[127.0.0.1:37267,DS-901ee2bd-038b-4b63-a68b-535217f08ec6,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1878397264-172.17.0.12-1599303377201:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44081,DS-b308e2c3-4447-4c3a-a062-a5c4d992ac6e,DISK], DatanodeInfoWithStorage[127.0.0.1:40549,DS-907bdb19-7655-4941-a784-bb2af2ba5db5,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1878397264-172.17.0.12-1599303377201:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44081,DS-b308e2c3-4447-4c3a-a062-a5c4d992ac6e,DISK], DatanodeInfoWithStorage[127.0.0.1:40549,DS-907bdb19-7655-4941-a784-bb2af2ba5db5,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1945743638-172.17.0.12-1599303845874:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37965,DS-4fa9d44f-7466-45ea-ab10-320ecff37d42,DISK], DatanodeInfoWithStorage[127.0.0.1:42198,DS-b338fa3e-2db7-4b87-81e9-68eb44a3e87e,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1945743638-172.17.0.12-1599303845874:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37965,DS-4fa9d44f-7466-45ea-ab10-320ecff37d42,DISK], DatanodeInfoWithStorage[127.0.0.1:42198,DS-b338fa3e-2db7-4b87-81e9-68eb44a3e87e,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-507772576-172.17.0.12-1599304020028:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42627,DS-b87ddf10-e35f-42bd-9d7b-2f4141f3c35f,DISK], DatanodeInfoWithStorage[127.0.0.1:33918,DS-3489607a-4b94-43b9-9b56-f95d59e2bd52,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-507772576-172.17.0.12-1599304020028:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42627,DS-b87ddf10-e35f-42bd-9d7b-2f4141f3c35f,DISK], DatanodeInfoWithStorage[127.0.0.1:33918,DS-3489607a-4b94-43b9-9b56-f95d59e2bd52,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1581012622-172.17.0.12-1599304323517:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43863,DS-66886fcb-e8ad-474b-a200-3ccd36675385,DISK], DatanodeInfoWithStorage[127.0.0.1:45728,DS-7e4afb29-4380-4f68-9a57-77a90d7d6814,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1581012622-172.17.0.12-1599304323517:blk_1073741826_1003; getBlockSize()=2; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43863,DS-66886fcb-e8ad-474b-a200-3ccd36675385,DISK], DatanodeInfoWithStorage[127.0.0.1:45728,DS-7e4afb29-4380-4f68-9a57-77a90d7d6814,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication#testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF
reconfPoint: -1
result: -1
failureMessage: Cannot obtain block length for LocatedBlock{BP-1554152928-172.17.0.12-1599304478752:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37337,DS-54727409-493e-4ad6-ae94-6e29facb2061,DISK], DatanodeInfoWithStorage[127.0.0.1:38386,DS-1d26439d-8ccc-4bf9-b5de-7c99ee805eb0,DISK]]}
stackTrace: org.apache.hadoop.hdfs.CannotObtainBlockLengthException: Cannot obtain block length for LocatedBlock{BP-1554152928-172.17.0.12-1599304478752:blk_1073741825_1002; getBlockSize()=1; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37337,DS-54727409-493e-4ad6-ae94-6e29facb2061,DISK], DatanodeInfoWithStorage[127.0.0.1:38386,DS-1d26439d-8ccc-4bf9-b5de-7c99ee805eb0,DISK]]}
	at org.apache.hadoop.hdfs.DFSInputStream.readBlockLength(DFSInputStream.java:363)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:201)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:185)
	at org.apache.hadoop.hdfs.DFSClient.openInternal(DFSClient.java:1048)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1011)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:319)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:327)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:899)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.verifyFileContent(TestReplaceDatanodeFailureReplication.java:227)
	at org.apache.hadoop.hdfs.TestReplaceDatanodeFailureReplication.testLessNumberOfLiveDatanodesThanWriteReplaceDatanodeOnFailureRF(TestReplaceDatanodeFailureReplication.java:145)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 27 out of 50
v1v1v2v2 failed with probability 11 out of 50
result: might be true error
Total execution time in seconds : 8020

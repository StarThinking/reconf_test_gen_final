reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithMixedConfig
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithMixedConfig
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46264,DS-78e0e476-67e4-413a-80f7-031d45687893,DISK], DatanodeInfoWithStorage[127.0.0.1:34029,DS-6d5ee762-6e15-48c1-b962-c7582157168e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46264,DS-78e0e476-67e4-413a-80f7-031d45687893,DISK], DatanodeInfoWithStorage[127.0.0.1:34029,DS-6d5ee762-6e15-48c1-b962-c7582157168e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46264,DS-78e0e476-67e4-413a-80f7-031d45687893,DISK], DatanodeInfoWithStorage[127.0.0.1:34029,DS-6d5ee762-6e15-48c1-b962-c7582157168e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46264,DS-78e0e476-67e4-413a-80f7-031d45687893,DISK], DatanodeInfoWithStorage[127.0.0.1:34029,DS-6d5ee762-6e15-48c1-b962-c7582157168e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithMixedConfig
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41712,DS-68303c6f-211f-46fc-8ec5-cc601082b831,DISK], DatanodeInfoWithStorage[127.0.0.1:44185,DS-e78c149b-bf2a-4a84-a900-7849d0d04ab3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44185,DS-e78c149b-bf2a-4a84-a900-7849d0d04ab3,DISK], DatanodeInfoWithStorage[127.0.0.1:41712,DS-68303c6f-211f-46fc-8ec5-cc601082b831,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41712,DS-68303c6f-211f-46fc-8ec5-cc601082b831,DISK], DatanodeInfoWithStorage[127.0.0.1:44185,DS-e78c149b-bf2a-4a84-a900-7849d0d04ab3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44185,DS-e78c149b-bf2a-4a84-a900-7849d0d04ab3,DISK], DatanodeInfoWithStorage[127.0.0.1:41712,DS-68303c6f-211f-46fc-8ec5-cc601082b831,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithMixedConfig
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44195,DS-6e7b5e98-30e8-4d86-9502-751caa9ece79,DISK], DatanodeInfoWithStorage[127.0.0.1:36143,DS-19a98c85-a4a1-45c0-9e5a-78a7988392cd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44195,DS-6e7b5e98-30e8-4d86-9502-751caa9ece79,DISK], DatanodeInfoWithStorage[127.0.0.1:36143,DS-19a98c85-a4a1-45c0-9e5a-78a7988392cd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44195,DS-6e7b5e98-30e8-4d86-9502-751caa9ece79,DISK], DatanodeInfoWithStorage[127.0.0.1:36143,DS-19a98c85-a4a1-45c0-9e5a-78a7988392cd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44195,DS-6e7b5e98-30e8-4d86-9502-751caa9ece79,DISK], DatanodeInfoWithStorage[127.0.0.1:36143,DS-19a98c85-a4a1-45c0-9e5a-78a7988392cd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithMixedConfig
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43773,DS-4a6c4380-f554-4d1b-9f3f-924bbb63abfa,DISK], DatanodeInfoWithStorage[127.0.0.1:46700,DS-14bd8f2f-3cc1-4646-b190-79b622760369,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43773,DS-4a6c4380-f554-4d1b-9f3f-924bbb63abfa,DISK], DatanodeInfoWithStorage[127.0.0.1:46700,DS-14bd8f2f-3cc1-4646-b190-79b622760369,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43773,DS-4a6c4380-f554-4d1b-9f3f-924bbb63abfa,DISK], DatanodeInfoWithStorage[127.0.0.1:46700,DS-14bd8f2f-3cc1-4646-b190-79b622760369,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43773,DS-4a6c4380-f554-4d1b-9f3f-924bbb63abfa,DISK], DatanodeInfoWithStorage[127.0.0.1:46700,DS-14bd8f2f-3cc1-4646-b190-79b622760369,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithMixedConfig
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44042,DS-23308fe5-becf-4d59-845e-be397a03b242,DISK], DatanodeInfoWithStorage[127.0.0.1:34966,DS-17618f42-a8a9-418e-93ed-2afe34bd426b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34966,DS-17618f42-a8a9-418e-93ed-2afe34bd426b,DISK], DatanodeInfoWithStorage[127.0.0.1:44042,DS-23308fe5-becf-4d59-845e-be397a03b242,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44042,DS-23308fe5-becf-4d59-845e-be397a03b242,DISK], DatanodeInfoWithStorage[127.0.0.1:34966,DS-17618f42-a8a9-418e-93ed-2afe34bd426b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34966,DS-17618f42-a8a9-418e-93ed-2afe34bd426b,DISK], DatanodeInfoWithStorage[127.0.0.1:44042,DS-23308fe5-becf-4d59-845e-be397a03b242,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithMixedConfig
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42693,DS-dc7b579c-0d98-4a0d-94e5-6a4c42f45b18,DISK], DatanodeInfoWithStorage[127.0.0.1:38969,DS-44eaf75d-dd9d-4e0e-b22e-e6d7fc68ce5c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42693,DS-dc7b579c-0d98-4a0d-94e5-6a4c42f45b18,DISK], DatanodeInfoWithStorage[127.0.0.1:38969,DS-44eaf75d-dd9d-4e0e-b22e-e6d7fc68ce5c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42693,DS-dc7b579c-0d98-4a0d-94e5-6a4c42f45b18,DISK], DatanodeInfoWithStorage[127.0.0.1:38969,DS-44eaf75d-dd9d-4e0e-b22e-e6d7fc68ce5c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42693,DS-dc7b579c-0d98-4a0d-94e5-6a4c42f45b18,DISK], DatanodeInfoWithStorage[127.0.0.1:38969,DS-44eaf75d-dd9d-4e0e-b22e-e6d7fc68ce5c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithMixedConfig
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37110,DS-c504bf7f-7376-4d98-9d2b-ec847516eb05,DISK], DatanodeInfoWithStorage[127.0.0.1:39110,DS-13eac533-14b3-4461-909f-10166a1650b0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39110,DS-13eac533-14b3-4461-909f-10166a1650b0,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-c504bf7f-7376-4d98-9d2b-ec847516eb05,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37110,DS-c504bf7f-7376-4d98-9d2b-ec847516eb05,DISK], DatanodeInfoWithStorage[127.0.0.1:39110,DS-13eac533-14b3-4461-909f-10166a1650b0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39110,DS-13eac533-14b3-4461-909f-10166a1650b0,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-c504bf7f-7376-4d98-9d2b-ec847516eb05,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithMixedConfig
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34785,DS-30164c9a-5a44-429f-bb0c-500b514d10e8,DISK], DatanodeInfoWithStorage[127.0.0.1:45471,DS-f568e798-41a0-4c69-bfb2-9725fa5d4ff6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45471,DS-f568e798-41a0-4c69-bfb2-9725fa5d4ff6,DISK], DatanodeInfoWithStorage[127.0.0.1:34785,DS-30164c9a-5a44-429f-bb0c-500b514d10e8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34785,DS-30164c9a-5a44-429f-bb0c-500b514d10e8,DISK], DatanodeInfoWithStorage[127.0.0.1:45471,DS-f568e798-41a0-4c69-bfb2-9725fa5d4ff6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45471,DS-f568e798-41a0-4c69-bfb2-9725fa5d4ff6,DISK], DatanodeInfoWithStorage[127.0.0.1:34785,DS-30164c9a-5a44-429f-bb0c-500b514d10e8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithMixedConfig
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43537,DS-862e3504-7f77-4562-ae71-fda3d8034c0c,DISK], DatanodeInfoWithStorage[127.0.0.1:39147,DS-905e717c-c876-4ad2-b4d8-a382338325db,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43537,DS-862e3504-7f77-4562-ae71-fda3d8034c0c,DISK], DatanodeInfoWithStorage[127.0.0.1:39147,DS-905e717c-c876-4ad2-b4d8-a382338325db,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43537,DS-862e3504-7f77-4562-ae71-fda3d8034c0c,DISK], DatanodeInfoWithStorage[127.0.0.1:39147,DS-905e717c-c876-4ad2-b4d8-a382338325db,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43537,DS-862e3504-7f77-4562-ae71-fda3d8034c0c,DISK], DatanodeInfoWithStorage[127.0.0.1:39147,DS-905e717c-c876-4ad2-b4d8-a382338325db,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testOpenFilesWithMixedConfig
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43665,DS-4976e858-bef3-4cee-9b05-0f5121fad4df,DISK], DatanodeInfoWithStorage[127.0.0.1:43058,DS-260a200b-5c23-4a2f-872a-b0e3abccae84,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43665,DS-4976e858-bef3-4cee-9b05-0f5121fad4df,DISK], DatanodeInfoWithStorage[127.0.0.1:43058,DS-260a200b-5c23-4a2f-872a-b0e3abccae84,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43665,DS-4976e858-bef3-4cee-9b05-0f5121fad4df,DISK], DatanodeInfoWithStorage[127.0.0.1:43058,DS-260a200b-5c23-4a2f-872a-b0e3abccae84,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43665,DS-4976e858-bef3-4cee-9b05-0f5121fad4df,DISK], DatanodeInfoWithStorage[127.0.0.1:43058,DS-260a200b-5c23-4a2f-872a-b0e3abccae84,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 16
v1v1v2v2 failed with probability 0 out of 16
result: might be true error
Total execution time in seconds : 1046

reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35541,DS-b987358b-35df-45f0-bf28-de3a37279a37,DISK], DatanodeInfoWithStorage[127.0.0.1:43057,DS-7b2c216f-203a-45af-b7d7-3df2d780b765,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35541,DS-b987358b-35df-45f0-bf28-de3a37279a37,DISK], DatanodeInfoWithStorage[127.0.0.1:43057,DS-7b2c216f-203a-45af-b7d7-3df2d780b765,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35541,DS-b987358b-35df-45f0-bf28-de3a37279a37,DISK], DatanodeInfoWithStorage[127.0.0.1:43057,DS-7b2c216f-203a-45af-b7d7-3df2d780b765,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35541,DS-b987358b-35df-45f0-bf28-de3a37279a37,DISK], DatanodeInfoWithStorage[127.0.0.1:43057,DS-7b2c216f-203a-45af-b7d7-3df2d780b765,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36035,DS-f18b4292-6a18-48cc-9480-b672b1759404,DISK], DatanodeInfoWithStorage[127.0.0.1:42282,DS-23b354a7-427e-4a29-ac2c-cc6e06c0bd0e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36035,DS-f18b4292-6a18-48cc-9480-b672b1759404,DISK], DatanodeInfoWithStorage[127.0.0.1:42282,DS-23b354a7-427e-4a29-ac2c-cc6e06c0bd0e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36035,DS-f18b4292-6a18-48cc-9480-b672b1759404,DISK], DatanodeInfoWithStorage[127.0.0.1:42282,DS-23b354a7-427e-4a29-ac2c-cc6e06c0bd0e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36035,DS-f18b4292-6a18-48cc-9480-b672b1759404,DISK], DatanodeInfoWithStorage[127.0.0.1:42282,DS-23b354a7-427e-4a29-ac2c-cc6e06c0bd0e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39977,DS-bbe90ff6-fedc-419c-a6de-c30e9ed326b7,DISK], DatanodeInfoWithStorage[127.0.0.1:34226,DS-36b7f10d-bb86-4c22-9fdb-7968accc0272,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34226,DS-36b7f10d-bb86-4c22-9fdb-7968accc0272,DISK], DatanodeInfoWithStorage[127.0.0.1:39977,DS-bbe90ff6-fedc-419c-a6de-c30e9ed326b7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39977,DS-bbe90ff6-fedc-419c-a6de-c30e9ed326b7,DISK], DatanodeInfoWithStorage[127.0.0.1:34226,DS-36b7f10d-bb86-4c22-9fdb-7968accc0272,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34226,DS-36b7f10d-bb86-4c22-9fdb-7968accc0272,DISK], DatanodeInfoWithStorage[127.0.0.1:39977,DS-bbe90ff6-fedc-419c-a6de-c30e9ed326b7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39350,DS-01da19ec-fa94-4bf4-b600-daa06f39bbb4,DISK], DatanodeInfoWithStorage[127.0.0.1:34343,DS-4a1b7522-ed57-429f-8151-6dedfb6e45e7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39350,DS-01da19ec-fa94-4bf4-b600-daa06f39bbb4,DISK], DatanodeInfoWithStorage[127.0.0.1:34343,DS-4a1b7522-ed57-429f-8151-6dedfb6e45e7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39350,DS-01da19ec-fa94-4bf4-b600-daa06f39bbb4,DISK], DatanodeInfoWithStorage[127.0.0.1:34343,DS-4a1b7522-ed57-429f-8151-6dedfb6e45e7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39350,DS-01da19ec-fa94-4bf4-b600-daa06f39bbb4,DISK], DatanodeInfoWithStorage[127.0.0.1:34343,DS-4a1b7522-ed57-429f-8151-6dedfb6e45e7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44126,DS-890f76a7-b8d5-431a-8889-3e2b7a810f55,DISK], DatanodeInfoWithStorage[127.0.0.1:37163,DS-b84d45b1-2c21-4b73-ab92-0473e19151f6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44126,DS-890f76a7-b8d5-431a-8889-3e2b7a810f55,DISK], DatanodeInfoWithStorage[127.0.0.1:37163,DS-b84d45b1-2c21-4b73-ab92-0473e19151f6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44126,DS-890f76a7-b8d5-431a-8889-3e2b7a810f55,DISK], DatanodeInfoWithStorage[127.0.0.1:37163,DS-b84d45b1-2c21-4b73-ab92-0473e19151f6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44126,DS-890f76a7-b8d5-431a-8889-3e2b7a810f55,DISK], DatanodeInfoWithStorage[127.0.0.1:37163,DS-b84d45b1-2c21-4b73-ab92-0473e19151f6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38922,DS-90119750-12c3-4f9e-a302-632488c19cbf,DISK], DatanodeInfoWithStorage[127.0.0.1:42362,DS-90de16fe-e843-4047-a029-2314118e77a2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38922,DS-90119750-12c3-4f9e-a302-632488c19cbf,DISK], DatanodeInfoWithStorage[127.0.0.1:42362,DS-90de16fe-e843-4047-a029-2314118e77a2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38922,DS-90119750-12c3-4f9e-a302-632488c19cbf,DISK], DatanodeInfoWithStorage[127.0.0.1:42362,DS-90de16fe-e843-4047-a029-2314118e77a2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38922,DS-90119750-12c3-4f9e-a302-632488c19cbf,DISK], DatanodeInfoWithStorage[127.0.0.1:42362,DS-90de16fe-e843-4047-a029-2314118e77a2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42317,DS-dd05f77a-f3cc-4f6a-8a47-c37d0da19576,DISK], DatanodeInfoWithStorage[127.0.0.1:46049,DS-0e0c575e-8da4-44ec-8531-b64c27b37dd8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46049,DS-0e0c575e-8da4-44ec-8531-b64c27b37dd8,DISK], DatanodeInfoWithStorage[127.0.0.1:42317,DS-dd05f77a-f3cc-4f6a-8a47-c37d0da19576,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42317,DS-dd05f77a-f3cc-4f6a-8a47-c37d0da19576,DISK], DatanodeInfoWithStorage[127.0.0.1:46049,DS-0e0c575e-8da4-44ec-8531-b64c27b37dd8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46049,DS-0e0c575e-8da4-44ec-8531-b64c27b37dd8,DISK], DatanodeInfoWithStorage[127.0.0.1:42317,DS-dd05f77a-f3cc-4f6a-8a47-c37d0da19576,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37488,DS-22b58f94-fa30-4cec-8856-ade0ceae84d1,DISK], DatanodeInfoWithStorage[127.0.0.1:44938,DS-f9bb7602-d212-4165-8dee-285597fdd360,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37488,DS-22b58f94-fa30-4cec-8856-ade0ceae84d1,DISK], DatanodeInfoWithStorage[127.0.0.1:44938,DS-f9bb7602-d212-4165-8dee-285597fdd360,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37488,DS-22b58f94-fa30-4cec-8856-ade0ceae84d1,DISK], DatanodeInfoWithStorage[127.0.0.1:44938,DS-f9bb7602-d212-4165-8dee-285597fdd360,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37488,DS-22b58f94-fa30-4cec-8856-ade0ceae84d1,DISK], DatanodeInfoWithStorage[127.0.0.1:44938,DS-f9bb7602-d212-4165-8dee-285597fdd360,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44795,DS-4735851f-fe42-4047-ae68-0343b79e9ced,DISK], DatanodeInfoWithStorage[127.0.0.1:39747,DS-47ec76c4-0921-40c0-ab8f-99fae8725b7e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44795,DS-4735851f-fe42-4047-ae68-0343b79e9ced,DISK], DatanodeInfoWithStorage[127.0.0.1:39747,DS-47ec76c4-0921-40c0-ab8f-99fae8725b7e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44795,DS-4735851f-fe42-4047-ae68-0343b79e9ced,DISK], DatanodeInfoWithStorage[127.0.0.1:39747,DS-47ec76c4-0921-40c0-ab8f-99fae8725b7e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44795,DS-4735851f-fe42-4047-ae68-0343b79e9ced,DISK], DatanodeInfoWithStorage[127.0.0.1:39747,DS-47ec76c4-0921-40c0-ab8f-99fae8725b7e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.snapshot.TestOpenFilesWithSnapshot#testSnapshotsForOpenFilesAndDeletion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40585,DS-c11a30f3-1a84-4cdf-80f3-96c5245796cf,DISK], DatanodeInfoWithStorage[127.0.0.1:36863,DS-91f554da-5ded-4645-8a4a-22b81950978e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40585,DS-c11a30f3-1a84-4cdf-80f3-96c5245796cf,DISK], DatanodeInfoWithStorage[127.0.0.1:36863,DS-91f554da-5ded-4645-8a4a-22b81950978e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40585,DS-c11a30f3-1a84-4cdf-80f3-96c5245796cf,DISK], DatanodeInfoWithStorage[127.0.0.1:36863,DS-91f554da-5ded-4645-8a4a-22b81950978e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40585,DS-c11a30f3-1a84-4cdf-80f3-96c5245796cf,DISK], DatanodeInfoWithStorage[127.0.0.1:36863,DS-91f554da-5ded-4645-8a4a-22b81950978e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 780

reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35513,DS-7fc257a8-1892-4fa8-89ac-c14b64a366fc,DISK], DatanodeInfoWithStorage[127.0.0.1:38027,DS-f5c12faa-9523-4c71-afc5-50f74b74294a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35513,DS-7fc257a8-1892-4fa8-89ac-c14b64a366fc,DISK], DatanodeInfoWithStorage[127.0.0.1:38027,DS-f5c12faa-9523-4c71-afc5-50f74b74294a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35513,DS-7fc257a8-1892-4fa8-89ac-c14b64a366fc,DISK], DatanodeInfoWithStorage[127.0.0.1:38027,DS-f5c12faa-9523-4c71-afc5-50f74b74294a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35513,DS-7fc257a8-1892-4fa8-89ac-c14b64a366fc,DISK], DatanodeInfoWithStorage[127.0.0.1:38027,DS-f5c12faa-9523-4c71-afc5-50f74b74294a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45347,DS-15d3bc79-b25f-4801-a071-bc017de98450,DISK], DatanodeInfoWithStorage[127.0.0.1:45791,DS-7baf997a-2a7b-4fdd-aaec-d779ef52702d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45791,DS-7baf997a-2a7b-4fdd-aaec-d779ef52702d,DISK], DatanodeInfoWithStorage[127.0.0.1:45347,DS-15d3bc79-b25f-4801-a071-bc017de98450,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45347,DS-15d3bc79-b25f-4801-a071-bc017de98450,DISK], DatanodeInfoWithStorage[127.0.0.1:45791,DS-7baf997a-2a7b-4fdd-aaec-d779ef52702d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45791,DS-7baf997a-2a7b-4fdd-aaec-d779ef52702d,DISK], DatanodeInfoWithStorage[127.0.0.1:45347,DS-15d3bc79-b25f-4801-a071-bc017de98450,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43377,DS-e4d4a8ff-8f48-4610-b181-eb4abeef4dba,DISK], DatanodeInfoWithStorage[127.0.0.1:34066,DS-b476f452-5530-47b3-8af5-4bb463ed8933,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43377,DS-e4d4a8ff-8f48-4610-b181-eb4abeef4dba,DISK], DatanodeInfoWithStorage[127.0.0.1:34066,DS-b476f452-5530-47b3-8af5-4bb463ed8933,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43377,DS-e4d4a8ff-8f48-4610-b181-eb4abeef4dba,DISK], DatanodeInfoWithStorage[127.0.0.1:34066,DS-b476f452-5530-47b3-8af5-4bb463ed8933,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43377,DS-e4d4a8ff-8f48-4610-b181-eb4abeef4dba,DISK], DatanodeInfoWithStorage[127.0.0.1:34066,DS-b476f452-5530-47b3-8af5-4bb463ed8933,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34550,DS-9899c531-f383-44e9-93dc-402d79beff1d,DISK], DatanodeInfoWithStorage[127.0.0.1:33121,DS-54a84a7d-808c-45b4-aa3d-fd4f92129979,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33121,DS-54a84a7d-808c-45b4-aa3d-fd4f92129979,DISK], DatanodeInfoWithStorage[127.0.0.1:34550,DS-9899c531-f383-44e9-93dc-402d79beff1d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34550,DS-9899c531-f383-44e9-93dc-402d79beff1d,DISK], DatanodeInfoWithStorage[127.0.0.1:33121,DS-54a84a7d-808c-45b4-aa3d-fd4f92129979,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33121,DS-54a84a7d-808c-45b4-aa3d-fd4f92129979,DISK], DatanodeInfoWithStorage[127.0.0.1:34550,DS-9899c531-f383-44e9-93dc-402d79beff1d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37145,DS-b15d0b80-1998-4f36-aa1c-0980162a87d8,DISK], DatanodeInfoWithStorage[127.0.0.1:38382,DS-91dd093b-e89f-45a6-910f-eb444992ad16,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37145,DS-b15d0b80-1998-4f36-aa1c-0980162a87d8,DISK], DatanodeInfoWithStorage[127.0.0.1:38382,DS-91dd093b-e89f-45a6-910f-eb444992ad16,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37145,DS-b15d0b80-1998-4f36-aa1c-0980162a87d8,DISK], DatanodeInfoWithStorage[127.0.0.1:38382,DS-91dd093b-e89f-45a6-910f-eb444992ad16,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37145,DS-b15d0b80-1998-4f36-aa1c-0980162a87d8,DISK], DatanodeInfoWithStorage[127.0.0.1:38382,DS-91dd093b-e89f-45a6-910f-eb444992ad16,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40041,DS-d5ed82c3-b208-48a4-b432-7005f7e4173b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40041,DS-d5ed82c3-b208-48a4-b432-7005f7e4173b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40041,DS-d5ed82c3-b208-48a4-b432-7005f7e4173b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40041,DS-d5ed82c3-b208-48a4-b432-7005f7e4173b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44399,DS-9f86e45e-802e-419c-8360-faa762701380,DISK], DatanodeInfoWithStorage[127.0.0.1:46550,DS-c241a779-fc72-4c45-8b63-69456d4cf055,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46550,DS-c241a779-fc72-4c45-8b63-69456d4cf055,DISK], DatanodeInfoWithStorage[127.0.0.1:44399,DS-9f86e45e-802e-419c-8360-faa762701380,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44399,DS-9f86e45e-802e-419c-8360-faa762701380,DISK], DatanodeInfoWithStorage[127.0.0.1:46550,DS-c241a779-fc72-4c45-8b63-69456d4cf055,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46550,DS-c241a779-fc72-4c45-8b63-69456d4cf055,DISK], DatanodeInfoWithStorage[127.0.0.1:44399,DS-9f86e45e-802e-419c-8360-faa762701380,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42270,DS-c8be9029-5b62-486f-b402-57823cb9b0f8,DISK], DatanodeInfoWithStorage[127.0.0.1:44653,DS-aff5a009-99d7-43c9-a031-9dc1368e004d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42270,DS-c8be9029-5b62-486f-b402-57823cb9b0f8,DISK], DatanodeInfoWithStorage[127.0.0.1:44653,DS-aff5a009-99d7-43c9-a031-9dc1368e004d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42270,DS-c8be9029-5b62-486f-b402-57823cb9b0f8,DISK], DatanodeInfoWithStorage[127.0.0.1:44653,DS-aff5a009-99d7-43c9-a031-9dc1368e004d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42270,DS-c8be9029-5b62-486f-b402-57823cb9b0f8,DISK], DatanodeInfoWithStorage[127.0.0.1:44653,DS-aff5a009-99d7-43c9-a031-9dc1368e004d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36644,DS-c5944dfd-793d-4f51-b913-b1547fb1d536,DISK], DatanodeInfoWithStorage[127.0.0.1:43144,DS-b81c8627-ceb7-4bb7-9b8b-6f9fabed0dcc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36644,DS-c5944dfd-793d-4f51-b913-b1547fb1d536,DISK], DatanodeInfoWithStorage[127.0.0.1:43144,DS-b81c8627-ceb7-4bb7-9b8b-6f9fabed0dcc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36644,DS-c5944dfd-793d-4f51-b913-b1547fb1d536,DISK], DatanodeInfoWithStorage[127.0.0.1:43144,DS-b81c8627-ceb7-4bb7-9b8b-6f9fabed0dcc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36644,DS-c5944dfd-793d-4f51-b913-b1547fb1d536,DISK], DatanodeInfoWithStorage[127.0.0.1:43144,DS-b81c8627-ceb7-4bb7-9b8b-6f9fabed0dcc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44342,DS-779246fd-11e6-40d4-adcc-297323afd03c,DISK], DatanodeInfoWithStorage[127.0.0.1:33612,DS-92bc5793-c420-4e3e-8c37-b290dbadb8f3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44342,DS-779246fd-11e6-40d4-adcc-297323afd03c,DISK], DatanodeInfoWithStorage[127.0.0.1:33612,DS-92bc5793-c420-4e3e-8c37-b290dbadb8f3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44342,DS-779246fd-11e6-40d4-adcc-297323afd03c,DISK], DatanodeInfoWithStorage[127.0.0.1:33612,DS-92bc5793-c420-4e3e-8c37-b290dbadb8f3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44342,DS-779246fd-11e6-40d4-adcc-297323afd03c,DISK], DatanodeInfoWithStorage[127.0.0.1:33612,DS-92bc5793-c420-4e3e-8c37-b290dbadb8f3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 2450

reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35236,DS-bad12b40-0068-4ed9-926b-ebc8a74315e5,DISK], DatanodeInfoWithStorage[127.0.0.1:41927,DS-0685b9ba-4d03-4295-a1e2-e1b7162f0095,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35236,DS-bad12b40-0068-4ed9-926b-ebc8a74315e5,DISK], DatanodeInfoWithStorage[127.0.0.1:41927,DS-0685b9ba-4d03-4295-a1e2-e1b7162f0095,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35236,DS-bad12b40-0068-4ed9-926b-ebc8a74315e5,DISK], DatanodeInfoWithStorage[127.0.0.1:41927,DS-0685b9ba-4d03-4295-a1e2-e1b7162f0095,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35236,DS-bad12b40-0068-4ed9-926b-ebc8a74315e5,DISK], DatanodeInfoWithStorage[127.0.0.1:41927,DS-0685b9ba-4d03-4295-a1e2-e1b7162f0095,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34505,DS-1821f098-dfb8-401f-8c3a-b10671af9750,DISK], DatanodeInfoWithStorage[127.0.0.1:41354,DS-74e8252e-d5d1-49fc-a872-ae9343283928,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34505,DS-1821f098-dfb8-401f-8c3a-b10671af9750,DISK], DatanodeInfoWithStorage[127.0.0.1:41354,DS-74e8252e-d5d1-49fc-a872-ae9343283928,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34505,DS-1821f098-dfb8-401f-8c3a-b10671af9750,DISK], DatanodeInfoWithStorage[127.0.0.1:41354,DS-74e8252e-d5d1-49fc-a872-ae9343283928,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34505,DS-1821f098-dfb8-401f-8c3a-b10671af9750,DISK], DatanodeInfoWithStorage[127.0.0.1:41354,DS-74e8252e-d5d1-49fc-a872-ae9343283928,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41371,DS-3a72c70c-6dc8-4f6e-adf3-b4a26a76d12e,DISK], DatanodeInfoWithStorage[127.0.0.1:40210,DS-62a58a0c-5754-4dfc-98a7-528aef410e3e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40210,DS-62a58a0c-5754-4dfc-98a7-528aef410e3e,DISK], DatanodeInfoWithStorage[127.0.0.1:41371,DS-3a72c70c-6dc8-4f6e-adf3-b4a26a76d12e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41371,DS-3a72c70c-6dc8-4f6e-adf3-b4a26a76d12e,DISK], DatanodeInfoWithStorage[127.0.0.1:40210,DS-62a58a0c-5754-4dfc-98a7-528aef410e3e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40210,DS-62a58a0c-5754-4dfc-98a7-528aef410e3e,DISK], DatanodeInfoWithStorage[127.0.0.1:41371,DS-3a72c70c-6dc8-4f6e-adf3-b4a26a76d12e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33249,DS-3e150781-538b-4800-b465-997a203862a9,DISK], DatanodeInfoWithStorage[127.0.0.1:33144,DS-f2c1efcd-5e4d-4871-9daf-a270cbcb1447,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33249,DS-3e150781-538b-4800-b465-997a203862a9,DISK], DatanodeInfoWithStorage[127.0.0.1:33144,DS-f2c1efcd-5e4d-4871-9daf-a270cbcb1447,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33249,DS-3e150781-538b-4800-b465-997a203862a9,DISK], DatanodeInfoWithStorage[127.0.0.1:33144,DS-f2c1efcd-5e4d-4871-9daf-a270cbcb1447,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33249,DS-3e150781-538b-4800-b465-997a203862a9,DISK], DatanodeInfoWithStorage[127.0.0.1:33144,DS-f2c1efcd-5e4d-4871-9daf-a270cbcb1447,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34516,DS-f1066cb8-fde8-4cf5-a9e8-176c97f4b68a,DISK], DatanodeInfoWithStorage[127.0.0.1:45346,DS-32fcd7ea-ed51-430e-b70c-bac2fd2243be,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34516,DS-f1066cb8-fde8-4cf5-a9e8-176c97f4b68a,DISK], DatanodeInfoWithStorage[127.0.0.1:45346,DS-32fcd7ea-ed51-430e-b70c-bac2fd2243be,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34516,DS-f1066cb8-fde8-4cf5-a9e8-176c97f4b68a,DISK], DatanodeInfoWithStorage[127.0.0.1:45346,DS-32fcd7ea-ed51-430e-b70c-bac2fd2243be,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34516,DS-f1066cb8-fde8-4cf5-a9e8-176c97f4b68a,DISK], DatanodeInfoWithStorage[127.0.0.1:45346,DS-32fcd7ea-ed51-430e-b70c-bac2fd2243be,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44474,DS-188c87ba-7dcf-4499-bc95-81b8a19df23c,DISK], DatanodeInfoWithStorage[127.0.0.1:35403,DS-fe2177e6-65e6-4bcd-97fb-e9ee84803c47,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44474,DS-188c87ba-7dcf-4499-bc95-81b8a19df23c,DISK], DatanodeInfoWithStorage[127.0.0.1:35403,DS-fe2177e6-65e6-4bcd-97fb-e9ee84803c47,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44474,DS-188c87ba-7dcf-4499-bc95-81b8a19df23c,DISK], DatanodeInfoWithStorage[127.0.0.1:35403,DS-fe2177e6-65e6-4bcd-97fb-e9ee84803c47,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44474,DS-188c87ba-7dcf-4499-bc95-81b8a19df23c,DISK], DatanodeInfoWithStorage[127.0.0.1:35403,DS-fe2177e6-65e6-4bcd-97fb-e9ee84803c47,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36123,DS-d6a9f902-39dd-46e2-96d1-76bbbf2ce539,DISK], DatanodeInfoWithStorage[127.0.0.1:40444,DS-217e0c91-baca-40cf-955c-ab999ba55f16,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40444,DS-217e0c91-baca-40cf-955c-ab999ba55f16,DISK], DatanodeInfoWithStorage[127.0.0.1:36123,DS-d6a9f902-39dd-46e2-96d1-76bbbf2ce539,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36123,DS-d6a9f902-39dd-46e2-96d1-76bbbf2ce539,DISK], DatanodeInfoWithStorage[127.0.0.1:40444,DS-217e0c91-baca-40cf-955c-ab999ba55f16,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40444,DS-217e0c91-baca-40cf-955c-ab999ba55f16,DISK], DatanodeInfoWithStorage[127.0.0.1:36123,DS-d6a9f902-39dd-46e2-96d1-76bbbf2ce539,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42900,DS-8673064e-27c3-40a8-9442-e7b88b0fa7ac,DISK], DatanodeInfoWithStorage[127.0.0.1:35857,DS-34a91f39-3768-40de-8e16-8b51b78d775e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35857,DS-34a91f39-3768-40de-8e16-8b51b78d775e,DISK], DatanodeInfoWithStorage[127.0.0.1:42900,DS-8673064e-27c3-40a8-9442-e7b88b0fa7ac,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42900,DS-8673064e-27c3-40a8-9442-e7b88b0fa7ac,DISK], DatanodeInfoWithStorage[127.0.0.1:35857,DS-34a91f39-3768-40de-8e16-8b51b78d775e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35857,DS-34a91f39-3768-40de-8e16-8b51b78d775e,DISK], DatanodeInfoWithStorage[127.0.0.1:42900,DS-8673064e-27c3-40a8-9442-e7b88b0fa7ac,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41947,DS-4feb4e5b-80ac-4bce-93c7-3c3897617cf4,DISK], DatanodeInfoWithStorage[127.0.0.1:42106,DS-90e917dc-c38d-451b-8c21-b38a916c4101,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41947,DS-4feb4e5b-80ac-4bce-93c7-3c3897617cf4,DISK], DatanodeInfoWithStorage[127.0.0.1:42106,DS-90e917dc-c38d-451b-8c21-b38a916c4101,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41947,DS-4feb4e5b-80ac-4bce-93c7-3c3897617cf4,DISK], DatanodeInfoWithStorage[127.0.0.1:42106,DS-90e917dc-c38d-451b-8c21-b38a916c4101,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41947,DS-4feb4e5b-80ac-4bce-93c7-3c3897617cf4,DISK], DatanodeInfoWithStorage[127.0.0.1:42106,DS-90e917dc-c38d-451b-8c21-b38a916c4101,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34295,DS-d07b537c-12dd-43a7-86d3-9483e64c6adf,DISK], DatanodeInfoWithStorage[127.0.0.1:39322,DS-ede11c47-96eb-496d-9dc6-b077ef0fbcc5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39322,DS-ede11c47-96eb-496d-9dc6-b077ef0fbcc5,DISK], DatanodeInfoWithStorage[127.0.0.1:34295,DS-d07b537c-12dd-43a7-86d3-9483e64c6adf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34295,DS-d07b537c-12dd-43a7-86d3-9483e64c6adf,DISK], DatanodeInfoWithStorage[127.0.0.1:39322,DS-ede11c47-96eb-496d-9dc6-b077ef0fbcc5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39322,DS-ede11c47-96eb-496d-9dc6-b077ef0fbcc5,DISK], DatanodeInfoWithStorage[127.0.0.1:34295,DS-d07b537c-12dd-43a7-86d3-9483e64c6adf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34240,DS-aef3ceda-76c7-4376-81fe-c29cfe5e016c,DISK], DatanodeInfoWithStorage[127.0.0.1:44873,DS-b2dcdbe4-226d-46b8-b9e5-3ba79060ebc4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44873,DS-b2dcdbe4-226d-46b8-b9e5-3ba79060ebc4,DISK], DatanodeInfoWithStorage[127.0.0.1:34240,DS-aef3ceda-76c7-4376-81fe-c29cfe5e016c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34240,DS-aef3ceda-76c7-4376-81fe-c29cfe5e016c,DISK], DatanodeInfoWithStorage[127.0.0.1:44873,DS-b2dcdbe4-226d-46b8-b9e5-3ba79060ebc4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44873,DS-b2dcdbe4-226d-46b8-b9e5-3ba79060ebc4,DISK], DatanodeInfoWithStorage[127.0.0.1:34240,DS-aef3ceda-76c7-4376-81fe-c29cfe5e016c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32875,DS-34f6582b-0cdc-4646-b195-6be812cc9b32,DISK], DatanodeInfoWithStorage[127.0.0.1:43333,DS-3ffd730b-0c41-4711-8169-87cb8119fcdf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32875,DS-34f6582b-0cdc-4646-b195-6be812cc9b32,DISK], DatanodeInfoWithStorage[127.0.0.1:43333,DS-3ffd730b-0c41-4711-8169-87cb8119fcdf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32875,DS-34f6582b-0cdc-4646-b195-6be812cc9b32,DISK], DatanodeInfoWithStorage[127.0.0.1:43333,DS-3ffd730b-0c41-4711-8169-87cb8119fcdf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32875,DS-34f6582b-0cdc-4646-b195-6be812cc9b32,DISK], DatanodeInfoWithStorage[127.0.0.1:43333,DS-3ffd730b-0c41-4711-8169-87cb8119fcdf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33105,DS-05b8515a-bbda-45ef-b0e1-8b60c92deb71,DISK], DatanodeInfoWithStorage[127.0.0.1:36478,DS-313ebd16-43b3-4469-8549-3336f5077c85,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33105,DS-05b8515a-bbda-45ef-b0e1-8b60c92deb71,DISK], DatanodeInfoWithStorage[127.0.0.1:36478,DS-313ebd16-43b3-4469-8549-3336f5077c85,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33105,DS-05b8515a-bbda-45ef-b0e1-8b60c92deb71,DISK], DatanodeInfoWithStorage[127.0.0.1:36478,DS-313ebd16-43b3-4469-8549-3336f5077c85,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33105,DS-05b8515a-bbda-45ef-b0e1-8b60c92deb71,DISK], DatanodeInfoWithStorage[127.0.0.1:36478,DS-313ebd16-43b3-4469-8549-3336f5077c85,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45050,DS-d34a84d5-f7e3-4ebe-b0c2-0e35a05c12cb,DISK], DatanodeInfoWithStorage[127.0.0.1:41192,DS-6ecb6a40-e8c8-4278-98e3-e96c03e9972a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45050,DS-d34a84d5-f7e3-4ebe-b0c2-0e35a05c12cb,DISK], DatanodeInfoWithStorage[127.0.0.1:41192,DS-6ecb6a40-e8c8-4278-98e3-e96c03e9972a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45050,DS-d34a84d5-f7e3-4ebe-b0c2-0e35a05c12cb,DISK], DatanodeInfoWithStorage[127.0.0.1:41192,DS-6ecb6a40-e8c8-4278-98e3-e96c03e9972a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45050,DS-d34a84d5-f7e3-4ebe-b0c2-0e35a05c12cb,DISK], DatanodeInfoWithStorage[127.0.0.1:41192,DS-6ecb6a40-e8c8-4278-98e3-e96c03e9972a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.pipeline.ecn
component: hdfs:DataNode
v1: true
v2: false
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testWALComparator
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44117,DS-f70894fe-b184-4e10-afaa-4f86040ede4c,DISK], DatanodeInfoWithStorage[127.0.0.1:36842,DS-c6318225-c2e7-4457-a49b-cd4468ef095f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44117,DS-f70894fe-b184-4e10-afaa-4f86040ede4c,DISK], DatanodeInfoWithStorage[127.0.0.1:36842,DS-c6318225-c2e7-4457-a49b-cd4468ef095f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44117,DS-f70894fe-b184-4e10-afaa-4f86040ede4c,DISK], DatanodeInfoWithStorage[127.0.0.1:36842,DS-c6318225-c2e7-4457-a49b-cd4468ef095f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44117,DS-f70894fe-b184-4e10-afaa-4f86040ede4c,DISK], DatanodeInfoWithStorage[127.0.0.1:36842,DS-c6318225-c2e7-4457-a49b-cd4468ef095f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 8 out of 50
v1v1v2v2 failed with probability 7 out of 50
result: might be true error
Total execution time in seconds : 8595

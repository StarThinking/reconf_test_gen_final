reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestGetFileChecksum#testGetFileChecksum
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestGetFileChecksum#testGetFileChecksum
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42961,DS-e6a4b6ee-b625-4432-bb41-b182f6a27843,DISK], DatanodeInfoWithStorage[127.0.0.1:38570,DS-ed14e543-78d9-43fd-b8b2-ba273ec8bf2c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38570,DS-ed14e543-78d9-43fd-b8b2-ba273ec8bf2c,DISK], DatanodeInfoWithStorage[127.0.0.1:42961,DS-e6a4b6ee-b625-4432-bb41-b182f6a27843,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42961,DS-e6a4b6ee-b625-4432-bb41-b182f6a27843,DISK], DatanodeInfoWithStorage[127.0.0.1:38570,DS-ed14e543-78d9-43fd-b8b2-ba273ec8bf2c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38570,DS-ed14e543-78d9-43fd-b8b2-ba273ec8bf2c,DISK], DatanodeInfoWithStorage[127.0.0.1:42961,DS-e6a4b6ee-b625-4432-bb41-b182f6a27843,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestGetFileChecksum#testGetFileChecksum
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35462,DS-719aa7a3-8dcd-4839-84de-cf8754c721c3,DISK], DatanodeInfoWithStorage[127.0.0.1:40727,DS-38702897-3635-407d-bf77-52c0594e289d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35462,DS-719aa7a3-8dcd-4839-84de-cf8754c721c3,DISK], DatanodeInfoWithStorage[127.0.0.1:40727,DS-38702897-3635-407d-bf77-52c0594e289d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35462,DS-719aa7a3-8dcd-4839-84de-cf8754c721c3,DISK], DatanodeInfoWithStorage[127.0.0.1:40727,DS-38702897-3635-407d-bf77-52c0594e289d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35462,DS-719aa7a3-8dcd-4839-84de-cf8754c721c3,DISK], DatanodeInfoWithStorage[127.0.0.1:40727,DS-38702897-3635-407d-bf77-52c0594e289d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestGetFileChecksum#testGetFileChecksum
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44125,DS-13936559-1f73-4a72-bab5-300afc9ce117,DISK], DatanodeInfoWithStorage[127.0.0.1:35010,DS-31ef3be8-c556-4dbf-885b-212a8d908fe9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44125,DS-13936559-1f73-4a72-bab5-300afc9ce117,DISK], DatanodeInfoWithStorage[127.0.0.1:35010,DS-31ef3be8-c556-4dbf-885b-212a8d908fe9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44125,DS-13936559-1f73-4a72-bab5-300afc9ce117,DISK], DatanodeInfoWithStorage[127.0.0.1:35010,DS-31ef3be8-c556-4dbf-885b-212a8d908fe9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44125,DS-13936559-1f73-4a72-bab5-300afc9ce117,DISK], DatanodeInfoWithStorage[127.0.0.1:35010,DS-31ef3be8-c556-4dbf-885b-212a8d908fe9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestGetFileChecksum#testGetFileChecksum
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40696,DS-5b82dafd-8ab0-4034-9bea-81d291112d36,DISK], DatanodeInfoWithStorage[127.0.0.1:40119,DS-4774b82f-af5e-495a-8be8-dfd1660aa3db,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40696,DS-5b82dafd-8ab0-4034-9bea-81d291112d36,DISK], DatanodeInfoWithStorage[127.0.0.1:40119,DS-4774b82f-af5e-495a-8be8-dfd1660aa3db,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40696,DS-5b82dafd-8ab0-4034-9bea-81d291112d36,DISK], DatanodeInfoWithStorage[127.0.0.1:40119,DS-4774b82f-af5e-495a-8be8-dfd1660aa3db,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40696,DS-5b82dafd-8ab0-4034-9bea-81d291112d36,DISK], DatanodeInfoWithStorage[127.0.0.1:40119,DS-4774b82f-af5e-495a-8be8-dfd1660aa3db,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestGetFileChecksum#testGetFileChecksum
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41926,DS-79f35cea-5eac-4f1d-98d2-314acf3e1d73,DISK], DatanodeInfoWithStorage[127.0.0.1:46376,DS-4b60b336-4ff6-4939-93dc-cd174ecbe026,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46376,DS-4b60b336-4ff6-4939-93dc-cd174ecbe026,DISK], DatanodeInfoWithStorage[127.0.0.1:41926,DS-79f35cea-5eac-4f1d-98d2-314acf3e1d73,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41926,DS-79f35cea-5eac-4f1d-98d2-314acf3e1d73,DISK], DatanodeInfoWithStorage[127.0.0.1:46376,DS-4b60b336-4ff6-4939-93dc-cd174ecbe026,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46376,DS-4b60b336-4ff6-4939-93dc-cd174ecbe026,DISK], DatanodeInfoWithStorage[127.0.0.1:41926,DS-79f35cea-5eac-4f1d-98d2-314acf3e1d73,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestGetFileChecksum#testGetFileChecksum
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46387,DS-8c55f318-cdc4-4572-844b-4d5225c9da5d,DISK], DatanodeInfoWithStorage[127.0.0.1:45001,DS-1e85df23-acf0-44dd-b63d-4f8be05c838e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46387,DS-8c55f318-cdc4-4572-844b-4d5225c9da5d,DISK], DatanodeInfoWithStorage[127.0.0.1:45001,DS-1e85df23-acf0-44dd-b63d-4f8be05c838e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46387,DS-8c55f318-cdc4-4572-844b-4d5225c9da5d,DISK], DatanodeInfoWithStorage[127.0.0.1:45001,DS-1e85df23-acf0-44dd-b63d-4f8be05c838e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46387,DS-8c55f318-cdc4-4572-844b-4d5225c9da5d,DISK], DatanodeInfoWithStorage[127.0.0.1:45001,DS-1e85df23-acf0-44dd-b63d-4f8be05c838e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestGetFileChecksum#testGetFileChecksum
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33421,DS-351594a6-9c32-469b-8da1-299f18b04920,DISK], DatanodeInfoWithStorage[127.0.0.1:45480,DS-91ec6d48-b3cf-402f-823c-537ad68c1af1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33421,DS-351594a6-9c32-469b-8da1-299f18b04920,DISK], DatanodeInfoWithStorage[127.0.0.1:45480,DS-91ec6d48-b3cf-402f-823c-537ad68c1af1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33421,DS-351594a6-9c32-469b-8da1-299f18b04920,DISK], DatanodeInfoWithStorage[127.0.0.1:45480,DS-91ec6d48-b3cf-402f-823c-537ad68c1af1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33421,DS-351594a6-9c32-469b-8da1-299f18b04920,DISK], DatanodeInfoWithStorage[127.0.0.1:45480,DS-91ec6d48-b3cf-402f-823c-537ad68c1af1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestGetFileChecksum#testGetFileChecksum
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34668,DS-21c9ce82-8457-4ce0-9b27-0396e9521057,DISK], DatanodeInfoWithStorage[127.0.0.1:33573,DS-e4fa2f74-2b11-4825-9031-c8c41f6c9672,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34668,DS-21c9ce82-8457-4ce0-9b27-0396e9521057,DISK], DatanodeInfoWithStorage[127.0.0.1:33573,DS-e4fa2f74-2b11-4825-9031-c8c41f6c9672,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34668,DS-21c9ce82-8457-4ce0-9b27-0396e9521057,DISK], DatanodeInfoWithStorage[127.0.0.1:33573,DS-e4fa2f74-2b11-4825-9031-c8c41f6c9672,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34668,DS-21c9ce82-8457-4ce0-9b27-0396e9521057,DISK], DatanodeInfoWithStorage[127.0.0.1:33573,DS-e4fa2f74-2b11-4825-9031-c8c41f6c9672,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestGetFileChecksum#testGetFileChecksum
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44997,DS-b9eef02e-ea40-42a0-be68-8127cd806734,DISK], DatanodeInfoWithStorage[127.0.0.1:39272,DS-6998eaef-9ccf-46f6-8515-a5921ebe3278,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44997,DS-b9eef02e-ea40-42a0-be68-8127cd806734,DISK], DatanodeInfoWithStorage[127.0.0.1:39272,DS-6998eaef-9ccf-46f6-8515-a5921ebe3278,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44997,DS-b9eef02e-ea40-42a0-be68-8127cd806734,DISK], DatanodeInfoWithStorage[127.0.0.1:39272,DS-6998eaef-9ccf-46f6-8515-a5921ebe3278,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44997,DS-b9eef02e-ea40-42a0-be68-8127cd806734,DISK], DatanodeInfoWithStorage[127.0.0.1:39272,DS-6998eaef-9ccf-46f6-8515-a5921ebe3278,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.incremental.intervalMsec
component: hdfs:DataNode
v1: 0
v2: 10000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestGetFileChecksum#testGetFileChecksum
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36539,DS-28d05e2b-7575-4d5f-96a1-be3e28fc2b2c,DISK], DatanodeInfoWithStorage[127.0.0.1:34072,DS-0077a737-9e5d-46b8-a0e8-ec60dc077f48,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34072,DS-0077a737-9e5d-46b8-a0e8-ec60dc077f48,DISK], DatanodeInfoWithStorage[127.0.0.1:36539,DS-28d05e2b-7575-4d5f-96a1-be3e28fc2b2c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36539,DS-28d05e2b-7575-4d5f-96a1-be3e28fc2b2c,DISK], DatanodeInfoWithStorage[127.0.0.1:34072,DS-0077a737-9e5d-46b8-a0e8-ec60dc077f48,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34072,DS-0077a737-9e5d-46b8-a0e8-ec60dc077f48,DISK], DatanodeInfoWithStorage[127.0.0.1:36539,DS-28d05e2b-7575-4d5f-96a1-be3e28fc2b2c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 8264

reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4]
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4]
reconfPoint: -2
result: -1
failureMessage: Call From bb5963b5fdf9/172.17.0.6 to localhost:36456 failed on socket timeout exception: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:38126 remote=localhost/127.0.0.1:36456]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
stackTrace: java.net.SocketTimeoutException: Call From bb5963b5fdf9/172.17.0.6 to localhost:36456 failed on socket timeout exception: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:38126 remote=localhost/127.0.0.1:36456]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy28.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy29.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.allocateNewBlock(DFSStripedOutputStream.java:480)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:526)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:865)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:214)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
	Suppressed: java.io.IOException: Unable to close file because dfsclient  was unable to contact the HDFS servers. clientRunning true hdfsTimeout 100
		at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:959)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1218)
		at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
		at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
		at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
		at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
		... 11 more
Caused by: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:38126 remote=localhost/127.0.0.1:36456]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4]
reconfPoint: -2
result: -1
failureMessage: Call From bb5963b5fdf9/172.17.0.6 to localhost:38504 failed on socket timeout exception: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:54888 remote=localhost/127.0.0.1:38504]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
stackTrace: java.net.SocketTimeoutException: Call From bb5963b5fdf9/172.17.0.6 to localhost:38504 failed on socket timeout exception: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:54888 remote=localhost/127.0.0.1:38504]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy28.create(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:369)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy29.create(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:276)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1217)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1196)
	at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1134)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:530)
	at org.apache.hadoop.hdfs.DistributedFileSystem$8.doCall(DistributedFileSystem.java:527)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:541)
	at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:468)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1118)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1098)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:987)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:975)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:864)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:214)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:54888 remote=localhost/127.0.0.1:38504]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4]
reconfPoint: -2
result: -1
failureMessage: Unable to close file because dfsclient  was unable to contact the HDFS servers. clientRunning true hdfsTimeout 100
stackTrace: java.io.IOException: Unable to close file because dfsclient  was unable to contact the HDFS servers. clientRunning true hdfsTimeout 100
	at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:959)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1218)
	at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:214)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1413462889-172.17.0.6-1595611826682:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39339,DS-c968f67d-f128-4ba4-bf2b-e8b5ba970846,DISK], DatanodeInfoWithStorage[127.0.0.1:40869,DS-76c2d026-97fb-46ff-905f-dc85a979f13a,DISK], DatanodeInfoWithStorage[127.0.0.1:35358,DS-df56acfb-2d58-460e-b62c-12e55d655afc,DISK], DatanodeInfoWithStorage[127.0.0.1:44144,DS-c3a91475-843c-4d88-8c0f-1a688aa8bc51,DISK], DatanodeInfoWithStorage[127.0.0.1:35437,DS-df938e0f-4fd9-4427-9e58-492e1daddf42,DISK], DatanodeInfoWithStorage[127.0.0.1:41407,DS-8554ed4d-fca2-4db9-9e6c-2b409ea322c3,DISK], DatanodeInfoWithStorage[127.0.0.1:38934,DS-2d0247f4-8c2f-4f61-83d8-4690801e1ea4,DISK], DatanodeInfoWithStorage[127.0.0.1:36522,DS-4dfbf2d4-5532-422d-9ce5-7a5a6ca6b0fc,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1413462889-172.17.0.6-1595611826682:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39339,DS-c968f67d-f128-4ba4-bf2b-e8b5ba970846,DISK], DatanodeInfoWithStorage[127.0.0.1:40869,DS-76c2d026-97fb-46ff-905f-dc85a979f13a,DISK], DatanodeInfoWithStorage[127.0.0.1:35358,DS-df56acfb-2d58-460e-b62c-12e55d655afc,DISK], DatanodeInfoWithStorage[127.0.0.1:44144,DS-c3a91475-843c-4d88-8c0f-1a688aa8bc51,DISK], DatanodeInfoWithStorage[127.0.0.1:35437,DS-df938e0f-4fd9-4427-9e58-492e1daddf42,DISK], DatanodeInfoWithStorage[127.0.0.1:41407,DS-8554ed4d-fca2-4db9-9e6c-2b409ea322c3,DISK], DatanodeInfoWithStorage[127.0.0.1:38934,DS-2d0247f4-8c2f-4f61-83d8-4690801e1ea4,DISK], DatanodeInfoWithStorage[127.0.0.1:36522,DS-4dfbf2d4-5532-422d-9ce5-7a5a6ca6b0fc,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1413462889-172.17.0.6-1595611826682:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39339,DS-c968f67d-f128-4ba4-bf2b-e8b5ba970846,DISK], DatanodeInfoWithStorage[127.0.0.1:40869,DS-76c2d026-97fb-46ff-905f-dc85a979f13a,DISK], DatanodeInfoWithStorage[127.0.0.1:35358,DS-df56acfb-2d58-460e-b62c-12e55d655afc,DISK], DatanodeInfoWithStorage[127.0.0.1:44144,DS-c3a91475-843c-4d88-8c0f-1a688aa8bc51,DISK], DatanodeInfoWithStorage[127.0.0.1:35437,DS-df938e0f-4fd9-4427-9e58-492e1daddf42,DISK], DatanodeInfoWithStorage[127.0.0.1:41407,DS-8554ed4d-fca2-4db9-9e6c-2b409ea322c3,DISK], DatanodeInfoWithStorage[127.0.0.1:38934,DS-2d0247f4-8c2f-4f61-83d8-4690801e1ea4,DISK], DatanodeInfoWithStorage[127.0.0.1:36522,DS-4dfbf2d4-5532-422d-9ce5-7a5a6ca6b0fc,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1413462889-172.17.0.6-1595611826682:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39339,DS-c968f67d-f128-4ba4-bf2b-e8b5ba970846,DISK], DatanodeInfoWithStorage[127.0.0.1:40869,DS-76c2d026-97fb-46ff-905f-dc85a979f13a,DISK], DatanodeInfoWithStorage[127.0.0.1:35358,DS-df56acfb-2d58-460e-b62c-12e55d655afc,DISK], DatanodeInfoWithStorage[127.0.0.1:44144,DS-c3a91475-843c-4d88-8c0f-1a688aa8bc51,DISK], DatanodeInfoWithStorage[127.0.0.1:35437,DS-df938e0f-4fd9-4427-9e58-492e1daddf42,DISK], DatanodeInfoWithStorage[127.0.0.1:41407,DS-8554ed4d-fca2-4db9-9e6c-2b409ea322c3,DISK], DatanodeInfoWithStorage[127.0.0.1:38934,DS-2d0247f4-8c2f-4f61-83d8-4690801e1ea4,DISK], DatanodeInfoWithStorage[127.0.0.1:36522,DS-4dfbf2d4-5532-422d-9ce5-7a5a6ca6b0fc,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-424625114-172.17.0.6-1595612448607:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40499,DS-d4ec370d-eac3-4eba-9c7c-b76235c14dc0,DISK], DatanodeInfoWithStorage[127.0.0.1:36370,DS-113a1bb3-1204-4f85-b12b-2b63376b605d,DISK], DatanodeInfoWithStorage[127.0.0.1:40613,DS-097b3fa9-9d87-4a8c-9c5f-6a7566d9c771,DISK], DatanodeInfoWithStorage[127.0.0.1:40595,DS-afd63b69-4ea7-4b2f-af4a-572d1c9c891f,DISK], DatanodeInfoWithStorage[127.0.0.1:36357,DS-ad93c969-bac8-45b8-99df-f377e95510ad,DISK], DatanodeInfoWithStorage[127.0.0.1:44232,DS-2a3d0068-5570-4828-89bc-b78c823883f4,DISK], DatanodeInfoWithStorage[127.0.0.1:38467,DS-40fa00c6-e88e-430b-91d5-dd026cf3ab04,DISK], DatanodeInfoWithStorage[127.0.0.1:34937,DS-95ed2911-6609-48fd-9ffe-e7b39246d84e,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-424625114-172.17.0.6-1595612448607:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40499,DS-d4ec370d-eac3-4eba-9c7c-b76235c14dc0,DISK], DatanodeInfoWithStorage[127.0.0.1:36370,DS-113a1bb3-1204-4f85-b12b-2b63376b605d,DISK], DatanodeInfoWithStorage[127.0.0.1:40613,DS-097b3fa9-9d87-4a8c-9c5f-6a7566d9c771,DISK], DatanodeInfoWithStorage[127.0.0.1:40595,DS-afd63b69-4ea7-4b2f-af4a-572d1c9c891f,DISK], DatanodeInfoWithStorage[127.0.0.1:36357,DS-ad93c969-bac8-45b8-99df-f377e95510ad,DISK], DatanodeInfoWithStorage[127.0.0.1:44232,DS-2a3d0068-5570-4828-89bc-b78c823883f4,DISK], DatanodeInfoWithStorage[127.0.0.1:38467,DS-40fa00c6-e88e-430b-91d5-dd026cf3ab04,DISK], DatanodeInfoWithStorage[127.0.0.1:34937,DS-95ed2911-6609-48fd-9ffe-e7b39246d84e,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-424625114-172.17.0.6-1595612448607:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40499,DS-d4ec370d-eac3-4eba-9c7c-b76235c14dc0,DISK], DatanodeInfoWithStorage[127.0.0.1:36370,DS-113a1bb3-1204-4f85-b12b-2b63376b605d,DISK], DatanodeInfoWithStorage[127.0.0.1:40613,DS-097b3fa9-9d87-4a8c-9c5f-6a7566d9c771,DISK], DatanodeInfoWithStorage[127.0.0.1:40595,DS-afd63b69-4ea7-4b2f-af4a-572d1c9c891f,DISK], DatanodeInfoWithStorage[127.0.0.1:36357,DS-ad93c969-bac8-45b8-99df-f377e95510ad,DISK], DatanodeInfoWithStorage[127.0.0.1:44232,DS-2a3d0068-5570-4828-89bc-b78c823883f4,DISK], DatanodeInfoWithStorage[127.0.0.1:38467,DS-40fa00c6-e88e-430b-91d5-dd026cf3ab04,DISK], DatanodeInfoWithStorage[127.0.0.1:34937,DS-95ed2911-6609-48fd-9ffe-e7b39246d84e,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7]}];  lastLocatedBlock=LocatedStripedBlock{BP-424625114-172.17.0.6-1595612448607:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40499,DS-d4ec370d-eac3-4eba-9c7c-b76235c14dc0,DISK], DatanodeInfoWithStorage[127.0.0.1:36370,DS-113a1bb3-1204-4f85-b12b-2b63376b605d,DISK], DatanodeInfoWithStorage[127.0.0.1:40613,DS-097b3fa9-9d87-4a8c-9c5f-6a7566d9c771,DISK], DatanodeInfoWithStorage[127.0.0.1:40595,DS-afd63b69-4ea7-4b2f-af4a-572d1c9c891f,DISK], DatanodeInfoWithStorage[127.0.0.1:36357,DS-ad93c969-bac8-45b8-99df-f377e95510ad,DISK], DatanodeInfoWithStorage[127.0.0.1:44232,DS-2a3d0068-5570-4828-89bc-b78c823883f4,DISK], DatanodeInfoWithStorage[127.0.0.1:38467,DS-40fa00c6-e88e-430b-91d5-dd026cf3ab04,DISK], DatanodeInfoWithStorage[127.0.0.1:34937,DS-95ed2911-6609-48fd-9ffe-e7b39246d84e,DISK]]; indices=[0, 1, 2, 3, 4, 5, 6, 7]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readParityChunks(StripeReader.java:211)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:378)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4]
reconfPoint: -2
result: -1
failureMessage: No enough valid inputs are provided, not recoverable
stackTrace: org.apache.hadoop.HadoopIllegalArgumentException: No enough valid inputs are provided, not recoverable
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.checkInputBuffers(ByteBufferDecodingState.java:119)
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.<init>(ByteBufferDecodingState.java:47)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:86)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:170)
	at org.apache.hadoop.hdfs.StripeReader.decodeAndFillBuffer(StripeReader.java:433)
	at org.apache.hadoop.hdfs.StatefulStripeReader.decode(StatefulStripeReader.java:94)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:390)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readOneStripe(DFSStripedInputStream.java:326)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readWithStrategy(DFSStripedInputStream.java:419)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyStatefulRead(StripedFileTestUtil.java:126)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:141)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4]
reconfPoint: -2
result: -1
failureMessage: Call From bb5963b5fdf9/172.17.0.6 to localhost:42454 failed on socket timeout exception: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:50874 remote=localhost/127.0.0.1:42454]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
stackTrace: java.net.SocketTimeoutException: Call From bb5963b5fdf9/172.17.0.6 to localhost:42454 failed on socket timeout exception: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:50874 remote=localhost/127.0.0.1:42454]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy28.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy29.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.allocateNewBlock(DFSStripedOutputStream.java:480)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:526)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:865)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:214)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
	Suppressed: java.io.IOException: Unable to close file because dfsclient  was unable to contact the HDFS servers. clientRunning true hdfsTimeout 100
		at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:959)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1218)
		at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
		at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
		at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
		at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
		... 11 more
Caused by: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:50874 remote=localhost/127.0.0.1:42454]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4] has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4]
reconfPoint: -2
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1767114269-172.17.0.6-1595618002840:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34229,DS-f1fe4afc-a0a6-4060-bc38-b828232ce389,DISK], DatanodeInfoWithStorage[127.0.0.1:44558,DS-196db2c9-5e19-4185-8ffd-25b778812bf9,DISK], DatanodeInfoWithStorage[127.0.0.1:44063,DS-ee1004ae-0932-4883-bef4-65252682784f,DISK], DatanodeInfoWithStorage[127.0.0.1:42709,DS-6a0b142c-7a48-442a-8dd7-a65d0e0bbed8,DISK], DatanodeInfoWithStorage[127.0.0.1:40368,DS-9bf633bf-f6a6-4317-8b7f-717ee08dc283,DISK], DatanodeInfoWithStorage[127.0.0.1:39283,DS-af832b4b-1bf9-49bc-9a0a-634f36c2bd2f,DISK], DatanodeInfoWithStorage[127.0.0.1:37251,DS-ce877ce1-f9c3-46b8-b58d-03dac6aadc57,DISK], DatanodeInfoWithStorage[127.0.0.1:34755,DS-eab478ec-7471-4338-bfcb-a11b8b2085ed,DISK]]; indices=[0, 1, 2, 3, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1767114269-172.17.0.6-1595618002840:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34229,DS-f1fe4afc-a0a6-4060-bc38-b828232ce389,DISK], DatanodeInfoWithStorage[127.0.0.1:44558,DS-196db2c9-5e19-4185-8ffd-25b778812bf9,DISK], DatanodeInfoWithStorage[127.0.0.1:44063,DS-ee1004ae-0932-4883-bef4-65252682784f,DISK], DatanodeInfoWithStorage[127.0.0.1:42709,DS-6a0b142c-7a48-442a-8dd7-a65d0e0bbed8,DISK], DatanodeInfoWithStorage[127.0.0.1:40368,DS-9bf633bf-f6a6-4317-8b7f-717ee08dc283,DISK], DatanodeInfoWithStorage[127.0.0.1:39283,DS-af832b4b-1bf9-49bc-9a0a-634f36c2bd2f,DISK], DatanodeInfoWithStorage[127.0.0.1:37251,DS-ce877ce1-f9c3-46b8-b58d-03dac6aadc57,DISK], DatanodeInfoWithStorage[127.0.0.1:34755,DS-eab478ec-7471-4338-bfcb-a11b8b2085ed,DISK]]; indices=[0, 1, 2, 3, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-1767114269-172.17.0.6-1595618002840:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34229,DS-f1fe4afc-a0a6-4060-bc38-b828232ce389,DISK], DatanodeInfoWithStorage[127.0.0.1:44558,DS-196db2c9-5e19-4185-8ffd-25b778812bf9,DISK], DatanodeInfoWithStorage[127.0.0.1:44063,DS-ee1004ae-0932-4883-bef4-65252682784f,DISK], DatanodeInfoWithStorage[127.0.0.1:42709,DS-6a0b142c-7a48-442a-8dd7-a65d0e0bbed8,DISK], DatanodeInfoWithStorage[127.0.0.1:40368,DS-9bf633bf-f6a6-4317-8b7f-717ee08dc283,DISK], DatanodeInfoWithStorage[127.0.0.1:39283,DS-af832b4b-1bf9-49bc-9a0a-634f36c2bd2f,DISK], DatanodeInfoWithStorage[127.0.0.1:37251,DS-ce877ce1-f9c3-46b8-b58d-03dac6aadc57,DISK], DatanodeInfoWithStorage[127.0.0.1:34755,DS-eab478ec-7471-4338-bfcb-a11b8b2085ed,DISK]]; indices=[0, 1, 2, 3, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-1767114269-172.17.0.6-1595618002840:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34229,DS-f1fe4afc-a0a6-4060-bc38-b828232ce389,DISK], DatanodeInfoWithStorage[127.0.0.1:44558,DS-196db2c9-5e19-4185-8ffd-25b778812bf9,DISK], DatanodeInfoWithStorage[127.0.0.1:44063,DS-ee1004ae-0932-4883-bef4-65252682784f,DISK], DatanodeInfoWithStorage[127.0.0.1:42709,DS-6a0b142c-7a48-442a-8dd7-a65d0e0bbed8,DISK], DatanodeInfoWithStorage[127.0.0.1:40368,DS-9bf633bf-f6a6-4317-8b7f-717ee08dc283,DISK], DatanodeInfoWithStorage[127.0.0.1:39283,DS-af832b4b-1bf9-49bc-9a0a-634f36c2bd2f,DISK], DatanodeInfoWithStorage[127.0.0.1:37251,DS-ce877ce1-f9c3-46b8-b58d-03dac6aadc57,DISK], DatanodeInfoWithStorage[127.0.0.1:34755,DS-eab478ec-7471-4338-bfcb-a11b8b2085ed,DISK]]; indices=[0, 1, 2, 3, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4]
reconfPoint: -2
result: -1
failureMessage: No enough valid inputs are provided, not recoverable
stackTrace: org.apache.hadoop.HadoopIllegalArgumentException: No enough valid inputs are provided, not recoverable
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.checkInputBuffers(ByteBufferDecodingState.java:119)
	at org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState.<init>(ByteBufferDecodingState.java:47)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:86)
	at org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder.decode(RawErasureDecoder.java:170)
	at org.apache.hadoop.hdfs.StripeReader.decodeAndFillBuffer(StripeReader.java:433)
	at org.apache.hadoop.hdfs.StatefulStripeReader.decode(StatefulStripeReader.java:94)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:390)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readOneStripe(DFSStripedInputStream.java:326)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.readWithStrategy(DFSStripedInputStream.java:419)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:829)
	at java.io.DataInputStream.read(DataInputStream.java:149)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyStatefulRead(StripedFileTestUtil.java:126)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:141)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData#testReadCorruptedData[4]
reconfPoint: -2
result: -1
failureMessage: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-467139096-172.17.0.6-1595618172640:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38224,DS-f90b5e71-13b2-441a-8775-603dd799eb01,DISK], DatanodeInfoWithStorage[127.0.0.1:34283,DS-21abc1d3-7161-45d7-84c0-d5d43d27cdde,DISK], DatanodeInfoWithStorage[127.0.0.1:44487,DS-9383ac3d-8449-405a-8075-9ab591f13a55,DISK], DatanodeInfoWithStorage[127.0.0.1:43288,DS-fd28c027-3978-4844-aee9-df56da2fe706,DISK], DatanodeInfoWithStorage[127.0.0.1:36675,DS-076cb92d-0a21-46e7-a3c2-fd864284469a,DISK], DatanodeInfoWithStorage[127.0.0.1:38934,DS-eb750301-783b-495e-a084-7835264197de,DISK], DatanodeInfoWithStorage[127.0.0.1:34388,DS-086635be-6d90-4319-87a0-c62cd4927a09,DISK], DatanodeInfoWithStorage[127.0.0.1:35892,DS-0031a064-df8b-4cf6-9c7d-c608d5df0a9e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-467139096-172.17.0.6-1595618172640:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38224,DS-f90b5e71-13b2-441a-8775-603dd799eb01,DISK], DatanodeInfoWithStorage[127.0.0.1:34283,DS-21abc1d3-7161-45d7-84c0-d5d43d27cdde,DISK], DatanodeInfoWithStorage[127.0.0.1:44487,DS-9383ac3d-8449-405a-8075-9ab591f13a55,DISK], DatanodeInfoWithStorage[127.0.0.1:43288,DS-fd28c027-3978-4844-aee9-df56da2fe706,DISK], DatanodeInfoWithStorage[127.0.0.1:36675,DS-076cb92d-0a21-46e7-a3c2-fd864284469a,DISK], DatanodeInfoWithStorage[127.0.0.1:38934,DS-eb750301-783b-495e-a084-7835264197de,DISK], DatanodeInfoWithStorage[127.0.0.1:34388,DS-086635be-6d90-4319-87a0-c62cd4927a09,DISK], DatanodeInfoWithStorage[127.0.0.1:35892,DS-0031a064-df8b-4cf6-9c7d-c608d5df0a9e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
stackTrace: java.io.IOException: 4 missing blocks, the stripe is: AlignedStripe(Offset=0, length=4194181, fetchedChunksNum=4, missingChunksNum=4); locatedBlocks is: LocatedBlocks{;  fileLength=25165701;  underConstruction=false;  blocks=[LocatedStripedBlock{BP-467139096-172.17.0.6-1595618172640:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38224,DS-f90b5e71-13b2-441a-8775-603dd799eb01,DISK], DatanodeInfoWithStorage[127.0.0.1:34283,DS-21abc1d3-7161-45d7-84c0-d5d43d27cdde,DISK], DatanodeInfoWithStorage[127.0.0.1:44487,DS-9383ac3d-8449-405a-8075-9ab591f13a55,DISK], DatanodeInfoWithStorage[127.0.0.1:43288,DS-fd28c027-3978-4844-aee9-df56da2fe706,DISK], DatanodeInfoWithStorage[127.0.0.1:36675,DS-076cb92d-0a21-46e7-a3c2-fd864284469a,DISK], DatanodeInfoWithStorage[127.0.0.1:38934,DS-eb750301-783b-495e-a084-7835264197de,DISK], DatanodeInfoWithStorage[127.0.0.1:34388,DS-086635be-6d90-4319-87a0-c62cd4927a09,DISK], DatanodeInfoWithStorage[127.0.0.1:35892,DS-0031a064-df8b-4cf6-9c7d-c608d5df0a9e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}];  lastLocatedBlock=LocatedStripedBlock{BP-467139096-172.17.0.6-1595618172640:blk_-9223372036854775792_1001; getBlockSize()=25165701; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38224,DS-f90b5e71-13b2-441a-8775-603dd799eb01,DISK], DatanodeInfoWithStorage[127.0.0.1:34283,DS-21abc1d3-7161-45d7-84c0-d5d43d27cdde,DISK], DatanodeInfoWithStorage[127.0.0.1:44487,DS-9383ac3d-8449-405a-8075-9ab591f13a55,DISK], DatanodeInfoWithStorage[127.0.0.1:43288,DS-fd28c027-3978-4844-aee9-df56da2fe706,DISK], DatanodeInfoWithStorage[127.0.0.1:36675,DS-076cb92d-0a21-46e7-a3c2-fd864284469a,DISK], DatanodeInfoWithStorage[127.0.0.1:38934,DS-eb750301-783b-495e-a084-7835264197de,DISK], DatanodeInfoWithStorage[127.0.0.1:34388,DS-086635be-6d90-4319-87a0-c62cd4927a09,DISK], DatanodeInfoWithStorage[127.0.0.1:35892,DS-0031a064-df8b-4cf6-9c7d-c608d5df0a9e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]};  isLastBlockComplete=true;  ecPolicy=ErasureCodingPolicy=[Name=RS-6-3-1024k, Schema=[ECSchema=[Codec=rs, numDataUnits=6, numParityUnits=3]], CellSize=1048576, Id=1]}
	at org.apache.hadoop.hdfs.StripeReader.checkMissingBlocks(StripeReader.java:179)
	at org.apache.hadoop.hdfs.StripeReader.readStripe(StripeReader.java:375)
	at org.apache.hadoop.hdfs.DFSStripedInputStream.fetchBlockByteRange(DFSStripedInputStream.java:507)
	at org.apache.hadoop.hdfs.DFSInputStream.pread(DFSInputStream.java:1360)
	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1324)
	at org.apache.hadoop.fs.FSInputStream.readFully(FSInputStream.java:121)
	at org.apache.hadoop.fs.FSDataInputStream.readFully(FSDataInputStream.java:111)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:106)
	at org.apache.hadoop.hdfs.StripedFileTestUtil.verifyPread(StripedFileTestUtil.java:86)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.verifyRead(ReadStripedFileWithDecodingHelper.java:139)
	at org.apache.hadoop.hdfs.ReadStripedFileWithDecodingHelper.testReadWithBlockCorrupted(ReadStripedFileWithDecodingHelper.java:221)
	at org.apache.hadoop.hdfs.TestReadStripedFileWithDecodingCorruptData.testReadCorruptedData(TestReadStripedFileWithDecodingCorruptData.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 7 out of 50
v1v1v2v2 failed with probability 4 out of 50
result: might be true error
Total execution time in seconds : 10237

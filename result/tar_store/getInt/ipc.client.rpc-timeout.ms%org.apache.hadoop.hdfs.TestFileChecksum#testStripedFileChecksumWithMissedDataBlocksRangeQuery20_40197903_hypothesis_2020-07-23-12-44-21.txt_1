reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-771903473-172.17.0.10-1595508408973:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43317,DS-f8d7cd1c-4baa-4081-8aca-6149109df0cf,DISK], DatanodeInfoWithStorage[127.0.0.1:44615,DS-012602e1-bd8d-4445-a209-520efd94edbe,DISK], DatanodeInfoWithStorage[127.0.0.1:40563,DS-0375d20c-c959-4d36-995f-e12f83a3d556,DISK], DatanodeInfoWithStorage[127.0.0.1:41520,DS-6d04fca8-5f5a-4cca-a54e-552dc11df77f,DISK], DatanodeInfoWithStorage[127.0.0.1:35676,DS-8ed42ec6-da78-4ea4-9799-06f734d0afc9,DISK], DatanodeInfoWithStorage[127.0.0.1:35695,DS-2882d166-d2d0-4ef1-b291-a3eaf9a010c9,DISK], DatanodeInfoWithStorage[127.0.0.1:35966,DS-98300d36-9c3c-4a3d-846b-6edac6f1b516,DISK], DatanodeInfoWithStorage[127.0.0.1:33203,DS-9fdc54a4-eae0-46ea-85b7-7147b7a9ef5d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-771903473-172.17.0.10-1595508408973:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43317,DS-f8d7cd1c-4baa-4081-8aca-6149109df0cf,DISK], DatanodeInfoWithStorage[127.0.0.1:44615,DS-012602e1-bd8d-4445-a209-520efd94edbe,DISK], DatanodeInfoWithStorage[127.0.0.1:40563,DS-0375d20c-c959-4d36-995f-e12f83a3d556,DISK], DatanodeInfoWithStorage[127.0.0.1:41520,DS-6d04fca8-5f5a-4cca-a54e-552dc11df77f,DISK], DatanodeInfoWithStorage[127.0.0.1:35676,DS-8ed42ec6-da78-4ea4-9799-06f734d0afc9,DISK], DatanodeInfoWithStorage[127.0.0.1:35695,DS-2882d166-d2d0-4ef1-b291-a3eaf9a010c9,DISK], DatanodeInfoWithStorage[127.0.0.1:35966,DS-98300d36-9c3c-4a3d-846b-6edac6f1b516,DISK], DatanodeInfoWithStorage[127.0.0.1:33203,DS-9fdc54a4-eae0-46ea-85b7-7147b7a9ef5d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-305600614-172.17.0.10-1595509079641:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33397,DS-2e3e55ce-5942-4923-a938-693f4fe3558c,DISK], DatanodeInfoWithStorage[127.0.0.1:41843,DS-4f90c895-1cc7-4246-bdc5-56e284ff3099,DISK], DatanodeInfoWithStorage[127.0.0.1:33356,DS-70b1ae7b-2625-4783-98df-1821e91354c1,DISK], DatanodeInfoWithStorage[127.0.0.1:41261,DS-fef5c658-1ff4-435f-9428-e604d61166cb,DISK], DatanodeInfoWithStorage[127.0.0.1:38096,DS-d339ff1b-f7fe-465a-9a12-67ffc8f8d945,DISK], DatanodeInfoWithStorage[127.0.0.1:33224,DS-5e9f925e-20f5-4d79-9dd2-7cf4e1003477,DISK], DatanodeInfoWithStorage[127.0.0.1:35947,DS-0265856b-de2d-4765-96ec-2f6283499d1b,DISK], DatanodeInfoWithStorage[127.0.0.1:38478,DS-140f62ca-7705-4236-8bce-f54d2e52d83f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-305600614-172.17.0.10-1595509079641:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33397,DS-2e3e55ce-5942-4923-a938-693f4fe3558c,DISK], DatanodeInfoWithStorage[127.0.0.1:41843,DS-4f90c895-1cc7-4246-bdc5-56e284ff3099,DISK], DatanodeInfoWithStorage[127.0.0.1:33356,DS-70b1ae7b-2625-4783-98df-1821e91354c1,DISK], DatanodeInfoWithStorage[127.0.0.1:41261,DS-fef5c658-1ff4-435f-9428-e604d61166cb,DISK], DatanodeInfoWithStorage[127.0.0.1:38096,DS-d339ff1b-f7fe-465a-9a12-67ffc8f8d945,DISK], DatanodeInfoWithStorage[127.0.0.1:33224,DS-5e9f925e-20f5-4d79-9dd2-7cf4e1003477,DISK], DatanodeInfoWithStorage[127.0.0.1:35947,DS-0265856b-de2d-4765-96ec-2f6283499d1b,DISK], DatanodeInfoWithStorage[127.0.0.1:38478,DS-140f62ca-7705-4236-8bce-f54d2e52d83f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1217365310-172.17.0.10-1595509491550:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39726,DS-69918e4d-a417-4014-8b7c-afe40a93259c,DISK], DatanodeInfoWithStorage[127.0.0.1:42565,DS-42054d75-acec-4cd0-823e-d4bdb154741b,DISK], DatanodeInfoWithStorage[127.0.0.1:32820,DS-b35f6f98-f1c2-461b-b98f-e43cb635990d,DISK], DatanodeInfoWithStorage[127.0.0.1:46774,DS-633052d5-acfe-4ae6-bd2c-711e3c742fdf,DISK], DatanodeInfoWithStorage[127.0.0.1:45978,DS-e11894ea-2fa7-44be-b4cf-abb7e1ec1536,DISK], DatanodeInfoWithStorage[127.0.0.1:37807,DS-f403ae30-fa9a-43f4-9292-6f9cf4dde9ca,DISK], DatanodeInfoWithStorage[127.0.0.1:38374,DS-623e1dca-eb53-4080-8c1e-1591006058e7,DISK], DatanodeInfoWithStorage[127.0.0.1:42822,DS-472e24d7-90c6-45df-ac93-af84c2c7dabf,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1217365310-172.17.0.10-1595509491550:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39726,DS-69918e4d-a417-4014-8b7c-afe40a93259c,DISK], DatanodeInfoWithStorage[127.0.0.1:42565,DS-42054d75-acec-4cd0-823e-d4bdb154741b,DISK], DatanodeInfoWithStorage[127.0.0.1:32820,DS-b35f6f98-f1c2-461b-b98f-e43cb635990d,DISK], DatanodeInfoWithStorage[127.0.0.1:46774,DS-633052d5-acfe-4ae6-bd2c-711e3c742fdf,DISK], DatanodeInfoWithStorage[127.0.0.1:45978,DS-e11894ea-2fa7-44be-b4cf-abb7e1ec1536,DISK], DatanodeInfoWithStorage[127.0.0.1:37807,DS-f403ae30-fa9a-43f4-9292-6f9cf4dde9ca,DISK], DatanodeInfoWithStorage[127.0.0.1:38374,DS-623e1dca-eb53-4080-8c1e-1591006058e7,DISK], DatanodeInfoWithStorage[127.0.0.1:42822,DS-472e24d7-90c6-45df-ac93-af84c2c7dabf,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-291976517-172.17.0.10-1595509662686:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39666,DS-86e10e2a-551f-4cfd-819d-34dc785d8015,DISK], DatanodeInfoWithStorage[127.0.0.1:36235,DS-dfa5f52a-ae09-47b8-ab99-a44f0f35e52e,DISK], DatanodeInfoWithStorage[127.0.0.1:39640,DS-91ca49a1-2811-487b-9b0b-f68db0d7e080,DISK], DatanodeInfoWithStorage[127.0.0.1:33843,DS-4821ae66-9d7d-4a2d-81a7-af58fb6c63c6,DISK], DatanodeInfoWithStorage[127.0.0.1:36256,DS-f763e758-2c68-4649-a107-2386b01b22a8,DISK], DatanodeInfoWithStorage[127.0.0.1:40263,DS-583d0f51-963e-4978-b3e5-d793b625c7d5,DISK], DatanodeInfoWithStorage[127.0.0.1:33383,DS-2ed3585b-fd77-4293-aa0f-d0d1615dea69,DISK], DatanodeInfoWithStorage[127.0.0.1:34204,DS-918cdbd0-15fb-45ed-a869-c185d1f9968c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-291976517-172.17.0.10-1595509662686:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39666,DS-86e10e2a-551f-4cfd-819d-34dc785d8015,DISK], DatanodeInfoWithStorage[127.0.0.1:36235,DS-dfa5f52a-ae09-47b8-ab99-a44f0f35e52e,DISK], DatanodeInfoWithStorage[127.0.0.1:39640,DS-91ca49a1-2811-487b-9b0b-f68db0d7e080,DISK], DatanodeInfoWithStorage[127.0.0.1:33843,DS-4821ae66-9d7d-4a2d-81a7-af58fb6c63c6,DISK], DatanodeInfoWithStorage[127.0.0.1:36256,DS-f763e758-2c68-4649-a107-2386b01b22a8,DISK], DatanodeInfoWithStorage[127.0.0.1:40263,DS-583d0f51-963e-4978-b3e5-d793b625c7d5,DISK], DatanodeInfoWithStorage[127.0.0.1:33383,DS-2ed3585b-fd77-4293-aa0f-d0d1615dea69,DISK], DatanodeInfoWithStorage[127.0.0.1:34204,DS-918cdbd0-15fb-45ed-a869-c185d1f9968c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-332372149-172.17.0.10-1595509928528:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46619,DS-4cefffca-a5fe-4c92-8f97-03c60428c3a9,DISK], DatanodeInfoWithStorage[127.0.0.1:43789,DS-872c34d2-2550-49b0-9463-a0bfecd2663a,DISK], DatanodeInfoWithStorage[127.0.0.1:38257,DS-dd53ae7f-bb3f-4449-8023-683cbe76b74c,DISK], DatanodeInfoWithStorage[127.0.0.1:33583,DS-a03bb802-8c3e-4113-9c03-963cf42307c5,DISK], DatanodeInfoWithStorage[127.0.0.1:35893,DS-1c85e901-6ff6-425e-becd-a20995f1a13d,DISK], DatanodeInfoWithStorage[127.0.0.1:42953,DS-06a2a476-3feb-43bf-8a7b-5def94403b16,DISK], DatanodeInfoWithStorage[127.0.0.1:33420,DS-692dc85f-8d16-4cf7-b094-86ee86f097e7,DISK], DatanodeInfoWithStorage[127.0.0.1:37984,DS-17469bb9-8f11-4203-a961-e313732ec4ea,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-332372149-172.17.0.10-1595509928528:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46619,DS-4cefffca-a5fe-4c92-8f97-03c60428c3a9,DISK], DatanodeInfoWithStorage[127.0.0.1:43789,DS-872c34d2-2550-49b0-9463-a0bfecd2663a,DISK], DatanodeInfoWithStorage[127.0.0.1:38257,DS-dd53ae7f-bb3f-4449-8023-683cbe76b74c,DISK], DatanodeInfoWithStorage[127.0.0.1:33583,DS-a03bb802-8c3e-4113-9c03-963cf42307c5,DISK], DatanodeInfoWithStorage[127.0.0.1:35893,DS-1c85e901-6ff6-425e-becd-a20995f1a13d,DISK], DatanodeInfoWithStorage[127.0.0.1:42953,DS-06a2a476-3feb-43bf-8a7b-5def94403b16,DISK], DatanodeInfoWithStorage[127.0.0.1:33420,DS-692dc85f-8d16-4cf7-b094-86ee86f097e7,DISK], DatanodeInfoWithStorage[127.0.0.1:37984,DS-17469bb9-8f11-4203-a961-e313732ec4ea,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1871972740-172.17.0.10-1595510113506:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44607,DS-81f0dd34-9270-4ad5-9450-3f441392284e,DISK], DatanodeInfoWithStorage[127.0.0.1:35681,DS-10fba414-1ef8-4fa9-af77-6edbbd83448d,DISK], DatanodeInfoWithStorage[127.0.0.1:34034,DS-81fb3365-9c93-4471-adb6-cff0cc548825,DISK], DatanodeInfoWithStorage[127.0.0.1:33713,DS-e3fac676-e712-4ea3-a0ab-1b903ff48a5e,DISK], DatanodeInfoWithStorage[127.0.0.1:35216,DS-488c708c-4443-4e00-be3e-671e3f29c97a,DISK], DatanodeInfoWithStorage[127.0.0.1:45995,DS-72a93915-f90a-4ae5-acd9-9bfd2c230234,DISK], DatanodeInfoWithStorage[127.0.0.1:38809,DS-900b9be6-0dfb-4b03-a1b6-b04085340129,DISK], DatanodeInfoWithStorage[127.0.0.1:39497,DS-3581428b-1341-4231-a9a6-00b2663c1acb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1871972740-172.17.0.10-1595510113506:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44607,DS-81f0dd34-9270-4ad5-9450-3f441392284e,DISK], DatanodeInfoWithStorage[127.0.0.1:35681,DS-10fba414-1ef8-4fa9-af77-6edbbd83448d,DISK], DatanodeInfoWithStorage[127.0.0.1:34034,DS-81fb3365-9c93-4471-adb6-cff0cc548825,DISK], DatanodeInfoWithStorage[127.0.0.1:33713,DS-e3fac676-e712-4ea3-a0ab-1b903ff48a5e,DISK], DatanodeInfoWithStorage[127.0.0.1:35216,DS-488c708c-4443-4e00-be3e-671e3f29c97a,DISK], DatanodeInfoWithStorage[127.0.0.1:45995,DS-72a93915-f90a-4ae5-acd9-9bfd2c230234,DISK], DatanodeInfoWithStorage[127.0.0.1:38809,DS-900b9be6-0dfb-4b03-a1b6-b04085340129,DISK], DatanodeInfoWithStorage[127.0.0.1:39497,DS-3581428b-1341-4231-a9a6-00b2663c1acb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1461665771-172.17.0.10-1595510631944:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34270,DS-59f2614b-2d0a-404d-983f-39e16cc66ebd,DISK], DatanodeInfoWithStorage[127.0.0.1:37241,DS-b69531b9-2f9c-4a13-9f7c-1b2435b166e4,DISK], DatanodeInfoWithStorage[127.0.0.1:40032,DS-394ee9b9-c677-4f47-98a0-e191554d0eea,DISK], DatanodeInfoWithStorage[127.0.0.1:39656,DS-75a1be9b-cf62-458e-a0ac-c09f3eff073b,DISK], DatanodeInfoWithStorage[127.0.0.1:41303,DS-54168ac2-d9e0-4bba-9255-fc051421738d,DISK], DatanodeInfoWithStorage[127.0.0.1:40717,DS-9604d2ea-de3d-49d5-a06c-390e536dff10,DISK], DatanodeInfoWithStorage[127.0.0.1:35422,DS-3dcb4e29-417f-4322-ba36-9fe2bb0809b8,DISK], DatanodeInfoWithStorage[127.0.0.1:44001,DS-8fcf3844-b6e1-4636-b016-d8307d7bea26,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1461665771-172.17.0.10-1595510631944:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34270,DS-59f2614b-2d0a-404d-983f-39e16cc66ebd,DISK], DatanodeInfoWithStorage[127.0.0.1:37241,DS-b69531b9-2f9c-4a13-9f7c-1b2435b166e4,DISK], DatanodeInfoWithStorage[127.0.0.1:40032,DS-394ee9b9-c677-4f47-98a0-e191554d0eea,DISK], DatanodeInfoWithStorage[127.0.0.1:39656,DS-75a1be9b-cf62-458e-a0ac-c09f3eff073b,DISK], DatanodeInfoWithStorage[127.0.0.1:41303,DS-54168ac2-d9e0-4bba-9255-fc051421738d,DISK], DatanodeInfoWithStorage[127.0.0.1:40717,DS-9604d2ea-de3d-49d5-a06c-390e536dff10,DISK], DatanodeInfoWithStorage[127.0.0.1:35422,DS-3dcb4e29-417f-4322-ba36-9fe2bb0809b8,DISK], DatanodeInfoWithStorage[127.0.0.1:44001,DS-8fcf3844-b6e1-4636-b016-d8307d7bea26,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: Call From ed27b18c88cc/172.17.0.10 to localhost:35441 failed on socket timeout exception: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:51334 remote=localhost/127.0.0.1:35441]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
stackTrace: java.net.SocketTimeoutException: Call From ed27b18c88cc/172.17.0.10 to localhost:35441 failed on socket timeout exception: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:51334 remote=localhost/127.0.0.1:35441]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy28.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:517)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy29.addBlock(Unknown Source)
	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1081)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.allocateNewBlock(DFSStripedOutputStream.java:480)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:526)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:865)
	at org.apache.hadoop.hdfs.TestFileChecksum.prepareTestFiles(TestFileChecksum.java:602)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:292)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
	Suppressed: java.io.IOException: Unable to close file because dfsclient  was unable to contact the HDFS servers. clientRunning true hdfsTimeout 100
		at org.apache.hadoop.hdfs.DFSOutputStream.completeFile(DFSOutputStream.java:959)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1218)
		at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
		at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
		at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
		at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
		... 12 more
Caused by: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:51334 remote=localhost/127.0.0.1:35441]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1191534936-172.17.0.10-1595511294589:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42831,DS-fc731ff1-9fc9-462e-bf96-12aea5d68921,DISK], DatanodeInfoWithStorage[127.0.0.1:39513,DS-5d8a87ea-0921-4dee-80e5-2e554650a9b7,DISK], DatanodeInfoWithStorage[127.0.0.1:35877,DS-8e623c37-10af-4eb4-b1ff-68903473da83,DISK], DatanodeInfoWithStorage[127.0.0.1:40019,DS-384c7bbb-451d-400b-83c5-62c187110731,DISK], DatanodeInfoWithStorage[127.0.0.1:35663,DS-1ec9d166-c588-4561-b85a-a3ec7ce6be5d,DISK], DatanodeInfoWithStorage[127.0.0.1:43063,DS-ad372947-0115-4445-b973-80df9ec8f52d,DISK], DatanodeInfoWithStorage[127.0.0.1:40199,DS-2f5b3c38-3c3d-4d1c-89ef-ba3970460415,DISK], DatanodeInfoWithStorage[127.0.0.1:38326,DS-309b850c-8610-4a71-9275-7b2b09930e08,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1191534936-172.17.0.10-1595511294589:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42831,DS-fc731ff1-9fc9-462e-bf96-12aea5d68921,DISK], DatanodeInfoWithStorage[127.0.0.1:39513,DS-5d8a87ea-0921-4dee-80e5-2e554650a9b7,DISK], DatanodeInfoWithStorage[127.0.0.1:35877,DS-8e623c37-10af-4eb4-b1ff-68903473da83,DISK], DatanodeInfoWithStorage[127.0.0.1:40019,DS-384c7bbb-451d-400b-83c5-62c187110731,DISK], DatanodeInfoWithStorage[127.0.0.1:35663,DS-1ec9d166-c588-4561-b85a-a3ec7ce6be5d,DISK], DatanodeInfoWithStorage[127.0.0.1:43063,DS-ad372947-0115-4445-b973-80df9ec8f52d,DISK], DatanodeInfoWithStorage[127.0.0.1:40199,DS-2f5b3c38-3c3d-4d1c-89ef-ba3970460415,DISK], DatanodeInfoWithStorage[127.0.0.1:38326,DS-309b850c-8610-4a71-9275-7b2b09930e08,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-263714964-172.17.0.10-1595511581243:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38165,DS-c7c0b789-163c-4a16-951c-3f7be7cb9f6a,DISK], DatanodeInfoWithStorage[127.0.0.1:33561,DS-d5bdc8a0-8bf5-4a2e-beea-0a7bcfcdc4fc,DISK], DatanodeInfoWithStorage[127.0.0.1:41444,DS-12b55ea9-c6a8-465b-81a4-c4b73b282a15,DISK], DatanodeInfoWithStorage[127.0.0.1:45641,DS-ab33cc76-6ed1-44ce-858f-3675bd3cd69b,DISK], DatanodeInfoWithStorage[127.0.0.1:34291,DS-16b7e01b-9f6d-4e88-81f4-1e638fd5d966,DISK], DatanodeInfoWithStorage[127.0.0.1:40235,DS-20994fd2-acb0-49cc-9b2c-1301183f13d7,DISK], DatanodeInfoWithStorage[127.0.0.1:35613,DS-93772199-76ee-4917-9e25-addd4e762aaf,DISK], DatanodeInfoWithStorage[127.0.0.1:45469,DS-cd3cd6fc-8870-4c99-8b00-399ab3913b71,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-263714964-172.17.0.10-1595511581243:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38165,DS-c7c0b789-163c-4a16-951c-3f7be7cb9f6a,DISK], DatanodeInfoWithStorage[127.0.0.1:33561,DS-d5bdc8a0-8bf5-4a2e-beea-0a7bcfcdc4fc,DISK], DatanodeInfoWithStorage[127.0.0.1:41444,DS-12b55ea9-c6a8-465b-81a4-c4b73b282a15,DISK], DatanodeInfoWithStorage[127.0.0.1:45641,DS-ab33cc76-6ed1-44ce-858f-3675bd3cd69b,DISK], DatanodeInfoWithStorage[127.0.0.1:34291,DS-16b7e01b-9f6d-4e88-81f4-1e638fd5d966,DISK], DatanodeInfoWithStorage[127.0.0.1:40235,DS-20994fd2-acb0-49cc-9b2c-1301183f13d7,DISK], DatanodeInfoWithStorage[127.0.0.1:35613,DS-93772199-76ee-4917-9e25-addd4e762aaf,DISK], DatanodeInfoWithStorage[127.0.0.1:45469,DS-cd3cd6fc-8870-4c99-8b00-399ab3913b71,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1792241186-172.17.0.10-1595511824237:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45131,DS-5efb80b0-b6c6-4dcb-b8fc-515da44a1140,DISK], DatanodeInfoWithStorage[127.0.0.1:36239,DS-6cf8fb39-f9a7-4f65-9687-99c21d9d633a,DISK], DatanodeInfoWithStorage[127.0.0.1:46511,DS-a981a955-23a7-4f8d-ace9-3f958f4ed286,DISK], DatanodeInfoWithStorage[127.0.0.1:41882,DS-1b5507f4-b381-450d-8728-35875becef18,DISK], DatanodeInfoWithStorage[127.0.0.1:35432,DS-d2a9adae-703c-42c1-a3d3-0a35c7f65dd1,DISK], DatanodeInfoWithStorage[127.0.0.1:33454,DS-ccb4adcd-2db8-4f46-8629-6f214b749238,DISK], DatanodeInfoWithStorage[127.0.0.1:43681,DS-d60a6d74-99f2-49d3-85d7-21bc9c792c09,DISK], DatanodeInfoWithStorage[127.0.0.1:36656,DS-fc0da10f-8012-49ad-bdf4-98670e992477,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1792241186-172.17.0.10-1595511824237:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45131,DS-5efb80b0-b6c6-4dcb-b8fc-515da44a1140,DISK], DatanodeInfoWithStorage[127.0.0.1:36239,DS-6cf8fb39-f9a7-4f65-9687-99c21d9d633a,DISK], DatanodeInfoWithStorage[127.0.0.1:46511,DS-a981a955-23a7-4f8d-ace9-3f958f4ed286,DISK], DatanodeInfoWithStorage[127.0.0.1:41882,DS-1b5507f4-b381-450d-8728-35875becef18,DISK], DatanodeInfoWithStorage[127.0.0.1:35432,DS-d2a9adae-703c-42c1-a3d3-0a35c7f65dd1,DISK], DatanodeInfoWithStorage[127.0.0.1:33454,DS-ccb4adcd-2db8-4f46-8629-6f214b749238,DISK], DatanodeInfoWithStorage[127.0.0.1:43681,DS-d60a6d74-99f2-49d3-85d7-21bc9c792c09,DISK], DatanodeInfoWithStorage[127.0.0.1:36656,DS-fc0da10f-8012-49ad-bdf4-98670e992477,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: Call From ed27b18c88cc/172.17.0.10 to localhost:46795 failed on socket timeout exception: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:39670 remote=localhost/127.0.0.1:46795]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
stackTrace: java.net.SocketTimeoutException: Call From ed27b18c88cc/172.17.0.10 to localhost:46795 failed on socket timeout exception: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:39670 remote=localhost/127.0.0.1:46795]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy28.delete(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.delete(ClientNamenodeProtocolTranslatorPB.java:641)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy29.delete(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.delete(DFSClient.java:1607)
	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:946)
	at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:943)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.delete(DistributedFileSystem.java:953)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:861)
	at org.apache.hadoop.hdfs.TestFileChecksum.prepareTestFiles(TestFileChecksum.java:602)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:292)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
Caused by: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:39670 remote=localhost/127.0.0.1:46795]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-557288468-172.17.0.10-1595511928612:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35194,DS-e907546b-fcbe-4775-a94a-b6cd71a6cdf2,DISK], DatanodeInfoWithStorage[127.0.0.1:45401,DS-f8b831e9-11c9-449f-8c0b-0cc8b2fa0975,DISK], DatanodeInfoWithStorage[127.0.0.1:37871,DS-5c1de333-57c7-401f-bb62-1f0061547160,DISK], DatanodeInfoWithStorage[127.0.0.1:42607,DS-ac959b23-d40f-44c5-a44d-d4f27ba2664c,DISK], DatanodeInfoWithStorage[127.0.0.1:38159,DS-2dd2e41a-1f85-4f39-a193-9b842d65f743,DISK], DatanodeInfoWithStorage[127.0.0.1:39088,DS-9435cbb2-33bc-4381-965b-29b5c3db49fb,DISK], DatanodeInfoWithStorage[127.0.0.1:37879,DS-73a49d54-9357-45f7-bd6f-4fa166f73813,DISK], DatanodeInfoWithStorage[127.0.0.1:36467,DS-05cd3a3c-5d68-438b-8af8-8906c94c2925,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-557288468-172.17.0.10-1595511928612:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35194,DS-e907546b-fcbe-4775-a94a-b6cd71a6cdf2,DISK], DatanodeInfoWithStorage[127.0.0.1:45401,DS-f8b831e9-11c9-449f-8c0b-0cc8b2fa0975,DISK], DatanodeInfoWithStorage[127.0.0.1:37871,DS-5c1de333-57c7-401f-bb62-1f0061547160,DISK], DatanodeInfoWithStorage[127.0.0.1:42607,DS-ac959b23-d40f-44c5-a44d-d4f27ba2664c,DISK], DatanodeInfoWithStorage[127.0.0.1:38159,DS-2dd2e41a-1f85-4f39-a193-9b842d65f743,DISK], DatanodeInfoWithStorage[127.0.0.1:39088,DS-9435cbb2-33bc-4381-965b-29b5c3db49fb,DISK], DatanodeInfoWithStorage[127.0.0.1:37879,DS-73a49d54-9357-45f7-bd6f-4fa166f73813,DISK], DatanodeInfoWithStorage[127.0.0.1:36467,DS-05cd3a3c-5d68-438b-8af8-8906c94c2925,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1438139311-172.17.0.10-1595512085022:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41158,DS-c087b65d-b8f2-422c-bcd8-5f4f3c681e09,DISK], DatanodeInfoWithStorage[127.0.0.1:38654,DS-e0698dfc-435b-409d-b652-1d237f4c15f4,DISK], DatanodeInfoWithStorage[127.0.0.1:45240,DS-9c144cd4-1857-42db-bfc6-76a4a2b19a8c,DISK], DatanodeInfoWithStorage[127.0.0.1:34621,DS-3a09eb1d-6169-47c9-a55c-8236a34eb117,DISK], DatanodeInfoWithStorage[127.0.0.1:38921,DS-ed8eaa3b-fd46-4464-8d78-7ae3f179aa39,DISK], DatanodeInfoWithStorage[127.0.0.1:41424,DS-545e255c-5e50-4d8c-8177-b46a39fbabe9,DISK], DatanodeInfoWithStorage[127.0.0.1:40104,DS-64e885ac-2272-4b88-904e-e12780539237,DISK], DatanodeInfoWithStorage[127.0.0.1:36189,DS-0fd34c56-8fb1-4735-8611-387ad8603b66,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1438139311-172.17.0.10-1595512085022:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41158,DS-c087b65d-b8f2-422c-bcd8-5f4f3c681e09,DISK], DatanodeInfoWithStorage[127.0.0.1:38654,DS-e0698dfc-435b-409d-b652-1d237f4c15f4,DISK], DatanodeInfoWithStorage[127.0.0.1:45240,DS-9c144cd4-1857-42db-bfc6-76a4a2b19a8c,DISK], DatanodeInfoWithStorage[127.0.0.1:34621,DS-3a09eb1d-6169-47c9-a55c-8236a34eb117,DISK], DatanodeInfoWithStorage[127.0.0.1:38921,DS-ed8eaa3b-fd46-4464-8d78-7ae3f179aa39,DISK], DatanodeInfoWithStorage[127.0.0.1:41424,DS-545e255c-5e50-4d8c-8177-b46a39fbabe9,DISK], DatanodeInfoWithStorage[127.0.0.1:40104,DS-64e885ac-2272-4b88-904e-e12780539237,DISK], DatanodeInfoWithStorage[127.0.0.1:36189,DS-0fd34c56-8fb1-4735-8611-387ad8603b66,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1356372913-172.17.0.10-1595512157769:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33190,DS-85403ab2-2fb4-422b-83b8-9cc38236b7e5,DISK], DatanodeInfoWithStorage[127.0.0.1:37934,DS-4ebccd83-c51c-431c-a10c-8ee888fda129,DISK], DatanodeInfoWithStorage[127.0.0.1:35246,DS-67617f46-9c7f-44ec-b32b-28845ded09fe,DISK], DatanodeInfoWithStorage[127.0.0.1:42056,DS-14e4934d-3b46-44b5-b8f5-4f9aa48a7965,DISK], DatanodeInfoWithStorage[127.0.0.1:45108,DS-35cc6cda-7e0f-4a6e-a5a5-70eaaf1d361c,DISK], DatanodeInfoWithStorage[127.0.0.1:38761,DS-bac64a27-b62c-4993-912f-44087202a9fc,DISK], DatanodeInfoWithStorage[127.0.0.1:40441,DS-1f73b710-7176-49c3-befe-16d3873dbcc9,DISK], DatanodeInfoWithStorage[127.0.0.1:44609,DS-5e17ce6a-5478-4da4-b4b4-ed1889d2bae8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1356372913-172.17.0.10-1595512157769:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33190,DS-85403ab2-2fb4-422b-83b8-9cc38236b7e5,DISK], DatanodeInfoWithStorage[127.0.0.1:37934,DS-4ebccd83-c51c-431c-a10c-8ee888fda129,DISK], DatanodeInfoWithStorage[127.0.0.1:35246,DS-67617f46-9c7f-44ec-b32b-28845ded09fe,DISK], DatanodeInfoWithStorage[127.0.0.1:42056,DS-14e4934d-3b46-44b5-b8f5-4f9aa48a7965,DISK], DatanodeInfoWithStorage[127.0.0.1:45108,DS-35cc6cda-7e0f-4a6e-a5a5-70eaaf1d361c,DISK], DatanodeInfoWithStorage[127.0.0.1:38761,DS-bac64a27-b62c-4993-912f-44087202a9fc,DISK], DatanodeInfoWithStorage[127.0.0.1:40441,DS-1f73b710-7176-49c3-befe-16d3873dbcc9,DISK], DatanodeInfoWithStorage[127.0.0.1:44609,DS-5e17ce6a-5478-4da4-b4b4-ed1889d2bae8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-406974611-172.17.0.10-1595512296090:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46684,DS-69c75580-f33d-4adc-9714-574bb6df9f0b,DISK], DatanodeInfoWithStorage[127.0.0.1:42744,DS-a7d6f697-f874-47d3-adf4-238f04cb4a57,DISK], DatanodeInfoWithStorage[127.0.0.1:36740,DS-2ce44b60-3eee-49a1-8517-3a88dca15a66,DISK], DatanodeInfoWithStorage[127.0.0.1:45955,DS-e165fbdb-3a23-4c9d-9d77-e8eb83bd9e77,DISK], DatanodeInfoWithStorage[127.0.0.1:37159,DS-7129ab79-99a5-499b-ad6b-c85d73fd9e75,DISK], DatanodeInfoWithStorage[127.0.0.1:34132,DS-ad847f6b-949b-4b60-a0c5-e9a7b070f9ea,DISK], DatanodeInfoWithStorage[127.0.0.1:45571,DS-ff05c6e9-da9c-476b-8359-14ab9e763032,DISK], DatanodeInfoWithStorage[127.0.0.1:41448,DS-259a662e-4ead-4936-8b83-50a2d6e88a57,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-406974611-172.17.0.10-1595512296090:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46684,DS-69c75580-f33d-4adc-9714-574bb6df9f0b,DISK], DatanodeInfoWithStorage[127.0.0.1:42744,DS-a7d6f697-f874-47d3-adf4-238f04cb4a57,DISK], DatanodeInfoWithStorage[127.0.0.1:36740,DS-2ce44b60-3eee-49a1-8517-3a88dca15a66,DISK], DatanodeInfoWithStorage[127.0.0.1:45955,DS-e165fbdb-3a23-4c9d-9d77-e8eb83bd9e77,DISK], DatanodeInfoWithStorage[127.0.0.1:37159,DS-7129ab79-99a5-499b-ad6b-c85d73fd9e75,DISK], DatanodeInfoWithStorage[127.0.0.1:34132,DS-ad847f6b-949b-4b60-a0c5-e9a7b070f9ea,DISK], DatanodeInfoWithStorage[127.0.0.1:45571,DS-ff05c6e9-da9c-476b-8359-14ab9e763032,DISK], DatanodeInfoWithStorage[127.0.0.1:41448,DS-259a662e-4ead-4936-8b83-50a2d6e88a57,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-585989750-172.17.0.10-1595512552055:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40794,DS-1e16011f-e1f3-401e-bdf7-799be7b5ccb2,DISK], DatanodeInfoWithStorage[127.0.0.1:35486,DS-e2ce4aef-db2f-4334-929a-524d5ba4e0c0,DISK], DatanodeInfoWithStorage[127.0.0.1:42345,DS-46c87a91-9114-42ad-a544-fe5e12143b24,DISK], DatanodeInfoWithStorage[127.0.0.1:36408,DS-8cc51406-951f-49c6-b1fc-f87ae73501d7,DISK], DatanodeInfoWithStorage[127.0.0.1:44124,DS-5fbc3df4-38fb-49f6-99eb-04f82bcdc310,DISK], DatanodeInfoWithStorage[127.0.0.1:39162,DS-ab8b0862-54c6-4871-a3c3-85db057d11b4,DISK], DatanodeInfoWithStorage[127.0.0.1:40215,DS-bb155255-69c5-4729-a77d-4a5f8917e8fc,DISK], DatanodeInfoWithStorage[127.0.0.1:42839,DS-f559de36-ccec-43c9-8a65-92b83ec523c4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-585989750-172.17.0.10-1595512552055:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40794,DS-1e16011f-e1f3-401e-bdf7-799be7b5ccb2,DISK], DatanodeInfoWithStorage[127.0.0.1:35486,DS-e2ce4aef-db2f-4334-929a-524d5ba4e0c0,DISK], DatanodeInfoWithStorage[127.0.0.1:42345,DS-46c87a91-9114-42ad-a544-fe5e12143b24,DISK], DatanodeInfoWithStorage[127.0.0.1:36408,DS-8cc51406-951f-49c6-b1fc-f87ae73501d7,DISK], DatanodeInfoWithStorage[127.0.0.1:44124,DS-5fbc3df4-38fb-49f6-99eb-04f82bcdc310,DISK], DatanodeInfoWithStorage[127.0.0.1:39162,DS-ab8b0862-54c6-4871-a3c3-85db057d11b4,DISK], DatanodeInfoWithStorage[127.0.0.1:40215,DS-bb155255-69c5-4729-a77d-4a5f8917e8fc,DISK], DatanodeInfoWithStorage[127.0.0.1:42839,DS-f559de36-ccec-43c9-8a65-92b83ec523c4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: Call From ed27b18c88cc/172.17.0.10 to localhost:37411 failed on socket timeout exception: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:50030 remote=localhost/127.0.0.1:37411]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
stackTrace: java.net.SocketTimeoutException: Call From ed27b18c88cc/172.17.0.10 to localhost:37411 failed on socket timeout exception: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:50030 remote=localhost/127.0.0.1:37411]; For more details see:  http://wiki.apache.org/hadoop/SocketTimeout
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:833)
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:777)
	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1549)
	at org.apache.hadoop.ipc.Client.call(Client.java:1491)
	at org.apache.hadoop.ipc.Client.call(Client.java:1388)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:233)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
	at com.sun.proxy.$Proxy28.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:660)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy29.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2425)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2401)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1318)
	at org.apache.hadoop.hdfs.DistributedFileSystem$27.doCall(DistributedFileSystem.java:1315)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:1332)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdir(DistributedFileSystem.java:1291)
	at org.apache.hadoop.hdfs.TestFileChecksum.setup(TestFileChecksum.java:95)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.ExpectedException$ExpectedExceptionStatement.evaluate(ExpectedException.java:168)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)
Caused by: java.net.SocketTimeoutException: 100 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/127.0.0.1:50030 remote=localhost/127.0.0.1:37411]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:164)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:161)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:131)
	at java.io.FilterInputStream.read(FilterInputStream.java:133)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:265)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at java.io.FilterInputStream.read(FilterInputStream.java:83)
	at org.apache.hadoop.ipc.Client$Connection$PingInputStream.read(Client.java:567)
	at java.io.DataInputStream.readInt(DataInputStream.java:387)
	at org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1850)
	at org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1183)
	at org.apache.hadoop.ipc.Client$Connection.run(Client.java:1079)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-831812819-172.17.0.10-1595512940772:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42802,DS-454718f6-ec58-45d9-baab-221ff0ba6174,DISK], DatanodeInfoWithStorage[127.0.0.1:43943,DS-cfcc8901-1ed3-4e9b-b9bf-15fdf746e17b,DISK], DatanodeInfoWithStorage[127.0.0.1:36614,DS-ba983bd2-aefd-49c8-9232-bc214a5539b6,DISK], DatanodeInfoWithStorage[127.0.0.1:37474,DS-120dcf2c-24ad-4894-8eb3-db328db36c92,DISK], DatanodeInfoWithStorage[127.0.0.1:41545,DS-2e08f5fd-c90b-49f2-99f6-d87461af88f4,DISK], DatanodeInfoWithStorage[127.0.0.1:39837,DS-eaf7623a-221f-4df4-8008-051c90c9b378,DISK], DatanodeInfoWithStorage[127.0.0.1:41560,DS-b8e8a10d-a757-40ae-a995-e371240886c2,DISK], DatanodeInfoWithStorage[127.0.0.1:40266,DS-3b738366-d6d7-47de-b88d-65648087051d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-831812819-172.17.0.10-1595512940772:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42802,DS-454718f6-ec58-45d9-baab-221ff0ba6174,DISK], DatanodeInfoWithStorage[127.0.0.1:43943,DS-cfcc8901-1ed3-4e9b-b9bf-15fdf746e17b,DISK], DatanodeInfoWithStorage[127.0.0.1:36614,DS-ba983bd2-aefd-49c8-9232-bc214a5539b6,DISK], DatanodeInfoWithStorage[127.0.0.1:37474,DS-120dcf2c-24ad-4894-8eb3-db328db36c92,DISK], DatanodeInfoWithStorage[127.0.0.1:41545,DS-2e08f5fd-c90b-49f2-99f6-d87461af88f4,DISK], DatanodeInfoWithStorage[127.0.0.1:39837,DS-eaf7623a-221f-4df4-8008-051c90c9b378,DISK], DatanodeInfoWithStorage[127.0.0.1:41560,DS-b8e8a10d-a757-40ae-a995-e371240886c2,DISK], DatanodeInfoWithStorage[127.0.0.1:40266,DS-3b738366-d6d7-47de-b88d-65648087051d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.rpc-timeout.ms
component: hdfs:DataNode
v1: 100
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery20
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2001248646-172.17.0.10-1595513228935:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35112,DS-e9aa30f0-3a5c-4ede-ac2d-8be2c1911ba0,DISK], DatanodeInfoWithStorage[127.0.0.1:40626,DS-99825dc7-5421-4a9f-a68d-a71c8c1833ab,DISK], DatanodeInfoWithStorage[127.0.0.1:41488,DS-308f8040-069d-4179-9a08-e159ce205001,DISK], DatanodeInfoWithStorage[127.0.0.1:40640,DS-39cd4807-7717-4985-8ec4-8b94bf35c3cc,DISK], DatanodeInfoWithStorage[127.0.0.1:34999,DS-a2783a8e-08e1-44d0-a5ec-2beababc9525,DISK], DatanodeInfoWithStorage[127.0.0.1:33772,DS-9a729fd3-ac08-4545-a299-3a574885aca6,DISK], DatanodeInfoWithStorage[127.0.0.1:37696,DS-b34992a6-1679-41c4-8da5-7b483d108c85,DISK], DatanodeInfoWithStorage[127.0.0.1:35717,DS-13c3b04f-8a4f-456d-a233-c53182774b23,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2001248646-172.17.0.10-1595513228935:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35112,DS-e9aa30f0-3a5c-4ede-ac2d-8be2c1911ba0,DISK], DatanodeInfoWithStorage[127.0.0.1:40626,DS-99825dc7-5421-4a9f-a68d-a71c8c1833ab,DISK], DatanodeInfoWithStorage[127.0.0.1:41488,DS-308f8040-069d-4179-9a08-e159ce205001,DISK], DatanodeInfoWithStorage[127.0.0.1:40640,DS-39cd4807-7717-4985-8ec4-8b94bf35c3cc,DISK], DatanodeInfoWithStorage[127.0.0.1:34999,DS-a2783a8e-08e1-44d0-a5ec-2beababc9525,DISK], DatanodeInfoWithStorage[127.0.0.1:33772,DS-9a729fd3-ac08-4545-a299-3a574885aca6,DISK], DatanodeInfoWithStorage[127.0.0.1:37696,DS-b34992a6-1679-41c4-8da5-7b483d108c85,DISK], DatanodeInfoWithStorage[127.0.0.1:35717,DS-13c3b04f-8a4f-456d-a233-c53182774b23,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery20(TestFileChecksum.java:533)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 8 out of 50
v1v1v2v2 failed with probability 12 out of 50
result: false positive !!!
Total execution time in seconds : 5207

reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2019627645-172.17.0.7-1595883694531:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36434,DS-e8989c35-9752-4650-a38d-2b8e4c3ff838,DISK], DatanodeInfoWithStorage[127.0.0.1:45136,DS-fd5ff79c-3238-4881-a0a0-ba435445e932,DISK], DatanodeInfoWithStorage[127.0.0.1:35421,DS-3aae79de-19c6-4305-8391-be5dc563aa35,DISK], DatanodeInfoWithStorage[127.0.0.1:43514,DS-601f5435-9183-4486-bd8e-d6fad7376303,DISK], DatanodeInfoWithStorage[127.0.0.1:38748,DS-f8e20bbd-2d42-43bc-8b28-8ab72378a2f4,DISK], DatanodeInfoWithStorage[127.0.0.1:42680,DS-5962b669-86d1-4fda-b4e0-30ea364b9b06,DISK], DatanodeInfoWithStorage[127.0.0.1:38023,DS-5baa3469-1b9a-48fc-b53c-3d49ce4e83cc,DISK], DatanodeInfoWithStorage[127.0.0.1:39807,DS-906aa282-62d8-4e7a-b0e0-83f46c0cb6d0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2019627645-172.17.0.7-1595883694531:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36434,DS-e8989c35-9752-4650-a38d-2b8e4c3ff838,DISK], DatanodeInfoWithStorage[127.0.0.1:45136,DS-fd5ff79c-3238-4881-a0a0-ba435445e932,DISK], DatanodeInfoWithStorage[127.0.0.1:35421,DS-3aae79de-19c6-4305-8391-be5dc563aa35,DISK], DatanodeInfoWithStorage[127.0.0.1:43514,DS-601f5435-9183-4486-bd8e-d6fad7376303,DISK], DatanodeInfoWithStorage[127.0.0.1:38748,DS-f8e20bbd-2d42-43bc-8b28-8ab72378a2f4,DISK], DatanodeInfoWithStorage[127.0.0.1:42680,DS-5962b669-86d1-4fda-b4e0-30ea364b9b06,DISK], DatanodeInfoWithStorage[127.0.0.1:38023,DS-5baa3469-1b9a-48fc-b53c-3d49ce4e83cc,DISK], DatanodeInfoWithStorage[127.0.0.1:39807,DS-906aa282-62d8-4e7a-b0e0-83f46c0cb6d0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1257751604-172.17.0.7-1595884151353:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46477,DS-21db68f8-0a62-4896-be76-bb63171e18aa,DISK], DatanodeInfoWithStorage[127.0.0.1:39308,DS-efa4170c-46b1-4824-8be8-df1082491f9d,DISK], DatanodeInfoWithStorage[127.0.0.1:37681,DS-2f18b5b3-1ded-4aa6-897d-8ad3205e147a,DISK], DatanodeInfoWithStorage[127.0.0.1:42187,DS-baeb3333-49d2-4276-8f64-2327b8158604,DISK], DatanodeInfoWithStorage[127.0.0.1:41540,DS-bf756e98-54ae-414a-b164-9ad1d0cb76ad,DISK], DatanodeInfoWithStorage[127.0.0.1:37220,DS-c8f3e816-4fec-4583-b1dd-ced795e80235,DISK], DatanodeInfoWithStorage[127.0.0.1:38042,DS-8a9a16db-42b3-4301-a30c-a54a1a43b16a,DISK], DatanodeInfoWithStorage[127.0.0.1:41585,DS-23db520e-f37d-4675-a513-dd9833b9efdd,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1257751604-172.17.0.7-1595884151353:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46477,DS-21db68f8-0a62-4896-be76-bb63171e18aa,DISK], DatanodeInfoWithStorage[127.0.0.1:39308,DS-efa4170c-46b1-4824-8be8-df1082491f9d,DISK], DatanodeInfoWithStorage[127.0.0.1:37681,DS-2f18b5b3-1ded-4aa6-897d-8ad3205e147a,DISK], DatanodeInfoWithStorage[127.0.0.1:42187,DS-baeb3333-49d2-4276-8f64-2327b8158604,DISK], DatanodeInfoWithStorage[127.0.0.1:41540,DS-bf756e98-54ae-414a-b164-9ad1d0cb76ad,DISK], DatanodeInfoWithStorage[127.0.0.1:37220,DS-c8f3e816-4fec-4583-b1dd-ced795e80235,DISK], DatanodeInfoWithStorage[127.0.0.1:38042,DS-8a9a16db-42b3-4301-a30c-a54a1a43b16a,DISK], DatanodeInfoWithStorage[127.0.0.1:41585,DS-23db520e-f37d-4675-a513-dd9833b9efdd,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1434500637-172.17.0.7-1595884402670:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41452,DS-8d53ae74-b1fc-4048-8363-2a1c4609de59,DISK], DatanodeInfoWithStorage[127.0.0.1:33788,DS-e36c2b04-5c8e-461d-af5c-73f8a79ee144,DISK], DatanodeInfoWithStorage[127.0.0.1:33783,DS-242c15e4-6590-4f4d-9bd6-2630b48e0642,DISK], DatanodeInfoWithStorage[127.0.0.1:40769,DS-a10bb4d5-182a-4cd1-bab4-d5c43f548874,DISK], DatanodeInfoWithStorage[127.0.0.1:33328,DS-82d482e5-9dc0-4505-adf9-2b29b73e5899,DISK], DatanodeInfoWithStorage[127.0.0.1:40460,DS-24d07c74-4361-48d9-97de-60154b7950db,DISK], DatanodeInfoWithStorage[127.0.0.1:38024,DS-06cca739-f87f-4c33-9f98-0f2c23996911,DISK], DatanodeInfoWithStorage[127.0.0.1:45151,DS-5bcd5be9-434f-4758-a85d-9b9496a21375,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1434500637-172.17.0.7-1595884402670:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41452,DS-8d53ae74-b1fc-4048-8363-2a1c4609de59,DISK], DatanodeInfoWithStorage[127.0.0.1:33788,DS-e36c2b04-5c8e-461d-af5c-73f8a79ee144,DISK], DatanodeInfoWithStorage[127.0.0.1:33783,DS-242c15e4-6590-4f4d-9bd6-2630b48e0642,DISK], DatanodeInfoWithStorage[127.0.0.1:40769,DS-a10bb4d5-182a-4cd1-bab4-d5c43f548874,DISK], DatanodeInfoWithStorage[127.0.0.1:33328,DS-82d482e5-9dc0-4505-adf9-2b29b73e5899,DISK], DatanodeInfoWithStorage[127.0.0.1:40460,DS-24d07c74-4361-48d9-97de-60154b7950db,DISK], DatanodeInfoWithStorage[127.0.0.1:38024,DS-06cca739-f87f-4c33-9f98-0f2c23996911,DISK], DatanodeInfoWithStorage[127.0.0.1:45151,DS-5bcd5be9-434f-4758-a85d-9b9496a21375,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-688247055-172.17.0.7-1595884497187:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45863,DS-a319ff12-2bf3-4721-98b2-e37707e51348,DISK], DatanodeInfoWithStorage[127.0.0.1:35935,DS-ffd37d5e-3833-49d6-bea1-8aacb2cc77e9,DISK], DatanodeInfoWithStorage[127.0.0.1:43320,DS-75bcad95-bb2d-478d-98cc-d724c862fe44,DISK], DatanodeInfoWithStorage[127.0.0.1:37321,DS-46d31380-5b9c-428f-8d4d-476f6d4a789c,DISK], DatanodeInfoWithStorage[127.0.0.1:35721,DS-648f1d88-5d28-45d6-95ee-b41d162f90ec,DISK], DatanodeInfoWithStorage[127.0.0.1:42454,DS-4eb06b8e-cb31-4852-93ee-394ef54337e5,DISK], DatanodeInfoWithStorage[127.0.0.1:34365,DS-be80f04f-4a1f-454f-8326-b45538c246dd,DISK], DatanodeInfoWithStorage[127.0.0.1:42380,DS-9c55edc6-bb47-4b62-8e93-8b393d59c757,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-688247055-172.17.0.7-1595884497187:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45863,DS-a319ff12-2bf3-4721-98b2-e37707e51348,DISK], DatanodeInfoWithStorage[127.0.0.1:35935,DS-ffd37d5e-3833-49d6-bea1-8aacb2cc77e9,DISK], DatanodeInfoWithStorage[127.0.0.1:43320,DS-75bcad95-bb2d-478d-98cc-d724c862fe44,DISK], DatanodeInfoWithStorage[127.0.0.1:37321,DS-46d31380-5b9c-428f-8d4d-476f6d4a789c,DISK], DatanodeInfoWithStorage[127.0.0.1:35721,DS-648f1d88-5d28-45d6-95ee-b41d162f90ec,DISK], DatanodeInfoWithStorage[127.0.0.1:42454,DS-4eb06b8e-cb31-4852-93ee-394ef54337e5,DISK], DatanodeInfoWithStorage[127.0.0.1:34365,DS-be80f04f-4a1f-454f-8326-b45538c246dd,DISK], DatanodeInfoWithStorage[127.0.0.1:42380,DS-9c55edc6-bb47-4b62-8e93-8b393d59c757,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1909580158-172.17.0.7-1595884588863:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37448,DS-1f49b751-00e2-4c70-b22f-b10b9e06ca31,DISK], DatanodeInfoWithStorage[127.0.0.1:43626,DS-abb8d510-7806-43c4-a9d8-b8188d5bf0bb,DISK], DatanodeInfoWithStorage[127.0.0.1:45874,DS-46c9a86d-44f5-47d6-8156-70b3af14be78,DISK], DatanodeInfoWithStorage[127.0.0.1:46146,DS-12870e17-ab6f-41e1-bd5c-b7c2bf9ba7b3,DISK], DatanodeInfoWithStorage[127.0.0.1:40942,DS-043c2152-eb9c-4389-b322-3805f8f6209c,DISK], DatanodeInfoWithStorage[127.0.0.1:39574,DS-994b3493-2373-4a9b-bbdf-1ed4006bdafb,DISK], DatanodeInfoWithStorage[127.0.0.1:33632,DS-a9edccee-3f72-4d8c-bcb9-8c76ba2ff522,DISK], DatanodeInfoWithStorage[127.0.0.1:34552,DS-3bae9cf9-7198-4c20-a440-fe50d249a9db,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1909580158-172.17.0.7-1595884588863:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37448,DS-1f49b751-00e2-4c70-b22f-b10b9e06ca31,DISK], DatanodeInfoWithStorage[127.0.0.1:43626,DS-abb8d510-7806-43c4-a9d8-b8188d5bf0bb,DISK], DatanodeInfoWithStorage[127.0.0.1:45874,DS-46c9a86d-44f5-47d6-8156-70b3af14be78,DISK], DatanodeInfoWithStorage[127.0.0.1:46146,DS-12870e17-ab6f-41e1-bd5c-b7c2bf9ba7b3,DISK], DatanodeInfoWithStorage[127.0.0.1:40942,DS-043c2152-eb9c-4389-b322-3805f8f6209c,DISK], DatanodeInfoWithStorage[127.0.0.1:39574,DS-994b3493-2373-4a9b-bbdf-1ed4006bdafb,DISK], DatanodeInfoWithStorage[127.0.0.1:33632,DS-a9edccee-3f72-4d8c-bcb9-8c76ba2ff522,DISK], DatanodeInfoWithStorage[127.0.0.1:34552,DS-3bae9cf9-7198-4c20-a440-fe50d249a9db,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1145480885-172.17.0.7-1595884748707:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37870,DS-13905166-cb80-45c4-9db4-5aaa11d2c7d4,DISK], DatanodeInfoWithStorage[127.0.0.1:33806,DS-bf4e546c-4d0d-478d-a8d6-53475c8fad8b,DISK], DatanodeInfoWithStorage[127.0.0.1:34493,DS-1c802635-b699-4ea5-aff9-09bde4b06f2a,DISK], DatanodeInfoWithStorage[127.0.0.1:43358,DS-90da4d60-0b74-45f7-8a8c-de694693fd1c,DISK], DatanodeInfoWithStorage[127.0.0.1:40427,DS-c9de822d-1425-4111-a202-1ec00ac196f1,DISK], DatanodeInfoWithStorage[127.0.0.1:39453,DS-777f7d33-b5ba-422e-8a5f-6aeacaf99e83,DISK], DatanodeInfoWithStorage[127.0.0.1:41291,DS-652b9c9e-668f-420f-bbdd-13649c066d50,DISK], DatanodeInfoWithStorage[127.0.0.1:42409,DS-a894c077-dec6-4f72-82c1-d63139dbf4b2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1145480885-172.17.0.7-1595884748707:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37870,DS-13905166-cb80-45c4-9db4-5aaa11d2c7d4,DISK], DatanodeInfoWithStorage[127.0.0.1:33806,DS-bf4e546c-4d0d-478d-a8d6-53475c8fad8b,DISK], DatanodeInfoWithStorage[127.0.0.1:34493,DS-1c802635-b699-4ea5-aff9-09bde4b06f2a,DISK], DatanodeInfoWithStorage[127.0.0.1:43358,DS-90da4d60-0b74-45f7-8a8c-de694693fd1c,DISK], DatanodeInfoWithStorage[127.0.0.1:40427,DS-c9de822d-1425-4111-a202-1ec00ac196f1,DISK], DatanodeInfoWithStorage[127.0.0.1:39453,DS-777f7d33-b5ba-422e-8a5f-6aeacaf99e83,DISK], DatanodeInfoWithStorage[127.0.0.1:41291,DS-652b9c9e-668f-420f-bbdd-13649c066d50,DISK], DatanodeInfoWithStorage[127.0.0.1:42409,DS-a894c077-dec6-4f72-82c1-d63139dbf4b2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1311927476-172.17.0.7-1595884951805:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38348,DS-ee13eda1-d024-495a-b2cf-2abb339e64d4,DISK], DatanodeInfoWithStorage[127.0.0.1:41041,DS-8e355e37-dffb-4c61-8ec2-25e32eb29803,DISK], DatanodeInfoWithStorage[127.0.0.1:39153,DS-dbf552a3-9e0d-48d0-ad02-ad670fc0daef,DISK], DatanodeInfoWithStorage[127.0.0.1:34355,DS-b49b6099-47db-4ef9-af08-9bf4e5cc03a5,DISK], DatanodeInfoWithStorage[127.0.0.1:46325,DS-d4f5ee15-29ac-442d-9955-f9bd5c444451,DISK], DatanodeInfoWithStorage[127.0.0.1:41645,DS-0f597efe-0891-4716-a176-5896b5d1deca,DISK], DatanodeInfoWithStorage[127.0.0.1:43470,DS-52ff9ae4-1930-4ee6-8e50-6874e2cc1e4e,DISK], DatanodeInfoWithStorage[127.0.0.1:33144,DS-37a6de5d-ef7e-4451-8a6e-196dba42dc7a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1311927476-172.17.0.7-1595884951805:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38348,DS-ee13eda1-d024-495a-b2cf-2abb339e64d4,DISK], DatanodeInfoWithStorage[127.0.0.1:41041,DS-8e355e37-dffb-4c61-8ec2-25e32eb29803,DISK], DatanodeInfoWithStorage[127.0.0.1:39153,DS-dbf552a3-9e0d-48d0-ad02-ad670fc0daef,DISK], DatanodeInfoWithStorage[127.0.0.1:34355,DS-b49b6099-47db-4ef9-af08-9bf4e5cc03a5,DISK], DatanodeInfoWithStorage[127.0.0.1:46325,DS-d4f5ee15-29ac-442d-9955-f9bd5c444451,DISK], DatanodeInfoWithStorage[127.0.0.1:41645,DS-0f597efe-0891-4716-a176-5896b5d1deca,DISK], DatanodeInfoWithStorage[127.0.0.1:43470,DS-52ff9ae4-1930-4ee6-8e50-6874e2cc1e4e,DISK], DatanodeInfoWithStorage[127.0.0.1:33144,DS-37a6de5d-ef7e-4451-8a6e-196dba42dc7a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-198470210-172.17.0.7-1595885016437:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43455,DS-917fbe37-ec61-4aa0-bd20-dda7c2822bf3,DISK], DatanodeInfoWithStorage[127.0.0.1:33577,DS-8123d6d0-f83e-4a15-8c7b-f54140712d26,DISK], DatanodeInfoWithStorage[127.0.0.1:36214,DS-44bc7434-3c1e-444f-bdcf-9e0cf0c41940,DISK], DatanodeInfoWithStorage[127.0.0.1:35992,DS-22f7d49e-11af-4c3b-a77c-0ac0d9238393,DISK], DatanodeInfoWithStorage[127.0.0.1:42491,DS-7456a380-8656-4d7c-b7de-9170c0ff5fbf,DISK], DatanodeInfoWithStorage[127.0.0.1:37188,DS-b3a5419f-5717-45b8-aa66-9415f8284bd3,DISK], DatanodeInfoWithStorage[127.0.0.1:38470,DS-993f8fbd-ab7d-47be-a2c8-5d540260960c,DISK], DatanodeInfoWithStorage[127.0.0.1:45887,DS-6768352a-7e45-47d2-abfd-40eabfc80638,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-198470210-172.17.0.7-1595885016437:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43455,DS-917fbe37-ec61-4aa0-bd20-dda7c2822bf3,DISK], DatanodeInfoWithStorage[127.0.0.1:33577,DS-8123d6d0-f83e-4a15-8c7b-f54140712d26,DISK], DatanodeInfoWithStorage[127.0.0.1:36214,DS-44bc7434-3c1e-444f-bdcf-9e0cf0c41940,DISK], DatanodeInfoWithStorage[127.0.0.1:35992,DS-22f7d49e-11af-4c3b-a77c-0ac0d9238393,DISK], DatanodeInfoWithStorage[127.0.0.1:42491,DS-7456a380-8656-4d7c-b7de-9170c0ff5fbf,DISK], DatanodeInfoWithStorage[127.0.0.1:37188,DS-b3a5419f-5717-45b8-aa66-9415f8284bd3,DISK], DatanodeInfoWithStorage[127.0.0.1:38470,DS-993f8fbd-ab7d-47be-a2c8-5d540260960c,DISK], DatanodeInfoWithStorage[127.0.0.1:45887,DS-6768352a-7e45-47d2-abfd-40eabfc80638,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2006804190-172.17.0.7-1595885082536:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43260,DS-13fa69fb-f3d4-418e-9cd1-9edae067a6f4,DISK], DatanodeInfoWithStorage[127.0.0.1:40799,DS-615f4fb0-196b-4daf-bec9-37adaf5d74d7,DISK], DatanodeInfoWithStorage[127.0.0.1:35186,DS-4b060ed3-f283-4c73-9114-24e0939fd28b,DISK], DatanodeInfoWithStorage[127.0.0.1:44025,DS-e74e2f6c-76a4-4b7f-bc8c-feb6b939ebe5,DISK], DatanodeInfoWithStorage[127.0.0.1:44744,DS-e95df667-903f-4596-8052-4d88ec8c9ee7,DISK], DatanodeInfoWithStorage[127.0.0.1:38736,DS-1b4bb0fa-8046-473e-a2fc-e58fbbbef15e,DISK], DatanodeInfoWithStorage[127.0.0.1:34766,DS-45d1b8b6-3deb-404c-938c-be07c03e2e5a,DISK], DatanodeInfoWithStorage[127.0.0.1:44393,DS-ef744e8c-86ef-435d-b844-72aee2c5b342,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2006804190-172.17.0.7-1595885082536:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43260,DS-13fa69fb-f3d4-418e-9cd1-9edae067a6f4,DISK], DatanodeInfoWithStorage[127.0.0.1:40799,DS-615f4fb0-196b-4daf-bec9-37adaf5d74d7,DISK], DatanodeInfoWithStorage[127.0.0.1:35186,DS-4b060ed3-f283-4c73-9114-24e0939fd28b,DISK], DatanodeInfoWithStorage[127.0.0.1:44025,DS-e74e2f6c-76a4-4b7f-bc8c-feb6b939ebe5,DISK], DatanodeInfoWithStorage[127.0.0.1:44744,DS-e95df667-903f-4596-8052-4d88ec8c9ee7,DISK], DatanodeInfoWithStorage[127.0.0.1:38736,DS-1b4bb0fa-8046-473e-a2fc-e58fbbbef15e,DISK], DatanodeInfoWithStorage[127.0.0.1:34766,DS-45d1b8b6-3deb-404c-938c-be07c03e2e5a,DISK], DatanodeInfoWithStorage[127.0.0.1:44393,DS-ef744e8c-86ef-435d-b844-72aee2c5b342,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-966647569-172.17.0.7-1595885679805:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39196,DS-a9bc8622-e586-45e0-b305-5af09185ef1c,DISK], DatanodeInfoWithStorage[127.0.0.1:42288,DS-a7910ef6-e463-435b-b609-b5fe77eeb63f,DISK], DatanodeInfoWithStorage[127.0.0.1:38811,DS-e98a62b1-f2ff-4f81-a827-e9b4c096a7f6,DISK], DatanodeInfoWithStorage[127.0.0.1:43846,DS-16311803-7767-4c14-aab2-8d3a8947a46d,DISK], DatanodeInfoWithStorage[127.0.0.1:40587,DS-4c1ec839-cde3-4b53-8a32-6c72b1241efa,DISK], DatanodeInfoWithStorage[127.0.0.1:44004,DS-5b64917c-8051-405e-9c08-bdc30aa1fdca,DISK], DatanodeInfoWithStorage[127.0.0.1:36854,DS-7d65e709-de59-4a5f-8cf7-3fae30ea4d2e,DISK], DatanodeInfoWithStorage[127.0.0.1:34264,DS-2829e1d2-8b0d-41d7-ac05-463b1f669ef5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-966647569-172.17.0.7-1595885679805:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39196,DS-a9bc8622-e586-45e0-b305-5af09185ef1c,DISK], DatanodeInfoWithStorage[127.0.0.1:42288,DS-a7910ef6-e463-435b-b609-b5fe77eeb63f,DISK], DatanodeInfoWithStorage[127.0.0.1:38811,DS-e98a62b1-f2ff-4f81-a827-e9b4c096a7f6,DISK], DatanodeInfoWithStorage[127.0.0.1:43846,DS-16311803-7767-4c14-aab2-8d3a8947a46d,DISK], DatanodeInfoWithStorage[127.0.0.1:40587,DS-4c1ec839-cde3-4b53-8a32-6c72b1241efa,DISK], DatanodeInfoWithStorage[127.0.0.1:44004,DS-5b64917c-8051-405e-9c08-bdc30aa1fdca,DISK], DatanodeInfoWithStorage[127.0.0.1:36854,DS-7d65e709-de59-4a5f-8cf7-3fae30ea4d2e,DISK], DatanodeInfoWithStorage[127.0.0.1:34264,DS-2829e1d2-8b0d-41d7-ac05-463b1f669ef5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-295894734-172.17.0.7-1595885825915:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37073,DS-9744d413-815a-41dc-91e5-a4c3b956fb79,DISK], DatanodeInfoWithStorage[127.0.0.1:41954,DS-cbf35e9a-5a10-4ea2-80c1-e19311a6b4fc,DISK], DatanodeInfoWithStorage[127.0.0.1:46155,DS-ea0beaa8-5721-4f5a-8440-af886acd0bd8,DISK], DatanodeInfoWithStorage[127.0.0.1:46883,DS-0e21dcf8-c179-4637-88ed-fe06cfa5f0e0,DISK], DatanodeInfoWithStorage[127.0.0.1:40136,DS-ac534a5c-9768-41cd-b2f3-2bfae33d0c43,DISK], DatanodeInfoWithStorage[127.0.0.1:40937,DS-3640fd92-1c5b-4c9f-8461-54ecac2ccd8d,DISK], DatanodeInfoWithStorage[127.0.0.1:40587,DS-d9117f62-2c04-41d0-a75a-7faf3086b410,DISK], DatanodeInfoWithStorage[127.0.0.1:40527,DS-2cc04ef8-8aad-4aeb-a1cd-41102fb3521a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-295894734-172.17.0.7-1595885825915:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37073,DS-9744d413-815a-41dc-91e5-a4c3b956fb79,DISK], DatanodeInfoWithStorage[127.0.0.1:41954,DS-cbf35e9a-5a10-4ea2-80c1-e19311a6b4fc,DISK], DatanodeInfoWithStorage[127.0.0.1:46155,DS-ea0beaa8-5721-4f5a-8440-af886acd0bd8,DISK], DatanodeInfoWithStorage[127.0.0.1:46883,DS-0e21dcf8-c179-4637-88ed-fe06cfa5f0e0,DISK], DatanodeInfoWithStorage[127.0.0.1:40136,DS-ac534a5c-9768-41cd-b2f3-2bfae33d0c43,DISK], DatanodeInfoWithStorage[127.0.0.1:40937,DS-3640fd92-1c5b-4c9f-8461-54ecac2ccd8d,DISK], DatanodeInfoWithStorage[127.0.0.1:40587,DS-d9117f62-2c04-41d0-a75a-7faf3086b410,DISK], DatanodeInfoWithStorage[127.0.0.1:40527,DS-2cc04ef8-8aad-4aeb-a1cd-41102fb3521a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-101917266-172.17.0.7-1595886683773:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38577,DS-1ace0b5f-8fa6-4cfd-9a2a-a1b1a66e8d50,DISK], DatanodeInfoWithStorage[127.0.0.1:33971,DS-644ac71e-3672-4310-a02a-e54757372731,DISK], DatanodeInfoWithStorage[127.0.0.1:45166,DS-123723d0-134e-4815-88cd-2be4ea56e626,DISK], DatanodeInfoWithStorage[127.0.0.1:33298,DS-1aa18176-1460-4e22-ad7c-889c56963319,DISK], DatanodeInfoWithStorage[127.0.0.1:39776,DS-4c7b8fba-a313-4fa9-9f01-134e2281f252,DISK], DatanodeInfoWithStorage[127.0.0.1:43759,DS-1e5dd447-a430-4d54-b6a1-13193e178335,DISK], DatanodeInfoWithStorage[127.0.0.1:35813,DS-fd3e89fc-bc32-42fb-a74e-90985b01a0ed,DISK], DatanodeInfoWithStorage[127.0.0.1:36790,DS-3c116ac6-59f5-4239-b6dc-391f5cde8efb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-101917266-172.17.0.7-1595886683773:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38577,DS-1ace0b5f-8fa6-4cfd-9a2a-a1b1a66e8d50,DISK], DatanodeInfoWithStorage[127.0.0.1:33971,DS-644ac71e-3672-4310-a02a-e54757372731,DISK], DatanodeInfoWithStorage[127.0.0.1:45166,DS-123723d0-134e-4815-88cd-2be4ea56e626,DISK], DatanodeInfoWithStorage[127.0.0.1:33298,DS-1aa18176-1460-4e22-ad7c-889c56963319,DISK], DatanodeInfoWithStorage[127.0.0.1:39776,DS-4c7b8fba-a313-4fa9-9f01-134e2281f252,DISK], DatanodeInfoWithStorage[127.0.0.1:43759,DS-1e5dd447-a430-4d54-b6a1-13193e178335,DISK], DatanodeInfoWithStorage[127.0.0.1:35813,DS-fd3e89fc-bc32-42fb-a74e-90985b01a0ed,DISK], DatanodeInfoWithStorage[127.0.0.1:36790,DS-3c116ac6-59f5-4239-b6dc-391f5cde8efb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-969236517-172.17.0.7-1595887091746:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46409,DS-d3ae516f-e8ae-43d7-b325-320f23e47ac6,DISK], DatanodeInfoWithStorage[127.0.0.1:39499,DS-73f6b02a-aec9-4f12-a6d3-cc889688a1b7,DISK], DatanodeInfoWithStorage[127.0.0.1:34864,DS-a1631ff8-ddd3-4481-91d9-8b4efff9ff4c,DISK], DatanodeInfoWithStorage[127.0.0.1:41640,DS-dff7009d-e6b4-4532-9539-fa46b5926428,DISK], DatanodeInfoWithStorage[127.0.0.1:34965,DS-670f54c0-c76a-4ead-8917-2d16200daf01,DISK], DatanodeInfoWithStorage[127.0.0.1:37706,DS-d23670cb-79ef-42d6-9453-cd99468088c3,DISK], DatanodeInfoWithStorage[127.0.0.1:41315,DS-26c17f22-bebf-482e-aec7-188ab46a0ba6,DISK], DatanodeInfoWithStorage[127.0.0.1:37326,DS-c4682541-7fac-45bf-991c-151872594b2d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-969236517-172.17.0.7-1595887091746:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46409,DS-d3ae516f-e8ae-43d7-b325-320f23e47ac6,DISK], DatanodeInfoWithStorage[127.0.0.1:39499,DS-73f6b02a-aec9-4f12-a6d3-cc889688a1b7,DISK], DatanodeInfoWithStorage[127.0.0.1:34864,DS-a1631ff8-ddd3-4481-91d9-8b4efff9ff4c,DISK], DatanodeInfoWithStorage[127.0.0.1:41640,DS-dff7009d-e6b4-4532-9539-fa46b5926428,DISK], DatanodeInfoWithStorage[127.0.0.1:34965,DS-670f54c0-c76a-4ead-8917-2d16200daf01,DISK], DatanodeInfoWithStorage[127.0.0.1:37706,DS-d23670cb-79ef-42d6-9453-cd99468088c3,DISK], DatanodeInfoWithStorage[127.0.0.1:41315,DS-26c17f22-bebf-482e-aec7-188ab46a0ba6,DISK], DatanodeInfoWithStorage[127.0.0.1:37326,DS-c4682541-7fac-45bf-991c-151872594b2d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 32768
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery5
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1295515961-172.17.0.7-1595887877684:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46322,DS-cc51c8d8-5d0f-4af3-8753-a68f9c241ca2,DISK], DatanodeInfoWithStorage[127.0.0.1:41791,DS-a9cdbd70-dcaf-4cb2-b629-6baade8af332,DISK], DatanodeInfoWithStorage[127.0.0.1:33281,DS-275389df-0ef3-4c57-b0ac-691fe5b4ab01,DISK], DatanodeInfoWithStorage[127.0.0.1:41096,DS-de1ce430-15fb-4284-ada5-f9ffc6a412c4,DISK], DatanodeInfoWithStorage[127.0.0.1:42616,DS-ea0e8311-2f65-421a-bb2f-826030ec9b29,DISK], DatanodeInfoWithStorage[127.0.0.1:40932,DS-93d20468-74e4-4ffe-8f42-3d37d33e6e3e,DISK], DatanodeInfoWithStorage[127.0.0.1:38839,DS-e9580eec-8db6-45ea-bb10-1dc10797d71a,DISK], DatanodeInfoWithStorage[127.0.0.1:40001,DS-5908706b-4339-4924-8d06-e1cda3af355d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1295515961-172.17.0.7-1595887877684:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46322,DS-cc51c8d8-5d0f-4af3-8753-a68f9c241ca2,DISK], DatanodeInfoWithStorage[127.0.0.1:41791,DS-a9cdbd70-dcaf-4cb2-b629-6baade8af332,DISK], DatanodeInfoWithStorage[127.0.0.1:33281,DS-275389df-0ef3-4c57-b0ac-691fe5b4ab01,DISK], DatanodeInfoWithStorage[127.0.0.1:41096,DS-de1ce430-15fb-4284-ada5-f9ffc6a412c4,DISK], DatanodeInfoWithStorage[127.0.0.1:42616,DS-ea0e8311-2f65-421a-bb2f-826030ec9b29,DISK], DatanodeInfoWithStorage[127.0.0.1:40932,DS-93d20468-74e4-4ffe-8f42-3d37d33e6e3e,DISK], DatanodeInfoWithStorage[127.0.0.1:38839,DS-e9580eec-8db6-45ea-bb10-1dc10797d71a,DISK], DatanodeInfoWithStorage[127.0.0.1:40001,DS-5908706b-4339-4924-8d06-e1cda3af355d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery5(TestFileChecksum.java:355)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 5 out of 50
v1v1v2v2 failed with probability 9 out of 50
result: false positive !!!
Total execution time in seconds : 5158

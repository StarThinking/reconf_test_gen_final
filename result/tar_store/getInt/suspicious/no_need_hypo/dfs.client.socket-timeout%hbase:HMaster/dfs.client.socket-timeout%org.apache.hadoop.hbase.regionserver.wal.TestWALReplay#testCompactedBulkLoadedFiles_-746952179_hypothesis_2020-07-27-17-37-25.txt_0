reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39461,DS-d3f90758-5376-4d08-8381-b9b183a8e48c,DISK], DatanodeInfoWithStorage[127.0.0.1:41700,DS-9d929684-58bf-4d70-9d2d-8369ae091801,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41700,DS-9d929684-58bf-4d70-9d2d-8369ae091801,DISK], DatanodeInfoWithStorage[127.0.0.1:39461,DS-d3f90758-5376-4d08-8381-b9b183a8e48c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39461,DS-d3f90758-5376-4d08-8381-b9b183a8e48c,DISK], DatanodeInfoWithStorage[127.0.0.1:41700,DS-9d929684-58bf-4d70-9d2d-8369ae091801,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41700,DS-9d929684-58bf-4d70-9d2d-8369ae091801,DISK], DatanodeInfoWithStorage[127.0.0.1:39461,DS-d3f90758-5376-4d08-8381-b9b183a8e48c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41043,DS-4c7768be-fd37-46bd-ab8d-913a0bd0a5fc,DISK], DatanodeInfoWithStorage[127.0.0.1:36699,DS-91f5be02-4652-432e-8f25-f63327a24996,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41043,DS-4c7768be-fd37-46bd-ab8d-913a0bd0a5fc,DISK], DatanodeInfoWithStorage[127.0.0.1:36699,DS-91f5be02-4652-432e-8f25-f63327a24996,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41043,DS-4c7768be-fd37-46bd-ab8d-913a0bd0a5fc,DISK], DatanodeInfoWithStorage[127.0.0.1:36699,DS-91f5be02-4652-432e-8f25-f63327a24996,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41043,DS-4c7768be-fd37-46bd-ab8d-913a0bd0a5fc,DISK], DatanodeInfoWithStorage[127.0.0.1:36699,DS-91f5be02-4652-432e-8f25-f63327a24996,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40757,DS-29283de8-aa54-4ebd-aa82-942e972e0bb8,DISK], DatanodeInfoWithStorage[127.0.0.1:41316,DS-19770b16-40d5-491f-96ec-08316d73ebc0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41316,DS-19770b16-40d5-491f-96ec-08316d73ebc0,DISK], DatanodeInfoWithStorage[127.0.0.1:40757,DS-29283de8-aa54-4ebd-aa82-942e972e0bb8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40757,DS-29283de8-aa54-4ebd-aa82-942e972e0bb8,DISK], DatanodeInfoWithStorage[127.0.0.1:41316,DS-19770b16-40d5-491f-96ec-08316d73ebc0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41316,DS-19770b16-40d5-491f-96ec-08316d73ebc0,DISK], DatanodeInfoWithStorage[127.0.0.1:40757,DS-29283de8-aa54-4ebd-aa82-942e972e0bb8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36686,DS-c67336f7-2920-4ed8-b0a0-90ad3b91b69a,DISK], DatanodeInfoWithStorage[127.0.0.1:45118,DS-3faaf091-94d2-4f11-814e-6eb9c4d9b21a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45118,DS-3faaf091-94d2-4f11-814e-6eb9c4d9b21a,DISK], DatanodeInfoWithStorage[127.0.0.1:36686,DS-c67336f7-2920-4ed8-b0a0-90ad3b91b69a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36686,DS-c67336f7-2920-4ed8-b0a0-90ad3b91b69a,DISK], DatanodeInfoWithStorage[127.0.0.1:45118,DS-3faaf091-94d2-4f11-814e-6eb9c4d9b21a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45118,DS-3faaf091-94d2-4f11-814e-6eb9c4d9b21a,DISK], DatanodeInfoWithStorage[127.0.0.1:36686,DS-c67336f7-2920-4ed8-b0a0-90ad3b91b69a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles has not been updated !
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38869,DS-253d3cfd-58c3-488d-960c-c0b4baa92842,DISK], DatanodeInfoWithStorage[127.0.0.1:42649,DS-78ccd6e0-c245-4cf0-aec7-594cbcd18cee,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42649,DS-78ccd6e0-c245-4cf0-aec7-594cbcd18cee,DISK], DatanodeInfoWithStorage[127.0.0.1:38869,DS-253d3cfd-58c3-488d-960c-c0b4baa92842,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38869,DS-253d3cfd-58c3-488d-960c-c0b4baa92842,DISK], DatanodeInfoWithStorage[127.0.0.1:42649,DS-78ccd6e0-c245-4cf0-aec7-594cbcd18cee,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42649,DS-78ccd6e0-c245-4cf0-aec7-594cbcd18cee,DISK], DatanodeInfoWithStorage[127.0.0.1:38869,DS-253d3cfd-58c3-488d-960c-c0b4baa92842,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39585,DS-3cf5e243-e6fa-4123-82ef-5ecfc4fa9e30,DISK], DatanodeInfoWithStorage[127.0.0.1:38462,DS-b1830b19-2ead-47f9-b390-6d63517c2533,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39585,DS-3cf5e243-e6fa-4123-82ef-5ecfc4fa9e30,DISK], DatanodeInfoWithStorage[127.0.0.1:38462,DS-b1830b19-2ead-47f9-b390-6d63517c2533,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39585,DS-3cf5e243-e6fa-4123-82ef-5ecfc4fa9e30,DISK], DatanodeInfoWithStorage[127.0.0.1:38462,DS-b1830b19-2ead-47f9-b390-6d63517c2533,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39585,DS-3cf5e243-e6fa-4123-82ef-5ecfc4fa9e30,DISK], DatanodeInfoWithStorage[127.0.0.1:38462,DS-b1830b19-2ead-47f9-b390-6d63517c2533,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35249,DS-b481afa6-3fbb-407f-af04-6b4a556f982b,DISK], DatanodeInfoWithStorage[127.0.0.1:38547,DS-efc87643-8a50-486a-adfd-7e1710fc8856,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35249,DS-b481afa6-3fbb-407f-af04-6b4a556f982b,DISK], DatanodeInfoWithStorage[127.0.0.1:38547,DS-efc87643-8a50-486a-adfd-7e1710fc8856,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35249,DS-b481afa6-3fbb-407f-af04-6b4a556f982b,DISK], DatanodeInfoWithStorage[127.0.0.1:38547,DS-efc87643-8a50-486a-adfd-7e1710fc8856,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35249,DS-b481afa6-3fbb-407f-af04-6b4a556f982b,DISK], DatanodeInfoWithStorage[127.0.0.1:38547,DS-efc87643-8a50-486a-adfd-7e1710fc8856,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43546,DS-0f01914c-e71a-4d25-ab40-a9c2e23379c0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43546,DS-0f01914c-e71a-4d25-ab40-a9c2e23379c0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43546,DS-0f01914c-e71a-4d25-ab40-a9c2e23379c0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43546,DS-0f01914c-e71a-4d25-ab40-a9c2e23379c0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35854,DS-a7c84174-8452-4ebb-aadb-5a972fa54923,DISK], DatanodeInfoWithStorage[127.0.0.1:46746,DS-366d7624-c55f-4779-8bc2-cf6495a993d8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35854,DS-a7c84174-8452-4ebb-aadb-5a972fa54923,DISK], DatanodeInfoWithStorage[127.0.0.1:46746,DS-366d7624-c55f-4779-8bc2-cf6495a993d8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35854,DS-a7c84174-8452-4ebb-aadb-5a972fa54923,DISK], DatanodeInfoWithStorage[127.0.0.1:46746,DS-366d7624-c55f-4779-8bc2-cf6495a993d8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35854,DS-a7c84174-8452-4ebb-aadb-5a972fa54923,DISK], DatanodeInfoWithStorage[127.0.0.1:46746,DS-366d7624-c55f-4779-8bc2-cf6495a993d8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35248,DS-172f88e5-09fa-4bce-bd0c-f00b6d21ef9f,DISK], DatanodeInfoWithStorage[127.0.0.1:33227,DS-bc9f86d0-0213-4a4c-a749-764417cf8dc5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35248,DS-172f88e5-09fa-4bce-bd0c-f00b6d21ef9f,DISK], DatanodeInfoWithStorage[127.0.0.1:33227,DS-bc9f86d0-0213-4a4c-a749-764417cf8dc5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35248,DS-172f88e5-09fa-4bce-bd0c-f00b6d21ef9f,DISK], DatanodeInfoWithStorage[127.0.0.1:33227,DS-bc9f86d0-0213-4a4c-a749-764417cf8dc5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35248,DS-172f88e5-09fa-4bce-bd0c-f00b6d21ef9f,DISK], DatanodeInfoWithStorage[127.0.0.1:33227,DS-bc9f86d0-0213-4a4c-a749-764417cf8dc5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Append sequenceId=8, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=8, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45723,DS-28b0aac9-5d0e-4e5a-8015-b1195dc124b6,DISK], DatanodeInfoWithStorage[127.0.0.1:40671,DS-85e97d7e-f5f1-449b-9fe5-1b9a4f51d097,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45723,DS-28b0aac9-5d0e-4e5a-8015-b1195dc124b6,DISK], DatanodeInfoWithStorage[127.0.0.1:40671,DS-85e97d7e-f5f1-449b-9fe5-1b9a4f51d097,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34236,DS-672d4fb2-cd04-4eea-a09c-8ca94004e483,DISK], DatanodeInfoWithStorage[127.0.0.1:34823,DS-bec9c183-87c6-4ead-b99a-b0c37f4585c4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34236,DS-672d4fb2-cd04-4eea-a09c-8ca94004e483,DISK], DatanodeInfoWithStorage[127.0.0.1:34823,DS-bec9c183-87c6-4ead-b99a-b0c37f4585c4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34236,DS-672d4fb2-cd04-4eea-a09c-8ca94004e483,DISK], DatanodeInfoWithStorage[127.0.0.1:34823,DS-bec9c183-87c6-4ead-b99a-b0c37f4585c4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34236,DS-672d4fb2-cd04-4eea-a09c-8ca94004e483,DISK], DatanodeInfoWithStorage[127.0.0.1:34823,DS-bec9c183-87c6-4ead-b99a-b0c37f4585c4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40731,DS-9a58f0bf-ab95-41c7-9d51-abeb34348fcd,DISK], DatanodeInfoWithStorage[127.0.0.1:40547,DS-12853e0f-dd17-4373-b672-7c3a4cfd7c52,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40731,DS-9a58f0bf-ab95-41c7-9d51-abeb34348fcd,DISK], DatanodeInfoWithStorage[127.0.0.1:40547,DS-12853e0f-dd17-4373-b672-7c3a4cfd7c52,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40731,DS-9a58f0bf-ab95-41c7-9d51-abeb34348fcd,DISK], DatanodeInfoWithStorage[127.0.0.1:40547,DS-12853e0f-dd17-4373-b672-7c3a4cfd7c52,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40731,DS-9a58f0bf-ab95-41c7-9d51-abeb34348fcd,DISK], DatanodeInfoWithStorage[127.0.0.1:40547,DS-12853e0f-dd17-4373-b672-7c3a4cfd7c52,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41938,DS-718bc628-5adc-4429-860d-8e2639e7e791,DISK], DatanodeInfoWithStorage[127.0.0.1:39149,DS-682a9e6d-52cb-4214-beac-c4efd61c4d5d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41938,DS-718bc628-5adc-4429-860d-8e2639e7e791,DISK], DatanodeInfoWithStorage[127.0.0.1:39149,DS-682a9e6d-52cb-4214-beac-c4efd61c4d5d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41938,DS-718bc628-5adc-4429-860d-8e2639e7e791,DISK], DatanodeInfoWithStorage[127.0.0.1:39149,DS-682a9e6d-52cb-4214-beac-c4efd61c4d5d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41938,DS-718bc628-5adc-4429-860d-8e2639e7e791,DISK], DatanodeInfoWithStorage[127.0.0.1:39149,DS-682a9e6d-52cb-4214-beac-c4efd61c4d5d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43873,DS-c16ee579-509d-4b13-a47a-ff021d2ae12c,DISK], DatanodeInfoWithStorage[127.0.0.1:34537,DS-bddff13a-d8bc-4528-a4f4-7e7f4e8ca9bf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34537,DS-bddff13a-d8bc-4528-a4f4-7e7f4e8ca9bf,DISK], DatanodeInfoWithStorage[127.0.0.1:43873,DS-c16ee579-509d-4b13-a47a-ff021d2ae12c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43873,DS-c16ee579-509d-4b13-a47a-ff021d2ae12c,DISK], DatanodeInfoWithStorage[127.0.0.1:34537,DS-bddff13a-d8bc-4528-a4f4-7e7f4e8ca9bf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34537,DS-bddff13a-d8bc-4528-a4f4-7e7f4e8ca9bf,DISK], DatanodeInfoWithStorage[127.0.0.1:43873,DS-c16ee579-509d-4b13-a47a-ff021d2ae12c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39648,DS-72795627-a85f-46cc-9385-3e0a47196c24,DISK], DatanodeInfoWithStorage[127.0.0.1:45839,DS-0e817597-06e7-49f3-b4a6-63180d22a1ac,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39648,DS-72795627-a85f-46cc-9385-3e0a47196c24,DISK], DatanodeInfoWithStorage[127.0.0.1:45839,DS-0e817597-06e7-49f3-b4a6-63180d22a1ac,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39648,DS-72795627-a85f-46cc-9385-3e0a47196c24,DISK], DatanodeInfoWithStorage[127.0.0.1:45839,DS-0e817597-06e7-49f3-b4a6-63180d22a1ac,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39648,DS-72795627-a85f-46cc-9385-3e0a47196c24,DISK], DatanodeInfoWithStorage[127.0.0.1:45839,DS-0e817597-06e7-49f3-b4a6-63180d22a1ac,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36867,DS-56038364-e020-4d42-a3f3-d463da8da4ef,DISK], DatanodeInfoWithStorage[127.0.0.1:34194,DS-0b40c445-f5b0-44f6-b09f-d59244dd9320,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36867,DS-56038364-e020-4d42-a3f3-d463da8da4ef,DISK], DatanodeInfoWithStorage[127.0.0.1:34194,DS-0b40c445-f5b0-44f6-b09f-d59244dd9320,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36867,DS-56038364-e020-4d42-a3f3-d463da8da4ef,DISK], DatanodeInfoWithStorage[127.0.0.1:34194,DS-0b40c445-f5b0-44f6-b09f-d59244dd9320,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36867,DS-56038364-e020-4d42-a3f3-d463da8da4ef,DISK], DatanodeInfoWithStorage[127.0.0.1:34194,DS-0b40c445-f5b0-44f6-b09f-d59244dd9320,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35187,DS-3fb3351f-e0cb-43d2-9676-0dee69923bf2,DISK], DatanodeInfoWithStorage[127.0.0.1:41529,DS-41edc418-2c40-40f3-a2ce-5aad0a2eb319,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41529,DS-41edc418-2c40-40f3-a2ce-5aad0a2eb319,DISK], DatanodeInfoWithStorage[127.0.0.1:35187,DS-3fb3351f-e0cb-43d2-9676-0dee69923bf2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35187,DS-3fb3351f-e0cb-43d2-9676-0dee69923bf2,DISK], DatanodeInfoWithStorage[127.0.0.1:41529,DS-41edc418-2c40-40f3-a2ce-5aad0a2eb319,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41529,DS-41edc418-2c40-40f3-a2ce-5aad0a2eb319,DISK], DatanodeInfoWithStorage[127.0.0.1:35187,DS-3fb3351f-e0cb-43d2-9676-0dee69923bf2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36419,DS-f87c26f6-2434-4bd5-ab1a-8f4478a7764e,DISK], DatanodeInfoWithStorage[127.0.0.1:45641,DS-15bcbd56-fb22-4eb9-a621-81dada61ad21,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36419,DS-f87c26f6-2434-4bd5-ab1a-8f4478a7764e,DISK], DatanodeInfoWithStorage[127.0.0.1:45641,DS-15bcbd56-fb22-4eb9-a621-81dada61ad21,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36419,DS-f87c26f6-2434-4bd5-ab1a-8f4478a7764e,DISK], DatanodeInfoWithStorage[127.0.0.1:45641,DS-15bcbd56-fb22-4eb9-a621-81dada61ad21,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36419,DS-f87c26f6-2434-4bd5-ab1a-8f4478a7764e,DISK], DatanodeInfoWithStorage[127.0.0.1:45641,DS-15bcbd56-fb22-4eb9-a621-81dada61ad21,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34008,DS-e5e0c741-325f-4638-b96a-803e1be8322b,DISK], DatanodeInfoWithStorage[127.0.0.1:38438,DS-555d1801-2b09-4b71-81c5-d7858733ce97,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34008,DS-e5e0c741-325f-4638-b96a-803e1be8322b,DISK], DatanodeInfoWithStorage[127.0.0.1:38438,DS-555d1801-2b09-4b71-81c5-d7858733ce97,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34008,DS-e5e0c741-325f-4638-b96a-803e1be8322b,DISK], DatanodeInfoWithStorage[127.0.0.1:38438,DS-555d1801-2b09-4b71-81c5-d7858733ce97,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34008,DS-e5e0c741-325f-4638-b96a-803e1be8322b,DISK], DatanodeInfoWithStorage[127.0.0.1:38438,DS-555d1801-2b09-4b71-81c5-d7858733ce97,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37030,DS-ef2a3608-6eb4-476c-87ed-4588bf859e3a,DISK], DatanodeInfoWithStorage[127.0.0.1:40529,DS-57711fad-6a79-47cf-bda0-3a7d2b45ed2f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37030,DS-ef2a3608-6eb4-476c-87ed-4588bf859e3a,DISK], DatanodeInfoWithStorage[127.0.0.1:40529,DS-57711fad-6a79-47cf-bda0-3a7d2b45ed2f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37030,DS-ef2a3608-6eb4-476c-87ed-4588bf859e3a,DISK], DatanodeInfoWithStorage[127.0.0.1:40529,DS-57711fad-6a79-47cf-bda0-3a7d2b45ed2f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37030,DS-ef2a3608-6eb4-476c-87ed-4588bf859e3a,DISK], DatanodeInfoWithStorage[127.0.0.1:40529,DS-57711fad-6a79-47cf-bda0-3a7d2b45ed2f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles has not been updated !
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38484,DS-801d129b-d83a-42e1-a0bd-208f84278dcf,DISK], DatanodeInfoWithStorage[127.0.0.1:42616,DS-73a7888d-4585-475a-ad6b-5d78f33285c0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38484,DS-801d129b-d83a-42e1-a0bd-208f84278dcf,DISK], DatanodeInfoWithStorage[127.0.0.1:42616,DS-73a7888d-4585-475a-ad6b-5d78f33285c0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38484,DS-801d129b-d83a-42e1-a0bd-208f84278dcf,DISK], DatanodeInfoWithStorage[127.0.0.1:42616,DS-73a7888d-4585-475a-ad6b-5d78f33285c0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38484,DS-801d129b-d83a-42e1-a0bd-208f84278dcf,DISK], DatanodeInfoWithStorage[127.0.0.1:42616,DS-73a7888d-4585-475a-ad6b-5d78f33285c0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45043,DS-6e27c503-78b8-422c-bd43-92afd3388be3,DISK], DatanodeInfoWithStorage[127.0.0.1:37750,DS-531efc39-b7d7-48ac-a029-6fba1da1f7ca,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45043,DS-6e27c503-78b8-422c-bd43-92afd3388be3,DISK], DatanodeInfoWithStorage[127.0.0.1:37750,DS-531efc39-b7d7-48ac-a029-6fba1da1f7ca,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45043,DS-6e27c503-78b8-422c-bd43-92afd3388be3,DISK], DatanodeInfoWithStorage[127.0.0.1:37750,DS-531efc39-b7d7-48ac-a029-6fba1da1f7ca,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45043,DS-6e27c503-78b8-422c-bd43-92afd3388be3,DISK], DatanodeInfoWithStorage[127.0.0.1:37750,DS-531efc39-b7d7-48ac-a029-6fba1da1f7ca,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles has not been updated !
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34404,DS-5df0d7e5-5a12-4f33-8923-65eff3ed6318,DISK], DatanodeInfoWithStorage[127.0.0.1:36652,DS-17ec2ed3-e07a-4fdc-8e8e-609a0c7f11a8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34404,DS-5df0d7e5-5a12-4f33-8923-65eff3ed6318,DISK], DatanodeInfoWithStorage[127.0.0.1:36652,DS-17ec2ed3-e07a-4fdc-8e8e-609a0c7f11a8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34404,DS-5df0d7e5-5a12-4f33-8923-65eff3ed6318,DISK], DatanodeInfoWithStorage[127.0.0.1:36652,DS-17ec2ed3-e07a-4fdc-8e8e-609a0c7f11a8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34404,DS-5df0d7e5-5a12-4f33-8923-65eff3ed6318,DISK], DatanodeInfoWithStorage[127.0.0.1:36652,DS-17ec2ed3-e07a-4fdc-8e8e-609a0c7f11a8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43422,DS-49a3608e-d92f-4dc7-9059-c33b91ec23a0,DISK], DatanodeInfoWithStorage[127.0.0.1:40361,DS-267bb5de-b74d-49a2-8df6-ba290ad76fa8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43422,DS-49a3608e-d92f-4dc7-9059-c33b91ec23a0,DISK], DatanodeInfoWithStorage[127.0.0.1:40361,DS-267bb5de-b74d-49a2-8df6-ba290ad76fa8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43422,DS-49a3608e-d92f-4dc7-9059-c33b91ec23a0,DISK], DatanodeInfoWithStorage[127.0.0.1:40361,DS-267bb5de-b74d-49a2-8df6-ba290ad76fa8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43422,DS-49a3608e-d92f-4dc7-9059-c33b91ec23a0,DISK], DatanodeInfoWithStorage[127.0.0.1:40361,DS-267bb5de-b74d-49a2-8df6-ba290ad76fa8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45264,DS-12768b71-cb9a-4435-b2eb-72e77900b1fd,DISK], DatanodeInfoWithStorage[127.0.0.1:37978,DS-8ef46ab2-6f8f-41fd-be65-187aa6a417d1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45264,DS-12768b71-cb9a-4435-b2eb-72e77900b1fd,DISK], DatanodeInfoWithStorage[127.0.0.1:37978,DS-8ef46ab2-6f8f-41fd-be65-187aa6a417d1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45264,DS-12768b71-cb9a-4435-b2eb-72e77900b1fd,DISK], DatanodeInfoWithStorage[127.0.0.1:37978,DS-8ef46ab2-6f8f-41fd-be65-187aa6a417d1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45264,DS-12768b71-cb9a-4435-b2eb-72e77900b1fd,DISK], DatanodeInfoWithStorage[127.0.0.1:37978,DS-8ef46ab2-6f8f-41fd-be65-187aa6a417d1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37493,DS-9591f050-62e6-48b0-b8d0-48b409db409d,DISK], DatanodeInfoWithStorage[127.0.0.1:43599,DS-26314203-dc22-47b5-b22b-da2fa401094d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43599,DS-26314203-dc22-47b5-b22b-da2fa401094d,DISK], DatanodeInfoWithStorage[127.0.0.1:37493,DS-9591f050-62e6-48b0-b8d0-48b409db409d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37493,DS-9591f050-62e6-48b0-b8d0-48b409db409d,DISK], DatanodeInfoWithStorage[127.0.0.1:43599,DS-26314203-dc22-47b5-b22b-da2fa401094d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43599,DS-26314203-dc22-47b5-b22b-da2fa401094d,DISK], DatanodeInfoWithStorage[127.0.0.1:37493,DS-9591f050-62e6-48b0-b8d0-48b409db409d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: region: testCompactedBulkLoadedFiles,,1595879143940.38210833d5b7764f37f48294076e5400.
stackTrace: org.apache.hadoop.hbase.DroppedSnapshotException: region: testCompactedBulkLoadedFiles,,1595879143940.38210833d5b7764f37f48294076e5400.
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2858)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2527)
	at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2499)
	at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:2389)
	at org.apache.hadoop.hbase.regionserver.HRegion.bulkLoadHFiles(HRegion.java:6232)
	at org.apache.hadoop.hbase.regionserver.HRegion.bulkLoadHFiles(HRegion.java:6116)
	at org.apache.hadoop.hbase.regionserver.wal.AbstractTestWALReplay.testCompactedBulkLoadedFiles(AbstractTestWALReplay.java:431)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=6, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	... 1 more
Caused by: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43271,DS-45e7531b-6c9e-4f19-b76c-1d2616b298b1,DISK], DatanodeInfoWithStorage[127.0.0.1:46239,DS-81e68fcc-64a3-49d1-9116-d1178e9b8bfc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43271,DS-45e7531b-6c9e-4f19-b76c-1d2616b298b1,DISK], DatanodeInfoWithStorage[127.0.0.1:46239,DS-81e68fcc-64a3-49d1-9116-d1178e9b8bfc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43001,DS-64759eba-a8d7-4ea2-9b06-2737a84075d3,DISK], DatanodeInfoWithStorage[127.0.0.1:37514,DS-41a6de4b-c43e-4f58-a5f4-ff273ca877f4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37514,DS-41a6de4b-c43e-4f58-a5f4-ff273ca877f4,DISK], DatanodeInfoWithStorage[127.0.0.1:43001,DS-64759eba-a8d7-4ea2-9b06-2737a84075d3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43001,DS-64759eba-a8d7-4ea2-9b06-2737a84075d3,DISK], DatanodeInfoWithStorage[127.0.0.1:37514,DS-41a6de4b-c43e-4f58-a5f4-ff273ca877f4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37514,DS-41a6de4b-c43e-4f58-a5f4-ff273ca877f4,DISK], DatanodeInfoWithStorage[127.0.0.1:43001,DS-64759eba-a8d7-4ea2-9b06-2737a84075d3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42406,DS-25da8319-77c6-4a5c-91b1-aa00890b5efd,DISK], DatanodeInfoWithStorage[127.0.0.1:46140,DS-342b3f6f-745e-4477-998f-af5aeeabb73a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46140,DS-342b3f6f-745e-4477-998f-af5aeeabb73a,DISK], DatanodeInfoWithStorage[127.0.0.1:42406,DS-25da8319-77c6-4a5c-91b1-aa00890b5efd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42406,DS-25da8319-77c6-4a5c-91b1-aa00890b5efd,DISK], DatanodeInfoWithStorage[127.0.0.1:46140,DS-342b3f6f-745e-4477-998f-af5aeeabb73a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46140,DS-342b3f6f-745e-4477-998f-af5aeeabb73a,DISK], DatanodeInfoWithStorage[127.0.0.1:42406,DS-25da8319-77c6-4a5c-91b1-aa00890b5efd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41346,DS-9920a744-3542-47d6-a070-2cd03332c188,DISK], DatanodeInfoWithStorage[127.0.0.1:45078,DS-ec35583e-6050-4280-a539-daf86ca7e678,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41346,DS-9920a744-3542-47d6-a070-2cd03332c188,DISK], DatanodeInfoWithStorage[127.0.0.1:45078,DS-ec35583e-6050-4280-a539-daf86ca7e678,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41346,DS-9920a744-3542-47d6-a070-2cd03332c188,DISK], DatanodeInfoWithStorage[127.0.0.1:45078,DS-ec35583e-6050-4280-a539-daf86ca7e678,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41346,DS-9920a744-3542-47d6-a070-2cd03332c188,DISK], DatanodeInfoWithStorage[127.0.0.1:45078,DS-ec35583e-6050-4280-a539-daf86ca7e678,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles has not been updated !
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45051,DS-5dbd7995-4464-40b6-9585-1065cbc6d0f3,DISK], DatanodeInfoWithStorage[127.0.0.1:37246,DS-853779b0-cf94-487c-9cfc-7b3be6e21443,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37246,DS-853779b0-cf94-487c-9cfc-7b3be6e21443,DISK], DatanodeInfoWithStorage[127.0.0.1:45051,DS-5dbd7995-4464-40b6-9585-1065cbc6d0f3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45051,DS-5dbd7995-4464-40b6-9585-1065cbc6d0f3,DISK], DatanodeInfoWithStorage[127.0.0.1:37246,DS-853779b0-cf94-487c-9cfc-7b3be6e21443,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37246,DS-853779b0-cf94-487c-9cfc-7b3be6e21443,DISK], DatanodeInfoWithStorage[127.0.0.1:45051,DS-5dbd7995-4464-40b6-9585-1065cbc6d0f3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45464,DS-7807c2dd-eebe-4483-9420-6ea4c13472bd,DISK], DatanodeInfoWithStorage[127.0.0.1:32792,DS-8f28ad0e-11bf-41de-9a1b-3229b6839995,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32792,DS-8f28ad0e-11bf-41de-9a1b-3229b6839995,DISK], DatanodeInfoWithStorage[127.0.0.1:45464,DS-7807c2dd-eebe-4483-9420-6ea4c13472bd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45464,DS-7807c2dd-eebe-4483-9420-6ea4c13472bd,DISK], DatanodeInfoWithStorage[127.0.0.1:32792,DS-8f28ad0e-11bf-41de-9a1b-3229b6839995,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32792,DS-8f28ad0e-11bf-41de-9a1b-3229b6839995,DISK], DatanodeInfoWithStorage[127.0.0.1:45464,DS-7807c2dd-eebe-4483-9420-6ea4c13472bd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41174,DS-5e25a8bd-32ba-4263-857f-cd68d954b76a,DISK], DatanodeInfoWithStorage[127.0.0.1:34568,DS-fa05e1fb-aff0-4458-91e6-a6184c24e503,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34568,DS-fa05e1fb-aff0-4458-91e6-a6184c24e503,DISK], DatanodeInfoWithStorage[127.0.0.1:41174,DS-5e25a8bd-32ba-4263-857f-cd68d954b76a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41174,DS-5e25a8bd-32ba-4263-857f-cd68d954b76a,DISK], DatanodeInfoWithStorage[127.0.0.1:34568,DS-fa05e1fb-aff0-4458-91e6-a6184c24e503,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34568,DS-fa05e1fb-aff0-4458-91e6-a6184c24e503,DISK], DatanodeInfoWithStorage[127.0.0.1:41174,DS-5e25a8bd-32ba-4263-857f-cd68d954b76a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43150,DS-a42d3e49-eec0-4f7b-b6a9-a84dc107ba21,DISK], DatanodeInfoWithStorage[127.0.0.1:40645,DS-717b2dad-0d19-49f9-b7ad-01bb2dd214ad,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43150,DS-a42d3e49-eec0-4f7b-b6a9-a84dc107ba21,DISK], DatanodeInfoWithStorage[127.0.0.1:40645,DS-717b2dad-0d19-49f9-b7ad-01bb2dd214ad,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43150,DS-a42d3e49-eec0-4f7b-b6a9-a84dc107ba21,DISK], DatanodeInfoWithStorage[127.0.0.1:40645,DS-717b2dad-0d19-49f9-b7ad-01bb2dd214ad,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43150,DS-a42d3e49-eec0-4f7b-b6a9-a84dc107ba21,DISK], DatanodeInfoWithStorage[127.0.0.1:40645,DS-717b2dad-0d19-49f9-b7ad-01bb2dd214ad,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39343,DS-6b6bc050-e12b-4ab7-b8b4-f426f39e6d19,DISK], DatanodeInfoWithStorage[127.0.0.1:32794,DS-a02118fc-33aa-486a-acde-a5fb65dc894d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32794,DS-a02118fc-33aa-486a-acde-a5fb65dc894d,DISK], DatanodeInfoWithStorage[127.0.0.1:39343,DS-6b6bc050-e12b-4ab7-b8b4-f426f39e6d19,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39343,DS-6b6bc050-e12b-4ab7-b8b4-f426f39e6d19,DISK], DatanodeInfoWithStorage[127.0.0.1:32794,DS-a02118fc-33aa-486a-acde-a5fb65dc894d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32794,DS-a02118fc-33aa-486a-acde-a5fb65dc894d,DISK], DatanodeInfoWithStorage[127.0.0.1:39343,DS-6b6bc050-e12b-4ab7-b8b4-f426f39e6d19,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34399,DS-0e7ff37c-1bf3-48ec-8588-ee75878f73e0,DISK], DatanodeInfoWithStorage[127.0.0.1:46683,DS-9101a1f4-6c90-4797-abdb-b18a0f6386d7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46683,DS-9101a1f4-6c90-4797-abdb-b18a0f6386d7,DISK], DatanodeInfoWithStorage[127.0.0.1:34399,DS-0e7ff37c-1bf3-48ec-8588-ee75878f73e0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34399,DS-0e7ff37c-1bf3-48ec-8588-ee75878f73e0,DISK], DatanodeInfoWithStorage[127.0.0.1:46683,DS-9101a1f4-6c90-4797-abdb-b18a0f6386d7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46683,DS-9101a1f4-6c90-4797-abdb-b18a0f6386d7,DISK], DatanodeInfoWithStorage[127.0.0.1:34399,DS-0e7ff37c-1bf3-48ec-8588-ee75878f73e0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44631,DS-f888ad00-155b-42a7-ab28-2e6e4e881b57,DISK], DatanodeInfoWithStorage[127.0.0.1:33896,DS-271e34fb-ee4d-47d8-b766-98cc1ab7c381,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44631,DS-f888ad00-155b-42a7-ab28-2e6e4e881b57,DISK], DatanodeInfoWithStorage[127.0.0.1:33896,DS-271e34fb-ee4d-47d8-b766-98cc1ab7c381,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44631,DS-f888ad00-155b-42a7-ab28-2e6e4e881b57,DISK], DatanodeInfoWithStorage[127.0.0.1:33896,DS-271e34fb-ee4d-47d8-b766-98cc1ab7c381,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44631,DS-f888ad00-155b-42a7-ab28-2e6e4e881b57,DISK], DatanodeInfoWithStorage[127.0.0.1:33896,DS-271e34fb-ee4d-47d8-b766-98cc1ab7c381,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38900,DS-f43f384c-db2d-4f83-8b66-1c072769f774,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-76394b68-cddf-41d9-8370-5a55f924a229,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38900,DS-f43f384c-db2d-4f83-8b66-1c072769f774,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-76394b68-cddf-41d9-8370-5a55f924a229,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38900,DS-f43f384c-db2d-4f83-8b66-1c072769f774,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-76394b68-cddf-41d9-8370-5a55f924a229,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38900,DS-f43f384c-db2d-4f83-8b66-1c072769f774,DISK], DatanodeInfoWithStorage[127.0.0.1:37110,DS-76394b68-cddf-41d9-8370-5a55f924a229,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36665,DS-2aaa82bc-ab4f-4791-98a7-6fb5dc87dde7,DISK], DatanodeInfoWithStorage[127.0.0.1:36654,DS-b24bc745-81e2-4e18-bccb-7f5dd17844fc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36665,DS-2aaa82bc-ab4f-4791-98a7-6fb5dc87dde7,DISK], DatanodeInfoWithStorage[127.0.0.1:36654,DS-b24bc745-81e2-4e18-bccb-7f5dd17844fc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36665,DS-2aaa82bc-ab4f-4791-98a7-6fb5dc87dde7,DISK], DatanodeInfoWithStorage[127.0.0.1:36654,DS-b24bc745-81e2-4e18-bccb-7f5dd17844fc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36665,DS-2aaa82bc-ab4f-4791-98a7-6fb5dc87dde7,DISK], DatanodeInfoWithStorage[127.0.0.1:36654,DS-b24bc745-81e2-4e18-bccb-7f5dd17844fc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34961,DS-b6afc5dc-02f4-49a3-8a4c-53c157d95df8,DISK], DatanodeInfoWithStorage[127.0.0.1:36023,DS-745dc82d-5670-4c13-a9f5-02c0f8879a18,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34961,DS-b6afc5dc-02f4-49a3-8a4c-53c157d95df8,DISK], DatanodeInfoWithStorage[127.0.0.1:36023,DS-745dc82d-5670-4c13-a9f5-02c0f8879a18,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34961,DS-b6afc5dc-02f4-49a3-8a4c-53c157d95df8,DISK], DatanodeInfoWithStorage[127.0.0.1:36023,DS-745dc82d-5670-4c13-a9f5-02c0f8879a18,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34961,DS-b6afc5dc-02f4-49a3-8a4c-53c157d95df8,DISK], DatanodeInfoWithStorage[127.0.0.1:36023,DS-745dc82d-5670-4c13-a9f5-02c0f8879a18,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Append sequenceId=3, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=3, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42823,DS-e4f32e5a-7a53-4a83-bf3d-10340cbbb086,DISK], DatanodeInfoWithStorage[127.0.0.1:42503,DS-0ad0c241-34c8-4649-85a7-cd38c01250c7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42823,DS-e4f32e5a-7a53-4a83-bf3d-10340cbbb086,DISK], DatanodeInfoWithStorage[127.0.0.1:42503,DS-0ad0c241-34c8-4649-85a7-cd38c01250c7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37831,DS-a6544d32-b003-41cb-a44f-c49b7bccc4b1,DISK], DatanodeInfoWithStorage[127.0.0.1:35253,DS-4608e6c3-842b-4892-af8e-49fb82e4cc4f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37831,DS-a6544d32-b003-41cb-a44f-c49b7bccc4b1,DISK], DatanodeInfoWithStorage[127.0.0.1:35253,DS-4608e6c3-842b-4892-af8e-49fb82e4cc4f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37831,DS-a6544d32-b003-41cb-a44f-c49b7bccc4b1,DISK], DatanodeInfoWithStorage[127.0.0.1:35253,DS-4608e6c3-842b-4892-af8e-49fb82e4cc4f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37831,DS-a6544d32-b003-41cb-a44f-c49b7bccc4b1,DISK], DatanodeInfoWithStorage[127.0.0.1:35253,DS-4608e6c3-842b-4892-af8e-49fb82e4cc4f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40600,DS-c2cadfb4-6dc1-4544-b48e-68cf642d12e5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40600,DS-c2cadfb4-6dc1-4544-b48e-68cf642d12e5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40600,DS-c2cadfb4-6dc1-4544-b48e-68cf642d12e5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40600,DS-c2cadfb4-6dc1-4544-b48e-68cf642d12e5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35979,DS-de677160-e3eb-454b-aca1-ebcd58fc1fda,DISK], DatanodeInfoWithStorage[127.0.0.1:44006,DS-35100b02-cb5b-4811-ba97-4944be6c7ff7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35979,DS-de677160-e3eb-454b-aca1-ebcd58fc1fda,DISK], DatanodeInfoWithStorage[127.0.0.1:44006,DS-35100b02-cb5b-4811-ba97-4944be6c7ff7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35979,DS-de677160-e3eb-454b-aca1-ebcd58fc1fda,DISK], DatanodeInfoWithStorage[127.0.0.1:44006,DS-35100b02-cb5b-4811-ba97-4944be6c7ff7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35979,DS-de677160-e3eb-454b-aca1-ebcd58fc1fda,DISK], DatanodeInfoWithStorage[127.0.0.1:44006,DS-35100b02-cb5b-4811-ba97-4944be6c7ff7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39244,DS-15cb887d-f48a-4360-a5ee-f375ee302d0d,DISK], DatanodeInfoWithStorage[127.0.0.1:34290,DS-bfc91114-c24f-4be0-971b-9a8ce8893d05,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34290,DS-bfc91114-c24f-4be0-971b-9a8ce8893d05,DISK], DatanodeInfoWithStorage[127.0.0.1:39244,DS-15cb887d-f48a-4360-a5ee-f375ee302d0d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39244,DS-15cb887d-f48a-4360-a5ee-f375ee302d0d,DISK], DatanodeInfoWithStorage[127.0.0.1:34290,DS-bfc91114-c24f-4be0-971b-9a8ce8893d05,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34290,DS-bfc91114-c24f-4be0-971b-9a8ce8893d05,DISK], DatanodeInfoWithStorage[127.0.0.1:39244,DS-15cb887d-f48a-4360-a5ee-f375ee302d0d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33854,DS-c54623ad-b84e-4ad3-96e0-2ecb695810c4,DISK], DatanodeInfoWithStorage[127.0.0.1:36253,DS-7286e1bf-e830-49ab-9675-da6ef5a18504,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33854,DS-c54623ad-b84e-4ad3-96e0-2ecb695810c4,DISK], DatanodeInfoWithStorage[127.0.0.1:36253,DS-7286e1bf-e830-49ab-9675-da6ef5a18504,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33854,DS-c54623ad-b84e-4ad3-96e0-2ecb695810c4,DISK], DatanodeInfoWithStorage[127.0.0.1:36253,DS-7286e1bf-e830-49ab-9675-da6ef5a18504,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33854,DS-c54623ad-b84e-4ad3-96e0-2ecb695810c4,DISK], DatanodeInfoWithStorage[127.0.0.1:36253,DS-7286e1bf-e830-49ab-9675-da6ef5a18504,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42912,DS-5b644231-d3f0-49de-ad82-4b29d065b523,DISK], DatanodeInfoWithStorage[127.0.0.1:35504,DS-bc8e01bd-4156-417b-8974-1c361a43ce1b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35504,DS-bc8e01bd-4156-417b-8974-1c361a43ce1b,DISK], DatanodeInfoWithStorage[127.0.0.1:42912,DS-5b644231-d3f0-49de-ad82-4b29d065b523,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42912,DS-5b644231-d3f0-49de-ad82-4b29d065b523,DISK], DatanodeInfoWithStorage[127.0.0.1:35504,DS-bc8e01bd-4156-417b-8974-1c361a43ce1b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35504,DS-bc8e01bd-4156-417b-8974-1c361a43ce1b,DISK], DatanodeInfoWithStorage[127.0.0.1:42912,DS-5b644231-d3f0-49de-ad82-4b29d065b523,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41919,DS-d67380b2-64ca-4823-bb0c-326eed3a02d0,DISK], DatanodeInfoWithStorage[127.0.0.1:34106,DS-697bb7a3-646d-4eab-a428-f6fc13f94175,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34106,DS-697bb7a3-646d-4eab-a428-f6fc13f94175,DISK], DatanodeInfoWithStorage[127.0.0.1:41919,DS-d67380b2-64ca-4823-bb0c-326eed3a02d0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41919,DS-d67380b2-64ca-4823-bb0c-326eed3a02d0,DISK], DatanodeInfoWithStorage[127.0.0.1:34106,DS-697bb7a3-646d-4eab-a428-f6fc13f94175,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34106,DS-697bb7a3-646d-4eab-a428-f6fc13f94175,DISK], DatanodeInfoWithStorage[127.0.0.1:41919,DS-d67380b2-64ca-4823-bb0c-326eed3a02d0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35979,DS-5586eaf9-5d8f-4800-8581-f77178194325,DISK], DatanodeInfoWithStorage[127.0.0.1:44293,DS-5a375d6a-5a60-4e81-b789-a97f1324e270,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35979,DS-5586eaf9-5d8f-4800-8581-f77178194325,DISK], DatanodeInfoWithStorage[127.0.0.1:44293,DS-5a375d6a-5a60-4e81-b789-a97f1324e270,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35979,DS-5586eaf9-5d8f-4800-8581-f77178194325,DISK], DatanodeInfoWithStorage[127.0.0.1:44293,DS-5a375d6a-5a60-4e81-b789-a97f1324e270,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35979,DS-5586eaf9-5d8f-4800-8581-f77178194325,DISK], DatanodeInfoWithStorage[127.0.0.1:44293,DS-5a375d6a-5a60-4e81-b789-a97f1324e270,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34154,DS-ec1242de-9444-4566-8a2f-30637ee764e7,DISK], DatanodeInfoWithStorage[127.0.0.1:43357,DS-7910f5f4-dc85-4ad6-9e47-95434c146871,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43357,DS-7910f5f4-dc85-4ad6-9e47-95434c146871,DISK], DatanodeInfoWithStorage[127.0.0.1:34154,DS-ec1242de-9444-4566-8a2f-30637ee764e7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34154,DS-ec1242de-9444-4566-8a2f-30637ee764e7,DISK], DatanodeInfoWithStorage[127.0.0.1:43357,DS-7910f5f4-dc85-4ad6-9e47-95434c146871,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43357,DS-7910f5f4-dc85-4ad6-9e47-95434c146871,DISK], DatanodeInfoWithStorage[127.0.0.1:34154,DS-ec1242de-9444-4566-8a2f-30637ee764e7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43437,DS-cf697951-603b-4ca2-9f15-aa5c38e516ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37098,DS-bb1b10f4-22ae-4e93-ae89-f298dd9ce3e7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43437,DS-cf697951-603b-4ca2-9f15-aa5c38e516ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37098,DS-bb1b10f4-22ae-4e93-ae89-f298dd9ce3e7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43437,DS-cf697951-603b-4ca2-9f15-aa5c38e516ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37098,DS-bb1b10f4-22ae-4e93-ae89-f298dd9ce3e7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43437,DS-cf697951-603b-4ca2-9f15-aa5c38e516ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37098,DS-bb1b10f4-22ae-4e93-ae89-f298dd9ce3e7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39456,DS-be8ece9f-1a73-40c9-b580-7b81bce67457,DISK], DatanodeInfoWithStorage[127.0.0.1:39546,DS-2f466dde-007b-4432-b06a-0dff3d2a7ecf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39456,DS-be8ece9f-1a73-40c9-b580-7b81bce67457,DISK], DatanodeInfoWithStorage[127.0.0.1:39546,DS-2f466dde-007b-4432-b06a-0dff3d2a7ecf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39456,DS-be8ece9f-1a73-40c9-b580-7b81bce67457,DISK], DatanodeInfoWithStorage[127.0.0.1:39546,DS-2f466dde-007b-4432-b06a-0dff3d2a7ecf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39456,DS-be8ece9f-1a73-40c9-b580-7b81bce67457,DISK], DatanodeInfoWithStorage[127.0.0.1:39546,DS-2f466dde-007b-4432-b06a-0dff3d2a7ecf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42889,DS-e06c9aab-6583-4691-9b5b-bccc187057da,DISK], DatanodeInfoWithStorage[127.0.0.1:33083,DS-76b34855-d242-4a5b-a2f5-33d5894c5abf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42889,DS-e06c9aab-6583-4691-9b5b-bccc187057da,DISK], DatanodeInfoWithStorage[127.0.0.1:33083,DS-76b34855-d242-4a5b-a2f5-33d5894c5abf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42889,DS-e06c9aab-6583-4691-9b5b-bccc187057da,DISK], DatanodeInfoWithStorage[127.0.0.1:33083,DS-76b34855-d242-4a5b-a2f5-33d5894c5abf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42889,DS-e06c9aab-6583-4691-9b5b-bccc187057da,DISK], DatanodeInfoWithStorage[127.0.0.1:33083,DS-76b34855-d242-4a5b-a2f5-33d5894c5abf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38272,DS-8aef2270-49a0-47c3-8efe-e8fc139d1c8b,DISK], DatanodeInfoWithStorage[127.0.0.1:33188,DS-3be792e7-f895-44af-a63d-03a629c06639,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33188,DS-3be792e7-f895-44af-a63d-03a629c06639,DISK], DatanodeInfoWithStorage[127.0.0.1:38272,DS-8aef2270-49a0-47c3-8efe-e8fc139d1c8b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38272,DS-8aef2270-49a0-47c3-8efe-e8fc139d1c8b,DISK], DatanodeInfoWithStorage[127.0.0.1:33188,DS-3be792e7-f895-44af-a63d-03a629c06639,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33188,DS-3be792e7-f895-44af-a63d-03a629c06639,DISK], DatanodeInfoWithStorage[127.0.0.1:38272,DS-8aef2270-49a0-47c3-8efe-e8fc139d1c8b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43246,DS-e6391c29-6c0c-4405-8b3d-e990fa1a71e7,DISK], DatanodeInfoWithStorage[127.0.0.1:43418,DS-0e0e96f3-b8c9-45b7-9592-66e14b0e9a83,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43418,DS-0e0e96f3-b8c9-45b7-9592-66e14b0e9a83,DISK], DatanodeInfoWithStorage[127.0.0.1:43246,DS-e6391c29-6c0c-4405-8b3d-e990fa1a71e7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43246,DS-e6391c29-6c0c-4405-8b3d-e990fa1a71e7,DISK], DatanodeInfoWithStorage[127.0.0.1:43418,DS-0e0e96f3-b8c9-45b7-9592-66e14b0e9a83,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43418,DS-0e0e96f3-b8c9-45b7-9592-66e14b0e9a83,DISK], DatanodeInfoWithStorage[127.0.0.1:43246,DS-e6391c29-6c0c-4405-8b3d-e990fa1a71e7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36406,DS-34d2742e-57b1-40af-97c0-86271baa0345,DISK], DatanodeInfoWithStorage[127.0.0.1:34824,DS-89d68170-9c23-420c-859b-4cd11126bf14,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36406,DS-34d2742e-57b1-40af-97c0-86271baa0345,DISK], DatanodeInfoWithStorage[127.0.0.1:34824,DS-89d68170-9c23-420c-859b-4cd11126bf14,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36406,DS-34d2742e-57b1-40af-97c0-86271baa0345,DISK], DatanodeInfoWithStorage[127.0.0.1:34824,DS-89d68170-9c23-420c-859b-4cd11126bf14,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36406,DS-34d2742e-57b1-40af-97c0-86271baa0345,DISK], DatanodeInfoWithStorage[127.0.0.1:34824,DS-89d68170-9c23-420c-859b-4cd11126bf14,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38075,DS-8184879e-d584-431f-903f-bc76c4c3a36f,DISK], DatanodeInfoWithStorage[127.0.0.1:43874,DS-a07078c3-4729-45b3-8e4d-10612e37fa0d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38075,DS-8184879e-d584-431f-903f-bc76c4c3a36f,DISK], DatanodeInfoWithStorage[127.0.0.1:43874,DS-a07078c3-4729-45b3-8e4d-10612e37fa0d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38075,DS-8184879e-d584-431f-903f-bc76c4c3a36f,DISK], DatanodeInfoWithStorage[127.0.0.1:43874,DS-a07078c3-4729-45b3-8e4d-10612e37fa0d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38075,DS-8184879e-d584-431f-903f-bc76c4c3a36f,DISK], DatanodeInfoWithStorage[127.0.0.1:43874,DS-a07078c3-4729-45b3-8e4d-10612e37fa0d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37087,DS-3ddd9811-1018-4aff-adae-756bc8900aa5,DISK], DatanodeInfoWithStorage[127.0.0.1:46478,DS-d5d3cd85-9433-4bff-85c7-001dd7004db7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46478,DS-d5d3cd85-9433-4bff-85c7-001dd7004db7,DISK], DatanodeInfoWithStorage[127.0.0.1:37087,DS-3ddd9811-1018-4aff-adae-756bc8900aa5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37087,DS-3ddd9811-1018-4aff-adae-756bc8900aa5,DISK], DatanodeInfoWithStorage[127.0.0.1:46478,DS-d5d3cd85-9433-4bff-85c7-001dd7004db7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46478,DS-d5d3cd85-9433-4bff-85c7-001dd7004db7,DISK], DatanodeInfoWithStorage[127.0.0.1:37087,DS-3ddd9811-1018-4aff-adae-756bc8900aa5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44696,DS-780345ce-7ff8-42ef-89a4-3ea9e7feaf42,DISK], DatanodeInfoWithStorage[127.0.0.1:44972,DS-e799b602-78d3-4dd7-bc78-b95539b9e823,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44972,DS-e799b602-78d3-4dd7-bc78-b95539b9e823,DISK], DatanodeInfoWithStorage[127.0.0.1:44696,DS-780345ce-7ff8-42ef-89a4-3ea9e7feaf42,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44696,DS-780345ce-7ff8-42ef-89a4-3ea9e7feaf42,DISK], DatanodeInfoWithStorage[127.0.0.1:44972,DS-e799b602-78d3-4dd7-bc78-b95539b9e823,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44972,DS-e799b602-78d3-4dd7-bc78-b95539b9e823,DISK], DatanodeInfoWithStorage[127.0.0.1:44696,DS-780345ce-7ff8-42ef-89a4-3ea9e7feaf42,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testCompactedBulkLoadedFiles
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34532,DS-b206f824-ee5d-4531-a0cd-8138d52d8c59,DISK], DatanodeInfoWithStorage[127.0.0.1:41519,DS-b379c2c6-6c24-433f-9c99-6854063e305a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41519,DS-b379c2c6-6c24-433f-9c99-6854063e305a,DISK], DatanodeInfoWithStorage[127.0.0.1:34532,DS-b206f824-ee5d-4531-a0cd-8138d52d8c59,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34532,DS-b206f824-ee5d-4531-a0cd-8138d52d8c59,DISK], DatanodeInfoWithStorage[127.0.0.1:41519,DS-b379c2c6-6c24-433f-9c99-6854063e305a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41519,DS-b379c2c6-6c24-433f-9c99-6854063e305a,DISK], DatanodeInfoWithStorage[127.0.0.1:34532,DS-b206f824-ee5d-4531-a0cd-8138d52d8c59,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 50 out of 50
v1v1v2v2 failed with probability 17 out of 50
result: might be true error
Total execution time in seconds : 14979

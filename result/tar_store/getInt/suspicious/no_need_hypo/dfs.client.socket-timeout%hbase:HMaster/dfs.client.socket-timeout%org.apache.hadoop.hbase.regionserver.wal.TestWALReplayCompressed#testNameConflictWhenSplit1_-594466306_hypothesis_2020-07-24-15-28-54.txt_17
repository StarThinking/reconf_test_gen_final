reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38669,DS-eb29dbb1-6ed4-497e-a5d5-3c1bfcaa6aae,DISK], DatanodeInfoWithStorage[127.0.0.1:45286,DS-acea7412-b404-4efe-9ef4-f2071baa0ebb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38669,DS-eb29dbb1-6ed4-497e-a5d5-3c1bfcaa6aae,DISK], DatanodeInfoWithStorage[127.0.0.1:45286,DS-acea7412-b404-4efe-9ef4-f2071baa0ebb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38669,DS-eb29dbb1-6ed4-497e-a5d5-3c1bfcaa6aae,DISK], DatanodeInfoWithStorage[127.0.0.1:45286,DS-acea7412-b404-4efe-9ef4-f2071baa0ebb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38669,DS-eb29dbb1-6ed4-497e-a5d5-3c1bfcaa6aae,DISK], DatanodeInfoWithStorage[127.0.0.1:45286,DS-acea7412-b404-4efe-9ef4-f2071baa0ebb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36500,DS-d233ccfd-99be-4985-aca3-b48aa3242fb2,DISK], DatanodeInfoWithStorage[127.0.0.1:43027,DS-c7a43386-86c9-4ed7-8b04-5779bf92b3e3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36500,DS-d233ccfd-99be-4985-aca3-b48aa3242fb2,DISK], DatanodeInfoWithStorage[127.0.0.1:43027,DS-c7a43386-86c9-4ed7-8b04-5779bf92b3e3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36500,DS-d233ccfd-99be-4985-aca3-b48aa3242fb2,DISK], DatanodeInfoWithStorage[127.0.0.1:43027,DS-c7a43386-86c9-4ed7-8b04-5779bf92b3e3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36500,DS-d233ccfd-99be-4985-aca3-b48aa3242fb2,DISK], DatanodeInfoWithStorage[127.0.0.1:43027,DS-c7a43386-86c9-4ed7-8b04-5779bf92b3e3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37424,DS-2bdda87b-6f73-450d-9bff-5652409192e3,DISK], DatanodeInfoWithStorage[127.0.0.1:44976,DS-c5c159c1-e15a-493f-be94-7df38fe20ab2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37424,DS-2bdda87b-6f73-450d-9bff-5652409192e3,DISK], DatanodeInfoWithStorage[127.0.0.1:44976,DS-c5c159c1-e15a-493f-be94-7df38fe20ab2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37424,DS-2bdda87b-6f73-450d-9bff-5652409192e3,DISK], DatanodeInfoWithStorage[127.0.0.1:44976,DS-c5c159c1-e15a-493f-be94-7df38fe20ab2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37424,DS-2bdda87b-6f73-450d-9bff-5652409192e3,DISK], DatanodeInfoWithStorage[127.0.0.1:44976,DS-c5c159c1-e15a-493f-be94-7df38fe20ab2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44930,DS-6a8772b1-7cd2-4cb8-96db-b42429df296a,DISK], DatanodeInfoWithStorage[127.0.0.1:38878,DS-293d2328-52fd-433a-9fab-46a93abb9459,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44930,DS-6a8772b1-7cd2-4cb8-96db-b42429df296a,DISK], DatanodeInfoWithStorage[127.0.0.1:38878,DS-293d2328-52fd-433a-9fab-46a93abb9459,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44930,DS-6a8772b1-7cd2-4cb8-96db-b42429df296a,DISK], DatanodeInfoWithStorage[127.0.0.1:38878,DS-293d2328-52fd-433a-9fab-46a93abb9459,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44930,DS-6a8772b1-7cd2-4cb8-96db-b42429df296a,DISK], DatanodeInfoWithStorage[127.0.0.1:38878,DS-293d2328-52fd-433a-9fab-46a93abb9459,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34628,DS-f0253ed8-5639-4ef4-8dd3-5955f04bc079,DISK], DatanodeInfoWithStorage[127.0.0.1:42009,DS-70f4d072-d4eb-440a-aba6-51d257ddf2de,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34628,DS-f0253ed8-5639-4ef4-8dd3-5955f04bc079,DISK], DatanodeInfoWithStorage[127.0.0.1:42009,DS-70f4d072-d4eb-440a-aba6-51d257ddf2de,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34628,DS-f0253ed8-5639-4ef4-8dd3-5955f04bc079,DISK], DatanodeInfoWithStorage[127.0.0.1:42009,DS-70f4d072-d4eb-440a-aba6-51d257ddf2de,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34628,DS-f0253ed8-5639-4ef4-8dd3-5955f04bc079,DISK], DatanodeInfoWithStorage[127.0.0.1:42009,DS-70f4d072-d4eb-440a-aba6-51d257ddf2de,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40551,DS-7d73119b-fa85-4978-be6f-9b70da3afa60,DISK], DatanodeInfoWithStorage[127.0.0.1:39175,DS-a151e868-e350-4070-8d3e-23deeb0a2fa6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40551,DS-7d73119b-fa85-4978-be6f-9b70da3afa60,DISK], DatanodeInfoWithStorage[127.0.0.1:39175,DS-a151e868-e350-4070-8d3e-23deeb0a2fa6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40551,DS-7d73119b-fa85-4978-be6f-9b70da3afa60,DISK], DatanodeInfoWithStorage[127.0.0.1:39175,DS-a151e868-e350-4070-8d3e-23deeb0a2fa6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40551,DS-7d73119b-fa85-4978-be6f-9b70da3afa60,DISK], DatanodeInfoWithStorage[127.0.0.1:39175,DS-a151e868-e350-4070-8d3e-23deeb0a2fa6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45464,DS-c6033577-7874-4603-89ba-1514c9246d5a,DISK], DatanodeInfoWithStorage[127.0.0.1:43723,DS-8537fead-4223-4762-bf04-028b32fe8645,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45464,DS-c6033577-7874-4603-89ba-1514c9246d5a,DISK], DatanodeInfoWithStorage[127.0.0.1:43723,DS-8537fead-4223-4762-bf04-028b32fe8645,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45464,DS-c6033577-7874-4603-89ba-1514c9246d5a,DISK], DatanodeInfoWithStorage[127.0.0.1:43723,DS-8537fead-4223-4762-bf04-028b32fe8645,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45464,DS-c6033577-7874-4603-89ba-1514c9246d5a,DISK], DatanodeInfoWithStorage[127.0.0.1:43723,DS-8537fead-4223-4762-bf04-028b32fe8645,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41936,DS-4c04433a-ced7-444f-884c-59e224e4fad0,DISK], DatanodeInfoWithStorage[127.0.0.1:44017,DS-1dc87e20-eb63-4da9-bf74-46be4f12da15,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41936,DS-4c04433a-ced7-444f-884c-59e224e4fad0,DISK], DatanodeInfoWithStorage[127.0.0.1:44017,DS-1dc87e20-eb63-4da9-bf74-46be4f12da15,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41936,DS-4c04433a-ced7-444f-884c-59e224e4fad0,DISK], DatanodeInfoWithStorage[127.0.0.1:44017,DS-1dc87e20-eb63-4da9-bf74-46be4f12da15,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41936,DS-4c04433a-ced7-444f-884c-59e224e4fad0,DISK], DatanodeInfoWithStorage[127.0.0.1:44017,DS-1dc87e20-eb63-4da9-bf74-46be4f12da15,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37324,DS-1c29beb5-332f-4596-9168-e75518ee4696,DISK], DatanodeInfoWithStorage[127.0.0.1:38597,DS-06b82077-4be6-4ac4-8799-e0f20187af4f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37324,DS-1c29beb5-332f-4596-9168-e75518ee4696,DISK], DatanodeInfoWithStorage[127.0.0.1:38597,DS-06b82077-4be6-4ac4-8799-e0f20187af4f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37324,DS-1c29beb5-332f-4596-9168-e75518ee4696,DISK], DatanodeInfoWithStorage[127.0.0.1:38597,DS-06b82077-4be6-4ac4-8799-e0f20187af4f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37324,DS-1c29beb5-332f-4596-9168-e75518ee4696,DISK], DatanodeInfoWithStorage[127.0.0.1:38597,DS-06b82077-4be6-4ac4-8799-e0f20187af4f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32780,DS-58dc959f-8d46-4233-9dff-7157ec2fa71c,DISK], DatanodeInfoWithStorage[127.0.0.1:33840,DS-f2755357-8484-43ef-bfeb-60cd9f368fc8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33840,DS-f2755357-8484-43ef-bfeb-60cd9f368fc8,DISK], DatanodeInfoWithStorage[127.0.0.1:32780,DS-58dc959f-8d46-4233-9dff-7157ec2fa71c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32780,DS-58dc959f-8d46-4233-9dff-7157ec2fa71c,DISK], DatanodeInfoWithStorage[127.0.0.1:33840,DS-f2755357-8484-43ef-bfeb-60cd9f368fc8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33840,DS-f2755357-8484-43ef-bfeb-60cd9f368fc8,DISK], DatanodeInfoWithStorage[127.0.0.1:32780,DS-58dc959f-8d46-4233-9dff-7157ec2fa71c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38780,DS-c7d98c3e-7b94-4608-a8a7-166236a04bae,DISK], DatanodeInfoWithStorage[127.0.0.1:40762,DS-0e21b141-5d90-4eb7-a4ab-b4552ffea22a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38780,DS-c7d98c3e-7b94-4608-a8a7-166236a04bae,DISK], DatanodeInfoWithStorage[127.0.0.1:40762,DS-0e21b141-5d90-4eb7-a4ab-b4552ffea22a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38780,DS-c7d98c3e-7b94-4608-a8a7-166236a04bae,DISK], DatanodeInfoWithStorage[127.0.0.1:40762,DS-0e21b141-5d90-4eb7-a4ab-b4552ffea22a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38780,DS-c7d98c3e-7b94-4608-a8a7-166236a04bae,DISK], DatanodeInfoWithStorage[127.0.0.1:40762,DS-0e21b141-5d90-4eb7-a4ab-b4552ffea22a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36771,DS-52be14c6-1c55-404f-88e0-c7c3a22e0ad7,DISK], DatanodeInfoWithStorage[127.0.0.1:43664,DS-c653390e-2586-4701-8785-51a96423b5bd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36771,DS-52be14c6-1c55-404f-88e0-c7c3a22e0ad7,DISK], DatanodeInfoWithStorage[127.0.0.1:43664,DS-c653390e-2586-4701-8785-51a96423b5bd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36771,DS-52be14c6-1c55-404f-88e0-c7c3a22e0ad7,DISK], DatanodeInfoWithStorage[127.0.0.1:43664,DS-c653390e-2586-4701-8785-51a96423b5bd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36771,DS-52be14c6-1c55-404f-88e0-c7c3a22e0ad7,DISK], DatanodeInfoWithStorage[127.0.0.1:43664,DS-c653390e-2586-4701-8785-51a96423b5bd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36963,DS-e8a8816d-54a9-4c21-b9de-589a3ebd3019,DISK], DatanodeInfoWithStorage[127.0.0.1:45943,DS-c19eeb4c-b8a6-415d-8659-b024ea2d7b5b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36963,DS-e8a8816d-54a9-4c21-b9de-589a3ebd3019,DISK], DatanodeInfoWithStorage[127.0.0.1:45943,DS-c19eeb4c-b8a6-415d-8659-b024ea2d7b5b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36963,DS-e8a8816d-54a9-4c21-b9de-589a3ebd3019,DISK], DatanodeInfoWithStorage[127.0.0.1:45943,DS-c19eeb4c-b8a6-415d-8659-b024ea2d7b5b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36963,DS-e8a8816d-54a9-4c21-b9de-589a3ebd3019,DISK], DatanodeInfoWithStorage[127.0.0.1:45943,DS-c19eeb4c-b8a6-415d-8659-b024ea2d7b5b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34340,DS-e374547a-979b-4349-83fe-b1e1f839c175,DISK], DatanodeInfoWithStorage[127.0.0.1:43536,DS-9e114449-e68e-4a42-b2c4-075fe07f067d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34340,DS-e374547a-979b-4349-83fe-b1e1f839c175,DISK], DatanodeInfoWithStorage[127.0.0.1:43536,DS-9e114449-e68e-4a42-b2c4-075fe07f067d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34340,DS-e374547a-979b-4349-83fe-b1e1f839c175,DISK], DatanodeInfoWithStorage[127.0.0.1:43536,DS-9e114449-e68e-4a42-b2c4-075fe07f067d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34340,DS-e374547a-979b-4349-83fe-b1e1f839c175,DISK], DatanodeInfoWithStorage[127.0.0.1:43536,DS-9e114449-e68e-4a42-b2c4-075fe07f067d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41756,DS-36297cb6-444b-4d1d-9910-7bee2b65f461,DISK], DatanodeInfoWithStorage[127.0.0.1:39043,DS-07a677ab-e87e-4630-8ad9-c211f23815a1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41756,DS-36297cb6-444b-4d1d-9910-7bee2b65f461,DISK], DatanodeInfoWithStorage[127.0.0.1:39043,DS-07a677ab-e87e-4630-8ad9-c211f23815a1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41756,DS-36297cb6-444b-4d1d-9910-7bee2b65f461,DISK], DatanodeInfoWithStorage[127.0.0.1:39043,DS-07a677ab-e87e-4630-8ad9-c211f23815a1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41756,DS-36297cb6-444b-4d1d-9910-7bee2b65f461,DISK], DatanodeInfoWithStorage[127.0.0.1:39043,DS-07a677ab-e87e-4630-8ad9-c211f23815a1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42560,DS-d020b3db-0494-4a84-8338-c724c08a424f,DISK], DatanodeInfoWithStorage[127.0.0.1:44053,DS-aef573e3-03e4-4dbf-ae91-cf9376e98970,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44053,DS-aef573e3-03e4-4dbf-ae91-cf9376e98970,DISK], DatanodeInfoWithStorage[127.0.0.1:42560,DS-d020b3db-0494-4a84-8338-c724c08a424f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42560,DS-d020b3db-0494-4a84-8338-c724c08a424f,DISK], DatanodeInfoWithStorage[127.0.0.1:44053,DS-aef573e3-03e4-4dbf-ae91-cf9376e98970,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44053,DS-aef573e3-03e4-4dbf-ae91-cf9376e98970,DISK], DatanodeInfoWithStorage[127.0.0.1:42560,DS-d020b3db-0494-4a84-8338-c724c08a424f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33552,DS-c38b9b6c-1d81-490a-a04f-32d3979a8a2b,DISK], DatanodeInfoWithStorage[127.0.0.1:38588,DS-bf4efae9-3e33-4da7-a961-82b0b4c2039a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38588,DS-bf4efae9-3e33-4da7-a961-82b0b4c2039a,DISK], DatanodeInfoWithStorage[127.0.0.1:33552,DS-c38b9b6c-1d81-490a-a04f-32d3979a8a2b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33552,DS-c38b9b6c-1d81-490a-a04f-32d3979a8a2b,DISK], DatanodeInfoWithStorage[127.0.0.1:38588,DS-bf4efae9-3e33-4da7-a961-82b0b4c2039a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38588,DS-bf4efae9-3e33-4da7-a961-82b0b4c2039a,DISK], DatanodeInfoWithStorage[127.0.0.1:33552,DS-c38b9b6c-1d81-490a-a04f-32d3979a8a2b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43965,DS-cec6ee8f-fa2e-4a69-b499-c226bc7b6f9e,DISK], DatanodeInfoWithStorage[127.0.0.1:44223,DS-9b916d4b-bb86-4a9f-b61c-b3b1f138b573,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43965,DS-cec6ee8f-fa2e-4a69-b499-c226bc7b6f9e,DISK], DatanodeInfoWithStorage[127.0.0.1:44223,DS-9b916d4b-bb86-4a9f-b61c-b3b1f138b573,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43965,DS-cec6ee8f-fa2e-4a69-b499-c226bc7b6f9e,DISK], DatanodeInfoWithStorage[127.0.0.1:44223,DS-9b916d4b-bb86-4a9f-b61c-b3b1f138b573,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43965,DS-cec6ee8f-fa2e-4a69-b499-c226bc7b6f9e,DISK], DatanodeInfoWithStorage[127.0.0.1:44223,DS-9b916d4b-bb86-4a9f-b61c-b3b1f138b573,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45329,DS-e31ddae5-869d-40a6-8ea7-990c291d399f,DISK], DatanodeInfoWithStorage[127.0.0.1:35632,DS-c4fc1e05-2ff8-46b3-9457-fe7ef9648f45,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45329,DS-e31ddae5-869d-40a6-8ea7-990c291d399f,DISK], DatanodeInfoWithStorage[127.0.0.1:35632,DS-c4fc1e05-2ff8-46b3-9457-fe7ef9648f45,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45329,DS-e31ddae5-869d-40a6-8ea7-990c291d399f,DISK], DatanodeInfoWithStorage[127.0.0.1:35632,DS-c4fc1e05-2ff8-46b3-9457-fe7ef9648f45,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45329,DS-e31ddae5-869d-40a6-8ea7-990c291d399f,DISK], DatanodeInfoWithStorage[127.0.0.1:35632,DS-c4fc1e05-2ff8-46b3-9457-fe7ef9648f45,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44918,DS-ae47fc22-739a-4ffe-bc09-6bc07fb0d50a,DISK], DatanodeInfoWithStorage[127.0.0.1:33947,DS-09f37220-05b9-49f5-b825-ca5f895f07fd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44918,DS-ae47fc22-739a-4ffe-bc09-6bc07fb0d50a,DISK], DatanodeInfoWithStorage[127.0.0.1:33947,DS-09f37220-05b9-49f5-b825-ca5f895f07fd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44918,DS-ae47fc22-739a-4ffe-bc09-6bc07fb0d50a,DISK], DatanodeInfoWithStorage[127.0.0.1:33947,DS-09f37220-05b9-49f5-b825-ca5f895f07fd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44918,DS-ae47fc22-739a-4ffe-bc09-6bc07fb0d50a,DISK], DatanodeInfoWithStorage[127.0.0.1:33947,DS-09f37220-05b9-49f5-b825-ca5f895f07fd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33378,DS-8209cf92-d13e-418e-bcd9-d5df27012c34,DISK], DatanodeInfoWithStorage[127.0.0.1:37182,DS-d85b5823-39ad-4247-a4ef-e7d564389cc3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37182,DS-d85b5823-39ad-4247-a4ef-e7d564389cc3,DISK], DatanodeInfoWithStorage[127.0.0.1:33378,DS-8209cf92-d13e-418e-bcd9-d5df27012c34,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33378,DS-8209cf92-d13e-418e-bcd9-d5df27012c34,DISK], DatanodeInfoWithStorage[127.0.0.1:37182,DS-d85b5823-39ad-4247-a4ef-e7d564389cc3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37182,DS-d85b5823-39ad-4247-a4ef-e7d564389cc3,DISK], DatanodeInfoWithStorage[127.0.0.1:33378,DS-8209cf92-d13e-418e-bcd9-d5df27012c34,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45859,DS-5bf406c9-2dd4-4c2f-9f26-287eea2f369a,DISK], DatanodeInfoWithStorage[127.0.0.1:40153,DS-f9b622a4-1590-4190-af4a-bf2e8332df3d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45859,DS-5bf406c9-2dd4-4c2f-9f26-287eea2f369a,DISK], DatanodeInfoWithStorage[127.0.0.1:40153,DS-f9b622a4-1590-4190-af4a-bf2e8332df3d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45859,DS-5bf406c9-2dd4-4c2f-9f26-287eea2f369a,DISK], DatanodeInfoWithStorage[127.0.0.1:40153,DS-f9b622a4-1590-4190-af4a-bf2e8332df3d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45859,DS-5bf406c9-2dd4-4c2f-9f26-287eea2f369a,DISK], DatanodeInfoWithStorage[127.0.0.1:40153,DS-f9b622a4-1590-4190-af4a-bf2e8332df3d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1 has not been updated !
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35443,DS-7cac8234-fac8-4684-9bcc-17ddb58daead,DISK], DatanodeInfoWithStorage[127.0.0.1:40010,DS-ed25bfc1-a732-489c-8b2a-8edb636c5da8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35443,DS-7cac8234-fac8-4684-9bcc-17ddb58daead,DISK], DatanodeInfoWithStorage[127.0.0.1:40010,DS-ed25bfc1-a732-489c-8b2a-8edb636c5da8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35443,DS-7cac8234-fac8-4684-9bcc-17ddb58daead,DISK], DatanodeInfoWithStorage[127.0.0.1:40010,DS-ed25bfc1-a732-489c-8b2a-8edb636c5da8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35443,DS-7cac8234-fac8-4684-9bcc-17ddb58daead,DISK], DatanodeInfoWithStorage[127.0.0.1:40010,DS-ed25bfc1-a732-489c-8b2a-8edb636c5da8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45503,DS-3c024ad4-7269-4a62-bc72-5a522f455f87,DISK], DatanodeInfoWithStorage[127.0.0.1:35423,DS-c56f3d38-bdbf-4d72-908e-16e038ead385,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45503,DS-3c024ad4-7269-4a62-bc72-5a522f455f87,DISK], DatanodeInfoWithStorage[127.0.0.1:35423,DS-c56f3d38-bdbf-4d72-908e-16e038ead385,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45503,DS-3c024ad4-7269-4a62-bc72-5a522f455f87,DISK], DatanodeInfoWithStorage[127.0.0.1:35423,DS-c56f3d38-bdbf-4d72-908e-16e038ead385,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45503,DS-3c024ad4-7269-4a62-bc72-5a522f455f87,DISK], DatanodeInfoWithStorage[127.0.0.1:35423,DS-c56f3d38-bdbf-4d72-908e-16e038ead385,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35405,DS-bf86a3fc-b017-447a-941a-1f87cee0836a,DISK], DatanodeInfoWithStorage[127.0.0.1:36352,DS-041a5f9a-9953-4525-8575-589225e2924b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35405,DS-bf86a3fc-b017-447a-941a-1f87cee0836a,DISK], DatanodeInfoWithStorage[127.0.0.1:36352,DS-041a5f9a-9953-4525-8575-589225e2924b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35405,DS-bf86a3fc-b017-447a-941a-1f87cee0836a,DISK], DatanodeInfoWithStorage[127.0.0.1:36352,DS-041a5f9a-9953-4525-8575-589225e2924b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35405,DS-bf86a3fc-b017-447a-941a-1f87cee0836a,DISK], DatanodeInfoWithStorage[127.0.0.1:36352,DS-041a5f9a-9953-4525-8575-589225e2924b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44550,DS-43e1624d-ad47-4ae7-8aa5-d640959aba3b,DISK], DatanodeInfoWithStorage[127.0.0.1:39652,DS-9170a827-35ee-4eda-b707-bb2966d14560,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44550,DS-43e1624d-ad47-4ae7-8aa5-d640959aba3b,DISK], DatanodeInfoWithStorage[127.0.0.1:39652,DS-9170a827-35ee-4eda-b707-bb2966d14560,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44550,DS-43e1624d-ad47-4ae7-8aa5-d640959aba3b,DISK], DatanodeInfoWithStorage[127.0.0.1:39652,DS-9170a827-35ee-4eda-b707-bb2966d14560,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44550,DS-43e1624d-ad47-4ae7-8aa5-d640959aba3b,DISK], DatanodeInfoWithStorage[127.0.0.1:39652,DS-9170a827-35ee-4eda-b707-bb2966d14560,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35990,DS-158ca484-1baf-4f4a-99c7-80590d8a3c0a,DISK], DatanodeInfoWithStorage[127.0.0.1:42891,DS-633fcbba-4027-41f1-a076-8583488f03d1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42891,DS-633fcbba-4027-41f1-a076-8583488f03d1,DISK], DatanodeInfoWithStorage[127.0.0.1:35990,DS-158ca484-1baf-4f4a-99c7-80590d8a3c0a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35990,DS-158ca484-1baf-4f4a-99c7-80590d8a3c0a,DISK], DatanodeInfoWithStorage[127.0.0.1:42891,DS-633fcbba-4027-41f1-a076-8583488f03d1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42891,DS-633fcbba-4027-41f1-a076-8583488f03d1,DISK], DatanodeInfoWithStorage[127.0.0.1:35990,DS-158ca484-1baf-4f4a-99c7-80590d8a3c0a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41900,DS-dd2ca8d2-d949-4645-b5d9-fba084ca6cd9,DISK], DatanodeInfoWithStorage[127.0.0.1:41204,DS-1df94765-e489-41d6-b23f-a57e3385a350,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41900,DS-dd2ca8d2-d949-4645-b5d9-fba084ca6cd9,DISK], DatanodeInfoWithStorage[127.0.0.1:41204,DS-1df94765-e489-41d6-b23f-a57e3385a350,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41900,DS-dd2ca8d2-d949-4645-b5d9-fba084ca6cd9,DISK], DatanodeInfoWithStorage[127.0.0.1:41204,DS-1df94765-e489-41d6-b23f-a57e3385a350,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41900,DS-dd2ca8d2-d949-4645-b5d9-fba084ca6cd9,DISK], DatanodeInfoWithStorage[127.0.0.1:41204,DS-1df94765-e489-41d6-b23f-a57e3385a350,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41832,DS-69445977-e50f-484b-8b69-388345f6c000,DISK], DatanodeInfoWithStorage[127.0.0.1:34648,DS-b09a6eaf-9749-42d9-bd0b-ed130cec5ad2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41832,DS-69445977-e50f-484b-8b69-388345f6c000,DISK], DatanodeInfoWithStorage[127.0.0.1:34648,DS-b09a6eaf-9749-42d9-bd0b-ed130cec5ad2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41832,DS-69445977-e50f-484b-8b69-388345f6c000,DISK], DatanodeInfoWithStorage[127.0.0.1:34648,DS-b09a6eaf-9749-42d9-bd0b-ed130cec5ad2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41832,DS-69445977-e50f-484b-8b69-388345f6c000,DISK], DatanodeInfoWithStorage[127.0.0.1:34648,DS-b09a6eaf-9749-42d9-bd0b-ed130cec5ad2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39466,DS-5a8e1dea-a167-4667-afd3-94c3397af845,DISK], DatanodeInfoWithStorage[127.0.0.1:42717,DS-32b35712-0c89-445d-a9b6-e0ae08a3a28d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42717,DS-32b35712-0c89-445d-a9b6-e0ae08a3a28d,DISK], DatanodeInfoWithStorage[127.0.0.1:39466,DS-5a8e1dea-a167-4667-afd3-94c3397af845,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39466,DS-5a8e1dea-a167-4667-afd3-94c3397af845,DISK], DatanodeInfoWithStorage[127.0.0.1:42717,DS-32b35712-0c89-445d-a9b6-e0ae08a3a28d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42717,DS-32b35712-0c89-445d-a9b6-e0ae08a3a28d,DISK], DatanodeInfoWithStorage[127.0.0.1:39466,DS-5a8e1dea-a167-4667-afd3-94c3397af845,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43918,DS-d720025c-16c3-4f48-9541-8ba9c8831358,DISK], DatanodeInfoWithStorage[127.0.0.1:39292,DS-52a7c61c-59eb-4dd2-8b9a-0fb91f3cc22d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39292,DS-52a7c61c-59eb-4dd2-8b9a-0fb91f3cc22d,DISK], DatanodeInfoWithStorage[127.0.0.1:43918,DS-d720025c-16c3-4f48-9541-8ba9c8831358,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43918,DS-d720025c-16c3-4f48-9541-8ba9c8831358,DISK], DatanodeInfoWithStorage[127.0.0.1:39292,DS-52a7c61c-59eb-4dd2-8b9a-0fb91f3cc22d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39292,DS-52a7c61c-59eb-4dd2-8b9a-0fb91f3cc22d,DISK], DatanodeInfoWithStorage[127.0.0.1:43918,DS-d720025c-16c3-4f48-9541-8ba9c8831358,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34070,DS-43560208-e315-4c7d-b34a-fc0874e1445d,DISK], DatanodeInfoWithStorage[127.0.0.1:39229,DS-8e199ae8-a734-4119-a609-938677026687,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39229,DS-8e199ae8-a734-4119-a609-938677026687,DISK], DatanodeInfoWithStorage[127.0.0.1:34070,DS-43560208-e315-4c7d-b34a-fc0874e1445d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34070,DS-43560208-e315-4c7d-b34a-fc0874e1445d,DISK], DatanodeInfoWithStorage[127.0.0.1:39229,DS-8e199ae8-a734-4119-a609-938677026687,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39229,DS-8e199ae8-a734-4119-a609-938677026687,DISK], DatanodeInfoWithStorage[127.0.0.1:34070,DS-43560208-e315-4c7d-b34a-fc0874e1445d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37198,DS-e5773ccd-8c67-4ba1-899e-ab83db0d2ed2,DISK], DatanodeInfoWithStorage[127.0.0.1:43135,DS-f52456e3-8851-40c5-80e8-61ecc96a3955,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37198,DS-e5773ccd-8c67-4ba1-899e-ab83db0d2ed2,DISK], DatanodeInfoWithStorage[127.0.0.1:43135,DS-f52456e3-8851-40c5-80e8-61ecc96a3955,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37198,DS-e5773ccd-8c67-4ba1-899e-ab83db0d2ed2,DISK], DatanodeInfoWithStorage[127.0.0.1:43135,DS-f52456e3-8851-40c5-80e8-61ecc96a3955,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37198,DS-e5773ccd-8c67-4ba1-899e-ab83db0d2ed2,DISK], DatanodeInfoWithStorage[127.0.0.1:43135,DS-f52456e3-8851-40c5-80e8-61ecc96a3955,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39190,DS-16c52652-e895-4a86-a47c-fdd854e2d41c,DISK], DatanodeInfoWithStorage[127.0.0.1:43236,DS-9243e99b-13ff-4bbc-bffa-41a5e1d21316,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43236,DS-9243e99b-13ff-4bbc-bffa-41a5e1d21316,DISK], DatanodeInfoWithStorage[127.0.0.1:39190,DS-16c52652-e895-4a86-a47c-fdd854e2d41c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39190,DS-16c52652-e895-4a86-a47c-fdd854e2d41c,DISK], DatanodeInfoWithStorage[127.0.0.1:43236,DS-9243e99b-13ff-4bbc-bffa-41a5e1d21316,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43236,DS-9243e99b-13ff-4bbc-bffa-41a5e1d21316,DISK], DatanodeInfoWithStorage[127.0.0.1:39190,DS-16c52652-e895-4a86-a47c-fdd854e2d41c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46536,DS-58ebfaa7-1938-41ef-af15-d6644d3a0db9,DISK], DatanodeInfoWithStorage[127.0.0.1:36609,DS-df3a2236-7f4c-4827-ba30-6f185412b8ce,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46536,DS-58ebfaa7-1938-41ef-af15-d6644d3a0db9,DISK], DatanodeInfoWithStorage[127.0.0.1:36609,DS-df3a2236-7f4c-4827-ba30-6f185412b8ce,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46536,DS-58ebfaa7-1938-41ef-af15-d6644d3a0db9,DISK], DatanodeInfoWithStorage[127.0.0.1:36609,DS-df3a2236-7f4c-4827-ba30-6f185412b8ce,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46536,DS-58ebfaa7-1938-41ef-af15-d6644d3a0db9,DISK], DatanodeInfoWithStorage[127.0.0.1:36609,DS-df3a2236-7f4c-4827-ba30-6f185412b8ce,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45963,DS-6b441b32-5f2b-4362-a474-e32366d46287,DISK], DatanodeInfoWithStorage[127.0.0.1:44559,DS-da290c5e-6fc9-4365-a32b-4a91efd32ca7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44559,DS-da290c5e-6fc9-4365-a32b-4a91efd32ca7,DISK], DatanodeInfoWithStorage[127.0.0.1:45963,DS-6b441b32-5f2b-4362-a474-e32366d46287,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45963,DS-6b441b32-5f2b-4362-a474-e32366d46287,DISK], DatanodeInfoWithStorage[127.0.0.1:44559,DS-da290c5e-6fc9-4365-a32b-4a91efd32ca7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44559,DS-da290c5e-6fc9-4365-a32b-4a91efd32ca7,DISK], DatanodeInfoWithStorage[127.0.0.1:45963,DS-6b441b32-5f2b-4362-a474-e32366d46287,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33437,DS-dd941e15-0d08-4159-b782-87b9698156b0,DISK], DatanodeInfoWithStorage[127.0.0.1:40569,DS-e750b4c1-128e-4405-862d-a22ac288e2ec,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33437,DS-dd941e15-0d08-4159-b782-87b9698156b0,DISK], DatanodeInfoWithStorage[127.0.0.1:40569,DS-e750b4c1-128e-4405-862d-a22ac288e2ec,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33437,DS-dd941e15-0d08-4159-b782-87b9698156b0,DISK], DatanodeInfoWithStorage[127.0.0.1:40569,DS-e750b4c1-128e-4405-862d-a22ac288e2ec,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33437,DS-dd941e15-0d08-4159-b782-87b9698156b0,DISK], DatanodeInfoWithStorage[127.0.0.1:40569,DS-e750b4c1-128e-4405-862d-a22ac288e2ec,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36934,DS-fc7f9f8a-a69c-430f-bfd1-7aa9d9dafe50,DISK], DatanodeInfoWithStorage[127.0.0.1:35767,DS-9131ee49-3aad-4c24-ac62-0d33cbe327da,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36934,DS-fc7f9f8a-a69c-430f-bfd1-7aa9d9dafe50,DISK], DatanodeInfoWithStorage[127.0.0.1:35767,DS-9131ee49-3aad-4c24-ac62-0d33cbe327da,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36934,DS-fc7f9f8a-a69c-430f-bfd1-7aa9d9dafe50,DISK], DatanodeInfoWithStorage[127.0.0.1:35767,DS-9131ee49-3aad-4c24-ac62-0d33cbe327da,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36934,DS-fc7f9f8a-a69c-430f-bfd1-7aa9d9dafe50,DISK], DatanodeInfoWithStorage[127.0.0.1:35767,DS-9131ee49-3aad-4c24-ac62-0d33cbe327da,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32932,DS-12ce1699-5306-457a-8156-60518f057680,DISK], DatanodeInfoWithStorage[127.0.0.1:41574,DS-0b363b72-20ec-4d1f-903b-f007fa20baef,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32932,DS-12ce1699-5306-457a-8156-60518f057680,DISK], DatanodeInfoWithStorage[127.0.0.1:41574,DS-0b363b72-20ec-4d1f-903b-f007fa20baef,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32932,DS-12ce1699-5306-457a-8156-60518f057680,DISK], DatanodeInfoWithStorage[127.0.0.1:41574,DS-0b363b72-20ec-4d1f-903b-f007fa20baef,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32932,DS-12ce1699-5306-457a-8156-60518f057680,DISK], DatanodeInfoWithStorage[127.0.0.1:41574,DS-0b363b72-20ec-4d1f-903b-f007fa20baef,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34433,DS-7c904285-6f48-4dcf-9515-85085f90ac7f,DISK], DatanodeInfoWithStorage[127.0.0.1:37885,DS-631e5d16-ec66-4296-b083-fd2fe3774afa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37885,DS-631e5d16-ec66-4296-b083-fd2fe3774afa,DISK], DatanodeInfoWithStorage[127.0.0.1:34433,DS-7c904285-6f48-4dcf-9515-85085f90ac7f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34433,DS-7c904285-6f48-4dcf-9515-85085f90ac7f,DISK], DatanodeInfoWithStorage[127.0.0.1:37885,DS-631e5d16-ec66-4296-b083-fd2fe3774afa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37885,DS-631e5d16-ec66-4296-b083-fd2fe3774afa,DISK], DatanodeInfoWithStorage[127.0.0.1:34433,DS-7c904285-6f48-4dcf-9515-85085f90ac7f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45856,DS-52892332-944a-421a-9861-19f525bb0cd9,DISK], DatanodeInfoWithStorage[127.0.0.1:43305,DS-59662cdf-2e73-4ebe-8106-28fedbeb21f7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45856,DS-52892332-944a-421a-9861-19f525bb0cd9,DISK], DatanodeInfoWithStorage[127.0.0.1:43305,DS-59662cdf-2e73-4ebe-8106-28fedbeb21f7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45856,DS-52892332-944a-421a-9861-19f525bb0cd9,DISK], DatanodeInfoWithStorage[127.0.0.1:43305,DS-59662cdf-2e73-4ebe-8106-28fedbeb21f7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45856,DS-52892332-944a-421a-9861-19f525bb0cd9,DISK], DatanodeInfoWithStorage[127.0.0.1:43305,DS-59662cdf-2e73-4ebe-8106-28fedbeb21f7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38792,DS-7af2ae09-7263-43e0-b15e-0efdc18bd746,DISK], DatanodeInfoWithStorage[127.0.0.1:42679,DS-b4cad606-1116-4804-bc7b-8bf7c48e27e5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42679,DS-b4cad606-1116-4804-bc7b-8bf7c48e27e5,DISK], DatanodeInfoWithStorage[127.0.0.1:38792,DS-7af2ae09-7263-43e0-b15e-0efdc18bd746,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38792,DS-7af2ae09-7263-43e0-b15e-0efdc18bd746,DISK], DatanodeInfoWithStorage[127.0.0.1:42679,DS-b4cad606-1116-4804-bc7b-8bf7c48e27e5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42679,DS-b4cad606-1116-4804-bc7b-8bf7c48e27e5,DISK], DatanodeInfoWithStorage[127.0.0.1:38792,DS-7af2ae09-7263-43e0-b15e-0efdc18bd746,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36820,DS-ac509fb9-c8ec-40d1-81ca-0ed550e8317e,DISK], DatanodeInfoWithStorage[127.0.0.1:43473,DS-4c22f76d-17ac-4966-8c03-38901820787b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36820,DS-ac509fb9-c8ec-40d1-81ca-0ed550e8317e,DISK], DatanodeInfoWithStorage[127.0.0.1:43473,DS-4c22f76d-17ac-4966-8c03-38901820787b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36820,DS-ac509fb9-c8ec-40d1-81ca-0ed550e8317e,DISK], DatanodeInfoWithStorage[127.0.0.1:43473,DS-4c22f76d-17ac-4966-8c03-38901820787b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36820,DS-ac509fb9-c8ec-40d1-81ca-0ed550e8317e,DISK], DatanodeInfoWithStorage[127.0.0.1:43473,DS-4c22f76d-17ac-4966-8c03-38901820787b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33902,DS-a6b2c914-3c29-4890-a64b-0c16315889b1,DISK], DatanodeInfoWithStorage[127.0.0.1:43616,DS-c20ac0cb-3540-4d06-8e0e-04e7f8e5ac8c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33902,DS-a6b2c914-3c29-4890-a64b-0c16315889b1,DISK], DatanodeInfoWithStorage[127.0.0.1:43616,DS-c20ac0cb-3540-4d06-8e0e-04e7f8e5ac8c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33902,DS-a6b2c914-3c29-4890-a64b-0c16315889b1,DISK], DatanodeInfoWithStorage[127.0.0.1:43616,DS-c20ac0cb-3540-4d06-8e0e-04e7f8e5ac8c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33902,DS-a6b2c914-3c29-4890-a64b-0c16315889b1,DISK], DatanodeInfoWithStorage[127.0.0.1:43616,DS-c20ac0cb-3540-4d06-8e0e-04e7f8e5ac8c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38349,DS-167798f7-b309-4220-b8b0-3aeb442f6c25,DISK], DatanodeInfoWithStorage[127.0.0.1:43207,DS-53e18496-b9a5-4f28-b14f-f8af5137cac3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43207,DS-53e18496-b9a5-4f28-b14f-f8af5137cac3,DISK], DatanodeInfoWithStorage[127.0.0.1:38349,DS-167798f7-b309-4220-b8b0-3aeb442f6c25,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38349,DS-167798f7-b309-4220-b8b0-3aeb442f6c25,DISK], DatanodeInfoWithStorage[127.0.0.1:43207,DS-53e18496-b9a5-4f28-b14f-f8af5137cac3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43207,DS-53e18496-b9a5-4f28-b14f-f8af5137cac3,DISK], DatanodeInfoWithStorage[127.0.0.1:38349,DS-167798f7-b309-4220-b8b0-3aeb442f6c25,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37851,DS-2cc81afc-f80e-4e65-948c-de5214ccd08d,DISK], DatanodeInfoWithStorage[127.0.0.1:45647,DS-f794f100-5ec3-4e2d-983c-7311650d4093,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45647,DS-f794f100-5ec3-4e2d-983c-7311650d4093,DISK], DatanodeInfoWithStorage[127.0.0.1:37851,DS-2cc81afc-f80e-4e65-948c-de5214ccd08d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37851,DS-2cc81afc-f80e-4e65-948c-de5214ccd08d,DISK], DatanodeInfoWithStorage[127.0.0.1:45647,DS-f794f100-5ec3-4e2d-983c-7311650d4093,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45647,DS-f794f100-5ec3-4e2d-983c-7311650d4093,DISK], DatanodeInfoWithStorage[127.0.0.1:37851,DS-2cc81afc-f80e-4e65-948c-de5214ccd08d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34527,DS-586bd01d-892c-4dec-b12f-7eb4ed2fcfb3,DISK], DatanodeInfoWithStorage[127.0.0.1:41110,DS-4f641150-fa96-4085-93dc-c496275e1b61,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41110,DS-4f641150-fa96-4085-93dc-c496275e1b61,DISK], DatanodeInfoWithStorage[127.0.0.1:34527,DS-586bd01d-892c-4dec-b12f-7eb4ed2fcfb3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34527,DS-586bd01d-892c-4dec-b12f-7eb4ed2fcfb3,DISK], DatanodeInfoWithStorage[127.0.0.1:41110,DS-4f641150-fa96-4085-93dc-c496275e1b61,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41110,DS-4f641150-fa96-4085-93dc-c496275e1b61,DISK], DatanodeInfoWithStorage[127.0.0.1:34527,DS-586bd01d-892c-4dec-b12f-7eb4ed2fcfb3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44891,DS-419221ca-f8e0-4095-91e3-837d586ab44f,DISK], DatanodeInfoWithStorage[127.0.0.1:42072,DS-8dfcc61d-ad82-4e40-bb56-e122896a8e7b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44891,DS-419221ca-f8e0-4095-91e3-837d586ab44f,DISK], DatanodeInfoWithStorage[127.0.0.1:42072,DS-8dfcc61d-ad82-4e40-bb56-e122896a8e7b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44891,DS-419221ca-f8e0-4095-91e3-837d586ab44f,DISK], DatanodeInfoWithStorage[127.0.0.1:42072,DS-8dfcc61d-ad82-4e40-bb56-e122896a8e7b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44891,DS-419221ca-f8e0-4095-91e3-837d586ab44f,DISK], DatanodeInfoWithStorage[127.0.0.1:42072,DS-8dfcc61d-ad82-4e40-bb56-e122896a8e7b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42333,DS-b307f40e-f3c5-4673-a724-36ff6b6cfea8,DISK], DatanodeInfoWithStorage[127.0.0.1:36738,DS-48acd077-7bf8-4f4a-b2be-a94da8956f1b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36738,DS-48acd077-7bf8-4f4a-b2be-a94da8956f1b,DISK], DatanodeInfoWithStorage[127.0.0.1:42333,DS-b307f40e-f3c5-4673-a724-36ff6b6cfea8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42333,DS-b307f40e-f3c5-4673-a724-36ff6b6cfea8,DISK], DatanodeInfoWithStorage[127.0.0.1:36738,DS-48acd077-7bf8-4f4a-b2be-a94da8956f1b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36738,DS-48acd077-7bf8-4f4a-b2be-a94da8956f1b,DISK], DatanodeInfoWithStorage[127.0.0.1:42333,DS-b307f40e-f3c5-4673-a724-36ff6b6cfea8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43850,DS-b702de81-0081-45df-b0de-fd6e79e90a22,DISK], DatanodeInfoWithStorage[127.0.0.1:41993,DS-b80bb0e6-eba3-4c3d-9d52-3d78a8b8d8c6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41993,DS-b80bb0e6-eba3-4c3d-9d52-3d78a8b8d8c6,DISK], DatanodeInfoWithStorage[127.0.0.1:43850,DS-b702de81-0081-45df-b0de-fd6e79e90a22,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43850,DS-b702de81-0081-45df-b0de-fd6e79e90a22,DISK], DatanodeInfoWithStorage[127.0.0.1:41993,DS-b80bb0e6-eba3-4c3d-9d52-3d78a8b8d8c6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41993,DS-b80bb0e6-eba3-4c3d-9d52-3d78a8b8d8c6,DISK], DatanodeInfoWithStorage[127.0.0.1:43850,DS-b702de81-0081-45df-b0de-fd6e79e90a22,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44279,DS-6495e4ee-13b8-4da4-b20e-9a8e0513893b,DISK], DatanodeInfoWithStorage[127.0.0.1:33319,DS-fdb07fdc-981b-42dd-b3d8-3f5a77100d17,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44279,DS-6495e4ee-13b8-4da4-b20e-9a8e0513893b,DISK], DatanodeInfoWithStorage[127.0.0.1:33319,DS-fdb07fdc-981b-42dd-b3d8-3f5a77100d17,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44279,DS-6495e4ee-13b8-4da4-b20e-9a8e0513893b,DISK], DatanodeInfoWithStorage[127.0.0.1:33319,DS-fdb07fdc-981b-42dd-b3d8-3f5a77100d17,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44279,DS-6495e4ee-13b8-4da4-b20e-9a8e0513893b,DISK], DatanodeInfoWithStorage[127.0.0.1:33319,DS-fdb07fdc-981b-42dd-b3d8-3f5a77100d17,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43754,DS-f780abe8-e2ff-49c0-bb23-6628bbb1793f,DISK], DatanodeInfoWithStorage[127.0.0.1:36441,DS-8e30f07a-811c-4ece-9014-b7f250086955,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36441,DS-8e30f07a-811c-4ece-9014-b7f250086955,DISK], DatanodeInfoWithStorage[127.0.0.1:43754,DS-f780abe8-e2ff-49c0-bb23-6628bbb1793f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43754,DS-f780abe8-e2ff-49c0-bb23-6628bbb1793f,DISK], DatanodeInfoWithStorage[127.0.0.1:36441,DS-8e30f07a-811c-4ece-9014-b7f250086955,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36441,DS-8e30f07a-811c-4ece-9014-b7f250086955,DISK], DatanodeInfoWithStorage[127.0.0.1:43754,DS-f780abe8-e2ff-49c0-bb23-6628bbb1793f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36484,DS-71d8f889-0766-401e-998f-1ec49815429f,DISK], DatanodeInfoWithStorage[127.0.0.1:40089,DS-6eb83ea4-ed37-45be-a6e9-d188f05bec66,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36484,DS-71d8f889-0766-401e-998f-1ec49815429f,DISK], DatanodeInfoWithStorage[127.0.0.1:40089,DS-6eb83ea4-ed37-45be-a6e9-d188f05bec66,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36484,DS-71d8f889-0766-401e-998f-1ec49815429f,DISK], DatanodeInfoWithStorage[127.0.0.1:40089,DS-6eb83ea4-ed37-45be-a6e9-d188f05bec66,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36484,DS-71d8f889-0766-401e-998f-1ec49815429f,DISK], DatanodeInfoWithStorage[127.0.0.1:40089,DS-6eb83ea4-ed37-45be-a6e9-d188f05bec66,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37608,DS-26ff8e09-86e2-49c3-9efb-9e950855780b,DISK], DatanodeInfoWithStorage[127.0.0.1:45886,DS-0b678a93-1d4f-4402-93d9-d10fd3390691,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45886,DS-0b678a93-1d4f-4402-93d9-d10fd3390691,DISK], DatanodeInfoWithStorage[127.0.0.1:37608,DS-26ff8e09-86e2-49c3-9efb-9e950855780b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37608,DS-26ff8e09-86e2-49c3-9efb-9e950855780b,DISK], DatanodeInfoWithStorage[127.0.0.1:45886,DS-0b678a93-1d4f-4402-93d9-d10fd3390691,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45886,DS-0b678a93-1d4f-4402-93d9-d10fd3390691,DISK], DatanodeInfoWithStorage[127.0.0.1:37608,DS-26ff8e09-86e2-49c3-9efb-9e950855780b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33601,DS-ddc6867f-073f-4187-b7cf-49ad13ec7bcc,DISK], DatanodeInfoWithStorage[127.0.0.1:33318,DS-021f5e43-3459-4476-a72b-eb596c537fb8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33601,DS-ddc6867f-073f-4187-b7cf-49ad13ec7bcc,DISK], DatanodeInfoWithStorage[127.0.0.1:33318,DS-021f5e43-3459-4476-a72b-eb596c537fb8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33601,DS-ddc6867f-073f-4187-b7cf-49ad13ec7bcc,DISK], DatanodeInfoWithStorage[127.0.0.1:33318,DS-021f5e43-3459-4476-a72b-eb596c537fb8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33601,DS-ddc6867f-073f-4187-b7cf-49ad13ec7bcc,DISK], DatanodeInfoWithStorage[127.0.0.1:33318,DS-021f5e43-3459-4476-a72b-eb596c537fb8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40800,DS-e003dd13-f906-4a0c-beac-573f939c34d4,DISK], DatanodeInfoWithStorage[127.0.0.1:33478,DS-e38b78f3-4e1a-42dd-9edc-5f725b8c3218,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33478,DS-e38b78f3-4e1a-42dd-9edc-5f725b8c3218,DISK], DatanodeInfoWithStorage[127.0.0.1:40800,DS-e003dd13-f906-4a0c-beac-573f939c34d4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40800,DS-e003dd13-f906-4a0c-beac-573f939c34d4,DISK], DatanodeInfoWithStorage[127.0.0.1:33478,DS-e38b78f3-4e1a-42dd-9edc-5f725b8c3218,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33478,DS-e38b78f3-4e1a-42dd-9edc-5f725b8c3218,DISK], DatanodeInfoWithStorage[127.0.0.1:40800,DS-e003dd13-f906-4a0c-beac-573f939c34d4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35903,DS-3fb26fc6-5d77-477c-9fb9-37149baa8058,DISK], DatanodeInfoWithStorage[127.0.0.1:32972,DS-4ee174b1-a026-471a-8a52-46e9e929b45b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32972,DS-4ee174b1-a026-471a-8a52-46e9e929b45b,DISK], DatanodeInfoWithStorage[127.0.0.1:35903,DS-3fb26fc6-5d77-477c-9fb9-37149baa8058,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35903,DS-3fb26fc6-5d77-477c-9fb9-37149baa8058,DISK], DatanodeInfoWithStorage[127.0.0.1:32972,DS-4ee174b1-a026-471a-8a52-46e9e929b45b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32972,DS-4ee174b1-a026-471a-8a52-46e9e929b45b,DISK], DatanodeInfoWithStorage[127.0.0.1:35903,DS-3fb26fc6-5d77-477c-9fb9-37149baa8058,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 60
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testNameConflictWhenSplit1
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40842,DS-35b6314a-c4e0-4f1d-a926-1fbaedf5e3a3,DISK], DatanodeInfoWithStorage[127.0.0.1:34980,DS-8213435d-c6c6-4ed6-b70a-663c0170022a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40842,DS-35b6314a-c4e0-4f1d-a926-1fbaedf5e3a3,DISK], DatanodeInfoWithStorage[127.0.0.1:34980,DS-8213435d-c6c6-4ed6-b70a-663c0170022a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40842,DS-35b6314a-c4e0-4f1d-a926-1fbaedf5e3a3,DISK], DatanodeInfoWithStorage[127.0.0.1:34980,DS-8213435d-c6c6-4ed6-b70a-663c0170022a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40842,DS-35b6314a-c4e0-4f1d-a926-1fbaedf5e3a3,DISK], DatanodeInfoWithStorage[127.0.0.1:34980,DS-8213435d-c6c6-4ed6-b70a-663c0170022a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 50 out of 50
v1v1v2v2 failed with probability 9 out of 50
result: might be true error
Total execution time in seconds : 14703

reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-779442512-172.17.0.13-1595960473283:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34982,DS-8834bd39-db4f-4f19-866c-63b37feda3c5,DISK], DatanodeInfoWithStorage[127.0.0.1:33281,DS-1e77e003-881f-4db2-8481-ab6e6c083c28,DISK], DatanodeInfoWithStorage[127.0.0.1:33436,DS-2e646291-f9e3-4cb1-9559-47cabcaed64c,DISK], DatanodeInfoWithStorage[127.0.0.1:38583,DS-7cf174e3-3de8-4fa7-8ad9-f71652dd7171,DISK], DatanodeInfoWithStorage[127.0.0.1:40061,DS-6d223f94-c845-40cf-8570-f94f45e79df5,DISK], DatanodeInfoWithStorage[127.0.0.1:43788,DS-a33478f8-b53b-4fba-b63e-91138f812321,DISK], DatanodeInfoWithStorage[127.0.0.1:38343,DS-5f062682-e700-4fa2-bffa-63dcd2fa469f,DISK], DatanodeInfoWithStorage[127.0.0.1:45094,DS-f954f5b6-0026-4e85-bd14-ee8acb02ab0b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-779442512-172.17.0.13-1595960473283:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34982,DS-8834bd39-db4f-4f19-866c-63b37feda3c5,DISK], DatanodeInfoWithStorage[127.0.0.1:33281,DS-1e77e003-881f-4db2-8481-ab6e6c083c28,DISK], DatanodeInfoWithStorage[127.0.0.1:33436,DS-2e646291-f9e3-4cb1-9559-47cabcaed64c,DISK], DatanodeInfoWithStorage[127.0.0.1:38583,DS-7cf174e3-3de8-4fa7-8ad9-f71652dd7171,DISK], DatanodeInfoWithStorage[127.0.0.1:40061,DS-6d223f94-c845-40cf-8570-f94f45e79df5,DISK], DatanodeInfoWithStorage[127.0.0.1:43788,DS-a33478f8-b53b-4fba-b63e-91138f812321,DISK], DatanodeInfoWithStorage[127.0.0.1:38343,DS-5f062682-e700-4fa2-bffa-63dcd2fa469f,DISK], DatanodeInfoWithStorage[127.0.0.1:45094,DS-f954f5b6-0026-4e85-bd14-ee8acb02ab0b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-99353159-172.17.0.13-1595960548411:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46770,DS-d76dd551-08af-47a6-b8d3-60984bbf890b,DISK], DatanodeInfoWithStorage[127.0.0.1:34663,DS-41f886d4-8e51-4c2b-b3ef-7a0860bf88b4,DISK], DatanodeInfoWithStorage[127.0.0.1:40024,DS-6faa91ab-a366-4f01-aef9-aff4e12dbe6a,DISK], DatanodeInfoWithStorage[127.0.0.1:36387,DS-85d7abbf-40f7-45a5-9551-452873100740,DISK], DatanodeInfoWithStorage[127.0.0.1:33723,DS-4943056f-3e5d-4a35-94bb-71d78fcbfe6b,DISK], DatanodeInfoWithStorage[127.0.0.1:45394,DS-f73681f6-2c99-428f-bf49-a460a96d47e6,DISK], DatanodeInfoWithStorage[127.0.0.1:43242,DS-9648e14c-fa86-484b-bf8e-918bb9805fa3,DISK], DatanodeInfoWithStorage[127.0.0.1:35246,DS-ab090679-5f1b-4190-a3f3-9e21ab85f13a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-99353159-172.17.0.13-1595960548411:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46770,DS-d76dd551-08af-47a6-b8d3-60984bbf890b,DISK], DatanodeInfoWithStorage[127.0.0.1:34663,DS-41f886d4-8e51-4c2b-b3ef-7a0860bf88b4,DISK], DatanodeInfoWithStorage[127.0.0.1:40024,DS-6faa91ab-a366-4f01-aef9-aff4e12dbe6a,DISK], DatanodeInfoWithStorage[127.0.0.1:36387,DS-85d7abbf-40f7-45a5-9551-452873100740,DISK], DatanodeInfoWithStorage[127.0.0.1:33723,DS-4943056f-3e5d-4a35-94bb-71d78fcbfe6b,DISK], DatanodeInfoWithStorage[127.0.0.1:45394,DS-f73681f6-2c99-428f-bf49-a460a96d47e6,DISK], DatanodeInfoWithStorage[127.0.0.1:43242,DS-9648e14c-fa86-484b-bf8e-918bb9805fa3,DISK], DatanodeInfoWithStorage[127.0.0.1:35246,DS-ab090679-5f1b-4190-a3f3-9e21ab85f13a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-900165138-172.17.0.13-1595960664151:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38276,DS-82e76a65-a31a-4a3e-ac3b-918219097194,DISK], DatanodeInfoWithStorage[127.0.0.1:42494,DS-c4b64317-411f-4916-b0a1-361fb0665420,DISK], DatanodeInfoWithStorage[127.0.0.1:37850,DS-137cdfd2-eb63-4a07-8327-6aaecff48f83,DISK], DatanodeInfoWithStorage[127.0.0.1:42272,DS-e3ee6981-ecd0-4c61-9945-74ba127fc1ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37428,DS-22721667-858a-43b6-b2ff-f5c88e76ebb4,DISK], DatanodeInfoWithStorage[127.0.0.1:35443,DS-9f452b50-4d53-4183-b414-8529dec356bd,DISK], DatanodeInfoWithStorage[127.0.0.1:33526,DS-e5daccff-0cc3-4ec5-a39c-3ff64ad93b9e,DISK], DatanodeInfoWithStorage[127.0.0.1:45163,DS-c17ed0f8-7d22-40f4-87a1-491cb084240b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-900165138-172.17.0.13-1595960664151:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38276,DS-82e76a65-a31a-4a3e-ac3b-918219097194,DISK], DatanodeInfoWithStorage[127.0.0.1:42494,DS-c4b64317-411f-4916-b0a1-361fb0665420,DISK], DatanodeInfoWithStorage[127.0.0.1:37850,DS-137cdfd2-eb63-4a07-8327-6aaecff48f83,DISK], DatanodeInfoWithStorage[127.0.0.1:42272,DS-e3ee6981-ecd0-4c61-9945-74ba127fc1ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37428,DS-22721667-858a-43b6-b2ff-f5c88e76ebb4,DISK], DatanodeInfoWithStorage[127.0.0.1:35443,DS-9f452b50-4d53-4183-b414-8529dec356bd,DISK], DatanodeInfoWithStorage[127.0.0.1:33526,DS-e5daccff-0cc3-4ec5-a39c-3ff64ad93b9e,DISK], DatanodeInfoWithStorage[127.0.0.1:45163,DS-c17ed0f8-7d22-40f4-87a1-491cb084240b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2140133868-172.17.0.13-1595961013488:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36926,DS-6f205a85-4890-4d18-aefc-5a2924e1c1f0,DISK], DatanodeInfoWithStorage[127.0.0.1:36367,DS-63bd9204-3e7e-4f92-996c-b7bf189d3173,DISK], DatanodeInfoWithStorage[127.0.0.1:40593,DS-85b07932-a19d-441f-860a-82e2748ca58d,DISK], DatanodeInfoWithStorage[127.0.0.1:33256,DS-11f18da5-48f5-44c8-bd88-8590981d1c3f,DISK], DatanodeInfoWithStorage[127.0.0.1:43927,DS-9d0cfd3a-442e-41d7-8605-24840742e63c,DISK], DatanodeInfoWithStorage[127.0.0.1:39946,DS-e9b1eca3-9e1e-4cd4-9111-947e07b449f3,DISK], DatanodeInfoWithStorage[127.0.0.1:42303,DS-1486e8a9-ffb7-43e2-9373-9347afd7cb20,DISK], DatanodeInfoWithStorage[127.0.0.1:35884,DS-0678b8d7-43bf-4314-98f1-c01d706ddc73,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2140133868-172.17.0.13-1595961013488:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36926,DS-6f205a85-4890-4d18-aefc-5a2924e1c1f0,DISK], DatanodeInfoWithStorage[127.0.0.1:36367,DS-63bd9204-3e7e-4f92-996c-b7bf189d3173,DISK], DatanodeInfoWithStorage[127.0.0.1:40593,DS-85b07932-a19d-441f-860a-82e2748ca58d,DISK], DatanodeInfoWithStorage[127.0.0.1:33256,DS-11f18da5-48f5-44c8-bd88-8590981d1c3f,DISK], DatanodeInfoWithStorage[127.0.0.1:43927,DS-9d0cfd3a-442e-41d7-8605-24840742e63c,DISK], DatanodeInfoWithStorage[127.0.0.1:39946,DS-e9b1eca3-9e1e-4cd4-9111-947e07b449f3,DISK], DatanodeInfoWithStorage[127.0.0.1:42303,DS-1486e8a9-ffb7-43e2-9373-9347afd7cb20,DISK], DatanodeInfoWithStorage[127.0.0.1:35884,DS-0678b8d7-43bf-4314-98f1-c01d706ddc73,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2060082491-172.17.0.13-1595961056333:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45276,DS-6b0229f8-1697-41e9-93db-09e1229deb23,DISK], DatanodeInfoWithStorage[127.0.0.1:39427,DS-86a537e3-e7b3-48b4-81bf-182bb0551ac6,DISK], DatanodeInfoWithStorage[127.0.0.1:38227,DS-2eb81667-1ab7-4521-8794-9260f5e5b717,DISK], DatanodeInfoWithStorage[127.0.0.1:41905,DS-84a0a6d9-21e7-402a-8ac4-8c0082300306,DISK], DatanodeInfoWithStorage[127.0.0.1:41330,DS-fa6e1efb-6b5f-40c0-b447-fecd9e1d18f1,DISK], DatanodeInfoWithStorage[127.0.0.1:35799,DS-88707f49-c234-4921-aa5a-fae942f02e38,DISK], DatanodeInfoWithStorage[127.0.0.1:40125,DS-dc9bda82-2884-4d80-9a46-4a86c2dd3330,DISK], DatanodeInfoWithStorage[127.0.0.1:39805,DS-8052261a-1ea6-4358-aee4-4d3d8f49fff4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2060082491-172.17.0.13-1595961056333:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45276,DS-6b0229f8-1697-41e9-93db-09e1229deb23,DISK], DatanodeInfoWithStorage[127.0.0.1:39427,DS-86a537e3-e7b3-48b4-81bf-182bb0551ac6,DISK], DatanodeInfoWithStorage[127.0.0.1:38227,DS-2eb81667-1ab7-4521-8794-9260f5e5b717,DISK], DatanodeInfoWithStorage[127.0.0.1:41905,DS-84a0a6d9-21e7-402a-8ac4-8c0082300306,DISK], DatanodeInfoWithStorage[127.0.0.1:41330,DS-fa6e1efb-6b5f-40c0-b447-fecd9e1d18f1,DISK], DatanodeInfoWithStorage[127.0.0.1:35799,DS-88707f49-c234-4921-aa5a-fae942f02e38,DISK], DatanodeInfoWithStorage[127.0.0.1:40125,DS-dc9bda82-2884-4d80-9a46-4a86c2dd3330,DISK], DatanodeInfoWithStorage[127.0.0.1:39805,DS-8052261a-1ea6-4358-aee4-4d3d8f49fff4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1338568877-172.17.0.13-1595961302640:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34653,DS-b112b248-7f89-4931-b0f8-a9078bf5ef4f,DISK], DatanodeInfoWithStorage[127.0.0.1:35602,DS-b27a46a2-12a9-4ec8-a544-94b61869a47b,DISK], DatanodeInfoWithStorage[127.0.0.1:34275,DS-d52e22de-8cca-417c-8961-6f10db38668a,DISK], DatanodeInfoWithStorage[127.0.0.1:37548,DS-d72a6c57-3251-4b20-93a1-d15c73cf8793,DISK], DatanodeInfoWithStorage[127.0.0.1:39787,DS-6f12b727-61c7-4d18-80a3-f7b1bf3715c4,DISK], DatanodeInfoWithStorage[127.0.0.1:46068,DS-10b6902c-8df2-4c2d-b58b-ecd3dfff76ac,DISK], DatanodeInfoWithStorage[127.0.0.1:35064,DS-6a48282d-fd5f-49f5-9ed8-0b685e555b8d,DISK], DatanodeInfoWithStorage[127.0.0.1:41494,DS-fbfe3c34-8635-4386-ad79-c3847f27fb7d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1338568877-172.17.0.13-1595961302640:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34653,DS-b112b248-7f89-4931-b0f8-a9078bf5ef4f,DISK], DatanodeInfoWithStorage[127.0.0.1:35602,DS-b27a46a2-12a9-4ec8-a544-94b61869a47b,DISK], DatanodeInfoWithStorage[127.0.0.1:34275,DS-d52e22de-8cca-417c-8961-6f10db38668a,DISK], DatanodeInfoWithStorage[127.0.0.1:37548,DS-d72a6c57-3251-4b20-93a1-d15c73cf8793,DISK], DatanodeInfoWithStorage[127.0.0.1:39787,DS-6f12b727-61c7-4d18-80a3-f7b1bf3715c4,DISK], DatanodeInfoWithStorage[127.0.0.1:46068,DS-10b6902c-8df2-4c2d-b58b-ecd3dfff76ac,DISK], DatanodeInfoWithStorage[127.0.0.1:35064,DS-6a48282d-fd5f-49f5-9ed8-0b685e555b8d,DISK], DatanodeInfoWithStorage[127.0.0.1:41494,DS-fbfe3c34-8635-4386-ad79-c3847f27fb7d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-785137492-172.17.0.13-1595961420178:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37189,DS-3d02d4fe-7f00-4e5a-82f2-e451aa1a9ebd,DISK], DatanodeInfoWithStorage[127.0.0.1:42570,DS-f56e6e67-07be-46ac-85f6-3de7e2deaf11,DISK], DatanodeInfoWithStorage[127.0.0.1:40393,DS-f5145515-d75a-4c56-990a-1c19d0f198ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41279,DS-89e4b801-cbfa-4662-a7e9-c7883f765c71,DISK], DatanodeInfoWithStorage[127.0.0.1:36346,DS-64627b85-b142-477d-b859-d3c0fc5c138c,DISK], DatanodeInfoWithStorage[127.0.0.1:39198,DS-f49210c1-393f-4ff9-bce5-252182545f6f,DISK], DatanodeInfoWithStorage[127.0.0.1:44128,DS-35101702-5c47-4636-a094-6f5cf0420e29,DISK], DatanodeInfoWithStorage[127.0.0.1:43562,DS-12d7a6ff-aaae-40a8-b3cc-7069bdbbfd69,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-785137492-172.17.0.13-1595961420178:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37189,DS-3d02d4fe-7f00-4e5a-82f2-e451aa1a9ebd,DISK], DatanodeInfoWithStorage[127.0.0.1:42570,DS-f56e6e67-07be-46ac-85f6-3de7e2deaf11,DISK], DatanodeInfoWithStorage[127.0.0.1:40393,DS-f5145515-d75a-4c56-990a-1c19d0f198ae,DISK], DatanodeInfoWithStorage[127.0.0.1:41279,DS-89e4b801-cbfa-4662-a7e9-c7883f765c71,DISK], DatanodeInfoWithStorage[127.0.0.1:36346,DS-64627b85-b142-477d-b859-d3c0fc5c138c,DISK], DatanodeInfoWithStorage[127.0.0.1:39198,DS-f49210c1-393f-4ff9-bce5-252182545f6f,DISK], DatanodeInfoWithStorage[127.0.0.1:44128,DS-35101702-5c47-4636-a094-6f5cf0420e29,DISK], DatanodeInfoWithStorage[127.0.0.1:43562,DS-12d7a6ff-aaae-40a8-b3cc-7069bdbbfd69,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1369843452-172.17.0.13-1595961635701:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36898,DS-a5ae1791-174d-4ab1-b552-20c1bfc0037e,DISK], DatanodeInfoWithStorage[127.0.0.1:34292,DS-c4167c97-59c3-43a0-9587-883427c5421c,DISK], DatanodeInfoWithStorage[127.0.0.1:44928,DS-14773688-3964-4c0f-afeb-007de8c3608a,DISK], DatanodeInfoWithStorage[127.0.0.1:43393,DS-f5dd26bf-b9d3-424b-b3a0-53523df90fcb,DISK], DatanodeInfoWithStorage[127.0.0.1:35292,DS-08e31c24-9f96-4a3e-bc23-9102f94bf4c2,DISK], DatanodeInfoWithStorage[127.0.0.1:35168,DS-fb3b3aaa-b75b-4e28-8a24-bb0d94c39656,DISK], DatanodeInfoWithStorage[127.0.0.1:35922,DS-2ca83822-f8ea-4369-8ea5-ed1261c8f866,DISK], DatanodeInfoWithStorage[127.0.0.1:43934,DS-d72e8aa7-9d18-4de5-9c2d-6b85023b9ea3,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1369843452-172.17.0.13-1595961635701:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36898,DS-a5ae1791-174d-4ab1-b552-20c1bfc0037e,DISK], DatanodeInfoWithStorage[127.0.0.1:34292,DS-c4167c97-59c3-43a0-9587-883427c5421c,DISK], DatanodeInfoWithStorage[127.0.0.1:44928,DS-14773688-3964-4c0f-afeb-007de8c3608a,DISK], DatanodeInfoWithStorage[127.0.0.1:43393,DS-f5dd26bf-b9d3-424b-b3a0-53523df90fcb,DISK], DatanodeInfoWithStorage[127.0.0.1:35292,DS-08e31c24-9f96-4a3e-bc23-9102f94bf4c2,DISK], DatanodeInfoWithStorage[127.0.0.1:35168,DS-fb3b3aaa-b75b-4e28-8a24-bb0d94c39656,DISK], DatanodeInfoWithStorage[127.0.0.1:35922,DS-2ca83822-f8ea-4369-8ea5-ed1261c8f866,DISK], DatanodeInfoWithStorage[127.0.0.1:43934,DS-d72e8aa7-9d18-4de5-9c2d-6b85023b9ea3,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1003101744-172.17.0.13-1595961805068:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43089,DS-970e6689-a593-49ac-b871-c4b8d0f54663,DISK], DatanodeInfoWithStorage[127.0.0.1:37524,DS-dd034a6b-6732-49ff-9e65-989f6b009d15,DISK], DatanodeInfoWithStorage[127.0.0.1:40514,DS-f8297b75-e4c7-4de9-b97a-ccabcddf0709,DISK], DatanodeInfoWithStorage[127.0.0.1:37969,DS-b7c2d0e7-a3e6-4190-934d-8902cd217e4e,DISK], DatanodeInfoWithStorage[127.0.0.1:43489,DS-3fc4f6c3-72ba-49de-b541-f4bf9d4a39da,DISK], DatanodeInfoWithStorage[127.0.0.1:37166,DS-cbb17650-dc2a-470c-b9c6-384662001e79,DISK], DatanodeInfoWithStorage[127.0.0.1:37577,DS-3ae6fac8-484e-4a78-8d63-4cb28b165a0f,DISK], DatanodeInfoWithStorage[127.0.0.1:40079,DS-f5559283-d9f0-481a-9344-ab66f15b0f41,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1003101744-172.17.0.13-1595961805068:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43089,DS-970e6689-a593-49ac-b871-c4b8d0f54663,DISK], DatanodeInfoWithStorage[127.0.0.1:37524,DS-dd034a6b-6732-49ff-9e65-989f6b009d15,DISK], DatanodeInfoWithStorage[127.0.0.1:40514,DS-f8297b75-e4c7-4de9-b97a-ccabcddf0709,DISK], DatanodeInfoWithStorage[127.0.0.1:37969,DS-b7c2d0e7-a3e6-4190-934d-8902cd217e4e,DISK], DatanodeInfoWithStorage[127.0.0.1:43489,DS-3fc4f6c3-72ba-49de-b541-f4bf9d4a39da,DISK], DatanodeInfoWithStorage[127.0.0.1:37166,DS-cbb17650-dc2a-470c-b9c6-384662001e79,DISK], DatanodeInfoWithStorage[127.0.0.1:37577,DS-3ae6fac8-484e-4a78-8d63-4cb28b165a0f,DISK], DatanodeInfoWithStorage[127.0.0.1:40079,DS-f5559283-d9f0-481a-9344-ab66f15b0f41,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1745812921-172.17.0.13-1595961876602:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39996,DS-e1801aea-6228-4360-bb1e-b75dd1f33072,DISK], DatanodeInfoWithStorage[127.0.0.1:41276,DS-e7ea070e-e8b3-4019-bc66-ece78bfac3a7,DISK], DatanodeInfoWithStorage[127.0.0.1:34478,DS-7d02ee0a-9c19-443c-9901-ff8bef511dc6,DISK], DatanodeInfoWithStorage[127.0.0.1:33450,DS-ef2fad61-e618-4a79-bfec-1e5dce1fc616,DISK], DatanodeInfoWithStorage[127.0.0.1:45723,DS-82e673f1-df1a-4eda-818a-c3b5ab56f19a,DISK], DatanodeInfoWithStorage[127.0.0.1:38594,DS-5d3066e6-d1c7-48d6-a5c3-e359c1d65b52,DISK], DatanodeInfoWithStorage[127.0.0.1:37786,DS-8ca53ae0-bd4d-4cd6-9e64-76d19977a072,DISK], DatanodeInfoWithStorage[127.0.0.1:40866,DS-1b0c3a95-3a51-4811-ba31-c5d4b5d40458,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1745812921-172.17.0.13-1595961876602:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39996,DS-e1801aea-6228-4360-bb1e-b75dd1f33072,DISK], DatanodeInfoWithStorage[127.0.0.1:41276,DS-e7ea070e-e8b3-4019-bc66-ece78bfac3a7,DISK], DatanodeInfoWithStorage[127.0.0.1:34478,DS-7d02ee0a-9c19-443c-9901-ff8bef511dc6,DISK], DatanodeInfoWithStorage[127.0.0.1:33450,DS-ef2fad61-e618-4a79-bfec-1e5dce1fc616,DISK], DatanodeInfoWithStorage[127.0.0.1:45723,DS-82e673f1-df1a-4eda-818a-c3b5ab56f19a,DISK], DatanodeInfoWithStorage[127.0.0.1:38594,DS-5d3066e6-d1c7-48d6-a5c3-e359c1d65b52,DISK], DatanodeInfoWithStorage[127.0.0.1:37786,DS-8ca53ae0-bd4d-4cd6-9e64-76d19977a072,DISK], DatanodeInfoWithStorage[127.0.0.1:40866,DS-1b0c3a95-3a51-4811-ba31-c5d4b5d40458,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2072173751-172.17.0.13-1595962196545:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43890,DS-a7fb9e82-2de9-4f43-a300-8c5fc5b29866,DISK], DatanodeInfoWithStorage[127.0.0.1:45495,DS-e7447cc3-4a55-45bf-9296-b0c28fa346cd,DISK], DatanodeInfoWithStorage[127.0.0.1:38679,DS-1c181e61-f345-4804-8a18-9dd74c4336ed,DISK], DatanodeInfoWithStorage[127.0.0.1:43599,DS-a6988848-ece2-494e-a90a-3a590f46dc4c,DISK], DatanodeInfoWithStorage[127.0.0.1:46393,DS-8b61b3b2-e8d6-4358-8643-2b2117035ab2,DISK], DatanodeInfoWithStorage[127.0.0.1:40516,DS-b5b2cda2-02a9-4f8b-866b-3036d1d34fb2,DISK], DatanodeInfoWithStorage[127.0.0.1:37240,DS-81e4c4f0-d258-4c2d-9c4e-6e6c428e21ca,DISK], DatanodeInfoWithStorage[127.0.0.1:46349,DS-200f8b57-d7d6-435a-ae94-fbe7ed6bcb29,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2072173751-172.17.0.13-1595962196545:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43890,DS-a7fb9e82-2de9-4f43-a300-8c5fc5b29866,DISK], DatanodeInfoWithStorage[127.0.0.1:45495,DS-e7447cc3-4a55-45bf-9296-b0c28fa346cd,DISK], DatanodeInfoWithStorage[127.0.0.1:38679,DS-1c181e61-f345-4804-8a18-9dd74c4336ed,DISK], DatanodeInfoWithStorage[127.0.0.1:43599,DS-a6988848-ece2-494e-a90a-3a590f46dc4c,DISK], DatanodeInfoWithStorage[127.0.0.1:46393,DS-8b61b3b2-e8d6-4358-8643-2b2117035ab2,DISK], DatanodeInfoWithStorage[127.0.0.1:40516,DS-b5b2cda2-02a9-4f8b-866b-3036d1d34fb2,DISK], DatanodeInfoWithStorage[127.0.0.1:37240,DS-81e4c4f0-d258-4c2d-9c4e-6e6c428e21ca,DISK], DatanodeInfoWithStorage[127.0.0.1:46349,DS-200f8b57-d7d6-435a-ae94-fbe7ed6bcb29,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1380644099-172.17.0.13-1595962687981:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39005,DS-9c50de5e-b32f-4f12-bd35-b419b740e81a,DISK], DatanodeInfoWithStorage[127.0.0.1:46485,DS-cfa2aef9-4254-4b10-bc85-864739a4c826,DISK], DatanodeInfoWithStorage[127.0.0.1:45988,DS-d8fc988a-a267-43b4-b553-9ae1cc39a78b,DISK], DatanodeInfoWithStorage[127.0.0.1:34270,DS-e3988382-b1af-4d9d-9967-1f3728a0fda0,DISK], DatanodeInfoWithStorage[127.0.0.1:40862,DS-9e0547eb-e450-48c5-b241-0d88aaa36242,DISK], DatanodeInfoWithStorage[127.0.0.1:33902,DS-554bfe4e-7f36-4957-a8d7-c713029ddd28,DISK], DatanodeInfoWithStorage[127.0.0.1:44011,DS-a0ed33e4-50bc-4dc5-bd0f-eb1daf0219f4,DISK], DatanodeInfoWithStorage[127.0.0.1:39458,DS-cda6396d-be9f-47ed-bb4a-9470571469fc,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1380644099-172.17.0.13-1595962687981:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39005,DS-9c50de5e-b32f-4f12-bd35-b419b740e81a,DISK], DatanodeInfoWithStorage[127.0.0.1:46485,DS-cfa2aef9-4254-4b10-bc85-864739a4c826,DISK], DatanodeInfoWithStorage[127.0.0.1:45988,DS-d8fc988a-a267-43b4-b553-9ae1cc39a78b,DISK], DatanodeInfoWithStorage[127.0.0.1:34270,DS-e3988382-b1af-4d9d-9967-1f3728a0fda0,DISK], DatanodeInfoWithStorage[127.0.0.1:40862,DS-9e0547eb-e450-48c5-b241-0d88aaa36242,DISK], DatanodeInfoWithStorage[127.0.0.1:33902,DS-554bfe4e-7f36-4957-a8d7-c713029ddd28,DISK], DatanodeInfoWithStorage[127.0.0.1:44011,DS-a0ed33e4-50bc-4dc5-bd0f-eb1daf0219f4,DISK], DatanodeInfoWithStorage[127.0.0.1:39458,DS-cda6396d-be9f-47ed-bb4a-9470571469fc,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-394829988-172.17.0.13-1595962753959:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43520,DS-5a260bde-ade6-433e-8bbd-9e4497783972,DISK], DatanodeInfoWithStorage[127.0.0.1:41933,DS-dc2dea98-6cc6-495c-be11-21097d7fb9a3,DISK], DatanodeInfoWithStorage[127.0.0.1:34649,DS-912c2615-a69a-4d18-b8d5-69c40485c00e,DISK], DatanodeInfoWithStorage[127.0.0.1:45538,DS-dbe8801c-dbac-4734-ab13-3421eb2583c1,DISK], DatanodeInfoWithStorage[127.0.0.1:34891,DS-7db568a4-59fd-43f0-ac74-51842da95aed,DISK], DatanodeInfoWithStorage[127.0.0.1:43852,DS-80996d29-92bd-4413-95f3-091ae32041f3,DISK], DatanodeInfoWithStorage[127.0.0.1:44024,DS-0c2e6b75-1249-4559-a35a-8ac9239ac766,DISK], DatanodeInfoWithStorage[127.0.0.1:45650,DS-048726e4-c3b5-4e4d-b31d-eeb2bb957ab8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-394829988-172.17.0.13-1595962753959:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43520,DS-5a260bde-ade6-433e-8bbd-9e4497783972,DISK], DatanodeInfoWithStorage[127.0.0.1:41933,DS-dc2dea98-6cc6-495c-be11-21097d7fb9a3,DISK], DatanodeInfoWithStorage[127.0.0.1:34649,DS-912c2615-a69a-4d18-b8d5-69c40485c00e,DISK], DatanodeInfoWithStorage[127.0.0.1:45538,DS-dbe8801c-dbac-4734-ab13-3421eb2583c1,DISK], DatanodeInfoWithStorage[127.0.0.1:34891,DS-7db568a4-59fd-43f0-ac74-51842da95aed,DISK], DatanodeInfoWithStorage[127.0.0.1:43852,DS-80996d29-92bd-4413-95f3-091ae32041f3,DISK], DatanodeInfoWithStorage[127.0.0.1:44024,DS-0c2e6b75-1249-4559-a35a-8ac9239ac766,DISK], DatanodeInfoWithStorage[127.0.0.1:45650,DS-048726e4-c3b5-4e4d-b31d-eeb2bb957ab8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2119122885-172.17.0.13-1595962785736:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38371,DS-91d1d67e-2174-4174-9183-416c2f782064,DISK], DatanodeInfoWithStorage[127.0.0.1:44031,DS-857de226-5a8b-4f20-8fdb-443f19c19603,DISK], DatanodeInfoWithStorage[127.0.0.1:37814,DS-d9adde20-2064-46c6-b106-0e6228792c70,DISK], DatanodeInfoWithStorage[127.0.0.1:43227,DS-8011b5a9-78f1-4672-8369-87a7fb8dcfa2,DISK], DatanodeInfoWithStorage[127.0.0.1:38024,DS-8ffba25d-3768-4e34-821e-06b7ab038db7,DISK], DatanodeInfoWithStorage[127.0.0.1:39535,DS-e4c6f966-0a3f-4d20-96b5-234fe19bed99,DISK], DatanodeInfoWithStorage[127.0.0.1:37491,DS-4ea782d2-64b7-4fb5-8bc3-4bba9d7615fb,DISK], DatanodeInfoWithStorage[127.0.0.1:45593,DS-fe2a1556-10bd-4ab3-b554-7f1f0a8cc950,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2119122885-172.17.0.13-1595962785736:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38371,DS-91d1d67e-2174-4174-9183-416c2f782064,DISK], DatanodeInfoWithStorage[127.0.0.1:44031,DS-857de226-5a8b-4f20-8fdb-443f19c19603,DISK], DatanodeInfoWithStorage[127.0.0.1:37814,DS-d9adde20-2064-46c6-b106-0e6228792c70,DISK], DatanodeInfoWithStorage[127.0.0.1:43227,DS-8011b5a9-78f1-4672-8369-87a7fb8dcfa2,DISK], DatanodeInfoWithStorage[127.0.0.1:38024,DS-8ffba25d-3768-4e34-821e-06b7ab038db7,DISK], DatanodeInfoWithStorage[127.0.0.1:39535,DS-e4c6f966-0a3f-4d20-96b5-234fe19bed99,DISK], DatanodeInfoWithStorage[127.0.0.1:37491,DS-4ea782d2-64b7-4fb5-8bc3-4bba9d7615fb,DISK], DatanodeInfoWithStorage[127.0.0.1:45593,DS-fe2a1556-10bd-4ab3-b554-7f1f0a8cc950,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-598283604-172.17.0.13-1595962846334:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41528,DS-08e69b4d-a295-42bb-a176-c182196b1279,DISK], DatanodeInfoWithStorage[127.0.0.1:37918,DS-275d2465-889a-4fcc-9649-cbf8a56a2d2c,DISK], DatanodeInfoWithStorage[127.0.0.1:38938,DS-d034043e-c434-4ecb-bd51-991df0d74263,DISK], DatanodeInfoWithStorage[127.0.0.1:38883,DS-9c5a62d8-8571-4fc7-a78e-ef39716bac1d,DISK], DatanodeInfoWithStorage[127.0.0.1:33805,DS-9a1c9f7f-5d7b-4c13-bc57-d2f3ba88d0aa,DISK], DatanodeInfoWithStorage[127.0.0.1:41248,DS-6f1ae3a6-f51e-4c1f-a242-bdb9da8ad878,DISK], DatanodeInfoWithStorage[127.0.0.1:35687,DS-66e3e003-f15b-416f-abca-2c60b2c3b570,DISK], DatanodeInfoWithStorage[127.0.0.1:42562,DS-9fd667a2-769c-4d29-9664-4e047d2741a8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-598283604-172.17.0.13-1595962846334:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41528,DS-08e69b4d-a295-42bb-a176-c182196b1279,DISK], DatanodeInfoWithStorage[127.0.0.1:37918,DS-275d2465-889a-4fcc-9649-cbf8a56a2d2c,DISK], DatanodeInfoWithStorage[127.0.0.1:38938,DS-d034043e-c434-4ecb-bd51-991df0d74263,DISK], DatanodeInfoWithStorage[127.0.0.1:38883,DS-9c5a62d8-8571-4fc7-a78e-ef39716bac1d,DISK], DatanodeInfoWithStorage[127.0.0.1:33805,DS-9a1c9f7f-5d7b-4c13-bc57-d2f3ba88d0aa,DISK], DatanodeInfoWithStorage[127.0.0.1:41248,DS-6f1ae3a6-f51e-4c1f-a242-bdb9da8ad878,DISK], DatanodeInfoWithStorage[127.0.0.1:35687,DS-66e3e003-f15b-416f-abca-2c60b2c3b570,DISK], DatanodeInfoWithStorage[127.0.0.1:42562,DS-9fd667a2-769c-4d29-9664-4e047d2741a8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1738278465-172.17.0.13-1595963055984:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33327,DS-6543df60-be21-46f3-83b2-10b7dc5fcc1d,DISK], DatanodeInfoWithStorage[127.0.0.1:36140,DS-d2fcc445-dd25-49f1-b18c-70d2be426b8b,DISK], DatanodeInfoWithStorage[127.0.0.1:34823,DS-4fccd655-0a90-46d9-88de-c41ec8af6c3f,DISK], DatanodeInfoWithStorage[127.0.0.1:45887,DS-d0bcb72f-0d58-4322-b92d-757e6c185e3d,DISK], DatanodeInfoWithStorage[127.0.0.1:35744,DS-cf2cbd80-6b11-49cd-bc37-fe75af3824e9,DISK], DatanodeInfoWithStorage[127.0.0.1:45603,DS-721d0954-1655-467a-ac39-b5333fdd8e0d,DISK], DatanodeInfoWithStorage[127.0.0.1:34864,DS-4cb50790-1390-4da3-9f8d-1d5a7f942e4e,DISK], DatanodeInfoWithStorage[127.0.0.1:40705,DS-d9143ad3-a9ea-4efa-8431-4e00b45c5fa4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1738278465-172.17.0.13-1595963055984:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33327,DS-6543df60-be21-46f3-83b2-10b7dc5fcc1d,DISK], DatanodeInfoWithStorage[127.0.0.1:36140,DS-d2fcc445-dd25-49f1-b18c-70d2be426b8b,DISK], DatanodeInfoWithStorage[127.0.0.1:34823,DS-4fccd655-0a90-46d9-88de-c41ec8af6c3f,DISK], DatanodeInfoWithStorage[127.0.0.1:45887,DS-d0bcb72f-0d58-4322-b92d-757e6c185e3d,DISK], DatanodeInfoWithStorage[127.0.0.1:35744,DS-cf2cbd80-6b11-49cd-bc37-fe75af3824e9,DISK], DatanodeInfoWithStorage[127.0.0.1:45603,DS-721d0954-1655-467a-ac39-b5333fdd8e0d,DISK], DatanodeInfoWithStorage[127.0.0.1:34864,DS-4cb50790-1390-4da3-9f8d-1d5a7f942e4e,DISK], DatanodeInfoWithStorage[127.0.0.1:40705,DS-d9143ad3-a9ea-4efa-8431-4e00b45c5fa4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1284183432-172.17.0.13-1595963658084:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43886,DS-8e136134-6d45-432c-8e94-135adae9d2e6,DISK], DatanodeInfoWithStorage[127.0.0.1:43492,DS-16034740-bc41-489d-b732-145fec6190e9,DISK], DatanodeInfoWithStorage[127.0.0.1:44206,DS-c4d56dd3-0fd0-4b9d-b7ba-b607fb14fe26,DISK], DatanodeInfoWithStorage[127.0.0.1:34937,DS-e8effdaa-7f83-4586-aba0-63eacacd67ec,DISK], DatanodeInfoWithStorage[127.0.0.1:34895,DS-90da97b1-3d24-4a94-b8c8-d3e145b12c41,DISK], DatanodeInfoWithStorage[127.0.0.1:32917,DS-8cfa2c17-2ca7-4c51-88de-34069a2e1408,DISK], DatanodeInfoWithStorage[127.0.0.1:35966,DS-d6e36bbf-da04-43d5-a1e0-050c5aed64c7,DISK], DatanodeInfoWithStorage[127.0.0.1:38659,DS-744d90b9-917f-4773-bf3c-03c5475cfb9a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1284183432-172.17.0.13-1595963658084:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43886,DS-8e136134-6d45-432c-8e94-135adae9d2e6,DISK], DatanodeInfoWithStorage[127.0.0.1:43492,DS-16034740-bc41-489d-b732-145fec6190e9,DISK], DatanodeInfoWithStorage[127.0.0.1:44206,DS-c4d56dd3-0fd0-4b9d-b7ba-b607fb14fe26,DISK], DatanodeInfoWithStorage[127.0.0.1:34937,DS-e8effdaa-7f83-4586-aba0-63eacacd67ec,DISK], DatanodeInfoWithStorage[127.0.0.1:34895,DS-90da97b1-3d24-4a94-b8c8-d3e145b12c41,DISK], DatanodeInfoWithStorage[127.0.0.1:32917,DS-8cfa2c17-2ca7-4c51-88de-34069a2e1408,DISK], DatanodeInfoWithStorage[127.0.0.1:35966,DS-d6e36bbf-da04-43d5-a1e0-050c5aed64c7,DISK], DatanodeInfoWithStorage[127.0.0.1:38659,DS-744d90b9-917f-4773-bf3c-03c5475cfb9a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1422163087-172.17.0.13-1595964230287:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43323,DS-19671158-a24b-42fe-806e-bb8e9a48d8dd,DISK], DatanodeInfoWithStorage[127.0.0.1:41764,DS-3b314051-545e-46aa-a0b6-68bb64620fb8,DISK], DatanodeInfoWithStorage[127.0.0.1:42926,DS-894dfaaa-fa05-4040-a5d8-3f1ca225379b,DISK], DatanodeInfoWithStorage[127.0.0.1:44353,DS-510a8d68-9ce9-4a62-96a4-a89d7894ae2e,DISK], DatanodeInfoWithStorage[127.0.0.1:44008,DS-fb581349-7b2d-4f05-9c01-b6e1d7ce494d,DISK], DatanodeInfoWithStorage[127.0.0.1:43662,DS-993610e0-96d2-4125-ba0d-b4f4894e719e,DISK], DatanodeInfoWithStorage[127.0.0.1:36212,DS-4904fe67-2ec1-47fe-a02d-499d875dc48d,DISK], DatanodeInfoWithStorage[127.0.0.1:37501,DS-a0266073-fe1e-442e-b946-91e3dda3ffeb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1422163087-172.17.0.13-1595964230287:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43323,DS-19671158-a24b-42fe-806e-bb8e9a48d8dd,DISK], DatanodeInfoWithStorage[127.0.0.1:41764,DS-3b314051-545e-46aa-a0b6-68bb64620fb8,DISK], DatanodeInfoWithStorage[127.0.0.1:42926,DS-894dfaaa-fa05-4040-a5d8-3f1ca225379b,DISK], DatanodeInfoWithStorage[127.0.0.1:44353,DS-510a8d68-9ce9-4a62-96a4-a89d7894ae2e,DISK], DatanodeInfoWithStorage[127.0.0.1:44008,DS-fb581349-7b2d-4f05-9c01-b6e1d7ce494d,DISK], DatanodeInfoWithStorage[127.0.0.1:43662,DS-993610e0-96d2-4125-ba0d-b4f4894e719e,DISK], DatanodeInfoWithStorage[127.0.0.1:36212,DS-4904fe67-2ec1-47fe-a02d-499d875dc48d,DISK], DatanodeInfoWithStorage[127.0.0.1:37501,DS-a0266073-fe1e-442e-b946-91e3dda3ffeb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-643641894-172.17.0.13-1595964357086:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34290,DS-b04293e5-30d8-4f79-8071-825bca63994d,DISK], DatanodeInfoWithStorage[127.0.0.1:43755,DS-cc2777a5-af84-4b82-8a18-47b345a6b912,DISK], DatanodeInfoWithStorage[127.0.0.1:37450,DS-d0fd4612-7d86-40bd-9fab-a61b16021af7,DISK], DatanodeInfoWithStorage[127.0.0.1:41362,DS-01546264-e570-4a37-a977-7de1c368b3c5,DISK], DatanodeInfoWithStorage[127.0.0.1:41266,DS-138b41f5-e94b-4500-b1f4-1b8a6de7009f,DISK], DatanodeInfoWithStorage[127.0.0.1:33373,DS-1d69de4c-43e0-4f84-9fdb-2e0a7f316272,DISK], DatanodeInfoWithStorage[127.0.0.1:36787,DS-ee5e27b9-782a-468f-8b65-17ee12ce3e8b,DISK], DatanodeInfoWithStorage[127.0.0.1:36704,DS-59e39066-08b0-4560-b1ff-a6722dfcfc5a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-643641894-172.17.0.13-1595964357086:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34290,DS-b04293e5-30d8-4f79-8071-825bca63994d,DISK], DatanodeInfoWithStorage[127.0.0.1:43755,DS-cc2777a5-af84-4b82-8a18-47b345a6b912,DISK], DatanodeInfoWithStorage[127.0.0.1:37450,DS-d0fd4612-7d86-40bd-9fab-a61b16021af7,DISK], DatanodeInfoWithStorage[127.0.0.1:41362,DS-01546264-e570-4a37-a977-7de1c368b3c5,DISK], DatanodeInfoWithStorage[127.0.0.1:41266,DS-138b41f5-e94b-4500-b1f4-1b8a6de7009f,DISK], DatanodeInfoWithStorage[127.0.0.1:33373,DS-1d69de4c-43e0-4f84-9fdb-2e0a7f316272,DISK], DatanodeInfoWithStorage[127.0.0.1:36787,DS-ee5e27b9-782a-468f-8b65-17ee12ce3e8b,DISK], DatanodeInfoWithStorage[127.0.0.1:36704,DS-59e39066-08b0-4560-b1ff-a6722dfcfc5a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1056531919-172.17.0.13-1595964556053:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46257,DS-a2324321-2197-4190-812b-e97708813b7a,DISK], DatanodeInfoWithStorage[127.0.0.1:40619,DS-9af94192-98db-4fc3-8208-9e6fac2f9f14,DISK], DatanodeInfoWithStorage[127.0.0.1:33053,DS-fc3975cd-24e6-49da-a87f-9bc1e8fa0df2,DISK], DatanodeInfoWithStorage[127.0.0.1:42209,DS-22d297a0-f83a-42cf-b164-af6d4019f423,DISK], DatanodeInfoWithStorage[127.0.0.1:40871,DS-76a5db31-81db-46a7-a5c9-7a44fc376083,DISK], DatanodeInfoWithStorage[127.0.0.1:39722,DS-2b8576f8-4c04-4428-ace8-61f177176b5e,DISK], DatanodeInfoWithStorage[127.0.0.1:43064,DS-f9a5c028-d10a-440e-8785-7c523490f56d,DISK], DatanodeInfoWithStorage[127.0.0.1:34129,DS-798aa691-90d2-4037-a438-9db8f4007f5f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1056531919-172.17.0.13-1595964556053:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46257,DS-a2324321-2197-4190-812b-e97708813b7a,DISK], DatanodeInfoWithStorage[127.0.0.1:40619,DS-9af94192-98db-4fc3-8208-9e6fac2f9f14,DISK], DatanodeInfoWithStorage[127.0.0.1:33053,DS-fc3975cd-24e6-49da-a87f-9bc1e8fa0df2,DISK], DatanodeInfoWithStorage[127.0.0.1:42209,DS-22d297a0-f83a-42cf-b164-af6d4019f423,DISK], DatanodeInfoWithStorage[127.0.0.1:40871,DS-76a5db31-81db-46a7-a5c9-7a44fc376083,DISK], DatanodeInfoWithStorage[127.0.0.1:39722,DS-2b8576f8-4c04-4428-ace8-61f177176b5e,DISK], DatanodeInfoWithStorage[127.0.0.1:43064,DS-f9a5c028-d10a-440e-8785-7c523490f56d,DISK], DatanodeInfoWithStorage[127.0.0.1:34129,DS-798aa691-90d2-4037-a438-9db8f4007f5f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1130029696-172.17.0.13-1595964875072:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42267,DS-0c660a22-dd5b-49ce-b5d6-72faa1c74483,DISK], DatanodeInfoWithStorage[127.0.0.1:39531,DS-277c2ec3-b873-48fb-b3fa-8b3a674ceaa6,DISK], DatanodeInfoWithStorage[127.0.0.1:45876,DS-4dc244aa-b3c5-4276-a4f4-5be00be9d818,DISK], DatanodeInfoWithStorage[127.0.0.1:36197,DS-fcb82325-8049-4cff-bd68-25e8ea039b7b,DISK], DatanodeInfoWithStorage[127.0.0.1:43592,DS-2cf2a284-cb64-4243-84d4-901588abad6e,DISK], DatanodeInfoWithStorage[127.0.0.1:41477,DS-0b6f6e2b-42ae-4c8b-8658-63e279e9db5c,DISK], DatanodeInfoWithStorage[127.0.0.1:43934,DS-897905d1-99d5-44ca-9395-04b0d409c2b6,DISK], DatanodeInfoWithStorage[127.0.0.1:36792,DS-694a4406-c174-436a-b8e8-b87d97bea12d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1130029696-172.17.0.13-1595964875072:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42267,DS-0c660a22-dd5b-49ce-b5d6-72faa1c74483,DISK], DatanodeInfoWithStorage[127.0.0.1:39531,DS-277c2ec3-b873-48fb-b3fa-8b3a674ceaa6,DISK], DatanodeInfoWithStorage[127.0.0.1:45876,DS-4dc244aa-b3c5-4276-a4f4-5be00be9d818,DISK], DatanodeInfoWithStorage[127.0.0.1:36197,DS-fcb82325-8049-4cff-bd68-25e8ea039b7b,DISK], DatanodeInfoWithStorage[127.0.0.1:43592,DS-2cf2a284-cb64-4243-84d4-901588abad6e,DISK], DatanodeInfoWithStorage[127.0.0.1:41477,DS-0b6f6e2b-42ae-4c8b-8658-63e279e9db5c,DISK], DatanodeInfoWithStorage[127.0.0.1:43934,DS-897905d1-99d5-44ca-9395-04b0d409c2b6,DISK], DatanodeInfoWithStorage[127.0.0.1:36792,DS-694a4406-c174-436a-b8e8-b87d97bea12d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1572678234-172.17.0.13-1595964980542:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34794,DS-9241011b-33f2-448f-b31d-3a8bd0b2277c,DISK], DatanodeInfoWithStorage[127.0.0.1:39546,DS-71d35103-b3fe-4756-8204-f1839f7b8bff,DISK], DatanodeInfoWithStorage[127.0.0.1:39881,DS-4c0b483f-fb92-4cbd-98fa-a04c21e4a4b1,DISK], DatanodeInfoWithStorage[127.0.0.1:45021,DS-73bcd59c-1b4f-432b-9736-d38ce1f9b198,DISK], DatanodeInfoWithStorage[127.0.0.1:44536,DS-9035188a-2e29-46a4-8d25-e1b20eb14ce1,DISK], DatanodeInfoWithStorage[127.0.0.1:39473,DS-6bb64584-711b-475b-909c-b7aee6b08b33,DISK], DatanodeInfoWithStorage[127.0.0.1:41331,DS-2bf4cb9d-a6a8-4def-8bb5-f31b59cc6fa0,DISK], DatanodeInfoWithStorage[127.0.0.1:44642,DS-04d21c23-d173-4fb0-beb9-e7d3df432404,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1572678234-172.17.0.13-1595964980542:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34794,DS-9241011b-33f2-448f-b31d-3a8bd0b2277c,DISK], DatanodeInfoWithStorage[127.0.0.1:39546,DS-71d35103-b3fe-4756-8204-f1839f7b8bff,DISK], DatanodeInfoWithStorage[127.0.0.1:39881,DS-4c0b483f-fb92-4cbd-98fa-a04c21e4a4b1,DISK], DatanodeInfoWithStorage[127.0.0.1:45021,DS-73bcd59c-1b4f-432b-9736-d38ce1f9b198,DISK], DatanodeInfoWithStorage[127.0.0.1:44536,DS-9035188a-2e29-46a4-8d25-e1b20eb14ce1,DISK], DatanodeInfoWithStorage[127.0.0.1:39473,DS-6bb64584-711b-475b-909c-b7aee6b08b33,DISK], DatanodeInfoWithStorage[127.0.0.1:41331,DS-2bf4cb9d-a6a8-4def-8bb5-f31b59cc6fa0,DISK], DatanodeInfoWithStorage[127.0.0.1:44642,DS-04d21c23-d173-4fb0-beb9-e7d3df432404,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.parallel.volumes.load.threads.num
component: hdfs:DataNode
v1: 1000
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-555244419-172.17.0.13-1595965050336:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38314,DS-91799a2c-847b-498e-bb04-f89d69090ce5,DISK], DatanodeInfoWithStorage[127.0.0.1:43423,DS-64ca7ec4-c5ce-4221-8544-ba1c4002346e,DISK], DatanodeInfoWithStorage[127.0.0.1:40512,DS-fe5cfda9-e196-4c04-bfd6-0d2e7651b077,DISK], DatanodeInfoWithStorage[127.0.0.1:43755,DS-4e00d638-6189-4f82-be0d-54544d19e78d,DISK], DatanodeInfoWithStorage[127.0.0.1:42489,DS-9ee360f5-13c8-4f42-add7-56b7549c7507,DISK], DatanodeInfoWithStorage[127.0.0.1:37034,DS-1d841f46-8f99-4530-8ee0-891ff5505787,DISK], DatanodeInfoWithStorage[127.0.0.1:41848,DS-1f1b9cca-c59e-46d3-b5a3-b9ef06656751,DISK], DatanodeInfoWithStorage[127.0.0.1:35556,DS-d1e6c68c-fba7-4628-a83f-ba0f8025cef4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-555244419-172.17.0.13-1595965050336:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38314,DS-91799a2c-847b-498e-bb04-f89d69090ce5,DISK], DatanodeInfoWithStorage[127.0.0.1:43423,DS-64ca7ec4-c5ce-4221-8544-ba1c4002346e,DISK], DatanodeInfoWithStorage[127.0.0.1:40512,DS-fe5cfda9-e196-4c04-bfd6-0d2e7651b077,DISK], DatanodeInfoWithStorage[127.0.0.1:43755,DS-4e00d638-6189-4f82-be0d-54544d19e78d,DISK], DatanodeInfoWithStorage[127.0.0.1:42489,DS-9ee360f5-13c8-4f42-add7-56b7549c7507,DISK], DatanodeInfoWithStorage[127.0.0.1:37034,DS-1d841f46-8f99-4530-8ee0-891ff5505787,DISK], DatanodeInfoWithStorage[127.0.0.1:41848,DS-1f1b9cca-c59e-46d3-b5a3-b9ef06656751,DISK], DatanodeInfoWithStorage[127.0.0.1:35556,DS-d1e6c68c-fba7-4628-a83f-ba0f8025cef4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 7 out of 50
v1v1v2v2 failed with probability 15 out of 50
result: false positive !!!
Total execution time in seconds : 5227

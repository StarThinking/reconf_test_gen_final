reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1496631927-172.17.0.18-1595611385226:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35092,DS-5ed95eef-9386-4b72-9b79-cfee05cd64a5,DISK], DatanodeInfoWithStorage[127.0.0.1:35734,DS-19e769c4-7f01-4a00-a3c3-d68e1dfa1e70,DISK], DatanodeInfoWithStorage[127.0.0.1:44584,DS-5c430b0d-6345-44d2-aba2-0a9b54b5d73c,DISK], DatanodeInfoWithStorage[127.0.0.1:37410,DS-037d61fe-2165-403c-8127-9cf80004cf1a,DISK], DatanodeInfoWithStorage[127.0.0.1:42475,DS-780c98cd-8c46-4122-9cd8-dc5a9e3e9ddd,DISK], DatanodeInfoWithStorage[127.0.0.1:45255,DS-3655df8c-2142-45f0-8264-12f6360b44f3,DISK], DatanodeInfoWithStorage[127.0.0.1:34122,DS-7bbbbdb7-6c8b-4f79-b91d-c1aef80e2979,DISK], DatanodeInfoWithStorage[127.0.0.1:33781,DS-83efdcaa-6182-4fe8-ba23-37c56c9bd90a,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1496631927-172.17.0.18-1595611385226:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35092,DS-5ed95eef-9386-4b72-9b79-cfee05cd64a5,DISK], DatanodeInfoWithStorage[127.0.0.1:35734,DS-19e769c4-7f01-4a00-a3c3-d68e1dfa1e70,DISK], DatanodeInfoWithStorage[127.0.0.1:44584,DS-5c430b0d-6345-44d2-aba2-0a9b54b5d73c,DISK], DatanodeInfoWithStorage[127.0.0.1:37410,DS-037d61fe-2165-403c-8127-9cf80004cf1a,DISK], DatanodeInfoWithStorage[127.0.0.1:42475,DS-780c98cd-8c46-4122-9cd8-dc5a9e3e9ddd,DISK], DatanodeInfoWithStorage[127.0.0.1:45255,DS-3655df8c-2142-45f0-8264-12f6360b44f3,DISK], DatanodeInfoWithStorage[127.0.0.1:34122,DS-7bbbbdb7-6c8b-4f79-b91d-c1aef80e2979,DISK], DatanodeInfoWithStorage[127.0.0.1:33781,DS-83efdcaa-6182-4fe8-ba23-37c56c9bd90a,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2084632420-172.17.0.18-1595611490987:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39786,DS-34d5b221-6616-4624-b368-0c819fe8eb33,DISK], DatanodeInfoWithStorage[127.0.0.1:38926,DS-466d1c14-a160-406b-966e-e207aba52690,DISK], DatanodeInfoWithStorage[127.0.0.1:34218,DS-37015cba-0e53-4626-b0a7-e090d6b8a6e8,DISK], DatanodeInfoWithStorage[127.0.0.1:45425,DS-e71f3825-1cc6-4551-9e63-8a815b98c10a,DISK], DatanodeInfoWithStorage[127.0.0.1:41465,DS-30073d11-db1d-40ee-82d3-df11eeeba887,DISK], DatanodeInfoWithStorage[127.0.0.1:35743,DS-41c1585e-8e2e-44b1-878a-c0ac4cdde1aa,DISK], DatanodeInfoWithStorage[127.0.0.1:38151,DS-3d996ff6-c0e3-4bfa-b042-f94b8bb2c029,DISK], DatanodeInfoWithStorage[127.0.0.1:35712,DS-e8cbce74-50ec-4e19-a89b-0310680a4329,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2084632420-172.17.0.18-1595611490987:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39786,DS-34d5b221-6616-4624-b368-0c819fe8eb33,DISK], DatanodeInfoWithStorage[127.0.0.1:38926,DS-466d1c14-a160-406b-966e-e207aba52690,DISK], DatanodeInfoWithStorage[127.0.0.1:34218,DS-37015cba-0e53-4626-b0a7-e090d6b8a6e8,DISK], DatanodeInfoWithStorage[127.0.0.1:45425,DS-e71f3825-1cc6-4551-9e63-8a815b98c10a,DISK], DatanodeInfoWithStorage[127.0.0.1:41465,DS-30073d11-db1d-40ee-82d3-df11eeeba887,DISK], DatanodeInfoWithStorage[127.0.0.1:35743,DS-41c1585e-8e2e-44b1-878a-c0ac4cdde1aa,DISK], DatanodeInfoWithStorage[127.0.0.1:38151,DS-3d996ff6-c0e3-4bfa-b042-f94b8bb2c029,DISK], DatanodeInfoWithStorage[127.0.0.1:35712,DS-e8cbce74-50ec-4e19-a89b-0310680a4329,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1527721342-172.17.0.18-1595611557065:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33594,DS-5a7b6954-50ad-4006-a6a7-d54bea97ab9f,DISK], DatanodeInfoWithStorage[127.0.0.1:35860,DS-03c76fbb-68b8-4d1f-b7c3-2f7873a449a0,DISK], DatanodeInfoWithStorage[127.0.0.1:33311,DS-80209f5b-2211-4594-99b2-7e5bb2fa6394,DISK], DatanodeInfoWithStorage[127.0.0.1:36873,DS-153b9d42-5edb-49cf-9d0b-e0f3c51b9a64,DISK], DatanodeInfoWithStorage[127.0.0.1:44147,DS-e19128a7-a66e-459c-b39c-3087a6b16bf5,DISK], DatanodeInfoWithStorage[127.0.0.1:35614,DS-3a5e5c0e-5e20-4fcd-ad2e-9888c3591817,DISK], DatanodeInfoWithStorage[127.0.0.1:44114,DS-4ca1b40f-a171-4863-9936-701645fc6dff,DISK], DatanodeInfoWithStorage[127.0.0.1:46310,DS-77871a5b-0d3e-4d7e-b04a-4a745c7300c0,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1527721342-172.17.0.18-1595611557065:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33594,DS-5a7b6954-50ad-4006-a6a7-d54bea97ab9f,DISK], DatanodeInfoWithStorage[127.0.0.1:35860,DS-03c76fbb-68b8-4d1f-b7c3-2f7873a449a0,DISK], DatanodeInfoWithStorage[127.0.0.1:33311,DS-80209f5b-2211-4594-99b2-7e5bb2fa6394,DISK], DatanodeInfoWithStorage[127.0.0.1:36873,DS-153b9d42-5edb-49cf-9d0b-e0f3c51b9a64,DISK], DatanodeInfoWithStorage[127.0.0.1:44147,DS-e19128a7-a66e-459c-b39c-3087a6b16bf5,DISK], DatanodeInfoWithStorage[127.0.0.1:35614,DS-3a5e5c0e-5e20-4fcd-ad2e-9888c3591817,DISK], DatanodeInfoWithStorage[127.0.0.1:44114,DS-4ca1b40f-a171-4863-9936-701645fc6dff,DISK], DatanodeInfoWithStorage[127.0.0.1:46310,DS-77871a5b-0d3e-4d7e-b04a-4a745c7300c0,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-488506905-172.17.0.18-1595611872246:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32873,DS-68bd4058-16b0-4d59-acaf-abe7ad006c84,DISK], DatanodeInfoWithStorage[127.0.0.1:44110,DS-1ec44979-cacf-4ab0-93cf-1d2a9553352a,DISK], DatanodeInfoWithStorage[127.0.0.1:45946,DS-5e7c8cd7-ff74-45ef-8d8b-de542358cf9c,DISK], DatanodeInfoWithStorage[127.0.0.1:46617,DS-3bd25d40-b959-434c-b88e-da133f67d73a,DISK], DatanodeInfoWithStorage[127.0.0.1:39137,DS-b77e12d7-cc79-4cc4-99cb-7747f2b4ae3a,DISK], DatanodeInfoWithStorage[127.0.0.1:42277,DS-6dabf891-b0f8-4b1e-84fb-3f68db71e704,DISK], DatanodeInfoWithStorage[127.0.0.1:40282,DS-3be79f84-36a7-4508-8c1c-9e1137e12e10,DISK], DatanodeInfoWithStorage[127.0.0.1:35972,DS-ccd7e858-f66b-4c4c-bbb4-663e0e79aaa1,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-488506905-172.17.0.18-1595611872246:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32873,DS-68bd4058-16b0-4d59-acaf-abe7ad006c84,DISK], DatanodeInfoWithStorage[127.0.0.1:44110,DS-1ec44979-cacf-4ab0-93cf-1d2a9553352a,DISK], DatanodeInfoWithStorage[127.0.0.1:45946,DS-5e7c8cd7-ff74-45ef-8d8b-de542358cf9c,DISK], DatanodeInfoWithStorage[127.0.0.1:46617,DS-3bd25d40-b959-434c-b88e-da133f67d73a,DISK], DatanodeInfoWithStorage[127.0.0.1:39137,DS-b77e12d7-cc79-4cc4-99cb-7747f2b4ae3a,DISK], DatanodeInfoWithStorage[127.0.0.1:42277,DS-6dabf891-b0f8-4b1e-84fb-3f68db71e704,DISK], DatanodeInfoWithStorage[127.0.0.1:40282,DS-3be79f84-36a7-4508-8c1c-9e1137e12e10,DISK], DatanodeInfoWithStorage[127.0.0.1:35972,DS-ccd7e858-f66b-4c4c-bbb4-663e0e79aaa1,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-287772939-172.17.0.18-1595611901907:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44990,DS-fb429649-8b06-4d9d-aaf3-574fca0c239f,DISK], DatanodeInfoWithStorage[127.0.0.1:33136,DS-c052e98f-3fb6-41dc-98a9-c1a36ad0c26d,DISK], DatanodeInfoWithStorage[127.0.0.1:46670,DS-5e533fe9-4f28-4c7c-8f46-cff89efb3439,DISK], DatanodeInfoWithStorage[127.0.0.1:37769,DS-c7250b67-5823-4a9f-ad8f-bab87ff30856,DISK], DatanodeInfoWithStorage[127.0.0.1:39811,DS-f7e9b67c-109d-415f-b78e-682da8467ccf,DISK], DatanodeInfoWithStorage[127.0.0.1:36170,DS-ed1a9bbc-88c3-4d52-b6ff-e57cc8311dbc,DISK], DatanodeInfoWithStorage[127.0.0.1:45607,DS-472fa250-b404-43d2-9b09-625480d269b4,DISK], DatanodeInfoWithStorage[127.0.0.1:46223,DS-869d270b-3a02-46b2-a07b-8bd2d7d2e403,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-287772939-172.17.0.18-1595611901907:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44990,DS-fb429649-8b06-4d9d-aaf3-574fca0c239f,DISK], DatanodeInfoWithStorage[127.0.0.1:33136,DS-c052e98f-3fb6-41dc-98a9-c1a36ad0c26d,DISK], DatanodeInfoWithStorage[127.0.0.1:46670,DS-5e533fe9-4f28-4c7c-8f46-cff89efb3439,DISK], DatanodeInfoWithStorage[127.0.0.1:37769,DS-c7250b67-5823-4a9f-ad8f-bab87ff30856,DISK], DatanodeInfoWithStorage[127.0.0.1:39811,DS-f7e9b67c-109d-415f-b78e-682da8467ccf,DISK], DatanodeInfoWithStorage[127.0.0.1:36170,DS-ed1a9bbc-88c3-4d52-b6ff-e57cc8311dbc,DISK], DatanodeInfoWithStorage[127.0.0.1:45607,DS-472fa250-b404-43d2-9b09-625480d269b4,DISK], DatanodeInfoWithStorage[127.0.0.1:46223,DS-869d270b-3a02-46b2-a07b-8bd2d7d2e403,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1986385317-172.17.0.18-1595611972536:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43783,DS-dedbc690-fde0-4ab1-8e83-ff3bede0701d,DISK], DatanodeInfoWithStorage[127.0.0.1:41126,DS-4cccbdc5-79ce-480b-887e-2a91d8767d9d,DISK], DatanodeInfoWithStorage[127.0.0.1:46257,DS-9c5e1b5e-6172-400b-a9e2-dd4bd627cdae,DISK], DatanodeInfoWithStorage[127.0.0.1:32984,DS-d0612084-5351-4f10-bae9-441a3707ae41,DISK], DatanodeInfoWithStorage[127.0.0.1:36265,DS-9896cced-3016-4863-9779-b0d4f514c1eb,DISK], DatanodeInfoWithStorage[127.0.0.1:41737,DS-34c9f0f1-c50a-436b-8176-ef3782c3a8b4,DISK], DatanodeInfoWithStorage[127.0.0.1:44873,DS-cd336e24-5176-4642-bdd2-7dff0fd16bb6,DISK], DatanodeInfoWithStorage[127.0.0.1:32845,DS-cf0b8839-889e-49ff-94d6-36a474bec0f8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1986385317-172.17.0.18-1595611972536:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43783,DS-dedbc690-fde0-4ab1-8e83-ff3bede0701d,DISK], DatanodeInfoWithStorage[127.0.0.1:41126,DS-4cccbdc5-79ce-480b-887e-2a91d8767d9d,DISK], DatanodeInfoWithStorage[127.0.0.1:46257,DS-9c5e1b5e-6172-400b-a9e2-dd4bd627cdae,DISK], DatanodeInfoWithStorage[127.0.0.1:32984,DS-d0612084-5351-4f10-bae9-441a3707ae41,DISK], DatanodeInfoWithStorage[127.0.0.1:36265,DS-9896cced-3016-4863-9779-b0d4f514c1eb,DISK], DatanodeInfoWithStorage[127.0.0.1:41737,DS-34c9f0f1-c50a-436b-8176-ef3782c3a8b4,DISK], DatanodeInfoWithStorage[127.0.0.1:44873,DS-cd336e24-5176-4642-bdd2-7dff0fd16bb6,DISK], DatanodeInfoWithStorage[127.0.0.1:32845,DS-cf0b8839-889e-49ff-94d6-36a474bec0f8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1706996429-172.17.0.18-1595612040374:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43175,DS-7ee0135c-8cbf-4495-b210-e79bca36fce9,DISK], DatanodeInfoWithStorage[127.0.0.1:35338,DS-5534b64b-4a27-4042-aad1-16af94268e15,DISK], DatanodeInfoWithStorage[127.0.0.1:38116,DS-7d046068-7c6d-45b6-b6bb-10f5daf8bb59,DISK], DatanodeInfoWithStorage[127.0.0.1:36210,DS-e542ba6c-5c4a-4f9c-af9c-e8b7bff13449,DISK], DatanodeInfoWithStorage[127.0.0.1:43962,DS-e935a349-8d1d-46b3-bd4b-3caf52f1046b,DISK], DatanodeInfoWithStorage[127.0.0.1:37053,DS-d0f39742-47ce-4c8b-9b08-ba3f8b7cd340,DISK], DatanodeInfoWithStorage[127.0.0.1:41364,DS-523afedf-9bf3-4986-b433-500e139d4a3f,DISK], DatanodeInfoWithStorage[127.0.0.1:42148,DS-84c22fb1-c10b-470f-a0d0-d27b96d10d29,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1706996429-172.17.0.18-1595612040374:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43175,DS-7ee0135c-8cbf-4495-b210-e79bca36fce9,DISK], DatanodeInfoWithStorage[127.0.0.1:35338,DS-5534b64b-4a27-4042-aad1-16af94268e15,DISK], DatanodeInfoWithStorage[127.0.0.1:38116,DS-7d046068-7c6d-45b6-b6bb-10f5daf8bb59,DISK], DatanodeInfoWithStorage[127.0.0.1:36210,DS-e542ba6c-5c4a-4f9c-af9c-e8b7bff13449,DISK], DatanodeInfoWithStorage[127.0.0.1:43962,DS-e935a349-8d1d-46b3-bd4b-3caf52f1046b,DISK], DatanodeInfoWithStorage[127.0.0.1:37053,DS-d0f39742-47ce-4c8b-9b08-ba3f8b7cd340,DISK], DatanodeInfoWithStorage[127.0.0.1:41364,DS-523afedf-9bf3-4986-b433-500e139d4a3f,DISK], DatanodeInfoWithStorage[127.0.0.1:42148,DS-84c22fb1-c10b-470f-a0d0-d27b96d10d29,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-225413350-172.17.0.18-1595612070273:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37660,DS-c15fa094-8cf4-44b4-b485-4ed33370d56c,DISK], DatanodeInfoWithStorage[127.0.0.1:38920,DS-9562961a-4a21-4045-ad5e-6759e3b8ffd0,DISK], DatanodeInfoWithStorage[127.0.0.1:39316,DS-97359e0e-6403-47d8-8f5b-ffd98ee2d487,DISK], DatanodeInfoWithStorage[127.0.0.1:34927,DS-929729b4-ceed-432b-86b2-69f8b96341f3,DISK], DatanodeInfoWithStorage[127.0.0.1:36438,DS-48484c6a-832b-4431-81a9-da6043d60a38,DISK], DatanodeInfoWithStorage[127.0.0.1:44440,DS-4c4a56d3-651c-4642-a295-48c78fa46c09,DISK], DatanodeInfoWithStorage[127.0.0.1:44734,DS-fc2b1b16-6fa3-4bd0-b384-f2cdc273c260,DISK], DatanodeInfoWithStorage[127.0.0.1:33792,DS-20c9fd88-4b62-4822-a4d7-207e5ecd9d38,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-225413350-172.17.0.18-1595612070273:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37660,DS-c15fa094-8cf4-44b4-b485-4ed33370d56c,DISK], DatanodeInfoWithStorage[127.0.0.1:38920,DS-9562961a-4a21-4045-ad5e-6759e3b8ffd0,DISK], DatanodeInfoWithStorage[127.0.0.1:39316,DS-97359e0e-6403-47d8-8f5b-ffd98ee2d487,DISK], DatanodeInfoWithStorage[127.0.0.1:34927,DS-929729b4-ceed-432b-86b2-69f8b96341f3,DISK], DatanodeInfoWithStorage[127.0.0.1:36438,DS-48484c6a-832b-4431-81a9-da6043d60a38,DISK], DatanodeInfoWithStorage[127.0.0.1:44440,DS-4c4a56d3-651c-4642-a295-48c78fa46c09,DISK], DatanodeInfoWithStorage[127.0.0.1:44734,DS-fc2b1b16-6fa3-4bd0-b384-f2cdc273c260,DISK], DatanodeInfoWithStorage[127.0.0.1:33792,DS-20c9fd88-4b62-4822-a4d7-207e5ecd9d38,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-350521819-172.17.0.18-1595612334564:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45081,DS-ccb8394a-5dae-4ce5-8c37-6ff78d3819a7,DISK], DatanodeInfoWithStorage[127.0.0.1:44520,DS-f1375d50-f741-451b-a00f-456adaeca240,DISK], DatanodeInfoWithStorage[127.0.0.1:34786,DS-c6f33f39-a657-4923-af02-3c5073361bb1,DISK], DatanodeInfoWithStorage[127.0.0.1:38113,DS-bc0460ca-db63-4f2e-80ec-6c6abdfca050,DISK], DatanodeInfoWithStorage[127.0.0.1:35773,DS-fa69e0cf-c137-4852-9576-3e0a39732cfc,DISK], DatanodeInfoWithStorage[127.0.0.1:37270,DS-000849eb-7341-4926-a680-bd845a6205f8,DISK], DatanodeInfoWithStorage[127.0.0.1:33893,DS-cb9575db-7560-4be9-bf35-c5c0df9781ff,DISK], DatanodeInfoWithStorage[127.0.0.1:42900,DS-d481f5db-40f9-44bc-a53c-92488d61da72,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-350521819-172.17.0.18-1595612334564:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45081,DS-ccb8394a-5dae-4ce5-8c37-6ff78d3819a7,DISK], DatanodeInfoWithStorage[127.0.0.1:44520,DS-f1375d50-f741-451b-a00f-456adaeca240,DISK], DatanodeInfoWithStorage[127.0.0.1:34786,DS-c6f33f39-a657-4923-af02-3c5073361bb1,DISK], DatanodeInfoWithStorage[127.0.0.1:38113,DS-bc0460ca-db63-4f2e-80ec-6c6abdfca050,DISK], DatanodeInfoWithStorage[127.0.0.1:35773,DS-fa69e0cf-c137-4852-9576-3e0a39732cfc,DISK], DatanodeInfoWithStorage[127.0.0.1:37270,DS-000849eb-7341-4926-a680-bd845a6205f8,DISK], DatanodeInfoWithStorage[127.0.0.1:33893,DS-cb9575db-7560-4be9-bf35-c5c0df9781ff,DISK], DatanodeInfoWithStorage[127.0.0.1:42900,DS-d481f5db-40f9-44bc-a53c-92488d61da72,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1141394740-172.17.0.18-1595612402733:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38460,DS-b43584ec-32d8-489e-b199-29d1f05ac700,DISK], DatanodeInfoWithStorage[127.0.0.1:35776,DS-3f125baf-408d-40ee-991c-6e263f089e96,DISK], DatanodeInfoWithStorage[127.0.0.1:39924,DS-6badd2a7-6a40-4cc7-ae95-00a267fea297,DISK], DatanodeInfoWithStorage[127.0.0.1:34348,DS-06b81313-5bbd-444c-a12a-764afd461220,DISK], DatanodeInfoWithStorage[127.0.0.1:42695,DS-181a1059-7082-4a96-b10a-f6f712590b24,DISK], DatanodeInfoWithStorage[127.0.0.1:42257,DS-eb5ed310-fa03-482b-84fa-2d107a8740c5,DISK], DatanodeInfoWithStorage[127.0.0.1:35301,DS-48de1591-cbf0-4fde-bf40-c9371c0d6f11,DISK], DatanodeInfoWithStorage[127.0.0.1:44068,DS-2441ebc6-085d-41aa-8cea-e99b1d349dd1,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1141394740-172.17.0.18-1595612402733:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38460,DS-b43584ec-32d8-489e-b199-29d1f05ac700,DISK], DatanodeInfoWithStorage[127.0.0.1:35776,DS-3f125baf-408d-40ee-991c-6e263f089e96,DISK], DatanodeInfoWithStorage[127.0.0.1:39924,DS-6badd2a7-6a40-4cc7-ae95-00a267fea297,DISK], DatanodeInfoWithStorage[127.0.0.1:34348,DS-06b81313-5bbd-444c-a12a-764afd461220,DISK], DatanodeInfoWithStorage[127.0.0.1:42695,DS-181a1059-7082-4a96-b10a-f6f712590b24,DISK], DatanodeInfoWithStorage[127.0.0.1:42257,DS-eb5ed310-fa03-482b-84fa-2d107a8740c5,DISK], DatanodeInfoWithStorage[127.0.0.1:35301,DS-48de1591-cbf0-4fde-bf40-c9371c0d6f11,DISK], DatanodeInfoWithStorage[127.0.0.1:44068,DS-2441ebc6-085d-41aa-8cea-e99b1d349dd1,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-905863918-172.17.0.18-1595612544716:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42756,DS-a78987d2-d113-4d21-82fa-c94c8beb8702,DISK], DatanodeInfoWithStorage[127.0.0.1:40984,DS-9a973ae7-30f4-4c12-9d32-52fe2d9d5da6,DISK], DatanodeInfoWithStorage[127.0.0.1:36942,DS-d425d6fa-950c-4d7c-bc3b-014312df309b,DISK], DatanodeInfoWithStorage[127.0.0.1:42140,DS-66dafa2c-3f3c-4f79-a230-a238e74a335e,DISK], DatanodeInfoWithStorage[127.0.0.1:44565,DS-93641787-7017-4ca6-b8c0-e09b34ba6c1f,DISK], DatanodeInfoWithStorage[127.0.0.1:46868,DS-e7db2088-1773-4af6-b795-a1be7aa6f6de,DISK], DatanodeInfoWithStorage[127.0.0.1:44074,DS-9c8e7c93-3b3b-4ae1-979b-ea7ce25fc587,DISK], DatanodeInfoWithStorage[127.0.0.1:43668,DS-fe5973d4-03ad-4747-9960-79a65719d508,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-905863918-172.17.0.18-1595612544716:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42756,DS-a78987d2-d113-4d21-82fa-c94c8beb8702,DISK], DatanodeInfoWithStorage[127.0.0.1:40984,DS-9a973ae7-30f4-4c12-9d32-52fe2d9d5da6,DISK], DatanodeInfoWithStorage[127.0.0.1:36942,DS-d425d6fa-950c-4d7c-bc3b-014312df309b,DISK], DatanodeInfoWithStorage[127.0.0.1:42140,DS-66dafa2c-3f3c-4f79-a230-a238e74a335e,DISK], DatanodeInfoWithStorage[127.0.0.1:44565,DS-93641787-7017-4ca6-b8c0-e09b34ba6c1f,DISK], DatanodeInfoWithStorage[127.0.0.1:46868,DS-e7db2088-1773-4af6-b795-a1be7aa6f6de,DISK], DatanodeInfoWithStorage[127.0.0.1:44074,DS-9c8e7c93-3b3b-4ae1-979b-ea7ce25fc587,DISK], DatanodeInfoWithStorage[127.0.0.1:43668,DS-fe5973d4-03ad-4747-9960-79a65719d508,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2026597925-172.17.0.18-1595612687741:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42250,DS-88c2524f-c29c-4021-b03a-4545f4697519,DISK], DatanodeInfoWithStorage[127.0.0.1:40906,DS-ee8999e4-49df-4ee8-b02e-1c3309cec7c1,DISK], DatanodeInfoWithStorage[127.0.0.1:34579,DS-2aef3a84-79c4-4c41-957b-559996d4f512,DISK], DatanodeInfoWithStorage[127.0.0.1:35000,DS-57f85107-54be-4d12-a722-65c69c52069c,DISK], DatanodeInfoWithStorage[127.0.0.1:40099,DS-2ae7c2b7-0862-4cfa-b4b0-43bd990d79d7,DISK], DatanodeInfoWithStorage[127.0.0.1:33454,DS-4ff45316-7c85-4f76-8f31-4b7e94e11226,DISK], DatanodeInfoWithStorage[127.0.0.1:35419,DS-62ae9ed2-727c-4e60-88a4-ec606722737b,DISK], DatanodeInfoWithStorage[127.0.0.1:40823,DS-2c9d81a7-2301-43fb-b76e-0677619bf733,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2026597925-172.17.0.18-1595612687741:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42250,DS-88c2524f-c29c-4021-b03a-4545f4697519,DISK], DatanodeInfoWithStorage[127.0.0.1:40906,DS-ee8999e4-49df-4ee8-b02e-1c3309cec7c1,DISK], DatanodeInfoWithStorage[127.0.0.1:34579,DS-2aef3a84-79c4-4c41-957b-559996d4f512,DISK], DatanodeInfoWithStorage[127.0.0.1:35000,DS-57f85107-54be-4d12-a722-65c69c52069c,DISK], DatanodeInfoWithStorage[127.0.0.1:40099,DS-2ae7c2b7-0862-4cfa-b4b0-43bd990d79d7,DISK], DatanodeInfoWithStorage[127.0.0.1:33454,DS-4ff45316-7c85-4f76-8f31-4b7e94e11226,DISK], DatanodeInfoWithStorage[127.0.0.1:35419,DS-62ae9ed2-727c-4e60-88a4-ec606722737b,DISK], DatanodeInfoWithStorage[127.0.0.1:40823,DS-2c9d81a7-2301-43fb-b76e-0677619bf733,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-272818104-172.17.0.18-1595612730149:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39874,DS-ecb77ca8-b47c-4732-9a25-c52d042d1763,DISK], DatanodeInfoWithStorage[127.0.0.1:42013,DS-08cbcc1f-f13f-4c05-8ff2-98953a609b2f,DISK], DatanodeInfoWithStorage[127.0.0.1:40651,DS-0cb9a15f-5312-42a2-88ce-07294fb2881e,DISK], DatanodeInfoWithStorage[127.0.0.1:45910,DS-d45faa1c-a2ca-4fdc-acdb-c4f622348ab6,DISK], DatanodeInfoWithStorage[127.0.0.1:39281,DS-e17c4bc5-1120-4d3e-a0d1-b36ccaba288e,DISK], DatanodeInfoWithStorage[127.0.0.1:44760,DS-14c92070-a792-4339-a2a2-a9e04abcea1d,DISK], DatanodeInfoWithStorage[127.0.0.1:38655,DS-e5b408d8-552a-48de-8105-0825f4c2c778,DISK], DatanodeInfoWithStorage[127.0.0.1:36176,DS-bf8afce6-0e11-49df-a63b-8d14c239c73a,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-272818104-172.17.0.18-1595612730149:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39874,DS-ecb77ca8-b47c-4732-9a25-c52d042d1763,DISK], DatanodeInfoWithStorage[127.0.0.1:42013,DS-08cbcc1f-f13f-4c05-8ff2-98953a609b2f,DISK], DatanodeInfoWithStorage[127.0.0.1:40651,DS-0cb9a15f-5312-42a2-88ce-07294fb2881e,DISK], DatanodeInfoWithStorage[127.0.0.1:45910,DS-d45faa1c-a2ca-4fdc-acdb-c4f622348ab6,DISK], DatanodeInfoWithStorage[127.0.0.1:39281,DS-e17c4bc5-1120-4d3e-a0d1-b36ccaba288e,DISK], DatanodeInfoWithStorage[127.0.0.1:44760,DS-14c92070-a792-4339-a2a2-a9e04abcea1d,DISK], DatanodeInfoWithStorage[127.0.0.1:38655,DS-e5b408d8-552a-48de-8105-0825f4c2c778,DISK], DatanodeInfoWithStorage[127.0.0.1:36176,DS-bf8afce6-0e11-49df-a63b-8d14c239c73a,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1951258307-172.17.0.18-1595613104111:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37773,DS-8ce0a5d7-012d-410f-a2c5-3b7114fa53c9,DISK], DatanodeInfoWithStorage[127.0.0.1:43390,DS-9d583780-1232-442f-acf1-129335c5e80f,DISK], DatanodeInfoWithStorage[127.0.0.1:39938,DS-4232124a-94f2-473f-85b1-22185573fece,DISK], DatanodeInfoWithStorage[127.0.0.1:33697,DS-6f2f2b9a-fb72-4997-90d2-94d68c57366b,DISK], DatanodeInfoWithStorage[127.0.0.1:46237,DS-1d806ac4-02e4-4964-aac6-f1f15b517648,DISK], DatanodeInfoWithStorage[127.0.0.1:35698,DS-90e58f46-075f-4e82-aefd-bd5a33bc1ee7,DISK], DatanodeInfoWithStorage[127.0.0.1:37551,DS-77494f12-2356-4778-be05-1d500b0b572b,DISK], DatanodeInfoWithStorage[127.0.0.1:36053,DS-c70f4195-3fa8-4d4b-bf8c-5e3f554367a2,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1951258307-172.17.0.18-1595613104111:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37773,DS-8ce0a5d7-012d-410f-a2c5-3b7114fa53c9,DISK], DatanodeInfoWithStorage[127.0.0.1:43390,DS-9d583780-1232-442f-acf1-129335c5e80f,DISK], DatanodeInfoWithStorage[127.0.0.1:39938,DS-4232124a-94f2-473f-85b1-22185573fece,DISK], DatanodeInfoWithStorage[127.0.0.1:33697,DS-6f2f2b9a-fb72-4997-90d2-94d68c57366b,DISK], DatanodeInfoWithStorage[127.0.0.1:46237,DS-1d806ac4-02e4-4964-aac6-f1f15b517648,DISK], DatanodeInfoWithStorage[127.0.0.1:35698,DS-90e58f46-075f-4e82-aefd-bd5a33bc1ee7,DISK], DatanodeInfoWithStorage[127.0.0.1:37551,DS-77494f12-2356-4778-be05-1d500b0b572b,DISK], DatanodeInfoWithStorage[127.0.0.1:36053,DS-c70f4195-3fa8-4d4b-bf8c-5e3f554367a2,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-678275818-172.17.0.18-1595613663321:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46767,DS-e23bba10-1388-41e5-884f-7d041bbbca3b,DISK], DatanodeInfoWithStorage[127.0.0.1:35167,DS-7903e7f4-d7b6-450f-b3f5-a20559df738b,DISK], DatanodeInfoWithStorage[127.0.0.1:39785,DS-aa213f83-7054-4c9b-abcc-0d5c2664df4b,DISK], DatanodeInfoWithStorage[127.0.0.1:41421,DS-bfe80a4c-9012-4982-91e0-c9b0756597a6,DISK], DatanodeInfoWithStorage[127.0.0.1:45132,DS-fa07a132-2469-44b1-aab7-98263edfc3b7,DISK], DatanodeInfoWithStorage[127.0.0.1:35684,DS-b3c23ac4-f97a-40cf-85ca-75747deb8b9b,DISK], DatanodeInfoWithStorage[127.0.0.1:41637,DS-c38b7dd6-4da0-422b-b8d9-d49f85733a5a,DISK], DatanodeInfoWithStorage[127.0.0.1:37352,DS-3d6a4236-df06-40b3-83ae-3753b103f2b4,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-678275818-172.17.0.18-1595613663321:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46767,DS-e23bba10-1388-41e5-884f-7d041bbbca3b,DISK], DatanodeInfoWithStorage[127.0.0.1:35167,DS-7903e7f4-d7b6-450f-b3f5-a20559df738b,DISK], DatanodeInfoWithStorage[127.0.0.1:39785,DS-aa213f83-7054-4c9b-abcc-0d5c2664df4b,DISK], DatanodeInfoWithStorage[127.0.0.1:41421,DS-bfe80a4c-9012-4982-91e0-c9b0756597a6,DISK], DatanodeInfoWithStorage[127.0.0.1:45132,DS-fa07a132-2469-44b1-aab7-98263edfc3b7,DISK], DatanodeInfoWithStorage[127.0.0.1:35684,DS-b3c23ac4-f97a-40cf-85ca-75747deb8b9b,DISK], DatanodeInfoWithStorage[127.0.0.1:41637,DS-c38b7dd6-4da0-422b-b8d9-d49f85733a5a,DISK], DatanodeInfoWithStorage[127.0.0.1:37352,DS-3d6a4236-df06-40b3-83ae-3753b103f2b4,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1391135372-172.17.0.18-1595613791419:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37507,DS-a505c853-8982-47c4-b3bd-db67229690b4,DISK], DatanodeInfoWithStorage[127.0.0.1:40167,DS-28b85d84-ca16-4547-9a9a-270ac1d15e59,DISK], DatanodeInfoWithStorage[127.0.0.1:41570,DS-8702b849-f793-408a-8d5e-8175b8339b4b,DISK], DatanodeInfoWithStorage[127.0.0.1:38059,DS-f169b792-9e6b-4fc1-80bd-b123b92c1324,DISK], DatanodeInfoWithStorage[127.0.0.1:46349,DS-7896b316-403b-4a22-ad35-4effda7be134,DISK], DatanodeInfoWithStorage[127.0.0.1:42924,DS-0188b22c-14eb-46a0-b09c-4e3e5bbcacbf,DISK], DatanodeInfoWithStorage[127.0.0.1:34520,DS-2b7c47f5-9f13-4045-88be-4b3b93f6483e,DISK], DatanodeInfoWithStorage[127.0.0.1:39280,DS-0dcd8e36-3215-4742-a185-0a693fb0c440,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1391135372-172.17.0.18-1595613791419:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37507,DS-a505c853-8982-47c4-b3bd-db67229690b4,DISK], DatanodeInfoWithStorage[127.0.0.1:40167,DS-28b85d84-ca16-4547-9a9a-270ac1d15e59,DISK], DatanodeInfoWithStorage[127.0.0.1:41570,DS-8702b849-f793-408a-8d5e-8175b8339b4b,DISK], DatanodeInfoWithStorage[127.0.0.1:38059,DS-f169b792-9e6b-4fc1-80bd-b123b92c1324,DISK], DatanodeInfoWithStorage[127.0.0.1:46349,DS-7896b316-403b-4a22-ad35-4effda7be134,DISK], DatanodeInfoWithStorage[127.0.0.1:42924,DS-0188b22c-14eb-46a0-b09c-4e3e5bbcacbf,DISK], DatanodeInfoWithStorage[127.0.0.1:34520,DS-2b7c47f5-9f13-4045-88be-4b3b93f6483e,DISK], DatanodeInfoWithStorage[127.0.0.1:39280,DS-0dcd8e36-3215-4742-a185-0a693fb0c440,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2096521175-172.17.0.18-1595614176498:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38166,DS-872f8004-d823-4f5a-a02e-1b6f34c5c42c,DISK], DatanodeInfoWithStorage[127.0.0.1:37173,DS-7867b7ce-36bf-411e-ad24-8d72d031a6f9,DISK], DatanodeInfoWithStorage[127.0.0.1:38842,DS-e33b286c-0b6a-4728-b955-541bdab9617c,DISK], DatanodeInfoWithStorage[127.0.0.1:43244,DS-7a107571-45d5-4eac-8662-b3425f13c70a,DISK], DatanodeInfoWithStorage[127.0.0.1:38732,DS-dac7442f-4faa-4918-aa42-e5b9bf03ad6d,DISK], DatanodeInfoWithStorage[127.0.0.1:33909,DS-321c69ff-26a6-4634-a3f4-a4ecb3ef56c0,DISK], DatanodeInfoWithStorage[127.0.0.1:35425,DS-ef1c51d4-af9b-4644-a0ad-4b57c582d81d,DISK], DatanodeInfoWithStorage[127.0.0.1:38267,DS-5ab6fbe2-1f48-4dd1-840f-21f27a4b1ebd,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2096521175-172.17.0.18-1595614176498:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38166,DS-872f8004-d823-4f5a-a02e-1b6f34c5c42c,DISK], DatanodeInfoWithStorage[127.0.0.1:37173,DS-7867b7ce-36bf-411e-ad24-8d72d031a6f9,DISK], DatanodeInfoWithStorage[127.0.0.1:38842,DS-e33b286c-0b6a-4728-b955-541bdab9617c,DISK], DatanodeInfoWithStorage[127.0.0.1:43244,DS-7a107571-45d5-4eac-8662-b3425f13c70a,DISK], DatanodeInfoWithStorage[127.0.0.1:38732,DS-dac7442f-4faa-4918-aa42-e5b9bf03ad6d,DISK], DatanodeInfoWithStorage[127.0.0.1:33909,DS-321c69ff-26a6-4634-a3f4-a4ecb3ef56c0,DISK], DatanodeInfoWithStorage[127.0.0.1:35425,DS-ef1c51d4-af9b-4644-a0ad-4b57c582d81d,DISK], DatanodeInfoWithStorage[127.0.0.1:38267,DS-5ab6fbe2-1f48-4dd1-840f-21f27a4b1ebd,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-546817127-172.17.0.18-1595614333660:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45948,DS-9160772b-cfa0-47ab-ba47-18dfb2f19967,DISK], DatanodeInfoWithStorage[127.0.0.1:40618,DS-52a1a7ce-b6fd-4d60-be0d-3f351ffbb306,DISK], DatanodeInfoWithStorage[127.0.0.1:36687,DS-bd409fd3-b5de-4ffc-a258-c72c98d23299,DISK], DatanodeInfoWithStorage[127.0.0.1:46229,DS-41c3659c-45d8-4a0f-9057-1903dd47af8f,DISK], DatanodeInfoWithStorage[127.0.0.1:34150,DS-6373158a-6421-414e-80aa-27bcde8bf8e5,DISK], DatanodeInfoWithStorage[127.0.0.1:41608,DS-682101c2-5311-4afe-83a6-3c6d5b7deb7c,DISK], DatanodeInfoWithStorage[127.0.0.1:36288,DS-45e91d8f-04d4-49ec-87a1-b1c1b9f2a308,DISK], DatanodeInfoWithStorage[127.0.0.1:38812,DS-5b3a5de1-ec64-46a2-a61a-9791aa564a61,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-546817127-172.17.0.18-1595614333660:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45948,DS-9160772b-cfa0-47ab-ba47-18dfb2f19967,DISK], DatanodeInfoWithStorage[127.0.0.1:40618,DS-52a1a7ce-b6fd-4d60-be0d-3f351ffbb306,DISK], DatanodeInfoWithStorage[127.0.0.1:36687,DS-bd409fd3-b5de-4ffc-a258-c72c98d23299,DISK], DatanodeInfoWithStorage[127.0.0.1:46229,DS-41c3659c-45d8-4a0f-9057-1903dd47af8f,DISK], DatanodeInfoWithStorage[127.0.0.1:34150,DS-6373158a-6421-414e-80aa-27bcde8bf8e5,DISK], DatanodeInfoWithStorage[127.0.0.1:41608,DS-682101c2-5311-4afe-83a6-3c6d5b7deb7c,DISK], DatanodeInfoWithStorage[127.0.0.1:36288,DS-45e91d8f-04d4-49ec-87a1-b1c1b9f2a308,DISK], DatanodeInfoWithStorage[127.0.0.1:38812,DS-5b3a5de1-ec64-46a2-a61a-9791aa564a61,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1291233531-172.17.0.18-1595614370500:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46605,DS-e25c2d42-a8e2-4c55-abb0-a85869a09963,DISK], DatanodeInfoWithStorage[127.0.0.1:46521,DS-e337e69b-4510-4784-aac5-85cf7cf643ba,DISK], DatanodeInfoWithStorage[127.0.0.1:36621,DS-897faeb1-811b-4c5a-9a86-ef6b5d02090b,DISK], DatanodeInfoWithStorage[127.0.0.1:39113,DS-a23086dc-ab84-4894-8459-30a9eaed0661,DISK], DatanodeInfoWithStorage[127.0.0.1:45135,DS-e6724d45-e2fc-462c-8aa7-02958821fcf7,DISK], DatanodeInfoWithStorage[127.0.0.1:42798,DS-18839d5d-6cf4-4eb5-a57f-afc3072820a4,DISK], DatanodeInfoWithStorage[127.0.0.1:46328,DS-414a45cc-8a83-4cbb-9a46-3ff65500e581,DISK], DatanodeInfoWithStorage[127.0.0.1:35617,DS-4e8d37f0-8b38-4682-afd3-65b923e6bddf,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1291233531-172.17.0.18-1595614370500:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46605,DS-e25c2d42-a8e2-4c55-abb0-a85869a09963,DISK], DatanodeInfoWithStorage[127.0.0.1:46521,DS-e337e69b-4510-4784-aac5-85cf7cf643ba,DISK], DatanodeInfoWithStorage[127.0.0.1:36621,DS-897faeb1-811b-4c5a-9a86-ef6b5d02090b,DISK], DatanodeInfoWithStorage[127.0.0.1:39113,DS-a23086dc-ab84-4894-8459-30a9eaed0661,DISK], DatanodeInfoWithStorage[127.0.0.1:45135,DS-e6724d45-e2fc-462c-8aa7-02958821fcf7,DISK], DatanodeInfoWithStorage[127.0.0.1:42798,DS-18839d5d-6cf4-4eb5-a57f-afc3072820a4,DISK], DatanodeInfoWithStorage[127.0.0.1:46328,DS-414a45cc-8a83-4cbb-9a46-3ff65500e581,DISK], DatanodeInfoWithStorage[127.0.0.1:35617,DS-4e8d37f0-8b38-4682-afd3-65b923e6bddf,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-409506456-172.17.0.18-1595614517879:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45187,DS-54e6fd37-f24f-402e-9e18-0182daf7cf34,DISK], DatanodeInfoWithStorage[127.0.0.1:35103,DS-cd8b1224-2a09-44b6-9f1d-549c0f5f3bb0,DISK], DatanodeInfoWithStorage[127.0.0.1:46598,DS-d48f1c3f-a6d3-4b3a-af2f-dd1ef0241cc2,DISK], DatanodeInfoWithStorage[127.0.0.1:36869,DS-b2dc5a37-19fd-44f9-8604-889bcd15e84d,DISK], DatanodeInfoWithStorage[127.0.0.1:46685,DS-9a81f24a-7fe6-4294-8554-b36fbf8235be,DISK], DatanodeInfoWithStorage[127.0.0.1:45438,DS-0b41ec19-2a54-49e5-828d-731c0f4f6971,DISK], DatanodeInfoWithStorage[127.0.0.1:34966,DS-8dcd820d-040e-4eee-90cb-ba40f1e7fb8a,DISK], DatanodeInfoWithStorage[127.0.0.1:44335,DS-be87c409-2eaf-4e4e-9a35-7419ac2c0ad1,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-409506456-172.17.0.18-1595614517879:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45187,DS-54e6fd37-f24f-402e-9e18-0182daf7cf34,DISK], DatanodeInfoWithStorage[127.0.0.1:35103,DS-cd8b1224-2a09-44b6-9f1d-549c0f5f3bb0,DISK], DatanodeInfoWithStorage[127.0.0.1:46598,DS-d48f1c3f-a6d3-4b3a-af2f-dd1ef0241cc2,DISK], DatanodeInfoWithStorage[127.0.0.1:36869,DS-b2dc5a37-19fd-44f9-8604-889bcd15e84d,DISK], DatanodeInfoWithStorage[127.0.0.1:46685,DS-9a81f24a-7fe6-4294-8554-b36fbf8235be,DISK], DatanodeInfoWithStorage[127.0.0.1:45438,DS-0b41ec19-2a54-49e5-828d-731c0f4f6971,DISK], DatanodeInfoWithStorage[127.0.0.1:34966,DS-8dcd820d-040e-4eee-90cb-ba40f1e7fb8a,DISK], DatanodeInfoWithStorage[127.0.0.1:44335,DS-be87c409-2eaf-4e4e-9a35-7419ac2c0ad1,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-894422-172.17.0.18-1595614707326:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38506,DS-8141716d-ef9d-4f79-aa52-0942fbfca2de,DISK], DatanodeInfoWithStorage[127.0.0.1:35985,DS-519add91-fa2a-40b1-82bd-27c1914b9903,DISK], DatanodeInfoWithStorage[127.0.0.1:40276,DS-cf435877-3d96-4f78-be27-28ef73bdf4a2,DISK], DatanodeInfoWithStorage[127.0.0.1:41323,DS-b36c30e4-42d1-404d-86ea-5dd57d2f8c23,DISK], DatanodeInfoWithStorage[127.0.0.1:45230,DS-8c38729a-6726-4bfa-857f-ccc51102269f,DISK], DatanodeInfoWithStorage[127.0.0.1:43306,DS-f9dcbdab-e20e-4dde-a50b-7ac84ee24268,DISK], DatanodeInfoWithStorage[127.0.0.1:45042,DS-4ab011be-fee3-4f42-80b3-7d2233409765,DISK], DatanodeInfoWithStorage[127.0.0.1:34238,DS-edcc4bb9-5904-4fdc-a2c8-2cbb0b10a916,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-894422-172.17.0.18-1595614707326:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38506,DS-8141716d-ef9d-4f79-aa52-0942fbfca2de,DISK], DatanodeInfoWithStorage[127.0.0.1:35985,DS-519add91-fa2a-40b1-82bd-27c1914b9903,DISK], DatanodeInfoWithStorage[127.0.0.1:40276,DS-cf435877-3d96-4f78-be27-28ef73bdf4a2,DISK], DatanodeInfoWithStorage[127.0.0.1:41323,DS-b36c30e4-42d1-404d-86ea-5dd57d2f8c23,DISK], DatanodeInfoWithStorage[127.0.0.1:45230,DS-8c38729a-6726-4bfa-857f-ccc51102269f,DISK], DatanodeInfoWithStorage[127.0.0.1:43306,DS-f9dcbdab-e20e-4dde-a50b-7ac84ee24268,DISK], DatanodeInfoWithStorage[127.0.0.1:45042,DS-4ab011be-fee3-4f42-80b3-7d2233409765,DISK], DatanodeInfoWithStorage[127.0.0.1:34238,DS-edcc4bb9-5904-4fdc-a2c8-2cbb0b10a916,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-881781648-172.17.0.18-1595614784074:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45647,DS-5717eef6-289a-4a11-a93f-cf087d490a30,DISK], DatanodeInfoWithStorage[127.0.0.1:44536,DS-95a5713b-9306-4e0f-8b84-487741e5cdd9,DISK], DatanodeInfoWithStorage[127.0.0.1:38255,DS-a0f41b04-1adc-4b6a-84d3-74d0d6bed735,DISK], DatanodeInfoWithStorage[127.0.0.1:46012,DS-0aeda243-4731-45d9-bed8-abe7d3d3c7ca,DISK], DatanodeInfoWithStorage[127.0.0.1:33073,DS-99650144-4825-4b71-aa63-bfd2aa7e899c,DISK], DatanodeInfoWithStorage[127.0.0.1:37449,DS-45ba2307-64f8-4736-90b3-ec1de3bbee03,DISK], DatanodeInfoWithStorage[127.0.0.1:44703,DS-58aa432d-e93f-4618-af86-716f72167065,DISK], DatanodeInfoWithStorage[127.0.0.1:43254,DS-9b20683f-6072-47c6-bc12-eb47be4c6322,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-881781648-172.17.0.18-1595614784074:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45647,DS-5717eef6-289a-4a11-a93f-cf087d490a30,DISK], DatanodeInfoWithStorage[127.0.0.1:44536,DS-95a5713b-9306-4e0f-8b84-487741e5cdd9,DISK], DatanodeInfoWithStorage[127.0.0.1:38255,DS-a0f41b04-1adc-4b6a-84d3-74d0d6bed735,DISK], DatanodeInfoWithStorage[127.0.0.1:46012,DS-0aeda243-4731-45d9-bed8-abe7d3d3c7ca,DISK], DatanodeInfoWithStorage[127.0.0.1:33073,DS-99650144-4825-4b71-aa63-bfd2aa7e899c,DISK], DatanodeInfoWithStorage[127.0.0.1:37449,DS-45ba2307-64f8-4736-90b3-ec1de3bbee03,DISK], DatanodeInfoWithStorage[127.0.0.1:44703,DS-58aa432d-e93f-4618-af86-716f72167065,DISK], DatanodeInfoWithStorage[127.0.0.1:43254,DS-9b20683f-6072-47c6-bc12-eb47be4c6322,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-203692664-172.17.0.18-1595615096576:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35494,DS-be5b383c-b4ce-4470-a9d6-c30a3a586f9e,DISK], DatanodeInfoWithStorage[127.0.0.1:45078,DS-eed27d57-5ebd-487d-9bfc-1108d2d97753,DISK], DatanodeInfoWithStorage[127.0.0.1:44597,DS-54bc93cb-f0dc-4d0f-8847-ce66a167680e,DISK], DatanodeInfoWithStorage[127.0.0.1:44563,DS-7ef31fe3-9066-48e9-93bd-2d602a7e1edc,DISK], DatanodeInfoWithStorage[127.0.0.1:35814,DS-8d9ae97c-cf24-4af5-a6d7-6fcb73db6b74,DISK], DatanodeInfoWithStorage[127.0.0.1:40631,DS-bb5b73a5-0204-40f2-8fea-f63f5e4748d1,DISK], DatanodeInfoWithStorage[127.0.0.1:42611,DS-53e6fe55-53f5-4a0d-914c-f18432742a7f,DISK], DatanodeInfoWithStorage[127.0.0.1:46806,DS-df7db9a6-7b94-4ae9-ac91-8ef8f8e40241,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-203692664-172.17.0.18-1595615096576:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35494,DS-be5b383c-b4ce-4470-a9d6-c30a3a586f9e,DISK], DatanodeInfoWithStorage[127.0.0.1:45078,DS-eed27d57-5ebd-487d-9bfc-1108d2d97753,DISK], DatanodeInfoWithStorage[127.0.0.1:44597,DS-54bc93cb-f0dc-4d0f-8847-ce66a167680e,DISK], DatanodeInfoWithStorage[127.0.0.1:44563,DS-7ef31fe3-9066-48e9-93bd-2d602a7e1edc,DISK], DatanodeInfoWithStorage[127.0.0.1:35814,DS-8d9ae97c-cf24-4af5-a6d7-6fcb73db6b74,DISK], DatanodeInfoWithStorage[127.0.0.1:40631,DS-bb5b73a5-0204-40f2-8fea-f63f5e4748d1,DISK], DatanodeInfoWithStorage[127.0.0.1:42611,DS-53e6fe55-53f5-4a0d-914c-f18432742a7f,DISK], DatanodeInfoWithStorage[127.0.0.1:46806,DS-df7db9a6-7b94-4ae9-ac91-8ef8f8e40241,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1825480489-172.17.0.18-1595615127031:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44305,DS-1e87af9e-e4de-4df3-846e-7213ee42bfbe,DISK], DatanodeInfoWithStorage[127.0.0.1:45499,DS-26aca753-f16c-4dc4-94cb-c7b49c73030b,DISK], DatanodeInfoWithStorage[127.0.0.1:35342,DS-8658e3d9-29df-4f0d-a339-cac1eb962ad2,DISK], DatanodeInfoWithStorage[127.0.0.1:37062,DS-78b8cc27-c937-4832-bf51-4bdc846f9709,DISK], DatanodeInfoWithStorage[127.0.0.1:44455,DS-371d891b-2d14-4e2a-892f-6df31bdccb2f,DISK], DatanodeInfoWithStorage[127.0.0.1:36695,DS-08ff9680-14d1-4b5d-ac55-ec1d77eb8a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:37736,DS-4e319ae4-4a1e-425a-b053-a57a0327e57d,DISK], DatanodeInfoWithStorage[127.0.0.1:44675,DS-f78f50b5-d10a-4922-81ea-3fb01118de33,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1825480489-172.17.0.18-1595615127031:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44305,DS-1e87af9e-e4de-4df3-846e-7213ee42bfbe,DISK], DatanodeInfoWithStorage[127.0.0.1:45499,DS-26aca753-f16c-4dc4-94cb-c7b49c73030b,DISK], DatanodeInfoWithStorage[127.0.0.1:35342,DS-8658e3d9-29df-4f0d-a339-cac1eb962ad2,DISK], DatanodeInfoWithStorage[127.0.0.1:37062,DS-78b8cc27-c937-4832-bf51-4bdc846f9709,DISK], DatanodeInfoWithStorage[127.0.0.1:44455,DS-371d891b-2d14-4e2a-892f-6df31bdccb2f,DISK], DatanodeInfoWithStorage[127.0.0.1:36695,DS-08ff9680-14d1-4b5d-ac55-ec1d77eb8a1c,DISK], DatanodeInfoWithStorage[127.0.0.1:37736,DS-4e319ae4-4a1e-425a-b053-a57a0327e57d,DISK], DatanodeInfoWithStorage[127.0.0.1:44675,DS-f78f50b5-d10a-4922-81ea-3fb01118de33,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-140686409-172.17.0.18-1595615183713:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40695,DS-d1de1c20-ee3f-470a-b12f-0f85058f7058,DISK], DatanodeInfoWithStorage[127.0.0.1:40231,DS-4666f44a-b9aa-410a-a083-e65ce58cae1f,DISK], DatanodeInfoWithStorage[127.0.0.1:42936,DS-47fa0521-089e-4485-bf2c-7cacfb67a36f,DISK], DatanodeInfoWithStorage[127.0.0.1:45953,DS-ed5d3edf-bbf2-4bf5-a894-a644c85e989f,DISK], DatanodeInfoWithStorage[127.0.0.1:38196,DS-00e3f2cb-a729-4868-8c13-cb262030e32d,DISK], DatanodeInfoWithStorage[127.0.0.1:41730,DS-2b3e1f2e-f00b-4256-a749-5ab2433ad795,DISK], DatanodeInfoWithStorage[127.0.0.1:32838,DS-91b57ae0-8758-44a8-a304-c5723d2a6313,DISK], DatanodeInfoWithStorage[127.0.0.1:41597,DS-0609a017-55fc-4b29-9fd7-31746b48d006,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-140686409-172.17.0.18-1595615183713:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40695,DS-d1de1c20-ee3f-470a-b12f-0f85058f7058,DISK], DatanodeInfoWithStorage[127.0.0.1:40231,DS-4666f44a-b9aa-410a-a083-e65ce58cae1f,DISK], DatanodeInfoWithStorage[127.0.0.1:42936,DS-47fa0521-089e-4485-bf2c-7cacfb67a36f,DISK], DatanodeInfoWithStorage[127.0.0.1:45953,DS-ed5d3edf-bbf2-4bf5-a894-a644c85e989f,DISK], DatanodeInfoWithStorage[127.0.0.1:38196,DS-00e3f2cb-a729-4868-8c13-cb262030e32d,DISK], DatanodeInfoWithStorage[127.0.0.1:41730,DS-2b3e1f2e-f00b-4256-a749-5ab2433ad795,DISK], DatanodeInfoWithStorage[127.0.0.1:32838,DS-91b57ae0-8758-44a8-a304-c5723d2a6313,DISK], DatanodeInfoWithStorage[127.0.0.1:41597,DS-0609a017-55fc-4b29-9fd7-31746b48d006,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-881800425-172.17.0.18-1595615284106:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36213,DS-24ceaf81-4c49-4abd-93e0-9e8db7c74328,DISK], DatanodeInfoWithStorage[127.0.0.1:39751,DS-a569de2f-c789-456c-b371-57d016dbd8f3,DISK], DatanodeInfoWithStorage[127.0.0.1:36509,DS-3d8e16c4-e147-4822-be53-547bfccaccc2,DISK], DatanodeInfoWithStorage[127.0.0.1:37569,DS-070cc713-bc7d-4fca-9106-e021b64b0f1d,DISK], DatanodeInfoWithStorage[127.0.0.1:38959,DS-4bf2a4b0-72bc-4452-b071-47bcc1627ddf,DISK], DatanodeInfoWithStorage[127.0.0.1:40332,DS-417c97cf-77a0-4085-8d9f-e893f4d10c2e,DISK], DatanodeInfoWithStorage[127.0.0.1:34476,DS-38c71bc3-27f7-4b98-9c4c-20d2d0bae938,DISK], DatanodeInfoWithStorage[127.0.0.1:35876,DS-59c28af2-9c9b-4e32-ae7e-9da17f2df474,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-881800425-172.17.0.18-1595615284106:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36213,DS-24ceaf81-4c49-4abd-93e0-9e8db7c74328,DISK], DatanodeInfoWithStorage[127.0.0.1:39751,DS-a569de2f-c789-456c-b371-57d016dbd8f3,DISK], DatanodeInfoWithStorage[127.0.0.1:36509,DS-3d8e16c4-e147-4822-be53-547bfccaccc2,DISK], DatanodeInfoWithStorage[127.0.0.1:37569,DS-070cc713-bc7d-4fca-9106-e021b64b0f1d,DISK], DatanodeInfoWithStorage[127.0.0.1:38959,DS-4bf2a4b0-72bc-4452-b071-47bcc1627ddf,DISK], DatanodeInfoWithStorage[127.0.0.1:40332,DS-417c97cf-77a0-4085-8d9f-e893f4d10c2e,DISK], DatanodeInfoWithStorage[127.0.0.1:34476,DS-38c71bc3-27f7-4b98-9c4c-20d2d0bae938,DISK], DatanodeInfoWithStorage[127.0.0.1:35876,DS-59c28af2-9c9b-4e32-ae7e-9da17f2df474,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-640559603-172.17.0.18-1595615346689:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36050,DS-d3b01822-8499-4d03-9071-a7e4240a1930,DISK], DatanodeInfoWithStorage[127.0.0.1:42527,DS-27a8e7d6-9b2a-431e-ade1-553e4c2c181c,DISK], DatanodeInfoWithStorage[127.0.0.1:39985,DS-ff4b3de7-a412-456f-b0bb-5a852edfc366,DISK], DatanodeInfoWithStorage[127.0.0.1:36219,DS-89767cc7-fb85-4fd7-b669-7389c5de0906,DISK], DatanodeInfoWithStorage[127.0.0.1:41880,DS-f2d9b661-84b5-43ec-acc8-32f4895120e9,DISK], DatanodeInfoWithStorage[127.0.0.1:46227,DS-c9f6125a-d88a-4f43-b9a3-47799a526118,DISK], DatanodeInfoWithStorage[127.0.0.1:35067,DS-24656761-b3c5-4df3-925d-96649d0ea05b,DISK], DatanodeInfoWithStorage[127.0.0.1:33376,DS-26315146-cbce-4950-9dad-eb06934b067c,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-640559603-172.17.0.18-1595615346689:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36050,DS-d3b01822-8499-4d03-9071-a7e4240a1930,DISK], DatanodeInfoWithStorage[127.0.0.1:42527,DS-27a8e7d6-9b2a-431e-ade1-553e4c2c181c,DISK], DatanodeInfoWithStorage[127.0.0.1:39985,DS-ff4b3de7-a412-456f-b0bb-5a852edfc366,DISK], DatanodeInfoWithStorage[127.0.0.1:36219,DS-89767cc7-fb85-4fd7-b669-7389c5de0906,DISK], DatanodeInfoWithStorage[127.0.0.1:41880,DS-f2d9b661-84b5-43ec-acc8-32f4895120e9,DISK], DatanodeInfoWithStorage[127.0.0.1:46227,DS-c9f6125a-d88a-4f43-b9a3-47799a526118,DISK], DatanodeInfoWithStorage[127.0.0.1:35067,DS-24656761-b3c5-4df3-925d-96649d0ea05b,DISK], DatanodeInfoWithStorage[127.0.0.1:33376,DS-26315146-cbce-4950-9dad-eb06934b067c,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1318403675-172.17.0.18-1595615461327:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37874,DS-d7e1df48-1d03-4344-8fd0-b683c97622a0,DISK], DatanodeInfoWithStorage[127.0.0.1:44858,DS-6a5b16cb-823e-4085-ae9e-d5cde88f4b5d,DISK], DatanodeInfoWithStorage[127.0.0.1:35619,DS-c39b8932-13b9-4b25-8b91-13459a71d93b,DISK], DatanodeInfoWithStorage[127.0.0.1:38496,DS-50387953-4cc3-4714-b734-59f54106ad8a,DISK], DatanodeInfoWithStorage[127.0.0.1:35773,DS-fe01e9cc-0865-462a-9dfe-2a69d1c89c6c,DISK], DatanodeInfoWithStorage[127.0.0.1:45969,DS-7fd7d348-14d5-4b75-82c7-fc207f05f340,DISK], DatanodeInfoWithStorage[127.0.0.1:37988,DS-f081fd8b-c070-4710-a9bc-2a110ce1c171,DISK], DatanodeInfoWithStorage[127.0.0.1:38268,DS-f15420de-3bd7-4ff8-9298-2a9fc39f10a4,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1318403675-172.17.0.18-1595615461327:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37874,DS-d7e1df48-1d03-4344-8fd0-b683c97622a0,DISK], DatanodeInfoWithStorage[127.0.0.1:44858,DS-6a5b16cb-823e-4085-ae9e-d5cde88f4b5d,DISK], DatanodeInfoWithStorage[127.0.0.1:35619,DS-c39b8932-13b9-4b25-8b91-13459a71d93b,DISK], DatanodeInfoWithStorage[127.0.0.1:38496,DS-50387953-4cc3-4714-b734-59f54106ad8a,DISK], DatanodeInfoWithStorage[127.0.0.1:35773,DS-fe01e9cc-0865-462a-9dfe-2a69d1c89c6c,DISK], DatanodeInfoWithStorage[127.0.0.1:45969,DS-7fd7d348-14d5-4b75-82c7-fc207f05f340,DISK], DatanodeInfoWithStorage[127.0.0.1:37988,DS-f081fd8b-c070-4710-a9bc-2a110ce1c171,DISK], DatanodeInfoWithStorage[127.0.0.1:38268,DS-f15420de-3bd7-4ff8-9298-2a9fc39f10a4,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1558173158-172.17.0.18-1595615534355:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42269,DS-256eddfe-3368-440a-b491-87b2dc4102fb,DISK], DatanodeInfoWithStorage[127.0.0.1:45394,DS-17f78373-af7d-48be-8490-6b983148b8fc,DISK], DatanodeInfoWithStorage[127.0.0.1:39162,DS-402bfe56-c050-49e4-b240-2997c16ed758,DISK], DatanodeInfoWithStorage[127.0.0.1:40629,DS-fba87d72-713e-42b6-b767-25fa69a7586e,DISK], DatanodeInfoWithStorage[127.0.0.1:33853,DS-c2a07f0a-d335-47b2-a475-0239f35e63d9,DISK], DatanodeInfoWithStorage[127.0.0.1:39388,DS-5f511ce7-1e6d-4253-848c-a375800f9bc5,DISK], DatanodeInfoWithStorage[127.0.0.1:37321,DS-d9fcc801-46ba-4dba-8116-9cbb8ad6bfea,DISK], DatanodeInfoWithStorage[127.0.0.1:46025,DS-2f77e306-6ac7-422a-80cc-ded196ef7377,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1558173158-172.17.0.18-1595615534355:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42269,DS-256eddfe-3368-440a-b491-87b2dc4102fb,DISK], DatanodeInfoWithStorage[127.0.0.1:45394,DS-17f78373-af7d-48be-8490-6b983148b8fc,DISK], DatanodeInfoWithStorage[127.0.0.1:39162,DS-402bfe56-c050-49e4-b240-2997c16ed758,DISK], DatanodeInfoWithStorage[127.0.0.1:40629,DS-fba87d72-713e-42b6-b767-25fa69a7586e,DISK], DatanodeInfoWithStorage[127.0.0.1:33853,DS-c2a07f0a-d335-47b2-a475-0239f35e63d9,DISK], DatanodeInfoWithStorage[127.0.0.1:39388,DS-5f511ce7-1e6d-4253-848c-a375800f9bc5,DISK], DatanodeInfoWithStorage[127.0.0.1:37321,DS-d9fcc801-46ba-4dba-8116-9cbb8ad6bfea,DISK], DatanodeInfoWithStorage[127.0.0.1:46025,DS-2f77e306-6ac7-422a-80cc-ded196ef7377,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-366391432-172.17.0.18-1595615636666:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32862,DS-166c381e-0313-4c20-81bb-a1ca0e61836e,DISK], DatanodeInfoWithStorage[127.0.0.1:33985,DS-fc4e5776-78da-4fd0-ac54-5f54a983932d,DISK], DatanodeInfoWithStorage[127.0.0.1:40485,DS-d7793602-cd31-46e0-a7bc-5b6474ba58db,DISK], DatanodeInfoWithStorage[127.0.0.1:37058,DS-4f182411-dad3-48e9-a0c3-e781925fb6db,DISK], DatanodeInfoWithStorage[127.0.0.1:44862,DS-0bba8db7-d778-43bc-93ea-25abab25bb8c,DISK], DatanodeInfoWithStorage[127.0.0.1:36230,DS-cac1d2ff-57be-4c62-ad0f-1fed0d077de2,DISK], DatanodeInfoWithStorage[127.0.0.1:42342,DS-15c98f78-5b47-42c0-87fa-3686fd157320,DISK], DatanodeInfoWithStorage[127.0.0.1:45897,DS-abde6b86-7360-4e92-a564-5ce836d99295,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-366391432-172.17.0.18-1595615636666:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32862,DS-166c381e-0313-4c20-81bb-a1ca0e61836e,DISK], DatanodeInfoWithStorage[127.0.0.1:33985,DS-fc4e5776-78da-4fd0-ac54-5f54a983932d,DISK], DatanodeInfoWithStorage[127.0.0.1:40485,DS-d7793602-cd31-46e0-a7bc-5b6474ba58db,DISK], DatanodeInfoWithStorage[127.0.0.1:37058,DS-4f182411-dad3-48e9-a0c3-e781925fb6db,DISK], DatanodeInfoWithStorage[127.0.0.1:44862,DS-0bba8db7-d778-43bc-93ea-25abab25bb8c,DISK], DatanodeInfoWithStorage[127.0.0.1:36230,DS-cac1d2ff-57be-4c62-ad0f-1fed0d077de2,DISK], DatanodeInfoWithStorage[127.0.0.1:42342,DS-15c98f78-5b47-42c0-87fa-3686fd157320,DISK], DatanodeInfoWithStorage[127.0.0.1:45897,DS-abde6b86-7360-4e92-a564-5ce836d99295,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2097386225-172.17.0.18-1595615713832:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40294,DS-c836e731-7109-4059-9829-512d448e56dd,DISK], DatanodeInfoWithStorage[127.0.0.1:35172,DS-11180e0c-c047-4aa1-90d4-cc2d6451eab3,DISK], DatanodeInfoWithStorage[127.0.0.1:37803,DS-ca775152-5440-455e-9069-df083e585826,DISK], DatanodeInfoWithStorage[127.0.0.1:44972,DS-96b530a8-934f-4348-b0dd-82c48f5893d4,DISK], DatanodeInfoWithStorage[127.0.0.1:41947,DS-32ea5ecd-cac9-4bc9-bd63-c685e7d6d79e,DISK], DatanodeInfoWithStorage[127.0.0.1:45562,DS-89915e0a-f9da-441f-bdf3-9573247fd440,DISK], DatanodeInfoWithStorage[127.0.0.1:36676,DS-57a7c152-3db5-40d9-ba1f-056048072d39,DISK], DatanodeInfoWithStorage[127.0.0.1:37297,DS-59f84062-73e6-435c-988f-692c003c3cb8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2097386225-172.17.0.18-1595615713832:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40294,DS-c836e731-7109-4059-9829-512d448e56dd,DISK], DatanodeInfoWithStorage[127.0.0.1:35172,DS-11180e0c-c047-4aa1-90d4-cc2d6451eab3,DISK], DatanodeInfoWithStorage[127.0.0.1:37803,DS-ca775152-5440-455e-9069-df083e585826,DISK], DatanodeInfoWithStorage[127.0.0.1:44972,DS-96b530a8-934f-4348-b0dd-82c48f5893d4,DISK], DatanodeInfoWithStorage[127.0.0.1:41947,DS-32ea5ecd-cac9-4bc9-bd63-c685e7d6d79e,DISK], DatanodeInfoWithStorage[127.0.0.1:45562,DS-89915e0a-f9da-441f-bdf3-9573247fd440,DISK], DatanodeInfoWithStorage[127.0.0.1:36676,DS-57a7c152-3db5-40d9-ba1f-056048072d39,DISK], DatanodeInfoWithStorage[127.0.0.1:37297,DS-59f84062-73e6-435c-988f-692c003c3cb8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1920647040-172.17.0.18-1595615836393:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41415,DS-0b0e9bf1-7735-4bd5-8543-3ad21ea07676,DISK], DatanodeInfoWithStorage[127.0.0.1:35885,DS-1c73fdea-5752-4f4d-8732-b3f18304bb9d,DISK], DatanodeInfoWithStorage[127.0.0.1:44240,DS-fc6c65c5-3406-4593-b6ea-037fc06647fe,DISK], DatanodeInfoWithStorage[127.0.0.1:39690,DS-eaf4f08a-a03a-4c54-9f6a-19c3536635ec,DISK], DatanodeInfoWithStorage[127.0.0.1:44372,DS-5514318b-1de4-493a-87f2-6f4e700b08cf,DISK], DatanodeInfoWithStorage[127.0.0.1:45724,DS-15bec613-1f90-4b6c-9baf-460558639a08,DISK], DatanodeInfoWithStorage[127.0.0.1:43408,DS-eb662170-da87-478f-8472-6b504ae6371f,DISK], DatanodeInfoWithStorage[127.0.0.1:39850,DS-595dc9c3-090f-45f0-9a3f-b6358f73aa3f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1920647040-172.17.0.18-1595615836393:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41415,DS-0b0e9bf1-7735-4bd5-8543-3ad21ea07676,DISK], DatanodeInfoWithStorage[127.0.0.1:35885,DS-1c73fdea-5752-4f4d-8732-b3f18304bb9d,DISK], DatanodeInfoWithStorage[127.0.0.1:44240,DS-fc6c65c5-3406-4593-b6ea-037fc06647fe,DISK], DatanodeInfoWithStorage[127.0.0.1:39690,DS-eaf4f08a-a03a-4c54-9f6a-19c3536635ec,DISK], DatanodeInfoWithStorage[127.0.0.1:44372,DS-5514318b-1de4-493a-87f2-6f4e700b08cf,DISK], DatanodeInfoWithStorage[127.0.0.1:45724,DS-15bec613-1f90-4b6c-9baf-460558639a08,DISK], DatanodeInfoWithStorage[127.0.0.1:43408,DS-eb662170-da87-478f-8472-6b504ae6371f,DISK], DatanodeInfoWithStorage[127.0.0.1:39850,DS-595dc9c3-090f-45f0-9a3f-b6358f73aa3f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1169390759-172.17.0.18-1595615876989:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39781,DS-6a0cbcd1-9eb0-4228-8025-6d6c8b44640d,DISK], DatanodeInfoWithStorage[127.0.0.1:41804,DS-6c0288aa-1db1-44b9-a793-d0d5158fdfd4,DISK], DatanodeInfoWithStorage[127.0.0.1:35843,DS-317bcbc7-a56a-4d1a-bded-227128e87177,DISK], DatanodeInfoWithStorage[127.0.0.1:37412,DS-a3ea34c0-8417-415e-8b39-e4d09d966f33,DISK], DatanodeInfoWithStorage[127.0.0.1:45545,DS-3a4d613a-f11e-4a46-a65a-66c10c7d2d0c,DISK], DatanodeInfoWithStorage[127.0.0.1:34295,DS-28f489dc-57e2-44ce-8fc6-5b1f193fecce,DISK], DatanodeInfoWithStorage[127.0.0.1:45705,DS-6c568d93-e847-4db2-abea-b1be28255b07,DISK], DatanodeInfoWithStorage[127.0.0.1:39540,DS-fadc68c7-f332-456a-b5de-858eef3ce07d,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1169390759-172.17.0.18-1595615876989:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39781,DS-6a0cbcd1-9eb0-4228-8025-6d6c8b44640d,DISK], DatanodeInfoWithStorage[127.0.0.1:41804,DS-6c0288aa-1db1-44b9-a793-d0d5158fdfd4,DISK], DatanodeInfoWithStorage[127.0.0.1:35843,DS-317bcbc7-a56a-4d1a-bded-227128e87177,DISK], DatanodeInfoWithStorage[127.0.0.1:37412,DS-a3ea34c0-8417-415e-8b39-e4d09d966f33,DISK], DatanodeInfoWithStorage[127.0.0.1:45545,DS-3a4d613a-f11e-4a46-a65a-66c10c7d2d0c,DISK], DatanodeInfoWithStorage[127.0.0.1:34295,DS-28f489dc-57e2-44ce-8fc6-5b1f193fecce,DISK], DatanodeInfoWithStorage[127.0.0.1:45705,DS-6c568d93-e847-4db2-abea-b1be28255b07,DISK], DatanodeInfoWithStorage[127.0.0.1:39540,DS-fadc68c7-f332-456a-b5de-858eef3ce07d,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1010704249-172.17.0.18-1595616095716:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35263,DS-a346a475-15d1-49e5-b313-f76027050bbb,DISK], DatanodeInfoWithStorage[127.0.0.1:36304,DS-c10ec967-aaa9-4532-880a-5d0d074efaf3,DISK], DatanodeInfoWithStorage[127.0.0.1:40997,DS-55bdd7a7-9f06-4a28-a8b4-16b8db672ff4,DISK], DatanodeInfoWithStorage[127.0.0.1:43619,DS-71512747-764f-4d2e-9dd8-8b751b742be4,DISK], DatanodeInfoWithStorage[127.0.0.1:39123,DS-de2de38c-99ab-4925-bc0d-6dfecf592595,DISK], DatanodeInfoWithStorage[127.0.0.1:33633,DS-422c78ff-27e7-4d86-9c52-02dccc5d6162,DISK], DatanodeInfoWithStorage[127.0.0.1:46496,DS-15c8664f-3a6b-4bc0-afd6-2ad5cd30fd7e,DISK], DatanodeInfoWithStorage[127.0.0.1:44288,DS-0fa560aa-d37f-4a36-957a-330ee9f6fa75,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1010704249-172.17.0.18-1595616095716:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35263,DS-a346a475-15d1-49e5-b313-f76027050bbb,DISK], DatanodeInfoWithStorage[127.0.0.1:36304,DS-c10ec967-aaa9-4532-880a-5d0d074efaf3,DISK], DatanodeInfoWithStorage[127.0.0.1:40997,DS-55bdd7a7-9f06-4a28-a8b4-16b8db672ff4,DISK], DatanodeInfoWithStorage[127.0.0.1:43619,DS-71512747-764f-4d2e-9dd8-8b751b742be4,DISK], DatanodeInfoWithStorage[127.0.0.1:39123,DS-de2de38c-99ab-4925-bc0d-6dfecf592595,DISK], DatanodeInfoWithStorage[127.0.0.1:33633,DS-422c78ff-27e7-4d86-9c52-02dccc5d6162,DISK], DatanodeInfoWithStorage[127.0.0.1:46496,DS-15c8664f-3a6b-4bc0-afd6-2ad5cd30fd7e,DISK], DatanodeInfoWithStorage[127.0.0.1:44288,DS-0fa560aa-d37f-4a36-957a-330ee9f6fa75,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1127416185-172.17.0.18-1595616311320:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37904,DS-bbbdfe03-1d6a-41d2-8471-cf9d653bf1fa,DISK], DatanodeInfoWithStorage[127.0.0.1:38810,DS-7e809a9e-15bf-4c09-bf44-61b9f2171e1c,DISK], DatanodeInfoWithStorage[127.0.0.1:45403,DS-6e4c1173-6bd0-4842-ae27-b41c49522106,DISK], DatanodeInfoWithStorage[127.0.0.1:44957,DS-8f3eba9d-ddc7-4c90-9532-18b693c80cbd,DISK], DatanodeInfoWithStorage[127.0.0.1:43678,DS-c4ae91a5-23a3-4e3c-9a5a-c488ef6a0b49,DISK], DatanodeInfoWithStorage[127.0.0.1:34108,DS-08c0ead4-e650-41a7-b138-10d38d1e3948,DISK], DatanodeInfoWithStorage[127.0.0.1:39196,DS-7c5e79a5-ce16-44d6-b4c7-7df7ed7f0ea8,DISK], DatanodeInfoWithStorage[127.0.0.1:37112,DS-bb89e594-7663-4e72-94af-aed3f56d2553,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1127416185-172.17.0.18-1595616311320:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37904,DS-bbbdfe03-1d6a-41d2-8471-cf9d653bf1fa,DISK], DatanodeInfoWithStorage[127.0.0.1:38810,DS-7e809a9e-15bf-4c09-bf44-61b9f2171e1c,DISK], DatanodeInfoWithStorage[127.0.0.1:45403,DS-6e4c1173-6bd0-4842-ae27-b41c49522106,DISK], DatanodeInfoWithStorage[127.0.0.1:44957,DS-8f3eba9d-ddc7-4c90-9532-18b693c80cbd,DISK], DatanodeInfoWithStorage[127.0.0.1:43678,DS-c4ae91a5-23a3-4e3c-9a5a-c488ef6a0b49,DISK], DatanodeInfoWithStorage[127.0.0.1:34108,DS-08c0ead4-e650-41a7-b138-10d38d1e3948,DISK], DatanodeInfoWithStorage[127.0.0.1:39196,DS-7c5e79a5-ce16-44d6-b4c7-7df7ed7f0ea8,DISK], DatanodeInfoWithStorage[127.0.0.1:37112,DS-bb89e594-7663-4e72-94af-aed3f56d2553,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1888811709-172.17.0.18-1595616418897:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33093,DS-91c9423d-350f-4ae0-9c42-bcde4cccfb95,DISK], DatanodeInfoWithStorage[127.0.0.1:42657,DS-53fdeba9-0f33-44b1-beee-cb40c05744ad,DISK], DatanodeInfoWithStorage[127.0.0.1:36047,DS-80a8a568-ac4d-46e1-90e7-330ec7c8c7e2,DISK], DatanodeInfoWithStorage[127.0.0.1:36583,DS-ee06d281-449c-4671-a171-58a388f02d83,DISK], DatanodeInfoWithStorage[127.0.0.1:34914,DS-87a74680-d846-4a17-be29-efd0dcdff043,DISK], DatanodeInfoWithStorage[127.0.0.1:45136,DS-6060d050-57b1-4b95-a6f3-4a301bbcc8e2,DISK], DatanodeInfoWithStorage[127.0.0.1:34210,DS-f4a0088a-3e2b-4a2c-8f6f-8149ca0d62e9,DISK], DatanodeInfoWithStorage[127.0.0.1:44124,DS-963d92d1-d090-4854-8e15-b52a887aeb61,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1888811709-172.17.0.18-1595616418897:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33093,DS-91c9423d-350f-4ae0-9c42-bcde4cccfb95,DISK], DatanodeInfoWithStorage[127.0.0.1:42657,DS-53fdeba9-0f33-44b1-beee-cb40c05744ad,DISK], DatanodeInfoWithStorage[127.0.0.1:36047,DS-80a8a568-ac4d-46e1-90e7-330ec7c8c7e2,DISK], DatanodeInfoWithStorage[127.0.0.1:36583,DS-ee06d281-449c-4671-a171-58a388f02d83,DISK], DatanodeInfoWithStorage[127.0.0.1:34914,DS-87a74680-d846-4a17-be29-efd0dcdff043,DISK], DatanodeInfoWithStorage[127.0.0.1:45136,DS-6060d050-57b1-4b95-a6f3-4a301bbcc8e2,DISK], DatanodeInfoWithStorage[127.0.0.1:34210,DS-f4a0088a-3e2b-4a2c-8f6f-8149ca0d62e9,DISK], DatanodeInfoWithStorage[127.0.0.1:44124,DS-963d92d1-d090-4854-8e15-b52a887aeb61,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1949038191-172.17.0.18-1595616565367:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33292,DS-c517259b-dddb-4a13-9af5-c2b2f31f7fe7,DISK], DatanodeInfoWithStorage[127.0.0.1:36460,DS-26613717-c116-4971-88f7-a15534a1d3b9,DISK], DatanodeInfoWithStorage[127.0.0.1:35661,DS-8e558a1b-ed97-4583-83d8-1315185bb2e3,DISK], DatanodeInfoWithStorage[127.0.0.1:33778,DS-0bc271cf-a1ef-4bcd-be4d-d061aa1ba30f,DISK], DatanodeInfoWithStorage[127.0.0.1:33648,DS-ddbf3529-ee39-4677-a5bd-8bb9fcf5ce3e,DISK], DatanodeInfoWithStorage[127.0.0.1:38709,DS-79d042e6-440c-447c-8a84-2e1541766f51,DISK], DatanodeInfoWithStorage[127.0.0.1:39070,DS-7d3d0018-0501-4af8-8e13-21317539b615,DISK], DatanodeInfoWithStorage[127.0.0.1:46428,DS-14df5725-72bd-4340-80c0-459a9c427c20,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1949038191-172.17.0.18-1595616565367:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33292,DS-c517259b-dddb-4a13-9af5-c2b2f31f7fe7,DISK], DatanodeInfoWithStorage[127.0.0.1:36460,DS-26613717-c116-4971-88f7-a15534a1d3b9,DISK], DatanodeInfoWithStorage[127.0.0.1:35661,DS-8e558a1b-ed97-4583-83d8-1315185bb2e3,DISK], DatanodeInfoWithStorage[127.0.0.1:33778,DS-0bc271cf-a1ef-4bcd-be4d-d061aa1ba30f,DISK], DatanodeInfoWithStorage[127.0.0.1:33648,DS-ddbf3529-ee39-4677-a5bd-8bb9fcf5ce3e,DISK], DatanodeInfoWithStorage[127.0.0.1:38709,DS-79d042e6-440c-447c-8a84-2e1541766f51,DISK], DatanodeInfoWithStorage[127.0.0.1:39070,DS-7d3d0018-0501-4af8-8e13-21317539b615,DISK], DatanodeInfoWithStorage[127.0.0.1:46428,DS-14df5725-72bd-4340-80c0-459a9c427c20,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 15 out of 50
v1v1v2v2 failed with probability 20 out of 50
result: false positive !!!
Total execution time in seconds : 5414

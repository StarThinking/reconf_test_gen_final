reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-175844025-172.17.0.18-1595978650621:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34148,DS-06a8869b-7b39-45c5-ae81-c51479375d96,DISK], DatanodeInfoWithStorage[127.0.0.1:37460,DS-4caf9acd-309c-476a-857f-f1af19c95f77,DISK], DatanodeInfoWithStorage[127.0.0.1:43919,DS-d22e0a52-c67f-493a-9e89-12f898976aea,DISK], DatanodeInfoWithStorage[127.0.0.1:46458,DS-ca8c61d8-10ef-4709-8531-c0c4e1f53079,DISK], DatanodeInfoWithStorage[127.0.0.1:44633,DS-aa0fce6b-7577-4d6d-bb6e-03460e810374,DISK], DatanodeInfoWithStorage[127.0.0.1:40303,DS-a542734f-dfd9-4f18-8ea5-f24c2b738590,DISK], DatanodeInfoWithStorage[127.0.0.1:37612,DS-1246182a-59ed-447b-8714-b27d5d571f5b,DISK], DatanodeInfoWithStorage[127.0.0.1:37151,DS-f8e7b2af-06f8-4ab6-a00d-d1aa87fe4d12,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-175844025-172.17.0.18-1595978650621:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34148,DS-06a8869b-7b39-45c5-ae81-c51479375d96,DISK], DatanodeInfoWithStorage[127.0.0.1:37460,DS-4caf9acd-309c-476a-857f-f1af19c95f77,DISK], DatanodeInfoWithStorage[127.0.0.1:43919,DS-d22e0a52-c67f-493a-9e89-12f898976aea,DISK], DatanodeInfoWithStorage[127.0.0.1:46458,DS-ca8c61d8-10ef-4709-8531-c0c4e1f53079,DISK], DatanodeInfoWithStorage[127.0.0.1:44633,DS-aa0fce6b-7577-4d6d-bb6e-03460e810374,DISK], DatanodeInfoWithStorage[127.0.0.1:40303,DS-a542734f-dfd9-4f18-8ea5-f24c2b738590,DISK], DatanodeInfoWithStorage[127.0.0.1:37612,DS-1246182a-59ed-447b-8714-b27d5d571f5b,DISK], DatanodeInfoWithStorage[127.0.0.1:37151,DS-f8e7b2af-06f8-4ab6-a00d-d1aa87fe4d12,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-473303855-172.17.0.18-1595978791134:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39625,DS-d3dead63-abd3-4d72-8f1b-cfb74d4161fb,DISK], DatanodeInfoWithStorage[127.0.0.1:35292,DS-14d709d7-2742-475a-a7e5-b0fe8f5bcd4f,DISK], DatanodeInfoWithStorage[127.0.0.1:42727,DS-6aaf7198-05bb-4297-8aeb-aafea47d2774,DISK], DatanodeInfoWithStorage[127.0.0.1:42791,DS-787476a8-a142-4d34-96cf-c1d506da278b,DISK], DatanodeInfoWithStorage[127.0.0.1:33669,DS-d6542018-88cc-4441-a0ba-bd9e74a74060,DISK], DatanodeInfoWithStorage[127.0.0.1:38632,DS-68273e04-d6dd-48b0-bad7-54d92f2f70a2,DISK], DatanodeInfoWithStorage[127.0.0.1:40942,DS-92f22c3f-3f1f-4501-91e5-8d8368eaa9bd,DISK], DatanodeInfoWithStorage[127.0.0.1:41450,DS-61777f60-7207-4f31-8c6c-a7b3f9c4b0d2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-473303855-172.17.0.18-1595978791134:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39625,DS-d3dead63-abd3-4d72-8f1b-cfb74d4161fb,DISK], DatanodeInfoWithStorage[127.0.0.1:35292,DS-14d709d7-2742-475a-a7e5-b0fe8f5bcd4f,DISK], DatanodeInfoWithStorage[127.0.0.1:42727,DS-6aaf7198-05bb-4297-8aeb-aafea47d2774,DISK], DatanodeInfoWithStorage[127.0.0.1:42791,DS-787476a8-a142-4d34-96cf-c1d506da278b,DISK], DatanodeInfoWithStorage[127.0.0.1:33669,DS-d6542018-88cc-4441-a0ba-bd9e74a74060,DISK], DatanodeInfoWithStorage[127.0.0.1:38632,DS-68273e04-d6dd-48b0-bad7-54d92f2f70a2,DISK], DatanodeInfoWithStorage[127.0.0.1:40942,DS-92f22c3f-3f1f-4501-91e5-8d8368eaa9bd,DISK], DatanodeInfoWithStorage[127.0.0.1:41450,DS-61777f60-7207-4f31-8c6c-a7b3f9c4b0d2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2048076124-172.17.0.18-1595979400709:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38747,DS-bb72b097-c108-4782-a5e4-4c5bcc366d49,DISK], DatanodeInfoWithStorage[127.0.0.1:34013,DS-aaa30904-3776-4cc7-b58a-fca59cb96a97,DISK], DatanodeInfoWithStorage[127.0.0.1:44808,DS-8d78bc53-8764-45c2-bfb3-b3995bf6b0e5,DISK], DatanodeInfoWithStorage[127.0.0.1:41699,DS-d6012a6d-9b9c-4212-bf24-cd4cf361b72d,DISK], DatanodeInfoWithStorage[127.0.0.1:37904,DS-8ce0c7db-1f6d-4102-b321-b17d78e2a326,DISK], DatanodeInfoWithStorage[127.0.0.1:32850,DS-332ae86c-5abd-47cb-85a7-665677329ef4,DISK], DatanodeInfoWithStorage[127.0.0.1:37945,DS-4eb0ce88-87a7-45c1-b07d-40374c455863,DISK], DatanodeInfoWithStorage[127.0.0.1:45702,DS-12c16716-7b01-4e34-b48a-e80b67386d2b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2048076124-172.17.0.18-1595979400709:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38747,DS-bb72b097-c108-4782-a5e4-4c5bcc366d49,DISK], DatanodeInfoWithStorage[127.0.0.1:34013,DS-aaa30904-3776-4cc7-b58a-fca59cb96a97,DISK], DatanodeInfoWithStorage[127.0.0.1:44808,DS-8d78bc53-8764-45c2-bfb3-b3995bf6b0e5,DISK], DatanodeInfoWithStorage[127.0.0.1:41699,DS-d6012a6d-9b9c-4212-bf24-cd4cf361b72d,DISK], DatanodeInfoWithStorage[127.0.0.1:37904,DS-8ce0c7db-1f6d-4102-b321-b17d78e2a326,DISK], DatanodeInfoWithStorage[127.0.0.1:32850,DS-332ae86c-5abd-47cb-85a7-665677329ef4,DISK], DatanodeInfoWithStorage[127.0.0.1:37945,DS-4eb0ce88-87a7-45c1-b07d-40374c455863,DISK], DatanodeInfoWithStorage[127.0.0.1:45702,DS-12c16716-7b01-4e34-b48a-e80b67386d2b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-539957724-172.17.0.18-1595979467379:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43878,DS-bc32439a-936e-4e7d-9e03-ee91d5d62822,DISK], DatanodeInfoWithStorage[127.0.0.1:34207,DS-9556d394-04a3-40f7-a0b0-24b6a9dbd190,DISK], DatanodeInfoWithStorage[127.0.0.1:33767,DS-4c6a464c-abe1-4741-b7f0-691da961f58d,DISK], DatanodeInfoWithStorage[127.0.0.1:44730,DS-4ee8261b-fa3a-4d23-b801-3cb5ec0b93d4,DISK], DatanodeInfoWithStorage[127.0.0.1:36797,DS-d3b74674-2107-4a58-82f5-794dce4a7a92,DISK], DatanodeInfoWithStorage[127.0.0.1:36495,DS-bddad9a2-2db0-4463-b40d-f6fd6f7f7bde,DISK], DatanodeInfoWithStorage[127.0.0.1:40052,DS-83f0194f-9fd9-4108-9eb3-cf7424549f8b,DISK], DatanodeInfoWithStorage[127.0.0.1:37587,DS-6e980b38-9b8b-494f-962c-6ab214b7c22b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-539957724-172.17.0.18-1595979467379:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43878,DS-bc32439a-936e-4e7d-9e03-ee91d5d62822,DISK], DatanodeInfoWithStorage[127.0.0.1:34207,DS-9556d394-04a3-40f7-a0b0-24b6a9dbd190,DISK], DatanodeInfoWithStorage[127.0.0.1:33767,DS-4c6a464c-abe1-4741-b7f0-691da961f58d,DISK], DatanodeInfoWithStorage[127.0.0.1:44730,DS-4ee8261b-fa3a-4d23-b801-3cb5ec0b93d4,DISK], DatanodeInfoWithStorage[127.0.0.1:36797,DS-d3b74674-2107-4a58-82f5-794dce4a7a92,DISK], DatanodeInfoWithStorage[127.0.0.1:36495,DS-bddad9a2-2db0-4463-b40d-f6fd6f7f7bde,DISK], DatanodeInfoWithStorage[127.0.0.1:40052,DS-83f0194f-9fd9-4108-9eb3-cf7424549f8b,DISK], DatanodeInfoWithStorage[127.0.0.1:37587,DS-6e980b38-9b8b-494f-962c-6ab214b7c22b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-611616122-172.17.0.18-1595979532994:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43342,DS-ad39b0ea-ad5a-45cb-864f-425f2d411833,DISK], DatanodeInfoWithStorage[127.0.0.1:45646,DS-711e1fc5-2d91-4ac8-a8fe-54cd4f1284dd,DISK], DatanodeInfoWithStorage[127.0.0.1:40828,DS-8eae6c7f-af8f-46f7-b49c-271c7fa300b4,DISK], DatanodeInfoWithStorage[127.0.0.1:40269,DS-f920d31c-48ae-4040-b76e-8f27c0d172ee,DISK], DatanodeInfoWithStorage[127.0.0.1:37163,DS-dd3f3eef-1111-4d78-9b46-bb6f3d8d3d68,DISK], DatanodeInfoWithStorage[127.0.0.1:35425,DS-9fdde5e4-9df4-44ff-8de5-3062ffd9f31c,DISK], DatanodeInfoWithStorage[127.0.0.1:36823,DS-e6dc195e-8fb0-453a-a83a-0dc0fcf49881,DISK], DatanodeInfoWithStorage[127.0.0.1:38455,DS-b9505d32-7d35-4256-a7b2-497dba2265b2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-611616122-172.17.0.18-1595979532994:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43342,DS-ad39b0ea-ad5a-45cb-864f-425f2d411833,DISK], DatanodeInfoWithStorage[127.0.0.1:45646,DS-711e1fc5-2d91-4ac8-a8fe-54cd4f1284dd,DISK], DatanodeInfoWithStorage[127.0.0.1:40828,DS-8eae6c7f-af8f-46f7-b49c-271c7fa300b4,DISK], DatanodeInfoWithStorage[127.0.0.1:40269,DS-f920d31c-48ae-4040-b76e-8f27c0d172ee,DISK], DatanodeInfoWithStorage[127.0.0.1:37163,DS-dd3f3eef-1111-4d78-9b46-bb6f3d8d3d68,DISK], DatanodeInfoWithStorage[127.0.0.1:35425,DS-9fdde5e4-9df4-44ff-8de5-3062ffd9f31c,DISK], DatanodeInfoWithStorage[127.0.0.1:36823,DS-e6dc195e-8fb0-453a-a83a-0dc0fcf49881,DISK], DatanodeInfoWithStorage[127.0.0.1:38455,DS-b9505d32-7d35-4256-a7b2-497dba2265b2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1414665638-172.17.0.18-1595980896060:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43849,DS-c13c554c-3e7d-4023-b32a-f8f2e779ece1,DISK], DatanodeInfoWithStorage[127.0.0.1:33552,DS-a5628b31-91b4-45a1-af93-f76afc7180f4,DISK], DatanodeInfoWithStorage[127.0.0.1:45474,DS-3653ab40-4acd-4d6c-97f0-130ca943b8b5,DISK], DatanodeInfoWithStorage[127.0.0.1:34893,DS-88a78cc7-8717-4c44-97fd-7d288cf4fa81,DISK], DatanodeInfoWithStorage[127.0.0.1:41340,DS-bd19eb1c-2b0e-407a-916e-edf55e851046,DISK], DatanodeInfoWithStorage[127.0.0.1:40079,DS-83e1cdcf-0262-4f76-9c4b-7f15f3fba463,DISK], DatanodeInfoWithStorage[127.0.0.1:42335,DS-aa91b003-b820-48d7-a7db-9816a1b69adb,DISK], DatanodeInfoWithStorage[127.0.0.1:38233,DS-9b5ecea2-5911-401f-9a69-eb0259174de8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1414665638-172.17.0.18-1595980896060:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43849,DS-c13c554c-3e7d-4023-b32a-f8f2e779ece1,DISK], DatanodeInfoWithStorage[127.0.0.1:33552,DS-a5628b31-91b4-45a1-af93-f76afc7180f4,DISK], DatanodeInfoWithStorage[127.0.0.1:45474,DS-3653ab40-4acd-4d6c-97f0-130ca943b8b5,DISK], DatanodeInfoWithStorage[127.0.0.1:34893,DS-88a78cc7-8717-4c44-97fd-7d288cf4fa81,DISK], DatanodeInfoWithStorage[127.0.0.1:41340,DS-bd19eb1c-2b0e-407a-916e-edf55e851046,DISK], DatanodeInfoWithStorage[127.0.0.1:40079,DS-83e1cdcf-0262-4f76-9c4b-7f15f3fba463,DISK], DatanodeInfoWithStorage[127.0.0.1:42335,DS-aa91b003-b820-48d7-a7db-9816a1b69adb,DISK], DatanodeInfoWithStorage[127.0.0.1:38233,DS-9b5ecea2-5911-401f-9a69-eb0259174de8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-805768775-172.17.0.18-1595981112817:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36490,DS-76d9ba03-1178-4be1-aaa9-b4f17b38315f,DISK], DatanodeInfoWithStorage[127.0.0.1:41034,DS-fb7ea0f8-ef0a-4f90-9111-5d55ca0249da,DISK], DatanodeInfoWithStorage[127.0.0.1:39153,DS-ce076b38-d157-4f9d-83c8-273fb8cfd58c,DISK], DatanodeInfoWithStorage[127.0.0.1:46452,DS-31bde5cf-b6d1-4c25-91c4-5d1962e7b45d,DISK], DatanodeInfoWithStorage[127.0.0.1:41910,DS-e92ca664-5a0c-44bc-95f0-3d5d7159e891,DISK], DatanodeInfoWithStorage[127.0.0.1:34982,DS-209bde0d-f963-4831-95ec-15aa8900468c,DISK], DatanodeInfoWithStorage[127.0.0.1:32972,DS-01cbfa05-1167-45b3-a869-4ba0fb9fd550,DISK], DatanodeInfoWithStorage[127.0.0.1:36120,DS-979c3644-19e9-420f-b5fa-de6b78d133de,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-805768775-172.17.0.18-1595981112817:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36490,DS-76d9ba03-1178-4be1-aaa9-b4f17b38315f,DISK], DatanodeInfoWithStorage[127.0.0.1:41034,DS-fb7ea0f8-ef0a-4f90-9111-5d55ca0249da,DISK], DatanodeInfoWithStorage[127.0.0.1:39153,DS-ce076b38-d157-4f9d-83c8-273fb8cfd58c,DISK], DatanodeInfoWithStorage[127.0.0.1:46452,DS-31bde5cf-b6d1-4c25-91c4-5d1962e7b45d,DISK], DatanodeInfoWithStorage[127.0.0.1:41910,DS-e92ca664-5a0c-44bc-95f0-3d5d7159e891,DISK], DatanodeInfoWithStorage[127.0.0.1:34982,DS-209bde0d-f963-4831-95ec-15aa8900468c,DISK], DatanodeInfoWithStorage[127.0.0.1:32972,DS-01cbfa05-1167-45b3-a869-4ba0fb9fd550,DISK], DatanodeInfoWithStorage[127.0.0.1:36120,DS-979c3644-19e9-420f-b5fa-de6b78d133de,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-594574369-172.17.0.18-1595981193788:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46354,DS-4ef7c3c7-5038-47aa-8c80-3ada96b4b0df,DISK], DatanodeInfoWithStorage[127.0.0.1:34775,DS-193b4984-947b-4127-90b0-8268210a8cd6,DISK], DatanodeInfoWithStorage[127.0.0.1:40103,DS-f0401e6e-46ef-448f-a32e-3d69e38b2bbe,DISK], DatanodeInfoWithStorage[127.0.0.1:44436,DS-f29426f8-25ea-46fe-ab88-45febf40d670,DISK], DatanodeInfoWithStorage[127.0.0.1:36667,DS-07b933da-c8a2-4178-be35-ee861836ffd3,DISK], DatanodeInfoWithStorage[127.0.0.1:40869,DS-245cbce8-3f8e-4faf-8dac-f543af703474,DISK], DatanodeInfoWithStorage[127.0.0.1:33952,DS-82c949c3-a48a-47de-9886-34e4b34cf04e,DISK], DatanodeInfoWithStorage[127.0.0.1:46151,DS-19449f83-bc21-4a96-a3b5-08110e0161e7,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-594574369-172.17.0.18-1595981193788:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46354,DS-4ef7c3c7-5038-47aa-8c80-3ada96b4b0df,DISK], DatanodeInfoWithStorage[127.0.0.1:34775,DS-193b4984-947b-4127-90b0-8268210a8cd6,DISK], DatanodeInfoWithStorage[127.0.0.1:40103,DS-f0401e6e-46ef-448f-a32e-3d69e38b2bbe,DISK], DatanodeInfoWithStorage[127.0.0.1:44436,DS-f29426f8-25ea-46fe-ab88-45febf40d670,DISK], DatanodeInfoWithStorage[127.0.0.1:36667,DS-07b933da-c8a2-4178-be35-ee861836ffd3,DISK], DatanodeInfoWithStorage[127.0.0.1:40869,DS-245cbce8-3f8e-4faf-8dac-f543af703474,DISK], DatanodeInfoWithStorage[127.0.0.1:33952,DS-82c949c3-a48a-47de-9886-34e4b34cf04e,DISK], DatanodeInfoWithStorage[127.0.0.1:46151,DS-19449f83-bc21-4a96-a3b5-08110e0161e7,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-29398637-172.17.0.18-1595981344208:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41333,DS-9cb4e989-9d37-4804-a9c5-2f1e7f22058d,DISK], DatanodeInfoWithStorage[127.0.0.1:46352,DS-975255ec-af2a-43ef-a826-6964c5eab79d,DISK], DatanodeInfoWithStorage[127.0.0.1:45407,DS-6ef753e2-5ecc-43f8-b93c-afcb29fd48b9,DISK], DatanodeInfoWithStorage[127.0.0.1:34018,DS-f768543c-e653-4e0c-8d96-a5fc8bec7ea5,DISK], DatanodeInfoWithStorage[127.0.0.1:33854,DS-510617f2-5938-4a55-aadc-6d992b5c2899,DISK], DatanodeInfoWithStorage[127.0.0.1:44750,DS-cef4efb6-69dd-4fb3-8f2d-c598ea0fa173,DISK], DatanodeInfoWithStorage[127.0.0.1:38282,DS-a0444ad7-43ee-4d46-8b10-725edc077790,DISK], DatanodeInfoWithStorage[127.0.0.1:32855,DS-c9694eb5-cde9-4e48-be91-71a3f9735874,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-29398637-172.17.0.18-1595981344208:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41333,DS-9cb4e989-9d37-4804-a9c5-2f1e7f22058d,DISK], DatanodeInfoWithStorage[127.0.0.1:46352,DS-975255ec-af2a-43ef-a826-6964c5eab79d,DISK], DatanodeInfoWithStorage[127.0.0.1:45407,DS-6ef753e2-5ecc-43f8-b93c-afcb29fd48b9,DISK], DatanodeInfoWithStorage[127.0.0.1:34018,DS-f768543c-e653-4e0c-8d96-a5fc8bec7ea5,DISK], DatanodeInfoWithStorage[127.0.0.1:33854,DS-510617f2-5938-4a55-aadc-6d992b5c2899,DISK], DatanodeInfoWithStorage[127.0.0.1:44750,DS-cef4efb6-69dd-4fb3-8f2d-c598ea0fa173,DISK], DatanodeInfoWithStorage[127.0.0.1:38282,DS-a0444ad7-43ee-4d46-8b10-725edc077790,DISK], DatanodeInfoWithStorage[127.0.0.1:32855,DS-c9694eb5-cde9-4e48-be91-71a3f9735874,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-593890057-172.17.0.18-1595982249855:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42145,DS-852e94fb-026d-4fc3-b184-97d4678b3f9c,DISK], DatanodeInfoWithStorage[127.0.0.1:38356,DS-0262e8c3-baed-45e9-b999-8274adc7927d,DISK], DatanodeInfoWithStorage[127.0.0.1:37560,DS-4e0665d9-c717-4f47-9b88-db48fe727061,DISK], DatanodeInfoWithStorage[127.0.0.1:37066,DS-758757eb-5333-46f7-8331-6d6f80dd77fb,DISK], DatanodeInfoWithStorage[127.0.0.1:42640,DS-0ff63789-9ce3-44cd-bd22-b5dc2e8a0e13,DISK], DatanodeInfoWithStorage[127.0.0.1:40441,DS-53378abc-dfee-49ea-b5b2-f1f8a988c0d6,DISK], DatanodeInfoWithStorage[127.0.0.1:35696,DS-93998ae1-a020-46cf-8836-dc454ee552b2,DISK], DatanodeInfoWithStorage[127.0.0.1:37375,DS-bee56788-6acf-45fb-91e4-027bb024bb78,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-593890057-172.17.0.18-1595982249855:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42145,DS-852e94fb-026d-4fc3-b184-97d4678b3f9c,DISK], DatanodeInfoWithStorage[127.0.0.1:38356,DS-0262e8c3-baed-45e9-b999-8274adc7927d,DISK], DatanodeInfoWithStorage[127.0.0.1:37560,DS-4e0665d9-c717-4f47-9b88-db48fe727061,DISK], DatanodeInfoWithStorage[127.0.0.1:37066,DS-758757eb-5333-46f7-8331-6d6f80dd77fb,DISK], DatanodeInfoWithStorage[127.0.0.1:42640,DS-0ff63789-9ce3-44cd-bd22-b5dc2e8a0e13,DISK], DatanodeInfoWithStorage[127.0.0.1:40441,DS-53378abc-dfee-49ea-b5b2-f1f8a988c0d6,DISK], DatanodeInfoWithStorage[127.0.0.1:35696,DS-93998ae1-a020-46cf-8836-dc454ee552b2,DISK], DatanodeInfoWithStorage[127.0.0.1:37375,DS-bee56788-6acf-45fb-91e4-027bb024bb78,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1933997533-172.17.0.18-1595982318922:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46413,DS-5c8e00b3-8c5d-452e-a01f-883974978202,DISK], DatanodeInfoWithStorage[127.0.0.1:41073,DS-8ddfc436-787d-475e-8a69-9f09dcb3a811,DISK], DatanodeInfoWithStorage[127.0.0.1:45123,DS-857e434e-2855-4055-8694-0c53ced5aa56,DISK], DatanodeInfoWithStorage[127.0.0.1:33447,DS-10cb8023-97e1-468b-b038-236b34196ab9,DISK], DatanodeInfoWithStorage[127.0.0.1:37562,DS-7ff9ffc4-5c5f-49fe-aa10-2268ba71c508,DISK], DatanodeInfoWithStorage[127.0.0.1:43630,DS-0eec271c-9aa4-43d7-83a7-41b9133b9303,DISK], DatanodeInfoWithStorage[127.0.0.1:41935,DS-cdf9f85e-352c-40a5-b115-8583b51967cf,DISK], DatanodeInfoWithStorage[127.0.0.1:36662,DS-82be4746-9c08-4c39-9b2e-da1f4dba8850,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1933997533-172.17.0.18-1595982318922:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46413,DS-5c8e00b3-8c5d-452e-a01f-883974978202,DISK], DatanodeInfoWithStorage[127.0.0.1:41073,DS-8ddfc436-787d-475e-8a69-9f09dcb3a811,DISK], DatanodeInfoWithStorage[127.0.0.1:45123,DS-857e434e-2855-4055-8694-0c53ced5aa56,DISK], DatanodeInfoWithStorage[127.0.0.1:33447,DS-10cb8023-97e1-468b-b038-236b34196ab9,DISK], DatanodeInfoWithStorage[127.0.0.1:37562,DS-7ff9ffc4-5c5f-49fe-aa10-2268ba71c508,DISK], DatanodeInfoWithStorage[127.0.0.1:43630,DS-0eec271c-9aa4-43d7-83a7-41b9133b9303,DISK], DatanodeInfoWithStorage[127.0.0.1:41935,DS-cdf9f85e-352c-40a5-b115-8583b51967cf,DISK], DatanodeInfoWithStorage[127.0.0.1:36662,DS-82be4746-9c08-4c39-9b2e-da1f4dba8850,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-650357316-172.17.0.18-1595982379267:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42559,DS-8307edf9-7e56-41d6-9f98-e9f7a4c2850b,DISK], DatanodeInfoWithStorage[127.0.0.1:33017,DS-fcb5b040-8369-4e1e-baae-4c60dad7fc3e,DISK], DatanodeInfoWithStorage[127.0.0.1:42931,DS-bc687d2a-3ac1-4ba6-9bd4-145708b5543b,DISK], DatanodeInfoWithStorage[127.0.0.1:46570,DS-42950f66-e10e-4876-85f3-d88887496f1d,DISK], DatanodeInfoWithStorage[127.0.0.1:44406,DS-ecd3efb0-870f-40f3-8b0d-3254d8310730,DISK], DatanodeInfoWithStorage[127.0.0.1:40843,DS-e980f9f1-35a6-488a-ae90-e4d3795b9fb7,DISK], DatanodeInfoWithStorage[127.0.0.1:43237,DS-a6f92b17-c952-49b6-90df-d85b1c1a210a,DISK], DatanodeInfoWithStorage[127.0.0.1:41499,DS-1c121299-5ac2-4ea7-9f53-ade4b0f9c1bc,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-650357316-172.17.0.18-1595982379267:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42559,DS-8307edf9-7e56-41d6-9f98-e9f7a4c2850b,DISK], DatanodeInfoWithStorage[127.0.0.1:33017,DS-fcb5b040-8369-4e1e-baae-4c60dad7fc3e,DISK], DatanodeInfoWithStorage[127.0.0.1:42931,DS-bc687d2a-3ac1-4ba6-9bd4-145708b5543b,DISK], DatanodeInfoWithStorage[127.0.0.1:46570,DS-42950f66-e10e-4876-85f3-d88887496f1d,DISK], DatanodeInfoWithStorage[127.0.0.1:44406,DS-ecd3efb0-870f-40f3-8b0d-3254d8310730,DISK], DatanodeInfoWithStorage[127.0.0.1:40843,DS-e980f9f1-35a6-488a-ae90-e4d3795b9fb7,DISK], DatanodeInfoWithStorage[127.0.0.1:43237,DS-a6f92b17-c952-49b6-90df-d85b1c1a210a,DISK], DatanodeInfoWithStorage[127.0.0.1:41499,DS-1c121299-5ac2-4ea7-9f53-ade4b0f9c1bc,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-867565316-172.17.0.18-1595983255500:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39245,DS-1d17aab1-39e1-41f2-872e-745d40ce741b,DISK], DatanodeInfoWithStorage[127.0.0.1:34441,DS-0e8f8188-d9fc-4487-b1bd-ca1dfd9c6817,DISK], DatanodeInfoWithStorage[127.0.0.1:36156,DS-c931558c-8dc9-47b1-b8d0-e6f3d6f4bcd2,DISK], DatanodeInfoWithStorage[127.0.0.1:34341,DS-3d93bda2-96ab-4fc0-bf80-e2135fc0c9d3,DISK], DatanodeInfoWithStorage[127.0.0.1:34468,DS-d5631a6f-ae89-4507-aea8-a0688c77dbb2,DISK], DatanodeInfoWithStorage[127.0.0.1:38711,DS-b16fdce1-1c46-45e5-8711-cc676fad3703,DISK], DatanodeInfoWithStorage[127.0.0.1:37163,DS-ab2fbd5e-a2c6-43b5-a9b9-b04bf1bace45,DISK], DatanodeInfoWithStorage[127.0.0.1:36214,DS-1231c07e-cd79-44c8-a52b-4ff6d5cc5c25,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-867565316-172.17.0.18-1595983255500:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39245,DS-1d17aab1-39e1-41f2-872e-745d40ce741b,DISK], DatanodeInfoWithStorage[127.0.0.1:34441,DS-0e8f8188-d9fc-4487-b1bd-ca1dfd9c6817,DISK], DatanodeInfoWithStorage[127.0.0.1:36156,DS-c931558c-8dc9-47b1-b8d0-e6f3d6f4bcd2,DISK], DatanodeInfoWithStorage[127.0.0.1:34341,DS-3d93bda2-96ab-4fc0-bf80-e2135fc0c9d3,DISK], DatanodeInfoWithStorage[127.0.0.1:34468,DS-d5631a6f-ae89-4507-aea8-a0688c77dbb2,DISK], DatanodeInfoWithStorage[127.0.0.1:38711,DS-b16fdce1-1c46-45e5-8711-cc676fad3703,DISK], DatanodeInfoWithStorage[127.0.0.1:37163,DS-ab2fbd5e-a2c6-43b5-a9b9-b04bf1bace45,DISK], DatanodeInfoWithStorage[127.0.0.1:36214,DS-1231c07e-cd79-44c8-a52b-4ff6d5cc5c25,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1297505599-172.17.0.18-1595983400519:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35587,DS-3178942d-6aea-43ad-a409-bd797289aebb,DISK], DatanodeInfoWithStorage[127.0.0.1:41610,DS-60e1b686-7206-4920-97e1-9299f8961eba,DISK], DatanodeInfoWithStorage[127.0.0.1:45669,DS-f32882ae-dc2f-4575-9cf0-0e30a1a54a9a,DISK], DatanodeInfoWithStorage[127.0.0.1:38647,DS-cd7154b7-b09f-42cd-ac66-85c2347a11d0,DISK], DatanodeInfoWithStorage[127.0.0.1:40069,DS-cc6aaa58-9f9b-4b8c-a3bd-23c0e525383b,DISK], DatanodeInfoWithStorage[127.0.0.1:46476,DS-7cccf4fe-e04a-4a75-947f-80f2ca4d4215,DISK], DatanodeInfoWithStorage[127.0.0.1:37660,DS-fd189241-6dcc-476a-bcd1-86b7b9a4a838,DISK], DatanodeInfoWithStorage[127.0.0.1:36824,DS-c8135ff4-a1fe-4923-8905-37590d04b14e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1297505599-172.17.0.18-1595983400519:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35587,DS-3178942d-6aea-43ad-a409-bd797289aebb,DISK], DatanodeInfoWithStorage[127.0.0.1:41610,DS-60e1b686-7206-4920-97e1-9299f8961eba,DISK], DatanodeInfoWithStorage[127.0.0.1:45669,DS-f32882ae-dc2f-4575-9cf0-0e30a1a54a9a,DISK], DatanodeInfoWithStorage[127.0.0.1:38647,DS-cd7154b7-b09f-42cd-ac66-85c2347a11d0,DISK], DatanodeInfoWithStorage[127.0.0.1:40069,DS-cc6aaa58-9f9b-4b8c-a3bd-23c0e525383b,DISK], DatanodeInfoWithStorage[127.0.0.1:46476,DS-7cccf4fe-e04a-4a75-947f-80f2ca4d4215,DISK], DatanodeInfoWithStorage[127.0.0.1:37660,DS-fd189241-6dcc-476a-bcd1-86b7b9a4a838,DISK], DatanodeInfoWithStorage[127.0.0.1:36824,DS-c8135ff4-a1fe-4923-8905-37590d04b14e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.server.listen.queue.size
component: hdfs:DataNode
v1: 128
v2: 2
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1040229011-172.17.0.18-1595983543100:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39104,DS-0dddfd44-eaf1-4113-aeab-bae47f9477ce,DISK], DatanodeInfoWithStorage[127.0.0.1:43746,DS-66b7ff52-664f-410c-9f5c-7851e1b86d91,DISK], DatanodeInfoWithStorage[127.0.0.1:35282,DS-c0842105-1ace-45b9-9325-1bbc84eaa727,DISK], DatanodeInfoWithStorage[127.0.0.1:36029,DS-951dd4c7-525c-42b7-b816-5a2d7c7e8596,DISK], DatanodeInfoWithStorage[127.0.0.1:41984,DS-82879b13-5263-45d9-be24-ba723b549dc7,DISK], DatanodeInfoWithStorage[127.0.0.1:35941,DS-86d4b6eb-3516-4cc9-aebd-d617084c4f3c,DISK], DatanodeInfoWithStorage[127.0.0.1:35893,DS-9ae5df51-0f24-47a4-a0e7-8f843c75c0bd,DISK], DatanodeInfoWithStorage[127.0.0.1:38960,DS-7f9dc7af-aa35-4f0d-9ff7-4c426371ed18,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1040229011-172.17.0.18-1595983543100:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39104,DS-0dddfd44-eaf1-4113-aeab-bae47f9477ce,DISK], DatanodeInfoWithStorage[127.0.0.1:43746,DS-66b7ff52-664f-410c-9f5c-7851e1b86d91,DISK], DatanodeInfoWithStorage[127.0.0.1:35282,DS-c0842105-1ace-45b9-9325-1bbc84eaa727,DISK], DatanodeInfoWithStorage[127.0.0.1:36029,DS-951dd4c7-525c-42b7-b816-5a2d7c7e8596,DISK], DatanodeInfoWithStorage[127.0.0.1:41984,DS-82879b13-5263-45d9-be24-ba723b549dc7,DISK], DatanodeInfoWithStorage[127.0.0.1:35941,DS-86d4b6eb-3516-4cc9-aebd-d617084c4f3c,DISK], DatanodeInfoWithStorage[127.0.0.1:35893,DS-9ae5df51-0f24-47a4-a0e7-8f843c75c0bd,DISK], DatanodeInfoWithStorage[127.0.0.1:38960,DS-7f9dc7af-aa35-4f0d-9ff7-4c426371ed18,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


v1v2 failed with probability 2 out of 50
v1v1v2v2 failed with probability 13 out of 50
result: false positive !!!
Total execution time in seconds : 5207

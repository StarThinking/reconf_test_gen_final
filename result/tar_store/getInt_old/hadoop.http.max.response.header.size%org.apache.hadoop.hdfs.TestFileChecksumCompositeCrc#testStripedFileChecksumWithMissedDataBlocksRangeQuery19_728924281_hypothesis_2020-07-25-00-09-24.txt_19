reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1103561886-172.17.0.13-1595636215951:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38947,DS-83ac3ed5-4027-4410-a51b-9965fb8ccc22,DISK], DatanodeInfoWithStorage[127.0.0.1:32771,DS-d4af3919-7fe1-4d64-813b-74c906c5d8a6,DISK], DatanodeInfoWithStorage[127.0.0.1:39566,DS-7de5d0db-2804-47b0-8133-8ddbaa004c26,DISK], DatanodeInfoWithStorage[127.0.0.1:43836,DS-2901c3e7-8f77-4fd0-b24b-4b123bd91f29,DISK], DatanodeInfoWithStorage[127.0.0.1:43362,DS-7ddef727-eec4-4569-9a6d-1729ec89a097,DISK], DatanodeInfoWithStorage[127.0.0.1:40266,DS-2510f26b-3e66-4d5d-97d3-60498eecda8c,DISK], DatanodeInfoWithStorage[127.0.0.1:41793,DS-e6eb8cab-9f27-4bcd-b607-42cdf6defc8c,DISK], DatanodeInfoWithStorage[127.0.0.1:33482,DS-695dfbd6-1b6c-421d-b731-4d5f747f85e0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1103561886-172.17.0.13-1595636215951:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38947,DS-83ac3ed5-4027-4410-a51b-9965fb8ccc22,DISK], DatanodeInfoWithStorage[127.0.0.1:32771,DS-d4af3919-7fe1-4d64-813b-74c906c5d8a6,DISK], DatanodeInfoWithStorage[127.0.0.1:39566,DS-7de5d0db-2804-47b0-8133-8ddbaa004c26,DISK], DatanodeInfoWithStorage[127.0.0.1:43836,DS-2901c3e7-8f77-4fd0-b24b-4b123bd91f29,DISK], DatanodeInfoWithStorage[127.0.0.1:43362,DS-7ddef727-eec4-4569-9a6d-1729ec89a097,DISK], DatanodeInfoWithStorage[127.0.0.1:40266,DS-2510f26b-3e66-4d5d-97d3-60498eecda8c,DISK], DatanodeInfoWithStorage[127.0.0.1:41793,DS-e6eb8cab-9f27-4bcd-b607-42cdf6defc8c,DISK], DatanodeInfoWithStorage[127.0.0.1:33482,DS-695dfbd6-1b6c-421d-b731-4d5f747f85e0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-737488299-172.17.0.13-1595636257253:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40914,DS-793214d6-1b93-47af-b0c5-106ef8d1afa4,DISK], DatanodeInfoWithStorage[127.0.0.1:42958,DS-a9d72f74-afac-46b7-937b-dd253d80fe0a,DISK], DatanodeInfoWithStorage[127.0.0.1:41109,DS-b6b8ca19-6076-485a-ab30-4619db7be0f3,DISK], DatanodeInfoWithStorage[127.0.0.1:36636,DS-58493aaf-a0ce-4c19-b0d7-f6b9816c8b7e,DISK], DatanodeInfoWithStorage[127.0.0.1:46587,DS-3a409363-fd97-48c1-bfef-d99810163f9c,DISK], DatanodeInfoWithStorage[127.0.0.1:36170,DS-565f5eb6-4439-4385-8cc9-fdac4966c559,DISK], DatanodeInfoWithStorage[127.0.0.1:44300,DS-5e1981bd-93e1-49bc-bd8c-1d78dd4efa45,DISK], DatanodeInfoWithStorage[127.0.0.1:45619,DS-c762625a-bb42-41a4-b36c-bf4713373337,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-737488299-172.17.0.13-1595636257253:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40914,DS-793214d6-1b93-47af-b0c5-106ef8d1afa4,DISK], DatanodeInfoWithStorage[127.0.0.1:42958,DS-a9d72f74-afac-46b7-937b-dd253d80fe0a,DISK], DatanodeInfoWithStorage[127.0.0.1:41109,DS-b6b8ca19-6076-485a-ab30-4619db7be0f3,DISK], DatanodeInfoWithStorage[127.0.0.1:36636,DS-58493aaf-a0ce-4c19-b0d7-f6b9816c8b7e,DISK], DatanodeInfoWithStorage[127.0.0.1:46587,DS-3a409363-fd97-48c1-bfef-d99810163f9c,DISK], DatanodeInfoWithStorage[127.0.0.1:36170,DS-565f5eb6-4439-4385-8cc9-fdac4966c559,DISK], DatanodeInfoWithStorage[127.0.0.1:44300,DS-5e1981bd-93e1-49bc-bd8c-1d78dd4efa45,DISK], DatanodeInfoWithStorage[127.0.0.1:45619,DS-c762625a-bb42-41a4-b36c-bf4713373337,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1828552542-172.17.0.13-1595637077097:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39152,DS-fb86d38c-4237-44a4-94d5-d749514bcdde,DISK], DatanodeInfoWithStorage[127.0.0.1:34776,DS-3ec39a64-c21b-41fd-b748-34304a6b626d,DISK], DatanodeInfoWithStorage[127.0.0.1:46792,DS-bba4738a-55c6-442c-bf2d-b05f2226d616,DISK], DatanodeInfoWithStorage[127.0.0.1:43800,DS-2bffc877-fc0e-498d-a2b4-1bf722c1d115,DISK], DatanodeInfoWithStorage[127.0.0.1:33688,DS-2a96ca46-6c3e-42b7-bd60-e669c61c3a68,DISK], DatanodeInfoWithStorage[127.0.0.1:39755,DS-8ddfc2f4-dd94-4f70-9a39-e062ac646a18,DISK], DatanodeInfoWithStorage[127.0.0.1:40003,DS-ec36c0b8-6407-43a4-a339-d7ca0b72b0a7,DISK], DatanodeInfoWithStorage[127.0.0.1:39721,DS-dd8ba309-1a2b-42b1-8f3c-6c15dfed0a64,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1828552542-172.17.0.13-1595637077097:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39152,DS-fb86d38c-4237-44a4-94d5-d749514bcdde,DISK], DatanodeInfoWithStorage[127.0.0.1:34776,DS-3ec39a64-c21b-41fd-b748-34304a6b626d,DISK], DatanodeInfoWithStorage[127.0.0.1:46792,DS-bba4738a-55c6-442c-bf2d-b05f2226d616,DISK], DatanodeInfoWithStorage[127.0.0.1:43800,DS-2bffc877-fc0e-498d-a2b4-1bf722c1d115,DISK], DatanodeInfoWithStorage[127.0.0.1:33688,DS-2a96ca46-6c3e-42b7-bd60-e669c61c3a68,DISK], DatanodeInfoWithStorage[127.0.0.1:39755,DS-8ddfc2f4-dd94-4f70-9a39-e062ac646a18,DISK], DatanodeInfoWithStorage[127.0.0.1:40003,DS-ec36c0b8-6407-43a4-a339-d7ca0b72b0a7,DISK], DatanodeInfoWithStorage[127.0.0.1:39721,DS-dd8ba309-1a2b-42b1-8f3c-6c15dfed0a64,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2048077581-172.17.0.13-1595638276048:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43104,DS-bc764acc-cfd8-409f-aa3b-c6e9add55766,DISK], DatanodeInfoWithStorage[127.0.0.1:40922,DS-4d8cd384-904f-4d81-834b-710f19837769,DISK], DatanodeInfoWithStorage[127.0.0.1:46550,DS-50455775-704f-4f6d-b012-deedcef58678,DISK], DatanodeInfoWithStorage[127.0.0.1:33839,DS-08dff04d-3e76-4e34-ae7b-3d62a5e10dc0,DISK], DatanodeInfoWithStorage[127.0.0.1:44047,DS-fc71f9aa-965c-406a-b47c-920f18f05903,DISK], DatanodeInfoWithStorage[127.0.0.1:36843,DS-5ac57af9-8d3b-4203-b98b-7ad722fb7cbf,DISK], DatanodeInfoWithStorage[127.0.0.1:44392,DS-076c70c7-1db0-4953-9a1d-220b77f6f1bb,DISK], DatanodeInfoWithStorage[127.0.0.1:35652,DS-0bfabf8b-e771-4bdd-bdc1-ce248bff2543,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-2048077581-172.17.0.13-1595638276048:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43104,DS-bc764acc-cfd8-409f-aa3b-c6e9add55766,DISK], DatanodeInfoWithStorage[127.0.0.1:40922,DS-4d8cd384-904f-4d81-834b-710f19837769,DISK], DatanodeInfoWithStorage[127.0.0.1:46550,DS-50455775-704f-4f6d-b012-deedcef58678,DISK], DatanodeInfoWithStorage[127.0.0.1:33839,DS-08dff04d-3e76-4e34-ae7b-3d62a5e10dc0,DISK], DatanodeInfoWithStorage[127.0.0.1:44047,DS-fc71f9aa-965c-406a-b47c-920f18f05903,DISK], DatanodeInfoWithStorage[127.0.0.1:36843,DS-5ac57af9-8d3b-4203-b98b-7ad722fb7cbf,DISK], DatanodeInfoWithStorage[127.0.0.1:44392,DS-076c70c7-1db0-4953-9a1d-220b77f6f1bb,DISK], DatanodeInfoWithStorage[127.0.0.1:35652,DS-0bfabf8b-e771-4bdd-bdc1-ce248bff2543,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-281541760-172.17.0.13-1595639519218:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44922,DS-db08b571-b6d0-4e2e-a46d-1c835acfdbdb,DISK], DatanodeInfoWithStorage[127.0.0.1:38471,DS-c1c78ce5-1885-4cba-8e0b-b85b5ba2101f,DISK], DatanodeInfoWithStorage[127.0.0.1:38101,DS-aeaa1298-9de3-408c-9549-c21da8477917,DISK], DatanodeInfoWithStorage[127.0.0.1:41066,DS-98939e8a-1359-4d50-bbb7-78260cd100b5,DISK], DatanodeInfoWithStorage[127.0.0.1:46401,DS-6dc7fe31-e2b7-40df-b925-43fbeda60be5,DISK], DatanodeInfoWithStorage[127.0.0.1:37617,DS-8c41d3a8-eb31-4a55-89e2-408479e6fd09,DISK], DatanodeInfoWithStorage[127.0.0.1:33450,DS-6a279b6d-2411-449d-bd57-0a278fdc8f83,DISK], DatanodeInfoWithStorage[127.0.0.1:42771,DS-9bb7aa4f-a36a-4c21-9926-e3a59525cff8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-281541760-172.17.0.13-1595639519218:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44922,DS-db08b571-b6d0-4e2e-a46d-1c835acfdbdb,DISK], DatanodeInfoWithStorage[127.0.0.1:38471,DS-c1c78ce5-1885-4cba-8e0b-b85b5ba2101f,DISK], DatanodeInfoWithStorage[127.0.0.1:38101,DS-aeaa1298-9de3-408c-9549-c21da8477917,DISK], DatanodeInfoWithStorage[127.0.0.1:41066,DS-98939e8a-1359-4d50-bbb7-78260cd100b5,DISK], DatanodeInfoWithStorage[127.0.0.1:46401,DS-6dc7fe31-e2b7-40df-b925-43fbeda60be5,DISK], DatanodeInfoWithStorage[127.0.0.1:37617,DS-8c41d3a8-eb31-4a55-89e2-408479e6fd09,DISK], DatanodeInfoWithStorage[127.0.0.1:33450,DS-6a279b6d-2411-449d-bd57-0a278fdc8f83,DISK], DatanodeInfoWithStorage[127.0.0.1:42771,DS-9bb7aa4f-a36a-4c21-9926-e3a59525cff8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-589737681-172.17.0.13-1595639908561:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44926,DS-e026ded9-b152-43fd-a1b3-ac47c270f68e,DISK], DatanodeInfoWithStorage[127.0.0.1:38025,DS-7c60934d-5bc9-494d-aa73-ff370e485442,DISK], DatanodeInfoWithStorage[127.0.0.1:41036,DS-192d441e-a33b-4f35-a2a2-0b3d69db3020,DISK], DatanodeInfoWithStorage[127.0.0.1:40783,DS-6cf8320a-deb0-4e5c-b182-a4ba980ec210,DISK], DatanodeInfoWithStorage[127.0.0.1:41728,DS-fec2417d-340d-40a9-b0c7-004e90805788,DISK], DatanodeInfoWithStorage[127.0.0.1:44595,DS-d95ae4ee-236b-4098-ab48-0a356971b197,DISK], DatanodeInfoWithStorage[127.0.0.1:34173,DS-b336b57f-4856-469c-bb2c-a2518f2ebaec,DISK], DatanodeInfoWithStorage[127.0.0.1:42041,DS-ca1bdf7b-36c2-4df1-9d9e-e3d26298c9c5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-589737681-172.17.0.13-1595639908561:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44926,DS-e026ded9-b152-43fd-a1b3-ac47c270f68e,DISK], DatanodeInfoWithStorage[127.0.0.1:38025,DS-7c60934d-5bc9-494d-aa73-ff370e485442,DISK], DatanodeInfoWithStorage[127.0.0.1:41036,DS-192d441e-a33b-4f35-a2a2-0b3d69db3020,DISK], DatanodeInfoWithStorage[127.0.0.1:40783,DS-6cf8320a-deb0-4e5c-b182-a4ba980ec210,DISK], DatanodeInfoWithStorage[127.0.0.1:41728,DS-fec2417d-340d-40a9-b0c7-004e90805788,DISK], DatanodeInfoWithStorage[127.0.0.1:44595,DS-d95ae4ee-236b-4098-ab48-0a356971b197,DISK], DatanodeInfoWithStorage[127.0.0.1:34173,DS-b336b57f-4856-469c-bb2c-a2518f2ebaec,DISK], DatanodeInfoWithStorage[127.0.0.1:42041,DS-ca1bdf7b-36c2-4df1-9d9e-e3d26298c9c5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1908096265-172.17.0.13-1595640055137:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43746,DS-5bd34989-11ce-4fbd-835d-e6faf37ba0a8,DISK], DatanodeInfoWithStorage[127.0.0.1:42544,DS-bfd47399-e502-4d4a-a1ab-5a3fa731d77f,DISK], DatanodeInfoWithStorage[127.0.0.1:37701,DS-7dfe78e2-abd0-4f02-bcb6-28f76ba8a7db,DISK], DatanodeInfoWithStorage[127.0.0.1:36915,DS-1d58f03d-fa82-45fc-bf3b-aceaf3561ad0,DISK], DatanodeInfoWithStorage[127.0.0.1:37321,DS-c378c448-2e5b-48bc-b6cd-0a38e7e6f0ad,DISK], DatanodeInfoWithStorage[127.0.0.1:42870,DS-a5415448-f207-4e47-9154-f718551f504b,DISK], DatanodeInfoWithStorage[127.0.0.1:45930,DS-aadac6b2-21d6-4e51-9ed4-114b7c1a6e15,DISK], DatanodeInfoWithStorage[127.0.0.1:33371,DS-efab6f44-c12d-4cf2-860a-5bdd6b04986b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1908096265-172.17.0.13-1595640055137:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43746,DS-5bd34989-11ce-4fbd-835d-e6faf37ba0a8,DISK], DatanodeInfoWithStorage[127.0.0.1:42544,DS-bfd47399-e502-4d4a-a1ab-5a3fa731d77f,DISK], DatanodeInfoWithStorage[127.0.0.1:37701,DS-7dfe78e2-abd0-4f02-bcb6-28f76ba8a7db,DISK], DatanodeInfoWithStorage[127.0.0.1:36915,DS-1d58f03d-fa82-45fc-bf3b-aceaf3561ad0,DISK], DatanodeInfoWithStorage[127.0.0.1:37321,DS-c378c448-2e5b-48bc-b6cd-0a38e7e6f0ad,DISK], DatanodeInfoWithStorage[127.0.0.1:42870,DS-a5415448-f207-4e47-9154-f718551f504b,DISK], DatanodeInfoWithStorage[127.0.0.1:45930,DS-aadac6b2-21d6-4e51-9ed4-114b7c1a6e15,DISK], DatanodeInfoWithStorage[127.0.0.1:33371,DS-efab6f44-c12d-4cf2-860a-5bdd6b04986b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-496194607-172.17.0.13-1595640363037:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33858,DS-14b7c6b6-dc0c-4147-899a-1a768cc4f20c,DISK], DatanodeInfoWithStorage[127.0.0.1:40886,DS-a94fcd0d-6b58-45db-826b-c036603a11cb,DISK], DatanodeInfoWithStorage[127.0.0.1:45193,DS-4c5e3f5e-b63c-446c-b276-5341a95848b6,DISK], DatanodeInfoWithStorage[127.0.0.1:46393,DS-41166966-38d1-41b4-b4ab-a5b545bf430a,DISK], DatanodeInfoWithStorage[127.0.0.1:38075,DS-a88af820-54a6-4ea1-8a6d-4c3cfda3121a,DISK], DatanodeInfoWithStorage[127.0.0.1:39555,DS-0f80fd91-1c5f-4e3a-ad91-e727ebd318cd,DISK], DatanodeInfoWithStorage[127.0.0.1:37588,DS-5d3d4d56-7dbd-4673-9ef6-9ce14e97d0ad,DISK], DatanodeInfoWithStorage[127.0.0.1:32814,DS-757d677a-d6c1-4134-b664-ff386e7f8fde,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-496194607-172.17.0.13-1595640363037:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33858,DS-14b7c6b6-dc0c-4147-899a-1a768cc4f20c,DISK], DatanodeInfoWithStorage[127.0.0.1:40886,DS-a94fcd0d-6b58-45db-826b-c036603a11cb,DISK], DatanodeInfoWithStorage[127.0.0.1:45193,DS-4c5e3f5e-b63c-446c-b276-5341a95848b6,DISK], DatanodeInfoWithStorage[127.0.0.1:46393,DS-41166966-38d1-41b4-b4ab-a5b545bf430a,DISK], DatanodeInfoWithStorage[127.0.0.1:38075,DS-a88af820-54a6-4ea1-8a6d-4c3cfda3121a,DISK], DatanodeInfoWithStorage[127.0.0.1:39555,DS-0f80fd91-1c5f-4e3a-ad91-e727ebd318cd,DISK], DatanodeInfoWithStorage[127.0.0.1:37588,DS-5d3d4d56-7dbd-4673-9ef6-9ce14e97d0ad,DISK], DatanodeInfoWithStorage[127.0.0.1:32814,DS-757d677a-d6c1-4134-b664-ff386e7f8fde,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1404473154-172.17.0.13-1595640443746:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37129,DS-be833ff8-fed6-4b52-ab4a-3dbd3da59d77,DISK], DatanodeInfoWithStorage[127.0.0.1:46344,DS-6fd9bfe2-4067-4500-a550-5334b88bbe88,DISK], DatanodeInfoWithStorage[127.0.0.1:39356,DS-84fc7394-9615-4ab0-9d2d-fbff2ff8d541,DISK], DatanodeInfoWithStorage[127.0.0.1:46823,DS-b23a43b2-89a3-4de5-9aee-dfe925178576,DISK], DatanodeInfoWithStorage[127.0.0.1:39908,DS-b1c0b7a9-0637-4d03-9467-4c77ba3bffbe,DISK], DatanodeInfoWithStorage[127.0.0.1:37466,DS-a331bc4b-2a27-4f24-8ff3-1ff3120f74b8,DISK], DatanodeInfoWithStorage[127.0.0.1:37438,DS-bc298dec-675b-480d-9700-f825bf800f94,DISK], DatanodeInfoWithStorage[127.0.0.1:40843,DS-37d06416-25a9-484d-9574-e19649e225cf,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1404473154-172.17.0.13-1595640443746:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37129,DS-be833ff8-fed6-4b52-ab4a-3dbd3da59d77,DISK], DatanodeInfoWithStorage[127.0.0.1:46344,DS-6fd9bfe2-4067-4500-a550-5334b88bbe88,DISK], DatanodeInfoWithStorage[127.0.0.1:39356,DS-84fc7394-9615-4ab0-9d2d-fbff2ff8d541,DISK], DatanodeInfoWithStorage[127.0.0.1:46823,DS-b23a43b2-89a3-4de5-9aee-dfe925178576,DISK], DatanodeInfoWithStorage[127.0.0.1:39908,DS-b1c0b7a9-0637-4d03-9467-4c77ba3bffbe,DISK], DatanodeInfoWithStorage[127.0.0.1:37466,DS-a331bc4b-2a27-4f24-8ff3-1ff3120f74b8,DISK], DatanodeInfoWithStorage[127.0.0.1:37438,DS-bc298dec-675b-480d-9700-f825bf800f94,DISK], DatanodeInfoWithStorage[127.0.0.1:40843,DS-37d06416-25a9-484d-9574-e19649e225cf,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1235611127-172.17.0.13-1595641884975:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38297,DS-71f1242c-1857-419a-b5da-a602960a1469,DISK], DatanodeInfoWithStorage[127.0.0.1:37790,DS-96ce2929-1bbf-46aa-9138-ccd74a8ea617,DISK], DatanodeInfoWithStorage[127.0.0.1:36171,DS-4bad9f63-00e1-4fc5-a9e3-c5b09c65ddf1,DISK], DatanodeInfoWithStorage[127.0.0.1:34005,DS-a2dc66ac-b2a2-48b7-8b2f-a360b43c2e16,DISK], DatanodeInfoWithStorage[127.0.0.1:44872,DS-9367c325-1fbf-4fe6-97a0-b9acf09caaad,DISK], DatanodeInfoWithStorage[127.0.0.1:34868,DS-5815616a-4183-47c0-bb98-4953a14ad5ff,DISK], DatanodeInfoWithStorage[127.0.0.1:42680,DS-1f281395-0ce4-4dbb-9f4d-2436db1c5f66,DISK], DatanodeInfoWithStorage[127.0.0.1:45023,DS-0e390012-e187-476c-a25a-512cbe3c7633,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1235611127-172.17.0.13-1595641884975:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38297,DS-71f1242c-1857-419a-b5da-a602960a1469,DISK], DatanodeInfoWithStorage[127.0.0.1:37790,DS-96ce2929-1bbf-46aa-9138-ccd74a8ea617,DISK], DatanodeInfoWithStorage[127.0.0.1:36171,DS-4bad9f63-00e1-4fc5-a9e3-c5b09c65ddf1,DISK], DatanodeInfoWithStorage[127.0.0.1:34005,DS-a2dc66ac-b2a2-48b7-8b2f-a360b43c2e16,DISK], DatanodeInfoWithStorage[127.0.0.1:44872,DS-9367c325-1fbf-4fe6-97a0-b9acf09caaad,DISK], DatanodeInfoWithStorage[127.0.0.1:34868,DS-5815616a-4183-47c0-bb98-4953a14ad5ff,DISK], DatanodeInfoWithStorage[127.0.0.1:42680,DS-1f281395-0ce4-4dbb-9f4d-2436db1c5f66,DISK], DatanodeInfoWithStorage[127.0.0.1:45023,DS-0e390012-e187-476c-a25a-512cbe3c7633,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-945413433-172.17.0.13-1595641924490:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35047,DS-8c8e1ee6-30fe-40ae-bca9-c9831967f530,DISK], DatanodeInfoWithStorage[127.0.0.1:33753,DS-602df8d1-30c5-4141-8c09-d03dba912ec8,DISK], DatanodeInfoWithStorage[127.0.0.1:43277,DS-50817075-f667-4548-a768-36b40c6b25d2,DISK], DatanodeInfoWithStorage[127.0.0.1:45828,DS-fb07f808-a6ee-480e-9820-8b201094ced6,DISK], DatanodeInfoWithStorage[127.0.0.1:33759,DS-c314b79c-ec02-46d7-9b69-c8c47f150fc1,DISK], DatanodeInfoWithStorage[127.0.0.1:40551,DS-809d68d6-1e93-44bb-939f-692958259d91,DISK], DatanodeInfoWithStorage[127.0.0.1:44393,DS-89f03b0f-822c-4512-9927-b3273c0d74ea,DISK], DatanodeInfoWithStorage[127.0.0.1:45070,DS-5a9e22e9-72d9-46e8-9af1-6b8dea403879,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-945413433-172.17.0.13-1595641924490:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35047,DS-8c8e1ee6-30fe-40ae-bca9-c9831967f530,DISK], DatanodeInfoWithStorage[127.0.0.1:33753,DS-602df8d1-30c5-4141-8c09-d03dba912ec8,DISK], DatanodeInfoWithStorage[127.0.0.1:43277,DS-50817075-f667-4548-a768-36b40c6b25d2,DISK], DatanodeInfoWithStorage[127.0.0.1:45828,DS-fb07f808-a6ee-480e-9820-8b201094ced6,DISK], DatanodeInfoWithStorage[127.0.0.1:33759,DS-c314b79c-ec02-46d7-9b69-c8c47f150fc1,DISK], DatanodeInfoWithStorage[127.0.0.1:40551,DS-809d68d6-1e93-44bb-939f-692958259d91,DISK], DatanodeInfoWithStorage[127.0.0.1:44393,DS-89f03b0f-822c-4512-9927-b3273c0d74ea,DISK], DatanodeInfoWithStorage[127.0.0.1:45070,DS-5a9e22e9-72d9-46e8-9af1-6b8dea403879,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1701349297-172.17.0.13-1595642011214:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42092,DS-ae9c0ccf-21e1-479b-9fe0-adddf414f9c6,DISK], DatanodeInfoWithStorage[127.0.0.1:33488,DS-d0a9e3d9-cc78-4ae2-8dab-aa4d5adab35e,DISK], DatanodeInfoWithStorage[127.0.0.1:38325,DS-7c43f89a-9b6c-432d-b993-089a48983abe,DISK], DatanodeInfoWithStorage[127.0.0.1:41349,DS-d23bf8fc-65f1-41fb-8161-5fc7a9dff20b,DISK], DatanodeInfoWithStorage[127.0.0.1:40546,DS-a4ea29c5-4937-4757-b19c-8ad79d650e21,DISK], DatanodeInfoWithStorage[127.0.0.1:36244,DS-422660bd-b73b-4138-b80e-b9a96a860b14,DISK], DatanodeInfoWithStorage[127.0.0.1:38048,DS-4cd5faac-9704-430e-ad92-37fd1e9fe2e8,DISK], DatanodeInfoWithStorage[127.0.0.1:34797,DS-d72159d2-c624-40bc-adb8-9ed3c55a88f0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1701349297-172.17.0.13-1595642011214:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42092,DS-ae9c0ccf-21e1-479b-9fe0-adddf414f9c6,DISK], DatanodeInfoWithStorage[127.0.0.1:33488,DS-d0a9e3d9-cc78-4ae2-8dab-aa4d5adab35e,DISK], DatanodeInfoWithStorage[127.0.0.1:38325,DS-7c43f89a-9b6c-432d-b993-089a48983abe,DISK], DatanodeInfoWithStorage[127.0.0.1:41349,DS-d23bf8fc-65f1-41fb-8161-5fc7a9dff20b,DISK], DatanodeInfoWithStorage[127.0.0.1:40546,DS-a4ea29c5-4937-4757-b19c-8ad79d650e21,DISK], DatanodeInfoWithStorage[127.0.0.1:36244,DS-422660bd-b73b-4138-b80e-b9a96a860b14,DISK], DatanodeInfoWithStorage[127.0.0.1:38048,DS-4cd5faac-9704-430e-ad92-37fd1e9fe2e8,DISK], DatanodeInfoWithStorage[127.0.0.1:34797,DS-d72159d2-c624-40bc-adb8-9ed3c55a88f0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-878851693-172.17.0.13-1595642108712:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37122,DS-934a61cd-ec70-48a2-bfdc-b13c53ba4e2c,DISK], DatanodeInfoWithStorage[127.0.0.1:33993,DS-9e75181d-0eee-4e8e-b2a1-315a8bfc4ca1,DISK], DatanodeInfoWithStorage[127.0.0.1:36862,DS-9f4ecbc6-1856-4299-8b46-6d1b9a6c44d8,DISK], DatanodeInfoWithStorage[127.0.0.1:35214,DS-1ef460dc-cdee-451a-b259-7acd2d866228,DISK], DatanodeInfoWithStorage[127.0.0.1:37653,DS-74c69c6d-312a-482b-9873-cef985f2973b,DISK], DatanodeInfoWithStorage[127.0.0.1:36170,DS-0924a104-727e-4c1a-a942-a22d47b8ace4,DISK], DatanodeInfoWithStorage[127.0.0.1:41311,DS-196d02e7-45eb-42b0-82da-9698dc85e9a0,DISK], DatanodeInfoWithStorage[127.0.0.1:37032,DS-15af8890-1ebf-49a5-9607-9a8ab0192134,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-878851693-172.17.0.13-1595642108712:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37122,DS-934a61cd-ec70-48a2-bfdc-b13c53ba4e2c,DISK], DatanodeInfoWithStorage[127.0.0.1:33993,DS-9e75181d-0eee-4e8e-b2a1-315a8bfc4ca1,DISK], DatanodeInfoWithStorage[127.0.0.1:36862,DS-9f4ecbc6-1856-4299-8b46-6d1b9a6c44d8,DISK], DatanodeInfoWithStorage[127.0.0.1:35214,DS-1ef460dc-cdee-451a-b259-7acd2d866228,DISK], DatanodeInfoWithStorage[127.0.0.1:37653,DS-74c69c6d-312a-482b-9873-cef985f2973b,DISK], DatanodeInfoWithStorage[127.0.0.1:36170,DS-0924a104-727e-4c1a-a942-a22d47b8ace4,DISK], DatanodeInfoWithStorage[127.0.0.1:41311,DS-196d02e7-45eb-42b0-82da-9698dc85e9a0,DISK], DatanodeInfoWithStorage[127.0.0.1:37032,DS-15af8890-1ebf-49a5-9607-9a8ab0192134,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-188450316-172.17.0.13-1595642152869:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37292,DS-c3b2954c-1161-437e-8455-e7a6a1774a9d,DISK], DatanodeInfoWithStorage[127.0.0.1:40262,DS-85a24482-e42b-4149-9882-78e6ad2c9025,DISK], DatanodeInfoWithStorage[127.0.0.1:41812,DS-26e103f3-17e1-4f1d-a58d-a6bf80abd71f,DISK], DatanodeInfoWithStorage[127.0.0.1:36258,DS-25521793-99f8-4d98-a15f-e6c5fcc81567,DISK], DatanodeInfoWithStorage[127.0.0.1:33114,DS-6caf61eb-9e58-4eb5-84aa-ef4fd07b163c,DISK], DatanodeInfoWithStorage[127.0.0.1:42531,DS-77041489-af29-4d50-b6a2-142f857f267f,DISK], DatanodeInfoWithStorage[127.0.0.1:39104,DS-8bbffb47-c518-4605-b1ed-0ab2a0000678,DISK], DatanodeInfoWithStorage[127.0.0.1:33214,DS-c5aab203-d982-44d1-bdec-0704a8e45a1e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-188450316-172.17.0.13-1595642152869:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37292,DS-c3b2954c-1161-437e-8455-e7a6a1774a9d,DISK], DatanodeInfoWithStorage[127.0.0.1:40262,DS-85a24482-e42b-4149-9882-78e6ad2c9025,DISK], DatanodeInfoWithStorage[127.0.0.1:41812,DS-26e103f3-17e1-4f1d-a58d-a6bf80abd71f,DISK], DatanodeInfoWithStorage[127.0.0.1:36258,DS-25521793-99f8-4d98-a15f-e6c5fcc81567,DISK], DatanodeInfoWithStorage[127.0.0.1:33114,DS-6caf61eb-9e58-4eb5-84aa-ef4fd07b163c,DISK], DatanodeInfoWithStorage[127.0.0.1:42531,DS-77041489-af29-4d50-b6a2-142f857f267f,DISK], DatanodeInfoWithStorage[127.0.0.1:39104,DS-8bbffb47-c518-4605-b1ed-0ab2a0000678,DISK], DatanodeInfoWithStorage[127.0.0.1:33214,DS-c5aab203-d982-44d1-bdec-0704a8e45a1e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1752998813-172.17.0.13-1595642281826:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37628,DS-77345c01-84fd-48a5-a424-d832ef29cfb5,DISK], DatanodeInfoWithStorage[127.0.0.1:33493,DS-d0984163-2868-431f-8966-9262c6d76807,DISK], DatanodeInfoWithStorage[127.0.0.1:35252,DS-ec496f04-3219-4ec0-b896-e3432da80329,DISK], DatanodeInfoWithStorage[127.0.0.1:42856,DS-396aa32c-e347-4d75-81c0-7e38546757db,DISK], DatanodeInfoWithStorage[127.0.0.1:35055,DS-4decbebb-afab-4813-8211-b158638bce23,DISK], DatanodeInfoWithStorage[127.0.0.1:35176,DS-9891483a-41e7-4d2b-ae12-79c79561f020,DISK], DatanodeInfoWithStorage[127.0.0.1:36973,DS-e38b6c5f-8ea1-4fb1-80f8-53f77b7268f4,DISK], DatanodeInfoWithStorage[127.0.0.1:41739,DS-2ec2ad7f-d567-4b77-a4ac-207d8dd7b178,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1752998813-172.17.0.13-1595642281826:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37628,DS-77345c01-84fd-48a5-a424-d832ef29cfb5,DISK], DatanodeInfoWithStorage[127.0.0.1:33493,DS-d0984163-2868-431f-8966-9262c6d76807,DISK], DatanodeInfoWithStorage[127.0.0.1:35252,DS-ec496f04-3219-4ec0-b896-e3432da80329,DISK], DatanodeInfoWithStorage[127.0.0.1:42856,DS-396aa32c-e347-4d75-81c0-7e38546757db,DISK], DatanodeInfoWithStorage[127.0.0.1:35055,DS-4decbebb-afab-4813-8211-b158638bce23,DISK], DatanodeInfoWithStorage[127.0.0.1:35176,DS-9891483a-41e7-4d2b-ae12-79c79561f020,DISK], DatanodeInfoWithStorage[127.0.0.1:36973,DS-e38b6c5f-8ea1-4fb1-80f8-53f77b7268f4,DISK], DatanodeInfoWithStorage[127.0.0.1:41739,DS-2ec2ad7f-d567-4b77-a4ac-207d8dd7b178,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-929333067-172.17.0.13-1595642418845:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38560,DS-dce5b8c8-9757-4f53-abc7-0f3fba309e9c,DISK], DatanodeInfoWithStorage[127.0.0.1:37973,DS-8f16013a-db73-4c71-8399-78b909b1a7bd,DISK], DatanodeInfoWithStorage[127.0.0.1:43309,DS-53851105-e8ff-4629-abd7-7264609470f9,DISK], DatanodeInfoWithStorage[127.0.0.1:35222,DS-4b147d01-7342-4ab2-9dd0-5aa41ea0d507,DISK], DatanodeInfoWithStorage[127.0.0.1:33017,DS-034c9eb3-165f-4535-9573-5b1a48c29e38,DISK], DatanodeInfoWithStorage[127.0.0.1:35533,DS-b7c6f7e2-dfb1-48e5-9c5d-e535181bcd0d,DISK], DatanodeInfoWithStorage[127.0.0.1:44359,DS-bbb79594-5c38-4b01-91da-ebfe6d7cda65,DISK], DatanodeInfoWithStorage[127.0.0.1:35462,DS-edece4ad-ea7b-43ed-b380-1b9a75657e2d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-929333067-172.17.0.13-1595642418845:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38560,DS-dce5b8c8-9757-4f53-abc7-0f3fba309e9c,DISK], DatanodeInfoWithStorage[127.0.0.1:37973,DS-8f16013a-db73-4c71-8399-78b909b1a7bd,DISK], DatanodeInfoWithStorage[127.0.0.1:43309,DS-53851105-e8ff-4629-abd7-7264609470f9,DISK], DatanodeInfoWithStorage[127.0.0.1:35222,DS-4b147d01-7342-4ab2-9dd0-5aa41ea0d507,DISK], DatanodeInfoWithStorage[127.0.0.1:33017,DS-034c9eb3-165f-4535-9573-5b1a48c29e38,DISK], DatanodeInfoWithStorage[127.0.0.1:35533,DS-b7c6f7e2-dfb1-48e5-9c5d-e535181bcd0d,DISK], DatanodeInfoWithStorage[127.0.0.1:44359,DS-bbb79594-5c38-4b01-91da-ebfe6d7cda65,DISK], DatanodeInfoWithStorage[127.0.0.1:35462,DS-edece4ad-ea7b-43ed-b380-1b9a75657e2d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.response.header.size
component: hdfs:DataNode
v1: 65536
v2: 32768
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1975331229-172.17.0.13-1595642579397:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42396,DS-6c17e96b-c52e-4799-a119-cda101c0f055,DISK], DatanodeInfoWithStorage[127.0.0.1:34361,DS-cd97938f-b10b-4802-96ba-8fe3bcb68a3c,DISK], DatanodeInfoWithStorage[127.0.0.1:45125,DS-0c50387b-afdc-4f3f-9e07-1a44de914d50,DISK], DatanodeInfoWithStorage[127.0.0.1:46277,DS-dbd36eac-462d-4c09-9fd3-9b1e396b01b2,DISK], DatanodeInfoWithStorage[127.0.0.1:38235,DS-7a4bdc1d-bdb4-40e4-b80f-032e8f873e5e,DISK], DatanodeInfoWithStorage[127.0.0.1:45340,DS-20aa4d21-dc0a-4a6b-ab7b-b3d820dfbc6d,DISK], DatanodeInfoWithStorage[127.0.0.1:46643,DS-38e5e205-a900-402f-a6bc-5a0d6c33c8f8,DISK], DatanodeInfoWithStorage[127.0.0.1:46598,DS-50f28dda-56fd-41bb-b6b8-98789a61e2ec,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1975331229-172.17.0.13-1595642579397:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42396,DS-6c17e96b-c52e-4799-a119-cda101c0f055,DISK], DatanodeInfoWithStorage[127.0.0.1:34361,DS-cd97938f-b10b-4802-96ba-8fe3bcb68a3c,DISK], DatanodeInfoWithStorage[127.0.0.1:45125,DS-0c50387b-afdc-4f3f-9e07-1a44de914d50,DISK], DatanodeInfoWithStorage[127.0.0.1:46277,DS-dbd36eac-462d-4c09-9fd3-9b1e396b01b2,DISK], DatanodeInfoWithStorage[127.0.0.1:38235,DS-7a4bdc1d-bdb4-40e4-b80f-032e8f873e5e,DISK], DatanodeInfoWithStorage[127.0.0.1:45340,DS-20aa4d21-dc0a-4a6b-ab7b-b3d820dfbc6d,DISK], DatanodeInfoWithStorage[127.0.0.1:46643,DS-38e5e205-a900-402f-a6bc-5a0d6c33c8f8,DISK], DatanodeInfoWithStorage[127.0.0.1:46598,DS-50f28dda-56fd-41bb-b6b8-98789a61e2ec,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


v1v2 failed with probability 1 out of 50
v1v1v2v2 failed with probability 14 out of 50
result: false positive !!!
Total execution time in seconds : 6843

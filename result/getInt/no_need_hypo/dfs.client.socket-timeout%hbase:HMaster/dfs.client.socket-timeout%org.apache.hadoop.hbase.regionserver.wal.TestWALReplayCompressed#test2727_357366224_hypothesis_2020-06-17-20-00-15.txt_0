reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#test2727
reconfPoint: 1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#test2727
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35526,DS-75a66887-195b-4b2c-b25a-d6e155868844,DISK], DatanodeInfoWithStorage[127.0.0.1:41904,DS-ae9a7eb3-ba71-4205-b134-1ff21dbbb508,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35526,DS-75a66887-195b-4b2c-b25a-d6e155868844,DISK], DatanodeInfoWithStorage[127.0.0.1:41904,DS-ae9a7eb3-ba71-4205-b134-1ff21dbbb508,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35526,DS-75a66887-195b-4b2c-b25a-d6e155868844,DISK], DatanodeInfoWithStorage[127.0.0.1:41904,DS-ae9a7eb3-ba71-4205-b134-1ff21dbbb508,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35526,DS-75a66887-195b-4b2c-b25a-d6e155868844,DISK], DatanodeInfoWithStorage[127.0.0.1:41904,DS-ae9a7eb3-ba71-4205-b134-1ff21dbbb508,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#test2727
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34735,DS-ef7e7026-998d-4fbc-a9e1-faa3b0a7c762,DISK], DatanodeInfoWithStorage[127.0.0.1:45936,DS-8e17e5cd-4bee-42c5-8336-8a69c2002933,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34735,DS-ef7e7026-998d-4fbc-a9e1-faa3b0a7c762,DISK], DatanodeInfoWithStorage[127.0.0.1:45936,DS-8e17e5cd-4bee-42c5-8336-8a69c2002933,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34735,DS-ef7e7026-998d-4fbc-a9e1-faa3b0a7c762,DISK], DatanodeInfoWithStorage[127.0.0.1:45936,DS-8e17e5cd-4bee-42c5-8336-8a69c2002933,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34735,DS-ef7e7026-998d-4fbc-a9e1-faa3b0a7c762,DISK], DatanodeInfoWithStorage[127.0.0.1:45936,DS-8e17e5cd-4bee-42c5-8336-8a69c2002933,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#test2727
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37496,DS-07b3b0eb-e5fc-404e-b757-fa61664b29b7,DISK], DatanodeInfoWithStorage[127.0.0.1:33916,DS-2e9b6389-fcc5-4d94-987f-b6a5330ae1a9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37496,DS-07b3b0eb-e5fc-404e-b757-fa61664b29b7,DISK], DatanodeInfoWithStorage[127.0.0.1:33916,DS-2e9b6389-fcc5-4d94-987f-b6a5330ae1a9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37496,DS-07b3b0eb-e5fc-404e-b757-fa61664b29b7,DISK], DatanodeInfoWithStorage[127.0.0.1:33916,DS-2e9b6389-fcc5-4d94-987f-b6a5330ae1a9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37496,DS-07b3b0eb-e5fc-404e-b757-fa61664b29b7,DISK], DatanodeInfoWithStorage[127.0.0.1:33916,DS-2e9b6389-fcc5-4d94-987f-b6a5330ae1a9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#test2727
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38981,DS-2e4f9c9e-fa7f-4ce8-b415-26313ffea8df,DISK], DatanodeInfoWithStorage[127.0.0.1:38403,DS-afa5206f-acbb-49dd-a411-f7996b9079fc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38981,DS-2e4f9c9e-fa7f-4ce8-b415-26313ffea8df,DISK], DatanodeInfoWithStorage[127.0.0.1:38403,DS-afa5206f-acbb-49dd-a411-f7996b9079fc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38981,DS-2e4f9c9e-fa7f-4ce8-b415-26313ffea8df,DISK], DatanodeInfoWithStorage[127.0.0.1:38403,DS-afa5206f-acbb-49dd-a411-f7996b9079fc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38981,DS-2e4f9c9e-fa7f-4ce8-b415-26313ffea8df,DISK], DatanodeInfoWithStorage[127.0.0.1:38403,DS-afa5206f-acbb-49dd-a411-f7996b9079fc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#test2727
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44766,DS-a92e5e01-da69-418d-8a72-51e50b208005,DISK], DatanodeInfoWithStorage[127.0.0.1:37226,DS-4b964dbc-3a67-4e06-9a86-137ddfb950a6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44766,DS-a92e5e01-da69-418d-8a72-51e50b208005,DISK], DatanodeInfoWithStorage[127.0.0.1:37226,DS-4b964dbc-3a67-4e06-9a86-137ddfb950a6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44766,DS-a92e5e01-da69-418d-8a72-51e50b208005,DISK], DatanodeInfoWithStorage[127.0.0.1:37226,DS-4b964dbc-3a67-4e06-9a86-137ddfb950a6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44766,DS-a92e5e01-da69-418d-8a72-51e50b208005,DISK], DatanodeInfoWithStorage[127.0.0.1:37226,DS-4b964dbc-3a67-4e06-9a86-137ddfb950a6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#test2727
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46262,DS-fa2d868e-2778-4c87-83d2-64a7d55ab805,DISK], DatanodeInfoWithStorage[127.0.0.1:33277,DS-b35e2362-ccfd-4a72-a31e-583fe7941121,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33277,DS-b35e2362-ccfd-4a72-a31e-583fe7941121,DISK], DatanodeInfoWithStorage[127.0.0.1:46262,DS-fa2d868e-2778-4c87-83d2-64a7d55ab805,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46262,DS-fa2d868e-2778-4c87-83d2-64a7d55ab805,DISK], DatanodeInfoWithStorage[127.0.0.1:33277,DS-b35e2362-ccfd-4a72-a31e-583fe7941121,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33277,DS-b35e2362-ccfd-4a72-a31e-583fe7941121,DISK], DatanodeInfoWithStorage[127.0.0.1:46262,DS-fa2d868e-2778-4c87-83d2-64a7d55ab805,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#test2727
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45479,DS-4e86b924-a42b-461f-8b6a-d75de0ee80bf,DISK], DatanodeInfoWithStorage[127.0.0.1:35725,DS-2801a49c-6ebb-4513-883e-4c74d8d51e3c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35725,DS-2801a49c-6ebb-4513-883e-4c74d8d51e3c,DISK], DatanodeInfoWithStorage[127.0.0.1:45479,DS-4e86b924-a42b-461f-8b6a-d75de0ee80bf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45479,DS-4e86b924-a42b-461f-8b6a-d75de0ee80bf,DISK], DatanodeInfoWithStorage[127.0.0.1:35725,DS-2801a49c-6ebb-4513-883e-4c74d8d51e3c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35725,DS-2801a49c-6ebb-4513-883e-4c74d8d51e3c,DISK], DatanodeInfoWithStorage[127.0.0.1:45479,DS-4e86b924-a42b-461f-8b6a-d75de0ee80bf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#test2727
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40560,DS-b485f187-2dc5-41c2-8023-0be3400a8906,DISK], DatanodeInfoWithStorage[127.0.0.1:38197,DS-22afec39-e15f-4996-bd15-b4f7d4109c19,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40560,DS-b485f187-2dc5-41c2-8023-0be3400a8906,DISK], DatanodeInfoWithStorage[127.0.0.1:38197,DS-22afec39-e15f-4996-bd15-b4f7d4109c19,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40560,DS-b485f187-2dc5-41c2-8023-0be3400a8906,DISK], DatanodeInfoWithStorage[127.0.0.1:38197,DS-22afec39-e15f-4996-bd15-b4f7d4109c19,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40560,DS-b485f187-2dc5-41c2-8023-0be3400a8906,DISK], DatanodeInfoWithStorage[127.0.0.1:38197,DS-22afec39-e15f-4996-bd15-b4f7d4109c19,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#test2727
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37632,DS-6795007e-7759-4e09-8a66-d466a3261426,DISK], DatanodeInfoWithStorage[127.0.0.1:39161,DS-8b213503-c959-4a30-ad59-78d11cbbd701,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39161,DS-8b213503-c959-4a30-ad59-78d11cbbd701,DISK], DatanodeInfoWithStorage[127.0.0.1:37632,DS-6795007e-7759-4e09-8a66-d466a3261426,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37632,DS-6795007e-7759-4e09-8a66-d466a3261426,DISK], DatanodeInfoWithStorage[127.0.0.1:39161,DS-8b213503-c959-4a30-ad59-78d11cbbd701,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39161,DS-8b213503-c959-4a30-ad59-78d11cbbd701,DISK], DatanodeInfoWithStorage[127.0.0.1:37632,DS-6795007e-7759-4e09-8a66-d466a3261426,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#test2727
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45726,DS-eea403ea-7dab-46c0-9cbf-0cf990227528,DISK], DatanodeInfoWithStorage[127.0.0.1:45457,DS-8f2a4144-78df-482b-8c72-aa32b1a6e77d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45726,DS-eea403ea-7dab-46c0-9cbf-0cf990227528,DISK], DatanodeInfoWithStorage[127.0.0.1:45457,DS-8f2a4144-78df-482b-8c72-aa32b1a6e77d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45726,DS-eea403ea-7dab-46c0-9cbf-0cf990227528,DISK], DatanodeInfoWithStorage[127.0.0.1:45457,DS-8f2a4144-78df-482b-8c72-aa32b1a6e77d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45726,DS-eea403ea-7dab-46c0-9cbf-0cf990227528,DISK], DatanodeInfoWithStorage[127.0.0.1:45457,DS-8f2a4144-78df-482b-8c72-aa32b1a6e77d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 1786

reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45883,DS-84db96fc-ac41-413a-8075-91672dc46bee,DISK], DatanodeInfoWithStorage[127.0.0.1:41965,DS-7f554759-4984-4dc8-9376-35c9d40503a3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45883,DS-84db96fc-ac41-413a-8075-91672dc46bee,DISK], DatanodeInfoWithStorage[127.0.0.1:41965,DS-7f554759-4984-4dc8-9376-35c9d40503a3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45883,DS-84db96fc-ac41-413a-8075-91672dc46bee,DISK], DatanodeInfoWithStorage[127.0.0.1:41965,DS-7f554759-4984-4dc8-9376-35c9d40503a3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45883,DS-84db96fc-ac41-413a-8075-91672dc46bee,DISK], DatanodeInfoWithStorage[127.0.0.1:41965,DS-7f554759-4984-4dc8-9376-35c9d40503a3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40218,DS-c3661627-3e86-4d9d-8183-b9a3713c2596,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40218,DS-c3661627-3e86-4d9d-8183-b9a3713c2596,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40218,DS-c3661627-3e86-4d9d-8183-b9a3713c2596,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40218,DS-c3661627-3e86-4d9d-8183-b9a3713c2596,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43150,DS-e67e566d-67c4-4664-be6a-2d7f016b503f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43150,DS-e67e566d-67c4-4664-be6a-2d7f016b503f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43150,DS-e67e566d-67c4-4664-be6a-2d7f016b503f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43150,DS-e67e566d-67c4-4664-be6a-2d7f016b503f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38758,DS-9f1c0dbc-3b9a-4322-92c0-ad8517b098fb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38758,DS-9f1c0dbc-3b9a-4322-92c0-ad8517b098fb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38758,DS-9f1c0dbc-3b9a-4322-92c0-ad8517b098fb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38758,DS-9f1c0dbc-3b9a-4322-92c0-ad8517b098fb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36608,DS-914c6c94-309a-4118-886c-737dfe10d78c,DISK], DatanodeInfoWithStorage[127.0.0.1:40739,DS-feb9d2fb-96f2-451e-bbb8-f8a89b62f3e9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40739,DS-feb9d2fb-96f2-451e-bbb8-f8a89b62f3e9,DISK], DatanodeInfoWithStorage[127.0.0.1:36608,DS-914c6c94-309a-4118-886c-737dfe10d78c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36608,DS-914c6c94-309a-4118-886c-737dfe10d78c,DISK], DatanodeInfoWithStorage[127.0.0.1:40739,DS-feb9d2fb-96f2-451e-bbb8-f8a89b62f3e9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40739,DS-feb9d2fb-96f2-451e-bbb8-f8a89b62f3e9,DISK], DatanodeInfoWithStorage[127.0.0.1:36608,DS-914c6c94-309a-4118-886c-737dfe10d78c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36689,DS-50a6eb7d-086a-406c-b489-4b8ffcd4833e,DISK], DatanodeInfoWithStorage[127.0.0.1:41819,DS-fc54ee9f-b94d-42c5-9a25-96928b4608e1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36689,DS-50a6eb7d-086a-406c-b489-4b8ffcd4833e,DISK], DatanodeInfoWithStorage[127.0.0.1:41819,DS-fc54ee9f-b94d-42c5-9a25-96928b4608e1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36689,DS-50a6eb7d-086a-406c-b489-4b8ffcd4833e,DISK], DatanodeInfoWithStorage[127.0.0.1:41819,DS-fc54ee9f-b94d-42c5-9a25-96928b4608e1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36689,DS-50a6eb7d-086a-406c-b489-4b8ffcd4833e,DISK], DatanodeInfoWithStorage[127.0.0.1:41819,DS-fc54ee9f-b94d-42c5-9a25-96928b4608e1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44697,DS-4a381114-991a-4d0e-b243-97e4be4985fa,DISK], DatanodeInfoWithStorage[127.0.0.1:34063,DS-e1c6fb23-0fa2-47e6-8348-97d56183558a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44697,DS-4a381114-991a-4d0e-b243-97e4be4985fa,DISK], DatanodeInfoWithStorage[127.0.0.1:34063,DS-e1c6fb23-0fa2-47e6-8348-97d56183558a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44697,DS-4a381114-991a-4d0e-b243-97e4be4985fa,DISK], DatanodeInfoWithStorage[127.0.0.1:34063,DS-e1c6fb23-0fa2-47e6-8348-97d56183558a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44697,DS-4a381114-991a-4d0e-b243-97e4be4985fa,DISK], DatanodeInfoWithStorage[127.0.0.1:34063,DS-e1c6fb23-0fa2-47e6-8348-97d56183558a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46130,DS-770eaba1-a821-4171-92ec-33e3df01a7f5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46130,DS-770eaba1-a821-4171-92ec-33e3df01a7f5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46130,DS-770eaba1-a821-4171-92ec-33e3df01a7f5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46130,DS-770eaba1-a821-4171-92ec-33e3df01a7f5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45191,DS-cd0ef717-644c-4608-aa10-ab15ed4ec6a2,DISK], DatanodeInfoWithStorage[127.0.0.1:40332,DS-622b1e6a-66b1-4d82-bdeb-bc9224f65045,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45191,DS-cd0ef717-644c-4608-aa10-ab15ed4ec6a2,DISK], DatanodeInfoWithStorage[127.0.0.1:40332,DS-622b1e6a-66b1-4d82-bdeb-bc9224f65045,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45191,DS-cd0ef717-644c-4608-aa10-ab15ed4ec6a2,DISK], DatanodeInfoWithStorage[127.0.0.1:40332,DS-622b1e6a-66b1-4d82-bdeb-bc9224f65045,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45191,DS-cd0ef717-644c-4608-aa10-ab15ed4ec6a2,DISK], DatanodeInfoWithStorage[127.0.0.1:40332,DS-622b1e6a-66b1-4d82-bdeb-bc9224f65045,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38384,DS-f54f95de-33a7-4183-8242-4281d507121f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38384,DS-f54f95de-33a7-4183-8242-4281d507121f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38384,DS-f54f95de-33a7-4183-8242-4281d507121f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38384,DS-f54f95de-33a7-4183-8242-4281d507121f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46346,DS-e16364b4-a7c6-4551-a0c8-cf3e4c4cb13c,DISK], DatanodeInfoWithStorage[127.0.0.1:33576,DS-7f244729-a837-4276-8558-18f30bf8328e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46346,DS-e16364b4-a7c6-4551-a0c8-cf3e4c4cb13c,DISK], DatanodeInfoWithStorage[127.0.0.1:33576,DS-7f244729-a837-4276-8558-18f30bf8328e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46346,DS-e16364b4-a7c6-4551-a0c8-cf3e4c4cb13c,DISK], DatanodeInfoWithStorage[127.0.0.1:33576,DS-7f244729-a837-4276-8558-18f30bf8328e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46346,DS-e16364b4-a7c6-4551-a0c8-cf3e4c4cb13c,DISK], DatanodeInfoWithStorage[127.0.0.1:33576,DS-7f244729-a837-4276-8558-18f30bf8328e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35647,DS-927027c9-989d-4420-a9e3-2cdb3d058b5d,DISK], DatanodeInfoWithStorage[127.0.0.1:43205,DS-605bca03-a780-410d-84d7-f7374e6331aa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35647,DS-927027c9-989d-4420-a9e3-2cdb3d058b5d,DISK], DatanodeInfoWithStorage[127.0.0.1:43205,DS-605bca03-a780-410d-84d7-f7374e6331aa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35647,DS-927027c9-989d-4420-a9e3-2cdb3d058b5d,DISK], DatanodeInfoWithStorage[127.0.0.1:43205,DS-605bca03-a780-410d-84d7-f7374e6331aa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35647,DS-927027c9-989d-4420-a9e3-2cdb3d058b5d,DISK], DatanodeInfoWithStorage[127.0.0.1:43205,DS-605bca03-a780-410d-84d7-f7374e6331aa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41537,DS-cefff0e8-181c-433c-bc29-444353462a9c,DISK], DatanodeInfoWithStorage[127.0.0.1:45552,DS-471781b0-50e4-4044-a27c-f1a06a9a1f08,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41537,DS-cefff0e8-181c-433c-bc29-444353462a9c,DISK], DatanodeInfoWithStorage[127.0.0.1:45552,DS-471781b0-50e4-4044-a27c-f1a06a9a1f08,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41537,DS-cefff0e8-181c-433c-bc29-444353462a9c,DISK], DatanodeInfoWithStorage[127.0.0.1:45552,DS-471781b0-50e4-4044-a27c-f1a06a9a1f08,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41537,DS-cefff0e8-181c-433c-bc29-444353462a9c,DISK], DatanodeInfoWithStorage[127.0.0.1:45552,DS-471781b0-50e4-4044-a27c-f1a06a9a1f08,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39390,DS-8207eec6-742c-422d-8442-d475cd9bd4f2,DISK], DatanodeInfoWithStorage[127.0.0.1:43699,DS-37c74a4b-dc72-4c35-8c23-563ee4aefb39,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43699,DS-37c74a4b-dc72-4c35-8c23-563ee4aefb39,DISK], DatanodeInfoWithStorage[127.0.0.1:39390,DS-8207eec6-742c-422d-8442-d475cd9bd4f2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39390,DS-8207eec6-742c-422d-8442-d475cd9bd4f2,DISK], DatanodeInfoWithStorage[127.0.0.1:43699,DS-37c74a4b-dc72-4c35-8c23-563ee4aefb39,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43699,DS-37c74a4b-dc72-4c35-8c23-563ee4aefb39,DISK], DatanodeInfoWithStorage[127.0.0.1:39390,DS-8207eec6-742c-422d-8442-d475cd9bd4f2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41943,DS-3006cf8c-2e84-4017-a2a9-46cca850b721,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41943,DS-3006cf8c-2e84-4017-a2a9-46cca850b721,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41943,DS-3006cf8c-2e84-4017-a2a9-46cca850b721,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41943,DS-3006cf8c-2e84-4017-a2a9-46cca850b721,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38800,DS-f50e0303-99a0-48cf-b24f-f1b613435304,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38800,DS-f50e0303-99a0-48cf-b24f-f1b613435304,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38800,DS-f50e0303-99a0-48cf-b24f-f1b613435304,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38800,DS-f50e0303-99a0-48cf-b24f-f1b613435304,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34662,DS-cf5138cc-3d7f-4fef-aff6-df575e6fd476,DISK], DatanodeInfoWithStorage[127.0.0.1:35602,DS-3812529c-d5ab-4884-b6db-9b4531151b93,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34662,DS-cf5138cc-3d7f-4fef-aff6-df575e6fd476,DISK], DatanodeInfoWithStorage[127.0.0.1:35602,DS-3812529c-d5ab-4884-b6db-9b4531151b93,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34662,DS-cf5138cc-3d7f-4fef-aff6-df575e6fd476,DISK], DatanodeInfoWithStorage[127.0.0.1:35602,DS-3812529c-d5ab-4884-b6db-9b4531151b93,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34662,DS-cf5138cc-3d7f-4fef-aff6-df575e6fd476,DISK], DatanodeInfoWithStorage[127.0.0.1:35602,DS-3812529c-d5ab-4884-b6db-9b4531151b93,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42120,DS-ce486bc2-4372-4964-a11b-d90298774b4c,DISK], DatanodeInfoWithStorage[127.0.0.1:41671,DS-8b7435ee-2047-4cea-9e08-6d9f1b5d622a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41671,DS-8b7435ee-2047-4cea-9e08-6d9f1b5d622a,DISK], DatanodeInfoWithStorage[127.0.0.1:42120,DS-ce486bc2-4372-4964-a11b-d90298774b4c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42120,DS-ce486bc2-4372-4964-a11b-d90298774b4c,DISK], DatanodeInfoWithStorage[127.0.0.1:41671,DS-8b7435ee-2047-4cea-9e08-6d9f1b5d622a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41671,DS-8b7435ee-2047-4cea-9e08-6d9f1b5d622a,DISK], DatanodeInfoWithStorage[127.0.0.1:42120,DS-ce486bc2-4372-4964-a11b-d90298774b4c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43917,DS-ef97c1bd-d65e-4bbd-97ce-c21718866117,DISK], DatanodeInfoWithStorage[127.0.0.1:36972,DS-0f4e0d66-12a2-4d09-a3b5-4e5f4fa3f910,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43917,DS-ef97c1bd-d65e-4bbd-97ce-c21718866117,DISK], DatanodeInfoWithStorage[127.0.0.1:36972,DS-0f4e0d66-12a2-4d09-a3b5-4e5f4fa3f910,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43917,DS-ef97c1bd-d65e-4bbd-97ce-c21718866117,DISK], DatanodeInfoWithStorage[127.0.0.1:36972,DS-0f4e0d66-12a2-4d09-a3b5-4e5f4fa3f910,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43917,DS-ef97c1bd-d65e-4bbd-97ce-c21718866117,DISK], DatanodeInfoWithStorage[127.0.0.1:36972,DS-0f4e0d66-12a2-4d09-a3b5-4e5f4fa3f910,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46826,DS-b4c4861e-8437-4bd2-bd80-da4f1a6b4dc7,DISK], DatanodeInfoWithStorage[127.0.0.1:43997,DS-364213b1-b122-4104-b5dd-4508c92b56eb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43997,DS-364213b1-b122-4104-b5dd-4508c92b56eb,DISK], DatanodeInfoWithStorage[127.0.0.1:46826,DS-b4c4861e-8437-4bd2-bd80-da4f1a6b4dc7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46826,DS-b4c4861e-8437-4bd2-bd80-da4f1a6b4dc7,DISK], DatanodeInfoWithStorage[127.0.0.1:43997,DS-364213b1-b122-4104-b5dd-4508c92b56eb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43997,DS-364213b1-b122-4104-b5dd-4508c92b56eb,DISK], DatanodeInfoWithStorage[127.0.0.1:46826,DS-b4c4861e-8437-4bd2-bd80-da4f1a6b4dc7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44240,DS-1c184c22-7f11-4bca-9ba7-6501c6f921c0,DISK], DatanodeInfoWithStorage[127.0.0.1:43271,DS-255a3d81-1f68-4d5d-93a9-1bb762a8fc02,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43271,DS-255a3d81-1f68-4d5d-93a9-1bb762a8fc02,DISK], DatanodeInfoWithStorage[127.0.0.1:44240,DS-1c184c22-7f11-4bca-9ba7-6501c6f921c0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44240,DS-1c184c22-7f11-4bca-9ba7-6501c6f921c0,DISK], DatanodeInfoWithStorage[127.0.0.1:43271,DS-255a3d81-1f68-4d5d-93a9-1bb762a8fc02,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43271,DS-255a3d81-1f68-4d5d-93a9-1bb762a8fc02,DISK], DatanodeInfoWithStorage[127.0.0.1:44240,DS-1c184c22-7f11-4bca-9ba7-6501c6f921c0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39778,DS-2052ab04-b5b6-460d-bb3c-0ec938bed3c0,DISK], DatanodeInfoWithStorage[127.0.0.1:33958,DS-64ce30c8-f6a8-451f-9107-1c3ae3c870f9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39778,DS-2052ab04-b5b6-460d-bb3c-0ec938bed3c0,DISK], DatanodeInfoWithStorage[127.0.0.1:33958,DS-64ce30c8-f6a8-451f-9107-1c3ae3c870f9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39778,DS-2052ab04-b5b6-460d-bb3c-0ec938bed3c0,DISK], DatanodeInfoWithStorage[127.0.0.1:33958,DS-64ce30c8-f6a8-451f-9107-1c3ae3c870f9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39778,DS-2052ab04-b5b6-460d-bb3c-0ec938bed3c0,DISK], DatanodeInfoWithStorage[127.0.0.1:33958,DS-64ce30c8-f6a8-451f-9107-1c3ae3c870f9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38938,DS-51c54ed5-a1e8-4bf4-ba22-58450d82981b,DISK], DatanodeInfoWithStorage[127.0.0.1:43742,DS-75eaca5b-5db5-4348-ab65-702164c95cbf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38938,DS-51c54ed5-a1e8-4bf4-ba22-58450d82981b,DISK], DatanodeInfoWithStorage[127.0.0.1:43742,DS-75eaca5b-5db5-4348-ab65-702164c95cbf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38938,DS-51c54ed5-a1e8-4bf4-ba22-58450d82981b,DISK], DatanodeInfoWithStorage[127.0.0.1:43742,DS-75eaca5b-5db5-4348-ab65-702164c95cbf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38938,DS-51c54ed5-a1e8-4bf4-ba22-58450d82981b,DISK], DatanodeInfoWithStorage[127.0.0.1:43742,DS-75eaca5b-5db5-4348-ab65-702164c95cbf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35565,DS-db69bea7-d581-4987-bc04-d61e7c399a9f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35565,DS-db69bea7-d581-4987-bc04-d61e7c399a9f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35565,DS-db69bea7-d581-4987-bc04-d61e7c399a9f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35565,DS-db69bea7-d581-4987-bc04-d61e7c399a9f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37576,DS-7e63d56b-ec17-4082-913c-fe722263cff3,DISK], DatanodeInfoWithStorage[127.0.0.1:41700,DS-3cd416f2-abd9-415c-b9cc-af421d27ca14,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37576,DS-7e63d56b-ec17-4082-913c-fe722263cff3,DISK], DatanodeInfoWithStorage[127.0.0.1:41700,DS-3cd416f2-abd9-415c-b9cc-af421d27ca14,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37576,DS-7e63d56b-ec17-4082-913c-fe722263cff3,DISK], DatanodeInfoWithStorage[127.0.0.1:41700,DS-3cd416f2-abd9-415c-b9cc-af421d27ca14,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37576,DS-7e63d56b-ec17-4082-913c-fe722263cff3,DISK], DatanodeInfoWithStorage[127.0.0.1:41700,DS-3cd416f2-abd9-415c-b9cc-af421d27ca14,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38353,DS-869902de-6136-47ba-b28e-850dddb201eb,DISK], DatanodeInfoWithStorage[127.0.0.1:45737,DS-eb9be4d9-0168-463a-ac05-c0ba0afd13d3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38353,DS-869902de-6136-47ba-b28e-850dddb201eb,DISK], DatanodeInfoWithStorage[127.0.0.1:45737,DS-eb9be4d9-0168-463a-ac05-c0ba0afd13d3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38353,DS-869902de-6136-47ba-b28e-850dddb201eb,DISK], DatanodeInfoWithStorage[127.0.0.1:45737,DS-eb9be4d9-0168-463a-ac05-c0ba0afd13d3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38353,DS-869902de-6136-47ba-b28e-850dddb201eb,DISK], DatanodeInfoWithStorage[127.0.0.1:45737,DS-eb9be4d9-0168-463a-ac05-c0ba0afd13d3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36572,DS-6ee282a3-c854-4a1b-9f8e-74dd04720724,DISK], DatanodeInfoWithStorage[127.0.0.1:44923,DS-ecca7d82-75ae-4186-a400-5be50dfdf5f7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44923,DS-ecca7d82-75ae-4186-a400-5be50dfdf5f7,DISK], DatanodeInfoWithStorage[127.0.0.1:36572,DS-6ee282a3-c854-4a1b-9f8e-74dd04720724,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36572,DS-6ee282a3-c854-4a1b-9f8e-74dd04720724,DISK], DatanodeInfoWithStorage[127.0.0.1:44923,DS-ecca7d82-75ae-4186-a400-5be50dfdf5f7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44923,DS-ecca7d82-75ae-4186-a400-5be50dfdf5f7,DISK], DatanodeInfoWithStorage[127.0.0.1:36572,DS-6ee282a3-c854-4a1b-9f8e-74dd04720724,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44813,DS-31560cc2-4f52-4b9b-8662-38104fd72a4a,DISK], DatanodeInfoWithStorage[127.0.0.1:36531,DS-00f7b7de-a842-481c-9ea9-54a4a0ab3cdb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44813,DS-31560cc2-4f52-4b9b-8662-38104fd72a4a,DISK], DatanodeInfoWithStorage[127.0.0.1:36531,DS-00f7b7de-a842-481c-9ea9-54a4a0ab3cdb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44813,DS-31560cc2-4f52-4b9b-8662-38104fd72a4a,DISK], DatanodeInfoWithStorage[127.0.0.1:36531,DS-00f7b7de-a842-481c-9ea9-54a4a0ab3cdb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44813,DS-31560cc2-4f52-4b9b-8662-38104fd72a4a,DISK], DatanodeInfoWithStorage[127.0.0.1:36531,DS-00f7b7de-a842-481c-9ea9-54a4a0ab3cdb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37763,DS-fecebdea-cb59-4b05-bc39-3533bd4a4c30,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37763,DS-fecebdea-cb59-4b05-bc39-3533bd4a4c30,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37763,DS-fecebdea-cb59-4b05-bc39-3533bd4a4c30,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37763,DS-fecebdea-cb59-4b05-bc39-3533bd4a4c30,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46453,DS-3ab1a80b-4824-4143-930c-3eaa90dfcd73,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46453,DS-3ab1a80b-4824-4143-930c-3eaa90dfcd73,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46453,DS-3ab1a80b-4824-4143-930c-3eaa90dfcd73,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46453,DS-3ab1a80b-4824-4143-930c-3eaa90dfcd73,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41530,DS-33d3d805-0850-4614-8b76-b333eb03ff8e,DISK], DatanodeInfoWithStorage[127.0.0.1:34669,DS-ca105d22-ee36-4a02-9206-193cc12c674b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34669,DS-ca105d22-ee36-4a02-9206-193cc12c674b,DISK], DatanodeInfoWithStorage[127.0.0.1:41530,DS-33d3d805-0850-4614-8b76-b333eb03ff8e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41530,DS-33d3d805-0850-4614-8b76-b333eb03ff8e,DISK], DatanodeInfoWithStorage[127.0.0.1:34669,DS-ca105d22-ee36-4a02-9206-193cc12c674b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34669,DS-ca105d22-ee36-4a02-9206-193cc12c674b,DISK], DatanodeInfoWithStorage[127.0.0.1:41530,DS-33d3d805-0850-4614-8b76-b333eb03ff8e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44579,DS-f5bc7ab6-f4d7-4a9f-b411-b9fafa33e87a,DISK], DatanodeInfoWithStorage[127.0.0.1:33852,DS-81092460-9dc0-4470-be32-c420106ada25,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44579,DS-f5bc7ab6-f4d7-4a9f-b411-b9fafa33e87a,DISK], DatanodeInfoWithStorage[127.0.0.1:33852,DS-81092460-9dc0-4470-be32-c420106ada25,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44579,DS-f5bc7ab6-f4d7-4a9f-b411-b9fafa33e87a,DISK], DatanodeInfoWithStorage[127.0.0.1:33852,DS-81092460-9dc0-4470-be32-c420106ada25,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44579,DS-f5bc7ab6-f4d7-4a9f-b411-b9fafa33e87a,DISK], DatanodeInfoWithStorage[127.0.0.1:33852,DS-81092460-9dc0-4470-be32-c420106ada25,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41149,DS-8bcc4ae1-6293-4403-9fa2-fe1559bb2ddf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41149,DS-8bcc4ae1-6293-4403-9fa2-fe1559bb2ddf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41149,DS-8bcc4ae1-6293-4403-9fa2-fe1559bb2ddf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41149,DS-8bcc4ae1-6293-4403-9fa2-fe1559bb2ddf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33822,DS-6ca393d1-0bf1-4120-9e2e-d207f354cd39,DISK], DatanodeInfoWithStorage[127.0.0.1:36586,DS-92e1f425-61cd-4c26-8393-039f413c0432,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33822,DS-6ca393d1-0bf1-4120-9e2e-d207f354cd39,DISK], DatanodeInfoWithStorage[127.0.0.1:36586,DS-92e1f425-61cd-4c26-8393-039f413c0432,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33822,DS-6ca393d1-0bf1-4120-9e2e-d207f354cd39,DISK], DatanodeInfoWithStorage[127.0.0.1:36586,DS-92e1f425-61cd-4c26-8393-039f413c0432,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33822,DS-6ca393d1-0bf1-4120-9e2e-d207f354cd39,DISK], DatanodeInfoWithStorage[127.0.0.1:36586,DS-92e1f425-61cd-4c26-8393-039f413c0432,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34323,DS-4abc90fb-38d2-4f20-a531-fd77f54dde49,DISK], DatanodeInfoWithStorage[127.0.0.1:39578,DS-1b6873e3-9790-4831-833c-04f822ab4229,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34323,DS-4abc90fb-38d2-4f20-a531-fd77f54dde49,DISK], DatanodeInfoWithStorage[127.0.0.1:39578,DS-1b6873e3-9790-4831-833c-04f822ab4229,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34323,DS-4abc90fb-38d2-4f20-a531-fd77f54dde49,DISK], DatanodeInfoWithStorage[127.0.0.1:39578,DS-1b6873e3-9790-4831-833c-04f822ab4229,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34323,DS-4abc90fb-38d2-4f20-a531-fd77f54dde49,DISK], DatanodeInfoWithStorage[127.0.0.1:39578,DS-1b6873e3-9790-4831-833c-04f822ab4229,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33143,DS-c0d83525-af57-47a7-8957-d9cadbd90630,DISK], DatanodeInfoWithStorage[127.0.0.1:41954,DS-2c27e330-443f-4e2a-98db-d98cf017a0aa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33143,DS-c0d83525-af57-47a7-8957-d9cadbd90630,DISK], DatanodeInfoWithStorage[127.0.0.1:41954,DS-2c27e330-443f-4e2a-98db-d98cf017a0aa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33143,DS-c0d83525-af57-47a7-8957-d9cadbd90630,DISK], DatanodeInfoWithStorage[127.0.0.1:41954,DS-2c27e330-443f-4e2a-98db-d98cf017a0aa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33143,DS-c0d83525-af57-47a7-8957-d9cadbd90630,DISK], DatanodeInfoWithStorage[127.0.0.1:41954,DS-2c27e330-443f-4e2a-98db-d98cf017a0aa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36864,DS-d2080d02-df35-47d1-a857-75f75176e42b,DISK], DatanodeInfoWithStorage[127.0.0.1:46783,DS-641f7f01-b4b5-4c65-afb7-a88722a7c63a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46783,DS-641f7f01-b4b5-4c65-afb7-a88722a7c63a,DISK], DatanodeInfoWithStorage[127.0.0.1:36864,DS-d2080d02-df35-47d1-a857-75f75176e42b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36864,DS-d2080d02-df35-47d1-a857-75f75176e42b,DISK], DatanodeInfoWithStorage[127.0.0.1:46783,DS-641f7f01-b4b5-4c65-afb7-a88722a7c63a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46783,DS-641f7f01-b4b5-4c65-afb7-a88722a7c63a,DISK], DatanodeInfoWithStorage[127.0.0.1:36864,DS-d2080d02-df35-47d1-a857-75f75176e42b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44666,DS-12ed40bd-d3ec-498b-a285-d4a0cc947871,DISK], DatanodeInfoWithStorage[127.0.0.1:36070,DS-cf68df8c-a86f-4bec-87ca-9ab40df8724c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36070,DS-cf68df8c-a86f-4bec-87ca-9ab40df8724c,DISK], DatanodeInfoWithStorage[127.0.0.1:44666,DS-12ed40bd-d3ec-498b-a285-d4a0cc947871,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44666,DS-12ed40bd-d3ec-498b-a285-d4a0cc947871,DISK], DatanodeInfoWithStorage[127.0.0.1:36070,DS-cf68df8c-a86f-4bec-87ca-9ab40df8724c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36070,DS-cf68df8c-a86f-4bec-87ca-9ab40df8724c,DISK], DatanodeInfoWithStorage[127.0.0.1:44666,DS-12ed40bd-d3ec-498b-a285-d4a0cc947871,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34755,DS-8c08d851-ee63-4c4e-9fe6-6d529b1d199d,DISK], DatanodeInfoWithStorage[127.0.0.1:39505,DS-826e08de-5132-4b9b-b2d0-1532e2b00408,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34755,DS-8c08d851-ee63-4c4e-9fe6-6d529b1d199d,DISK], DatanodeInfoWithStorage[127.0.0.1:39505,DS-826e08de-5132-4b9b-b2d0-1532e2b00408,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34755,DS-8c08d851-ee63-4c4e-9fe6-6d529b1d199d,DISK], DatanodeInfoWithStorage[127.0.0.1:39505,DS-826e08de-5132-4b9b-b2d0-1532e2b00408,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34755,DS-8c08d851-ee63-4c4e-9fe6-6d529b1d199d,DISK], DatanodeInfoWithStorage[127.0.0.1:39505,DS-826e08de-5132-4b9b-b2d0-1532e2b00408,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37308,DS-3c584392-1005-43d3-bfc6-f06d527663b3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37308,DS-3c584392-1005-43d3-bfc6-f06d527663b3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37308,DS-3c584392-1005-43d3-bfc6-f06d527663b3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37308,DS-3c584392-1005-43d3-bfc6-f06d527663b3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36612,DS-cc931a72-ea5f-4008-bf51-9e95cf94860e,DISK], DatanodeInfoWithStorage[127.0.0.1:36403,DS-8e08c99a-d6e0-4ba9-b1cf-c301bc9a2254,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36612,DS-cc931a72-ea5f-4008-bf51-9e95cf94860e,DISK], DatanodeInfoWithStorage[127.0.0.1:36403,DS-8e08c99a-d6e0-4ba9-b1cf-c301bc9a2254,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36612,DS-cc931a72-ea5f-4008-bf51-9e95cf94860e,DISK], DatanodeInfoWithStorage[127.0.0.1:36403,DS-8e08c99a-d6e0-4ba9-b1cf-c301bc9a2254,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36612,DS-cc931a72-ea5f-4008-bf51-9e95cf94860e,DISK], DatanodeInfoWithStorage[127.0.0.1:36403,DS-8e08c99a-d6e0-4ba9-b1cf-c301bc9a2254,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46001,DS-0e9a2722-8ff7-4ac0-9876-30cf832db590,DISK], DatanodeInfoWithStorage[127.0.0.1:34740,DS-07aaf2a9-87a8-4968-a5c9-954018d0ac2c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46001,DS-0e9a2722-8ff7-4ac0-9876-30cf832db590,DISK], DatanodeInfoWithStorage[127.0.0.1:34740,DS-07aaf2a9-87a8-4968-a5c9-954018d0ac2c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46001,DS-0e9a2722-8ff7-4ac0-9876-30cf832db590,DISK], DatanodeInfoWithStorage[127.0.0.1:34740,DS-07aaf2a9-87a8-4968-a5c9-954018d0ac2c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46001,DS-0e9a2722-8ff7-4ac0-9876-30cf832db590,DISK], DatanodeInfoWithStorage[127.0.0.1:34740,DS-07aaf2a9-87a8-4968-a5c9-954018d0ac2c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38557,DS-bfeb7546-ff15-4386-8a19-9e1c51134559,DISK], DatanodeInfoWithStorage[127.0.0.1:41928,DS-553aeadf-c2d9-4658-8347-b6a9e76c0152,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41928,DS-553aeadf-c2d9-4658-8347-b6a9e76c0152,DISK], DatanodeInfoWithStorage[127.0.0.1:38557,DS-bfeb7546-ff15-4386-8a19-9e1c51134559,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38557,DS-bfeb7546-ff15-4386-8a19-9e1c51134559,DISK], DatanodeInfoWithStorage[127.0.0.1:41928,DS-553aeadf-c2d9-4658-8347-b6a9e76c0152,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41928,DS-553aeadf-c2d9-4658-8347-b6a9e76c0152,DISK], DatanodeInfoWithStorage[127.0.0.1:38557,DS-bfeb7546-ff15-4386-8a19-9e1c51134559,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34157,DS-c9b89656-d3a5-48ae-8aa5-3c469a2e5fd9,DISK], DatanodeInfoWithStorage[127.0.0.1:42743,DS-c59678dd-06ec-476b-8896-5869d169607f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34157,DS-c9b89656-d3a5-48ae-8aa5-3c469a2e5fd9,DISK], DatanodeInfoWithStorage[127.0.0.1:42743,DS-c59678dd-06ec-476b-8896-5869d169607f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34157,DS-c9b89656-d3a5-48ae-8aa5-3c469a2e5fd9,DISK], DatanodeInfoWithStorage[127.0.0.1:42743,DS-c59678dd-06ec-476b-8896-5869d169607f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34157,DS-c9b89656-d3a5-48ae-8aa5-3c469a2e5fd9,DISK], DatanodeInfoWithStorage[127.0.0.1:42743,DS-c59678dd-06ec-476b-8896-5869d169607f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33637,DS-64168b9e-1eb1-4121-878e-5ec1e89bec58,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33637,DS-64168b9e-1eb1-4121-878e-5ec1e89bec58,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33637,DS-64168b9e-1eb1-4121-878e-5ec1e89bec58,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33637,DS-64168b9e-1eb1-4121-878e-5ec1e89bec58,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34263,DS-8252ece3-bde1-4158-9638-705ac3feafc0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34263,DS-8252ece3-bde1-4158-9638-705ac3feafc0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34263,DS-8252ece3-bde1-4158-9638-705ac3feafc0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34263,DS-8252ece3-bde1-4158-9638-705ac3feafc0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35048,DS-324682c6-ffe0-4edc-8498-3e01999bfe2a,DISK], DatanodeInfoWithStorage[127.0.0.1:46182,DS-a40b7d69-280f-423c-83b5-492ae0232f63,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46182,DS-a40b7d69-280f-423c-83b5-492ae0232f63,DISK], DatanodeInfoWithStorage[127.0.0.1:35048,DS-324682c6-ffe0-4edc-8498-3e01999bfe2a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35048,DS-324682c6-ffe0-4edc-8498-3e01999bfe2a,DISK], DatanodeInfoWithStorage[127.0.0.1:46182,DS-a40b7d69-280f-423c-83b5-492ae0232f63,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46182,DS-a40b7d69-280f-423c-83b5-492ae0232f63,DISK], DatanodeInfoWithStorage[127.0.0.1:35048,DS-324682c6-ffe0-4edc-8498-3e01999bfe2a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42512,DS-4ee1e6b1-cfcb-41ac-ac55-3ff0b7e19410,DISK], DatanodeInfoWithStorage[127.0.0.1:42344,DS-bba8f202-16c3-4e37-9f2b-ae8015ce8eef,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42512,DS-4ee1e6b1-cfcb-41ac-ac55-3ff0b7e19410,DISK], DatanodeInfoWithStorage[127.0.0.1:42344,DS-bba8f202-16c3-4e37-9f2b-ae8015ce8eef,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42512,DS-4ee1e6b1-cfcb-41ac-ac55-3ff0b7e19410,DISK], DatanodeInfoWithStorage[127.0.0.1:42344,DS-bba8f202-16c3-4e37-9f2b-ae8015ce8eef,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42512,DS-4ee1e6b1-cfcb-41ac-ac55-3ff0b7e19410,DISK], DatanodeInfoWithStorage[127.0.0.1:42344,DS-bba8f202-16c3-4e37-9f2b-ae8015ce8eef,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37374,DS-1d8b8a0c-2231-4736-9b3e-05984703f090,DISK], DatanodeInfoWithStorage[127.0.0.1:32862,DS-00b4684e-5830-428b-b2c3-6407a55cf131,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32862,DS-00b4684e-5830-428b-b2c3-6407a55cf131,DISK], DatanodeInfoWithStorage[127.0.0.1:37374,DS-1d8b8a0c-2231-4736-9b3e-05984703f090,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37374,DS-1d8b8a0c-2231-4736-9b3e-05984703f090,DISK], DatanodeInfoWithStorage[127.0.0.1:32862,DS-00b4684e-5830-428b-b2c3-6407a55cf131,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32862,DS-00b4684e-5830-428b-b2c3-6407a55cf131,DISK], DatanodeInfoWithStorage[127.0.0.1:37374,DS-1d8b8a0c-2231-4736-9b3e-05984703f090,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38577,DS-721d5738-4fe9-4786-978d-790776c3e0bb,DISK], DatanodeInfoWithStorage[127.0.0.1:41023,DS-7dc1ad43-9343-4e2f-96e5-8dad8294b78c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38577,DS-721d5738-4fe9-4786-978d-790776c3e0bb,DISK], DatanodeInfoWithStorage[127.0.0.1:41023,DS-7dc1ad43-9343-4e2f-96e5-8dad8294b78c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38577,DS-721d5738-4fe9-4786-978d-790776c3e0bb,DISK], DatanodeInfoWithStorage[127.0.0.1:41023,DS-7dc1ad43-9343-4e2f-96e5-8dad8294b78c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38577,DS-721d5738-4fe9-4786-978d-790776c3e0bb,DISK], DatanodeInfoWithStorage[127.0.0.1:41023,DS-7dc1ad43-9343-4e2f-96e5-8dad8294b78c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38906,DS-9c6baf6d-65ec-49b2-8cf3-ffec84702e56,DISK], DatanodeInfoWithStorage[127.0.0.1:46659,DS-2b0a7d7b-eb5c-4ee8-a6f8-cbb38683214b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38906,DS-9c6baf6d-65ec-49b2-8cf3-ffec84702e56,DISK], DatanodeInfoWithStorage[127.0.0.1:46659,DS-2b0a7d7b-eb5c-4ee8-a6f8-cbb38683214b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38906,DS-9c6baf6d-65ec-49b2-8cf3-ffec84702e56,DISK], DatanodeInfoWithStorage[127.0.0.1:46659,DS-2b0a7d7b-eb5c-4ee8-a6f8-cbb38683214b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38906,DS-9c6baf6d-65ec-49b2-8cf3-ffec84702e56,DISK], DatanodeInfoWithStorage[127.0.0.1:46659,DS-2b0a7d7b-eb5c-4ee8-a6f8-cbb38683214b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 50 out of 50
v1v1v2v2 failed with probability 1 out of 50
result: might be true error
Total execution time in seconds : 3440

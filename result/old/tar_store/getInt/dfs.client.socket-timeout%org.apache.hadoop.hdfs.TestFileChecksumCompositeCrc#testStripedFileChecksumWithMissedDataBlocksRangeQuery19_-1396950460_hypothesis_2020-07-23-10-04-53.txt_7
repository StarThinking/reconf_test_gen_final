reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: Failed: the number of failed blocks = 7 > the number of parity blocks = 3
stackTrace: java.io.IOException: Failed: the number of failed blocks = 7 > the number of parity blocks = 3
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamers(DFSStripedOutputStream.java:396)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.handleStreamerFailure(DFSStripedOutputStream.java:414)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.flushAllInternals(DFSStripedOutputStream.java:1283)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamerFailures(DFSStripedOutputStream.java:631)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:567)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:865)
	at org.apache.hadoop.hdfs.TestFileChecksum.prepareTestFiles(TestFileChecksum.java:602)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:292)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
	Suppressed: java.lang.NullPointerException
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.flushAllInternals(DFSStripedOutputStream.java:1282)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1186)
		at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
		at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
		at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
		at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
		... 12 more



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1177650345-172.17.0.8-1595499088210:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45867,DS-10c927b9-34a1-423d-9b92-93524f3b9167,DISK], DatanodeInfoWithStorage[127.0.0.1:44322,DS-d7a1d559-be5a-4081-a63e-a7bff65c5d49,DISK], DatanodeInfoWithStorage[127.0.0.1:44095,DS-7c820e40-5d24-49e7-bc7e-a4c4384b96fe,DISK], DatanodeInfoWithStorage[127.0.0.1:39074,DS-0982f4fc-c0c0-475e-9bd4-bdfed6d3d36b,DISK], DatanodeInfoWithStorage[127.0.0.1:40822,DS-b59b11eb-f8d0-4c7d-bdda-89deb4f85187,DISK], DatanodeInfoWithStorage[127.0.0.1:36691,DS-2fc1c401-a3a9-4446-98d6-b65ae2071844,DISK], DatanodeInfoWithStorage[127.0.0.1:35630,DS-7e244c81-978d-4536-ab09-7d24b01bd512,DISK], DatanodeInfoWithStorage[127.0.0.1:35743,DS-60f57e0c-c1b8-4016-af96-78344c448e4b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1177650345-172.17.0.8-1595499088210:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45867,DS-10c927b9-34a1-423d-9b92-93524f3b9167,DISK], DatanodeInfoWithStorage[127.0.0.1:44322,DS-d7a1d559-be5a-4081-a63e-a7bff65c5d49,DISK], DatanodeInfoWithStorage[127.0.0.1:44095,DS-7c820e40-5d24-49e7-bc7e-a4c4384b96fe,DISK], DatanodeInfoWithStorage[127.0.0.1:39074,DS-0982f4fc-c0c0-475e-9bd4-bdfed6d3d36b,DISK], DatanodeInfoWithStorage[127.0.0.1:40822,DS-b59b11eb-f8d0-4c7d-bdda-89deb4f85187,DISK], DatanodeInfoWithStorage[127.0.0.1:36691,DS-2fc1c401-a3a9-4446-98d6-b65ae2071844,DISK], DatanodeInfoWithStorage[127.0.0.1:35630,DS-7e244c81-978d-4536-ab09-7d24b01bd512,DISK], DatanodeInfoWithStorage[127.0.0.1:35743,DS-60f57e0c-c1b8-4016-af96-78344c448e4b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1770506510-172.17.0.8-1595499420365:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45306,DS-94f1266c-21b3-4d6d-aed0-221355e9177e,DISK], DatanodeInfoWithStorage[127.0.0.1:36629,DS-2596de41-4040-44f5-9653-adb5b92ca1af,DISK], DatanodeInfoWithStorage[127.0.0.1:35572,DS-7da3ea4d-c8f2-435c-ba93-3e450794ec58,DISK], DatanodeInfoWithStorage[127.0.0.1:37941,DS-99620ed8-480b-4b67-a3ee-d1142e110e90,DISK], DatanodeInfoWithStorage[127.0.0.1:43829,DS-a3a8eb0e-df9d-483a-ab9a-2ea399d84365,DISK], DatanodeInfoWithStorage[127.0.0.1:44826,DS-0cba7465-16e8-4774-8434-1c99acc0c7ca,DISK], DatanodeInfoWithStorage[127.0.0.1:40857,DS-9db8161d-0dd2-4cfc-b679-4c66fb45e50e,DISK], DatanodeInfoWithStorage[127.0.0.1:43585,DS-0275a7cf-f671-4672-97d3-57576ba0701b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1770506510-172.17.0.8-1595499420365:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45306,DS-94f1266c-21b3-4d6d-aed0-221355e9177e,DISK], DatanodeInfoWithStorage[127.0.0.1:36629,DS-2596de41-4040-44f5-9653-adb5b92ca1af,DISK], DatanodeInfoWithStorage[127.0.0.1:35572,DS-7da3ea4d-c8f2-435c-ba93-3e450794ec58,DISK], DatanodeInfoWithStorage[127.0.0.1:37941,DS-99620ed8-480b-4b67-a3ee-d1142e110e90,DISK], DatanodeInfoWithStorage[127.0.0.1:43829,DS-a3a8eb0e-df9d-483a-ab9a-2ea399d84365,DISK], DatanodeInfoWithStorage[127.0.0.1:44826,DS-0cba7465-16e8-4774-8434-1c99acc0c7ca,DISK], DatanodeInfoWithStorage[127.0.0.1:40857,DS-9db8161d-0dd2-4cfc-b679-4c66fb45e50e,DISK], DatanodeInfoWithStorage[127.0.0.1:43585,DS-0275a7cf-f671-4672-97d3-57576ba0701b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1224134837-172.17.0.8-1595499644589:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46200,DS-f9f6cf89-2c87-4619-8d2e-d58fc8099680,DISK], DatanodeInfoWithStorage[127.0.0.1:43146,DS-03ae75ae-fadb-4854-acc0-b170849c1443,DISK], DatanodeInfoWithStorage[127.0.0.1:39169,DS-7dbcfd4a-ba66-45be-8403-92f4b9b0f32e,DISK], DatanodeInfoWithStorage[127.0.0.1:41122,DS-394e0402-fa4a-42ae-8db0-d837e907732d,DISK], DatanodeInfoWithStorage[127.0.0.1:36335,DS-e00b0dab-2f4e-495b-8f59-b214f9001b8a,DISK], DatanodeInfoWithStorage[127.0.0.1:40657,DS-c52212b2-f623-4c21-9fb0-9505682f6364,DISK], DatanodeInfoWithStorage[127.0.0.1:38962,DS-b41c7c09-d358-4633-a1b2-7589d9795929,DISK], DatanodeInfoWithStorage[127.0.0.1:46521,DS-2554ade4-99c8-43a9-9c63-bd3229ef8c46,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1224134837-172.17.0.8-1595499644589:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46200,DS-f9f6cf89-2c87-4619-8d2e-d58fc8099680,DISK], DatanodeInfoWithStorage[127.0.0.1:43146,DS-03ae75ae-fadb-4854-acc0-b170849c1443,DISK], DatanodeInfoWithStorage[127.0.0.1:39169,DS-7dbcfd4a-ba66-45be-8403-92f4b9b0f32e,DISK], DatanodeInfoWithStorage[127.0.0.1:41122,DS-394e0402-fa4a-42ae-8db0-d837e907732d,DISK], DatanodeInfoWithStorage[127.0.0.1:36335,DS-e00b0dab-2f4e-495b-8f59-b214f9001b8a,DISK], DatanodeInfoWithStorage[127.0.0.1:40657,DS-c52212b2-f623-4c21-9fb0-9505682f6364,DISK], DatanodeInfoWithStorage[127.0.0.1:38962,DS-b41c7c09-d358-4633-a1b2-7589d9795929,DISK], DatanodeInfoWithStorage[127.0.0.1:46521,DS-2554ade4-99c8-43a9-9c63-bd3229ef8c46,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-436861456-172.17.0.8-1595499798787:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44934,DS-3d90d567-ca7b-494e-aa95-d2a0cef7eca2,DISK], DatanodeInfoWithStorage[127.0.0.1:35127,DS-be4b310d-8e3e-4d8a-a481-2a4483a2af49,DISK], DatanodeInfoWithStorage[127.0.0.1:35751,DS-d2262bc9-76db-4945-b1b9-cbb321fb57ea,DISK], DatanodeInfoWithStorage[127.0.0.1:35146,DS-289c1ee2-6d43-4349-9690-2ea217277db3,DISK], DatanodeInfoWithStorage[127.0.0.1:43569,DS-66b54a3f-9170-48cc-91c6-118d73aa7ff7,DISK], DatanodeInfoWithStorage[127.0.0.1:40218,DS-ffe13439-52e0-4d9a-a741-b8dd819e6ffd,DISK], DatanodeInfoWithStorage[127.0.0.1:38047,DS-e8c02c68-f4ff-4a25-9119-4e889d35debe,DISK], DatanodeInfoWithStorage[127.0.0.1:42295,DS-5c1ea2ca-a1c2-4797-883e-858eb6dbaabd,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-436861456-172.17.0.8-1595499798787:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44934,DS-3d90d567-ca7b-494e-aa95-d2a0cef7eca2,DISK], DatanodeInfoWithStorage[127.0.0.1:35127,DS-be4b310d-8e3e-4d8a-a481-2a4483a2af49,DISK], DatanodeInfoWithStorage[127.0.0.1:35751,DS-d2262bc9-76db-4945-b1b9-cbb321fb57ea,DISK], DatanodeInfoWithStorage[127.0.0.1:35146,DS-289c1ee2-6d43-4349-9690-2ea217277db3,DISK], DatanodeInfoWithStorage[127.0.0.1:43569,DS-66b54a3f-9170-48cc-91c6-118d73aa7ff7,DISK], DatanodeInfoWithStorage[127.0.0.1:40218,DS-ffe13439-52e0-4d9a-a741-b8dd819e6ffd,DISK], DatanodeInfoWithStorage[127.0.0.1:38047,DS-e8c02c68-f4ff-4a25-9119-4e889d35debe,DISK], DatanodeInfoWithStorage[127.0.0.1:42295,DS-5c1ea2ca-a1c2-4797-883e-858eb6dbaabd,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1894149640-172.17.0.8-1595499868041:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37472,DS-f201496f-4935-40ba-a695-667f225dbfdb,DISK], DatanodeInfoWithStorage[127.0.0.1:46688,DS-d8a25989-93c0-4bb9-9131-220c8504448b,DISK], DatanodeInfoWithStorage[127.0.0.1:44030,DS-132d8116-7656-4af5-9e01-b82f6a02eb65,DISK], DatanodeInfoWithStorage[127.0.0.1:38449,DS-2947a80f-3dd5-4349-b301-5626df0642c3,DISK], DatanodeInfoWithStorage[127.0.0.1:40220,DS-b8e20e93-375d-4436-b9aa-6dfa91c86b17,DISK], DatanodeInfoWithStorage[127.0.0.1:39974,DS-d1123fdc-42f4-4a34-8448-c5136a7ea375,DISK], DatanodeInfoWithStorage[127.0.0.1:37085,DS-b0615fc5-fa3d-42f1-a80f-f29a6332f15c,DISK], DatanodeInfoWithStorage[127.0.0.1:33527,DS-111aa3a7-fbdb-4e75-ba6a-e25b58f0437f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1894149640-172.17.0.8-1595499868041:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37472,DS-f201496f-4935-40ba-a695-667f225dbfdb,DISK], DatanodeInfoWithStorage[127.0.0.1:46688,DS-d8a25989-93c0-4bb9-9131-220c8504448b,DISK], DatanodeInfoWithStorage[127.0.0.1:44030,DS-132d8116-7656-4af5-9e01-b82f6a02eb65,DISK], DatanodeInfoWithStorage[127.0.0.1:38449,DS-2947a80f-3dd5-4349-b301-5626df0642c3,DISK], DatanodeInfoWithStorage[127.0.0.1:40220,DS-b8e20e93-375d-4436-b9aa-6dfa91c86b17,DISK], DatanodeInfoWithStorage[127.0.0.1:39974,DS-d1123fdc-42f4-4a34-8448-c5136a7ea375,DISK], DatanodeInfoWithStorage[127.0.0.1:37085,DS-b0615fc5-fa3d-42f1-a80f-f29a6332f15c,DISK], DatanodeInfoWithStorage[127.0.0.1:33527,DS-111aa3a7-fbdb-4e75-ba6a-e25b58f0437f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: Failed: the number of failed blocks = 6 > the number of parity blocks = 3
stackTrace: java.io.IOException: Failed: the number of failed blocks = 6 > the number of parity blocks = 3
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamers(DFSStripedOutputStream.java:396)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamerFailures(DFSStripedOutputStream.java:624)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:567)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:865)
	at org.apache.hadoop.hdfs.TestFileChecksum.prepareTestFiles(TestFileChecksum.java:602)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:292)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
	Suppressed: java.io.IOException: Failed: the number of failed blocks = 6 > the number of parity blocks = 3
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamers(DFSStripedOutputStream.java:396)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamerFailures(DFSStripedOutputStream.java:624)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1188)
		at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
		at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
		at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
		at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
		... 12 more



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-756532715-172.17.0.8-1595500115603:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36633,DS-5d43c490-d0f7-4420-aa49-9378a42b1275,DISK], DatanodeInfoWithStorage[127.0.0.1:33981,DS-e6aee80a-f3d2-4069-baa1-bc4923b317f4,DISK], DatanodeInfoWithStorage[127.0.0.1:33913,DS-d1588ec6-2fe4-40b9-88ea-70dd6c910b51,DISK], DatanodeInfoWithStorage[127.0.0.1:40448,DS-988141cd-d587-4911-894f-24ac294ad271,DISK], DatanodeInfoWithStorage[127.0.0.1:44774,DS-14d382b4-9538-4b63-ac32-9518cf4000e7,DISK], DatanodeInfoWithStorage[127.0.0.1:43834,DS-c9df220a-0316-4fa5-b665-b7f748b75555,DISK], DatanodeInfoWithStorage[127.0.0.1:35993,DS-3b1c9efb-f870-47ac-9321-dbd1a3b40f23,DISK], DatanodeInfoWithStorage[127.0.0.1:40014,DS-73bc1db8-3445-472b-87ea-ba28f93e382d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-756532715-172.17.0.8-1595500115603:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36633,DS-5d43c490-d0f7-4420-aa49-9378a42b1275,DISK], DatanodeInfoWithStorage[127.0.0.1:33981,DS-e6aee80a-f3d2-4069-baa1-bc4923b317f4,DISK], DatanodeInfoWithStorage[127.0.0.1:33913,DS-d1588ec6-2fe4-40b9-88ea-70dd6c910b51,DISK], DatanodeInfoWithStorage[127.0.0.1:40448,DS-988141cd-d587-4911-894f-24ac294ad271,DISK], DatanodeInfoWithStorage[127.0.0.1:44774,DS-14d382b4-9538-4b63-ac32-9518cf4000e7,DISK], DatanodeInfoWithStorage[127.0.0.1:43834,DS-c9df220a-0316-4fa5-b665-b7f748b75555,DISK], DatanodeInfoWithStorage[127.0.0.1:35993,DS-3b1c9efb-f870-47ac-9321-dbd1a3b40f23,DISK], DatanodeInfoWithStorage[127.0.0.1:40014,DS-73bc1db8-3445-472b-87ea-ba28f93e382d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: Failed: the number of failed blocks = 4 > the number of parity blocks = 3
stackTrace: java.io.IOException: Failed: the number of failed blocks = 4 > the number of parity blocks = 3
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamers(DFSStripedOutputStream.java:396)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.handleStreamerFailure(DFSStripedOutputStream.java:414)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.flushAllInternals(DFSStripedOutputStream.java:1283)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:555)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:865)
	at org.apache.hadoop.hdfs.TestFileChecksum.prepareTestFiles(TestFileChecksum.java:602)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:292)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
	Suppressed: java.lang.NullPointerException
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.flushAllInternals(DFSStripedOutputStream.java:1282)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1186)
		at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
		at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
		at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
		at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
		... 12 more



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: Failed: the number of failed blocks = 5 > the number of parity blocks = 3
stackTrace: java.io.IOException: Failed: the number of failed blocks = 5 > the number of parity blocks = 3
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamers(DFSStripedOutputStream.java:396)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.handleStreamerFailure(DFSStripedOutputStream.java:414)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.flushAllInternals(DFSStripedOutputStream.java:1283)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamerFailures(DFSStripedOutputStream.java:631)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:567)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:865)
	at org.apache.hadoop.hdfs.TestFileChecksum.prepareTestFiles(TestFileChecksum.java:602)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:292)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
	Suppressed: java.lang.NullPointerException
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.flushAllInternals(DFSStripedOutputStream.java:1282)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1186)
		at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
		at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
		at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
		at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
		... 12 more



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: Failed: the number of failed blocks = 6 > the number of parity blocks = 3
stackTrace: java.io.IOException: Failed: the number of failed blocks = 6 > the number of parity blocks = 3
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamers(DFSStripedOutputStream.java:396)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamerFailures(DFSStripedOutputStream.java:624)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:567)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:865)
	at org.apache.hadoop.hdfs.TestFileChecksum.prepareTestFiles(TestFileChecksum.java:602)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:292)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
	Suppressed: java.io.IOException: Failed: the number of failed blocks = 7 > the number of parity blocks = 3
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamers(DFSStripedOutputStream.java:396)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.handleStreamerFailure(DFSStripedOutputStream.java:414)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.flushAllInternals(DFSStripedOutputStream.java:1283)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1186)
		at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
		at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
		at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
		at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
		... 12 more



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-593273173-172.17.0.8-1595500985061:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39215,DS-0a958b2d-22e6-4f9c-93a0-34abd038ba2f,DISK], DatanodeInfoWithStorage[127.0.0.1:33243,DS-358214f7-5934-4600-a5ed-af7923e6a68d,DISK], DatanodeInfoWithStorage[127.0.0.1:40517,DS-4bed92e2-3c22-495c-86f6-484ca07dd8b3,DISK], DatanodeInfoWithStorage[127.0.0.1:43365,DS-ebfd6770-5af6-4747-9ca8-4af2c6111dae,DISK], DatanodeInfoWithStorage[127.0.0.1:34990,DS-f2c08c0f-cfb8-4f78-b75b-679bee43aecf,DISK], DatanodeInfoWithStorage[127.0.0.1:36837,DS-920870e8-17c9-4d36-9df8-9cea59208b56,DISK], DatanodeInfoWithStorage[127.0.0.1:34186,DS-22dfce14-d92c-4df8-ac4f-4e7b9c173e71,DISK], DatanodeInfoWithStorage[127.0.0.1:41355,DS-1aa614f0-8ebf-49a8-b06b-65c18afac4d5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-593273173-172.17.0.8-1595500985061:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39215,DS-0a958b2d-22e6-4f9c-93a0-34abd038ba2f,DISK], DatanodeInfoWithStorage[127.0.0.1:33243,DS-358214f7-5934-4600-a5ed-af7923e6a68d,DISK], DatanodeInfoWithStorage[127.0.0.1:40517,DS-4bed92e2-3c22-495c-86f6-484ca07dd8b3,DISK], DatanodeInfoWithStorage[127.0.0.1:43365,DS-ebfd6770-5af6-4747-9ca8-4af2c6111dae,DISK], DatanodeInfoWithStorage[127.0.0.1:34990,DS-f2c08c0f-cfb8-4f78-b75b-679bee43aecf,DISK], DatanodeInfoWithStorage[127.0.0.1:36837,DS-920870e8-17c9-4d36-9df8-9cea59208b56,DISK], DatanodeInfoWithStorage[127.0.0.1:34186,DS-22dfce14-d92c-4df8-ac4f-4e7b9c173e71,DISK], DatanodeInfoWithStorage[127.0.0.1:41355,DS-1aa614f0-8ebf-49a8-b06b-65c18afac4d5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: Failed: the number of failed blocks = 5 > the number of parity blocks = 3
stackTrace: java.io.IOException: Failed: the number of failed blocks = 5 > the number of parity blocks = 3
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamers(DFSStripedOutputStream.java:396)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamerFailures(DFSStripedOutputStream.java:624)
	at org.apache.hadoop.hdfs.DFSStripedOutputStream.writeChunk(DFSStripedOutputStream.java:567)
	at org.apache.hadoop.fs.FSOutputSummer.writeChecksumChunks(FSOutputSummer.java:217)
	at org.apache.hadoop.fs.FSOutputSummer.write1(FSOutputSummer.java:125)
	at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:111)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:96)
	at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:865)
	at org.apache.hadoop.hdfs.TestFileChecksum.prepareTestFiles(TestFileChecksum.java:602)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:292)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
	Suppressed: java.io.IOException: Failed: the number of failed blocks = 7 > the number of parity blocks = 3
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.checkStreamers(DFSStripedOutputStream.java:396)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.handleStreamerFailure(DFSStripedOutputStream.java:414)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.flushAllInternals(DFSStripedOutputStream.java:1283)
		at org.apache.hadoop.hdfs.DFSStripedOutputStream.closeImpl(DFSStripedOutputStream.java:1186)
		at org.apache.hadoop.hdfs.DFSOutputStream.close(DFSOutputStream.java:845)
		at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
		at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:101)
		at org.apache.hadoop.hdfs.DFSTestUtil.writeFile(DFSTestUtil.java:866)
		... 12 more



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1721530411-172.17.0.8-1595501435186:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41641,DS-073739d3-af6d-44ce-bc0f-739fbefea2eb,DISK], DatanodeInfoWithStorage[127.0.0.1:42936,DS-df028d9a-1ac1-4661-891e-16c86ee4b56f,DISK], DatanodeInfoWithStorage[127.0.0.1:38569,DS-63ab2b50-12f1-4bad-a20a-34a9d1b4ccba,DISK], DatanodeInfoWithStorage[127.0.0.1:42316,DS-fea73367-f7e5-41df-b0da-e307f8ca1094,DISK], DatanodeInfoWithStorage[127.0.0.1:33253,DS-dc7c501c-1e40-4f39-89e1-bbdeea00918e,DISK], DatanodeInfoWithStorage[127.0.0.1:34005,DS-b40371f5-6a31-46f3-a375-58f8e30b4ecf,DISK], DatanodeInfoWithStorage[127.0.0.1:38768,DS-b12ef57a-9b4e-4ae2-bbfd-39e40f0b3b06,DISK], DatanodeInfoWithStorage[127.0.0.1:46056,DS-5402d133-0632-4432-a15a-dc5666727884,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1721530411-172.17.0.8-1595501435186:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41641,DS-073739d3-af6d-44ce-bc0f-739fbefea2eb,DISK], DatanodeInfoWithStorage[127.0.0.1:42936,DS-df028d9a-1ac1-4661-891e-16c86ee4b56f,DISK], DatanodeInfoWithStorage[127.0.0.1:38569,DS-63ab2b50-12f1-4bad-a20a-34a9d1b4ccba,DISK], DatanodeInfoWithStorage[127.0.0.1:42316,DS-fea73367-f7e5-41df-b0da-e307f8ca1094,DISK], DatanodeInfoWithStorage[127.0.0.1:33253,DS-dc7c501c-1e40-4f39-89e1-bbdeea00918e,DISK], DatanodeInfoWithStorage[127.0.0.1:34005,DS-b40371f5-6a31-46f3-a375-58f8e30b4ecf,DISK], DatanodeInfoWithStorage[127.0.0.1:38768,DS-b12ef57a-9b4e-4ae2-bbfd-39e40f0b3b06,DISK], DatanodeInfoWithStorage[127.0.0.1:46056,DS-5402d133-0632-4432-a15a-dc5666727884,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-553084345-172.17.0.8-1595501464815:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40651,DS-813a71a4-720c-44e8-872c-8a3b905bc063,DISK], DatanodeInfoWithStorage[127.0.0.1:37167,DS-0474de43-cd88-40a7-9e48-322d9f3432a2,DISK], DatanodeInfoWithStorage[127.0.0.1:33987,DS-6aced308-3bb2-4599-8e44-c7bc7da8d1bf,DISK], DatanodeInfoWithStorage[127.0.0.1:38173,DS-8f9a5d42-2ef6-43d1-ba22-9669aa77bf87,DISK], DatanodeInfoWithStorage[127.0.0.1:35722,DS-910ee92b-770f-47d1-aa4d-c228e920e50c,DISK], DatanodeInfoWithStorage[127.0.0.1:38010,DS-332ced81-c863-476a-a468-ac7ba6df6706,DISK], DatanodeInfoWithStorage[127.0.0.1:36929,DS-03d4904f-1d1f-46e9-8e8b-17d5c828159c,DISK], DatanodeInfoWithStorage[127.0.0.1:33220,DS-4bbe8f01-159e-44ea-9da8-c8b3faec572b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-553084345-172.17.0.8-1595501464815:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40651,DS-813a71a4-720c-44e8-872c-8a3b905bc063,DISK], DatanodeInfoWithStorage[127.0.0.1:37167,DS-0474de43-cd88-40a7-9e48-322d9f3432a2,DISK], DatanodeInfoWithStorage[127.0.0.1:33987,DS-6aced308-3bb2-4599-8e44-c7bc7da8d1bf,DISK], DatanodeInfoWithStorage[127.0.0.1:38173,DS-8f9a5d42-2ef6-43d1-ba22-9669aa77bf87,DISK], DatanodeInfoWithStorage[127.0.0.1:35722,DS-910ee92b-770f-47d1-aa4d-c228e920e50c,DISK], DatanodeInfoWithStorage[127.0.0.1:38010,DS-332ced81-c863-476a-a468-ac7ba6df6706,DISK], DatanodeInfoWithStorage[127.0.0.1:36929,DS-03d4904f-1d1f-46e9-8e8b-17d5c828159c,DISK], DatanodeInfoWithStorage[127.0.0.1:33220,DS-4bbe8f01-159e-44ea-9da8-c8b3faec572b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1413562982-172.17.0.8-1595501731095:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45580,DS-ea8551ed-37fd-40d8-9eb1-8fded6523e28,DISK], DatanodeInfoWithStorage[127.0.0.1:35973,DS-ce2376e7-0071-4d90-b703-066c801abfdd,DISK], DatanodeInfoWithStorage[127.0.0.1:36991,DS-85dcd428-3a21-494f-a250-5796cda2e99e,DISK], DatanodeInfoWithStorage[127.0.0.1:40167,DS-7a0f5690-4c6a-4221-bb6d-6b03a3498e54,DISK], DatanodeInfoWithStorage[127.0.0.1:36315,DS-0b205338-c166-4ea6-932e-4f305f15874f,DISK], DatanodeInfoWithStorage[127.0.0.1:43614,DS-f3954f89-267f-4bf0-8d68-c3e69fe51854,DISK], DatanodeInfoWithStorage[127.0.0.1:39045,DS-83772950-96a7-4514-96f2-e965f3346f17,DISK], DatanodeInfoWithStorage[127.0.0.1:37275,DS-f67009ae-b2c8-4a57-acff-9ff5ff07bd09,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1413562982-172.17.0.8-1595501731095:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45580,DS-ea8551ed-37fd-40d8-9eb1-8fded6523e28,DISK], DatanodeInfoWithStorage[127.0.0.1:35973,DS-ce2376e7-0071-4d90-b703-066c801abfdd,DISK], DatanodeInfoWithStorage[127.0.0.1:36991,DS-85dcd428-3a21-494f-a250-5796cda2e99e,DISK], DatanodeInfoWithStorage[127.0.0.1:40167,DS-7a0f5690-4c6a-4221-bb6d-6b03a3498e54,DISK], DatanodeInfoWithStorage[127.0.0.1:36315,DS-0b205338-c166-4ea6-932e-4f305f15874f,DISK], DatanodeInfoWithStorage[127.0.0.1:43614,DS-f3954f89-267f-4bf0-8d68-c3e69fe51854,DISK], DatanodeInfoWithStorage[127.0.0.1:39045,DS-83772950-96a7-4514-96f2-e965f3346f17,DISK], DatanodeInfoWithStorage[127.0.0.1:37275,DS-f67009ae-b2c8-4a57-acff-9ff5ff07bd09,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1916709406-172.17.0.8-1595501941472:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42932,DS-1c4e4a5e-bb94-4609-8f3a-45f2fd35df48,DISK], DatanodeInfoWithStorage[127.0.0.1:38296,DS-00bd492f-5185-4125-b0f7-43a7337f4539,DISK], DatanodeInfoWithStorage[127.0.0.1:38078,DS-183e3254-0014-4d7e-922c-3400b954f177,DISK], DatanodeInfoWithStorage[127.0.0.1:34154,DS-77f58cb1-6b76-4a58-80e4-5153689a266a,DISK], DatanodeInfoWithStorage[127.0.0.1:44276,DS-62d04178-0b1d-4c1c-b65b-8b1db4129134,DISK], DatanodeInfoWithStorage[127.0.0.1:45026,DS-39340713-01c3-42d0-b9f2-903119202c76,DISK], DatanodeInfoWithStorage[127.0.0.1:35308,DS-6ee3e7c1-fa04-4f64-bd96-8a9595f34103,DISK], DatanodeInfoWithStorage[127.0.0.1:41382,DS-1bbdaf1d-4e5e-4b8a-a819-8129c93d5718,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1916709406-172.17.0.8-1595501941472:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42932,DS-1c4e4a5e-bb94-4609-8f3a-45f2fd35df48,DISK], DatanodeInfoWithStorage[127.0.0.1:38296,DS-00bd492f-5185-4125-b0f7-43a7337f4539,DISK], DatanodeInfoWithStorage[127.0.0.1:38078,DS-183e3254-0014-4d7e-922c-3400b954f177,DISK], DatanodeInfoWithStorage[127.0.0.1:34154,DS-77f58cb1-6b76-4a58-80e4-5153689a266a,DISK], DatanodeInfoWithStorage[127.0.0.1:44276,DS-62d04178-0b1d-4c1c-b65b-8b1db4129134,DISK], DatanodeInfoWithStorage[127.0.0.1:45026,DS-39340713-01c3-42d0-b9f2-903119202c76,DISK], DatanodeInfoWithStorage[127.0.0.1:35308,DS-6ee3e7c1-fa04-4f64-bd96-8a9595f34103,DISK], DatanodeInfoWithStorage[127.0.0.1:41382,DS-1bbdaf1d-4e5e-4b8a-a819-8129c93d5718,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1419131908-172.17.0.8-1595502148123:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37605,DS-bba56fe4-24c3-472a-93bf-a6ca8061e6ce,DISK], DatanodeInfoWithStorage[127.0.0.1:37902,DS-5609ab69-4a51-4fe4-a4f3-835dbe9c79f7,DISK], DatanodeInfoWithStorage[127.0.0.1:34882,DS-9ddc95a6-49a3-4e42-8c82-f406a1600d4c,DISK], DatanodeInfoWithStorage[127.0.0.1:33864,DS-6ae648e7-b293-4869-96f8-595a3655b5a1,DISK], DatanodeInfoWithStorage[127.0.0.1:34302,DS-e8372a49-2781-48ac-8aa6-2eb3dc97c0f5,DISK], DatanodeInfoWithStorage[127.0.0.1:37765,DS-3f034e3a-a9de-4195-8d7d-b38ea41bc0f5,DISK], DatanodeInfoWithStorage[127.0.0.1:35279,DS-06b7097f-777c-4081-998f-cf20d0e0a347,DISK], DatanodeInfoWithStorage[127.0.0.1:44233,DS-01b9c92b-b6f0-4061-99a6-5160b9ae9d14,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1419131908-172.17.0.8-1595502148123:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37605,DS-bba56fe4-24c3-472a-93bf-a6ca8061e6ce,DISK], DatanodeInfoWithStorage[127.0.0.1:37902,DS-5609ab69-4a51-4fe4-a4f3-835dbe9c79f7,DISK], DatanodeInfoWithStorage[127.0.0.1:34882,DS-9ddc95a6-49a3-4e42-8c82-f406a1600d4c,DISK], DatanodeInfoWithStorage[127.0.0.1:33864,DS-6ae648e7-b293-4869-96f8-595a3655b5a1,DISK], DatanodeInfoWithStorage[127.0.0.1:34302,DS-e8372a49-2781-48ac-8aa6-2eb3dc97c0f5,DISK], DatanodeInfoWithStorage[127.0.0.1:37765,DS-3f034e3a-a9de-4195-8d7d-b38ea41bc0f5,DISK], DatanodeInfoWithStorage[127.0.0.1:35279,DS-06b7097f-777c-4081-998f-cf20d0e0a347,DISK], DatanodeInfoWithStorage[127.0.0.1:44233,DS-01b9c92b-b6f0-4061-99a6-5160b9ae9d14,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-310192266-172.17.0.8-1595502431678:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40160,DS-b085eec7-2421-4f9d-aa7a-6dedd82d75aa,DISK], DatanodeInfoWithStorage[127.0.0.1:40901,DS-bceb1ff7-5168-4237-b217-905e92d127a3,DISK], DatanodeInfoWithStorage[127.0.0.1:41034,DS-b2f48990-8fe1-49cb-bde2-c66ab33655e6,DISK], DatanodeInfoWithStorage[127.0.0.1:44829,DS-dbeb6d1d-0f78-40f9-bfd4-ea60372359e7,DISK], DatanodeInfoWithStorage[127.0.0.1:37584,DS-5da35a9b-267a-4752-a71d-de3d0f9f58aa,DISK], DatanodeInfoWithStorage[127.0.0.1:44704,DS-b2282751-4def-4237-a683-f362c1d46cc3,DISK], DatanodeInfoWithStorage[127.0.0.1:36576,DS-dfdf91a3-6867-410f-a9b8-9f60a3413898,DISK], DatanodeInfoWithStorage[127.0.0.1:35070,DS-5d8f2af2-0302-47ec-8f90-6db43c031ad2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-310192266-172.17.0.8-1595502431678:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40160,DS-b085eec7-2421-4f9d-aa7a-6dedd82d75aa,DISK], DatanodeInfoWithStorage[127.0.0.1:40901,DS-bceb1ff7-5168-4237-b217-905e92d127a3,DISK], DatanodeInfoWithStorage[127.0.0.1:41034,DS-b2f48990-8fe1-49cb-bde2-c66ab33655e6,DISK], DatanodeInfoWithStorage[127.0.0.1:44829,DS-dbeb6d1d-0f78-40f9-bfd4-ea60372359e7,DISK], DatanodeInfoWithStorage[127.0.0.1:37584,DS-5da35a9b-267a-4752-a71d-de3d0f9f58aa,DISK], DatanodeInfoWithStorage[127.0.0.1:44704,DS-b2282751-4def-4237-a683-f362c1d46cc3,DISK], DatanodeInfoWithStorage[127.0.0.1:36576,DS-dfdf91a3-6867-410f-a9b8-9f60a3413898,DISK], DatanodeInfoWithStorage[127.0.0.1:35070,DS-5d8f2af2-0302-47ec-8f90-6db43c031ad2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-256635757-172.17.0.8-1595502616616:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39702,DS-d4dd1fd3-8dac-4226-9ebf-18b4630d8039,DISK], DatanodeInfoWithStorage[127.0.0.1:43971,DS-0dc9a4bc-f65b-4ffa-b046-e28b6e5337d9,DISK], DatanodeInfoWithStorage[127.0.0.1:35383,DS-260b46db-0d20-4914-81b3-f5c863af348f,DISK], DatanodeInfoWithStorage[127.0.0.1:45725,DS-87f40484-ef30-4337-88a6-aab674e89c84,DISK], DatanodeInfoWithStorage[127.0.0.1:39758,DS-dbf8d86e-6b0c-4c1f-b47d-6695d7ed2ed0,DISK], DatanodeInfoWithStorage[127.0.0.1:41766,DS-acd4041b-7002-412b-9d64-ff1b2757ec3a,DISK], DatanodeInfoWithStorage[127.0.0.1:46332,DS-8dceea43-ea05-4fb6-9923-16dd60aa8a4a,DISK], DatanodeInfoWithStorage[127.0.0.1:37544,DS-390a8366-0e88-4411-ab42-b0a968435a9b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-256635757-172.17.0.8-1595502616616:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39702,DS-d4dd1fd3-8dac-4226-9ebf-18b4630d8039,DISK], DatanodeInfoWithStorage[127.0.0.1:43971,DS-0dc9a4bc-f65b-4ffa-b046-e28b6e5337d9,DISK], DatanodeInfoWithStorage[127.0.0.1:35383,DS-260b46db-0d20-4914-81b3-f5c863af348f,DISK], DatanodeInfoWithStorage[127.0.0.1:45725,DS-87f40484-ef30-4337-88a6-aab674e89c84,DISK], DatanodeInfoWithStorage[127.0.0.1:39758,DS-dbf8d86e-6b0c-4c1f-b47d-6695d7ed2ed0,DISK], DatanodeInfoWithStorage[127.0.0.1:41766,DS-acd4041b-7002-412b-9d64-ff1b2757ec3a,DISK], DatanodeInfoWithStorage[127.0.0.1:46332,DS-8dceea43-ea05-4fb6-9923-16dd60aa8a4a,DISK], DatanodeInfoWithStorage[127.0.0.1:37544,DS-390a8366-0e88-4411-ab42-b0a968435a9b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1544668519-172.17.0.8-1595503007312:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39984,DS-d6334e76-6a6f-4abe-8c8c-7dab22e7d693,DISK], DatanodeInfoWithStorage[127.0.0.1:36879,DS-2296bfa0-1dbd-4b9a-9fe4-18703bb51130,DISK], DatanodeInfoWithStorage[127.0.0.1:46667,DS-47cbfaba-6501-4e82-b27f-6f0d1392e90a,DISK], DatanodeInfoWithStorage[127.0.0.1:37763,DS-6b2e0e49-06b2-4bab-b934-6306353dd265,DISK], DatanodeInfoWithStorage[127.0.0.1:41906,DS-39d3105b-3b86-4b59-b3f3-d549d8c30302,DISK], DatanodeInfoWithStorage[127.0.0.1:37376,DS-af53848e-709d-4f0c-a4df-590a2ab0c49e,DISK], DatanodeInfoWithStorage[127.0.0.1:39215,DS-9dac808f-09c3-402a-b0d8-eb280f332f13,DISK], DatanodeInfoWithStorage[127.0.0.1:34895,DS-ff8e468c-a0f5-4270-91f4-9f599dfcead6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1544668519-172.17.0.8-1595503007312:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39984,DS-d6334e76-6a6f-4abe-8c8c-7dab22e7d693,DISK], DatanodeInfoWithStorage[127.0.0.1:36879,DS-2296bfa0-1dbd-4b9a-9fe4-18703bb51130,DISK], DatanodeInfoWithStorage[127.0.0.1:46667,DS-47cbfaba-6501-4e82-b27f-6f0d1392e90a,DISK], DatanodeInfoWithStorage[127.0.0.1:37763,DS-6b2e0e49-06b2-4bab-b934-6306353dd265,DISK], DatanodeInfoWithStorage[127.0.0.1:41906,DS-39d3105b-3b86-4b59-b3f3-d549d8c30302,DISK], DatanodeInfoWithStorage[127.0.0.1:37376,DS-af53848e-709d-4f0c-a4df-590a2ab0c49e,DISK], DatanodeInfoWithStorage[127.0.0.1:39215,DS-9dac808f-09c3-402a-b0d8-eb280f332f13,DISK], DatanodeInfoWithStorage[127.0.0.1:34895,DS-ff8e468c-a0f5-4270-91f4-9f599dfcead6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1305053710-172.17.0.8-1595503150948:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37716,DS-3ce3a131-9124-40b9-8a16-3f106951890e,DISK], DatanodeInfoWithStorage[127.0.0.1:43631,DS-921047b5-9f53-479b-8cd1-61d603df0996,DISK], DatanodeInfoWithStorage[127.0.0.1:40313,DS-fcbc3869-1475-47de-a714-dec2d62a72fc,DISK], DatanodeInfoWithStorage[127.0.0.1:39833,DS-c966f74f-fe4e-4e6e-8282-737b7392e7cc,DISK], DatanodeInfoWithStorage[127.0.0.1:45493,DS-db9c9e25-6e74-4f27-951b-9a4b47a4fcf4,DISK], DatanodeInfoWithStorage[127.0.0.1:43393,DS-0aaf13bd-9178-478b-8762-6ebdb1bd7d8d,DISK], DatanodeInfoWithStorage[127.0.0.1:39351,DS-6f6a09be-67a7-4cb9-81c7-94bfb9643b6f,DISK], DatanodeInfoWithStorage[127.0.0.1:46023,DS-4c8a72da-7aa9-4d13-bf02-9562f9f07d25,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1305053710-172.17.0.8-1595503150948:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37716,DS-3ce3a131-9124-40b9-8a16-3f106951890e,DISK], DatanodeInfoWithStorage[127.0.0.1:43631,DS-921047b5-9f53-479b-8cd1-61d603df0996,DISK], DatanodeInfoWithStorage[127.0.0.1:40313,DS-fcbc3869-1475-47de-a714-dec2d62a72fc,DISK], DatanodeInfoWithStorage[127.0.0.1:39833,DS-c966f74f-fe4e-4e6e-8282-737b7392e7cc,DISK], DatanodeInfoWithStorage[127.0.0.1:45493,DS-db9c9e25-6e74-4f27-951b-9a4b47a4fcf4,DISK], DatanodeInfoWithStorage[127.0.0.1:43393,DS-0aaf13bd-9178-478b-8762-6ebdb1bd7d8d,DISK], DatanodeInfoWithStorage[127.0.0.1:39351,DS-6f6a09be-67a7-4cb9-81c7-94bfb9643b6f,DISK], DatanodeInfoWithStorage[127.0.0.1:46023,DS-4c8a72da-7aa9-4d13-bf02-9562f9f07d25,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1831247108-172.17.0.8-1595503349494:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41089,DS-68193b71-1545-48a4-befa-d206958958df,DISK], DatanodeInfoWithStorage[127.0.0.1:41888,DS-3fa729a2-8583-4d9a-81f5-b4f541025f56,DISK], DatanodeInfoWithStorage[127.0.0.1:42296,DS-f5ff4c3f-852e-48ef-8a8f-7cea0ecf7cac,DISK], DatanodeInfoWithStorage[127.0.0.1:44643,DS-cb7a4bc5-0cdb-449e-bf0e-18f32cb8f4d2,DISK], DatanodeInfoWithStorage[127.0.0.1:40089,DS-959b6e46-3239-469f-89c2-079807f96580,DISK], DatanodeInfoWithStorage[127.0.0.1:34479,DS-dd91936b-ddbe-4e82-ab0f-427708cc3f8a,DISK], DatanodeInfoWithStorage[127.0.0.1:33535,DS-8aaff30d-866f-4e8f-927e-0ca3d23929c6,DISK], DatanodeInfoWithStorage[127.0.0.1:46789,DS-3d19c5c7-75c9-4d12-b3e8-7eb00c465ae9,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1831247108-172.17.0.8-1595503349494:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41089,DS-68193b71-1545-48a4-befa-d206958958df,DISK], DatanodeInfoWithStorage[127.0.0.1:41888,DS-3fa729a2-8583-4d9a-81f5-b4f541025f56,DISK], DatanodeInfoWithStorage[127.0.0.1:42296,DS-f5ff4c3f-852e-48ef-8a8f-7cea0ecf7cac,DISK], DatanodeInfoWithStorage[127.0.0.1:44643,DS-cb7a4bc5-0cdb-449e-bf0e-18f32cb8f4d2,DISK], DatanodeInfoWithStorage[127.0.0.1:40089,DS-959b6e46-3239-469f-89c2-079807f96580,DISK], DatanodeInfoWithStorage[127.0.0.1:34479,DS-dd91936b-ddbe-4e82-ab0f-427708cc3f8a,DISK], DatanodeInfoWithStorage[127.0.0.1:33535,DS-8aaff30d-866f-4e8f-927e-0ca3d23929c6,DISK], DatanodeInfoWithStorage[127.0.0.1:46789,DS-3d19c5c7-75c9-4d12-b3e8-7eb00c465ae9,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery19
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1869610405-172.17.0.8-1595503787647:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36293,DS-bfc45d15-8393-4c82-86fb-b1fe808906b2,DISK], DatanodeInfoWithStorage[127.0.0.1:35120,DS-008fdf84-d0cc-4614-8f7d-fa318d169000,DISK], DatanodeInfoWithStorage[127.0.0.1:39625,DS-6df62c2f-71a7-4447-ba98-8c730c23b884,DISK], DatanodeInfoWithStorage[127.0.0.1:38613,DS-2d88eafe-cf97-4700-b52c-810b9fcc8b72,DISK], DatanodeInfoWithStorage[127.0.0.1:46259,DS-5b761c2e-51d9-4c4b-bc0e-e0754ec407aa,DISK], DatanodeInfoWithStorage[127.0.0.1:34222,DS-7180d254-96cf-4e38-8d50-aa65c8ceb566,DISK], DatanodeInfoWithStorage[127.0.0.1:37842,DS-69e52976-55bd-40be-a053-891b5553186a,DISK], DatanodeInfoWithStorage[127.0.0.1:35259,DS-4e5d844d-8e60-483c-a16b-5e86bbdae40b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1869610405-172.17.0.8-1595503787647:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36293,DS-bfc45d15-8393-4c82-86fb-b1fe808906b2,DISK], DatanodeInfoWithStorage[127.0.0.1:35120,DS-008fdf84-d0cc-4614-8f7d-fa318d169000,DISK], DatanodeInfoWithStorage[127.0.0.1:39625,DS-6df62c2f-71a7-4447-ba98-8c730c23b884,DISK], DatanodeInfoWithStorage[127.0.0.1:38613,DS-2d88eafe-cf97-4700-b52c-810b9fcc8b72,DISK], DatanodeInfoWithStorage[127.0.0.1:46259,DS-5b761c2e-51d9-4c4b-bc0e-e0754ec407aa,DISK], DatanodeInfoWithStorage[127.0.0.1:34222,DS-7180d254-96cf-4e38-8d50-aa65c8ceb566,DISK], DatanodeInfoWithStorage[127.0.0.1:37842,DS-69e52976-55bd-40be-a053-891b5553186a,DISK], DatanodeInfoWithStorage[127.0.0.1:35259,DS-4e5d844d-8e60-483c-a16b-5e86bbdae40b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery19(TestFileChecksum.java:519)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


v1v2 failed with probability 11 out of 50
v1v1v2v2 failed with probability 13 out of 50
result: false positive !!!
Total execution time in seconds : 5116

reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1762396477-172.17.0.12-1595663838325:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38675,DS-17541320-6f74-4ad8-9fe1-46ed34c13389,DISK], DatanodeInfoWithStorage[127.0.0.1:34267,DS-dff9511c-cc0f-414b-8e68-17ee13f932a8,DISK], DatanodeInfoWithStorage[127.0.0.1:41287,DS-6230772f-d837-4d0e-ab9e-cc37d5da1853,DISK], DatanodeInfoWithStorage[127.0.0.1:41534,DS-d2c9db6f-30f6-45e2-923a-9a575a45fd9a,DISK], DatanodeInfoWithStorage[127.0.0.1:35627,DS-08e93fea-848c-4be5-8ffb-79685222b187,DISK], DatanodeInfoWithStorage[127.0.0.1:46688,DS-b1fa9e96-12fc-44a9-b2d5-0d2b10039867,DISK], DatanodeInfoWithStorage[127.0.0.1:34549,DS-d47491c4-065f-415b-8581-de21690d6db6,DISK], DatanodeInfoWithStorage[127.0.0.1:43350,DS-88bf9056-8c37-4c20-a640-11621158401a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1762396477-172.17.0.12-1595663838325:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38675,DS-17541320-6f74-4ad8-9fe1-46ed34c13389,DISK], DatanodeInfoWithStorage[127.0.0.1:34267,DS-dff9511c-cc0f-414b-8e68-17ee13f932a8,DISK], DatanodeInfoWithStorage[127.0.0.1:41287,DS-6230772f-d837-4d0e-ab9e-cc37d5da1853,DISK], DatanodeInfoWithStorage[127.0.0.1:41534,DS-d2c9db6f-30f6-45e2-923a-9a575a45fd9a,DISK], DatanodeInfoWithStorage[127.0.0.1:35627,DS-08e93fea-848c-4be5-8ffb-79685222b187,DISK], DatanodeInfoWithStorage[127.0.0.1:46688,DS-b1fa9e96-12fc-44a9-b2d5-0d2b10039867,DISK], DatanodeInfoWithStorage[127.0.0.1:34549,DS-d47491c4-065f-415b-8581-de21690d6db6,DISK], DatanodeInfoWithStorage[127.0.0.1:43350,DS-88bf9056-8c37-4c20-a640-11621158401a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1243517110-172.17.0.12-1595663903944:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46228,DS-5074931d-1568-473a-8e96-725db5ff7979,DISK], DatanodeInfoWithStorage[127.0.0.1:36461,DS-c555f209-33c0-4ba9-8bd6-1839e3157f6b,DISK], DatanodeInfoWithStorage[127.0.0.1:38901,DS-2b5a14e1-b8f8-4366-ac2d-02439af9098d,DISK], DatanodeInfoWithStorage[127.0.0.1:33400,DS-6afe1e47-13b3-47b8-8734-1c608dd18549,DISK], DatanodeInfoWithStorage[127.0.0.1:34333,DS-d53e044b-e416-446d-835d-7b7c2011afac,DISK], DatanodeInfoWithStorage[127.0.0.1:34619,DS-f4e77d05-bce6-4ea9-ae9f-2948b16fbefe,DISK], DatanodeInfoWithStorage[127.0.0.1:34765,DS-382f5a06-57a4-43aa-be61-ca29a9efc8fa,DISK], DatanodeInfoWithStorage[127.0.0.1:41543,DS-677fc359-0702-400f-bd25-4d56acb7d5f1,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1243517110-172.17.0.12-1595663903944:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46228,DS-5074931d-1568-473a-8e96-725db5ff7979,DISK], DatanodeInfoWithStorage[127.0.0.1:36461,DS-c555f209-33c0-4ba9-8bd6-1839e3157f6b,DISK], DatanodeInfoWithStorage[127.0.0.1:38901,DS-2b5a14e1-b8f8-4366-ac2d-02439af9098d,DISK], DatanodeInfoWithStorage[127.0.0.1:33400,DS-6afe1e47-13b3-47b8-8734-1c608dd18549,DISK], DatanodeInfoWithStorage[127.0.0.1:34333,DS-d53e044b-e416-446d-835d-7b7c2011afac,DISK], DatanodeInfoWithStorage[127.0.0.1:34619,DS-f4e77d05-bce6-4ea9-ae9f-2948b16fbefe,DISK], DatanodeInfoWithStorage[127.0.0.1:34765,DS-382f5a06-57a4-43aa-be61-ca29a9efc8fa,DISK], DatanodeInfoWithStorage[127.0.0.1:41543,DS-677fc359-0702-400f-bd25-4d56acb7d5f1,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-790818811-172.17.0.12-1595664148649:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38012,DS-5e002e45-3c67-487c-952a-c074d7ccd45f,DISK], DatanodeInfoWithStorage[127.0.0.1:44828,DS-d89697cb-093f-4ad2-ba57-900084dc59b9,DISK], DatanodeInfoWithStorage[127.0.0.1:33296,DS-a5e2de76-f389-4d91-9eae-eecb2b683ba2,DISK], DatanodeInfoWithStorage[127.0.0.1:38303,DS-9a46a179-09ed-4e3a-b7fd-2c226ae66091,DISK], DatanodeInfoWithStorage[127.0.0.1:45246,DS-19ab42c4-7f08-402c-9d5d-125d156926f3,DISK], DatanodeInfoWithStorage[127.0.0.1:37899,DS-6366a7a3-77e7-47b9-b4ee-dfdbbe7eaeff,DISK], DatanodeInfoWithStorage[127.0.0.1:35249,DS-5945f8d2-bbaa-426b-89c0-ddff6d2b587b,DISK], DatanodeInfoWithStorage[127.0.0.1:33986,DS-75eb4014-7294-462f-b11d-d34b93366333,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-790818811-172.17.0.12-1595664148649:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38012,DS-5e002e45-3c67-487c-952a-c074d7ccd45f,DISK], DatanodeInfoWithStorage[127.0.0.1:44828,DS-d89697cb-093f-4ad2-ba57-900084dc59b9,DISK], DatanodeInfoWithStorage[127.0.0.1:33296,DS-a5e2de76-f389-4d91-9eae-eecb2b683ba2,DISK], DatanodeInfoWithStorage[127.0.0.1:38303,DS-9a46a179-09ed-4e3a-b7fd-2c226ae66091,DISK], DatanodeInfoWithStorage[127.0.0.1:45246,DS-19ab42c4-7f08-402c-9d5d-125d156926f3,DISK], DatanodeInfoWithStorage[127.0.0.1:37899,DS-6366a7a3-77e7-47b9-b4ee-dfdbbe7eaeff,DISK], DatanodeInfoWithStorage[127.0.0.1:35249,DS-5945f8d2-bbaa-426b-89c0-ddff6d2b587b,DISK], DatanodeInfoWithStorage[127.0.0.1:33986,DS-75eb4014-7294-462f-b11d-d34b93366333,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1318766573-172.17.0.12-1595664752693:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38894,DS-cf65fbd9-3fbf-48ee-99b8-1efb5733cb20,DISK], DatanodeInfoWithStorage[127.0.0.1:42084,DS-e9ea4132-ce49-4e38-a128-9c91cca8b05b,DISK], DatanodeInfoWithStorage[127.0.0.1:40544,DS-f6969b80-1157-4da8-b4ba-d0fd9d48543f,DISK], DatanodeInfoWithStorage[127.0.0.1:37801,DS-67d66c74-7cbb-4e39-8834-f22ff6890edf,DISK], DatanodeInfoWithStorage[127.0.0.1:36539,DS-6889fffa-9d2f-462a-ac17-e6d9056c9c68,DISK], DatanodeInfoWithStorage[127.0.0.1:45575,DS-7c608730-adb6-4ef9-8ce2-0f59d382e8a2,DISK], DatanodeInfoWithStorage[127.0.0.1:33284,DS-a801665a-fe6d-470d-9f2e-c06bd6c73041,DISK], DatanodeInfoWithStorage[127.0.0.1:40655,DS-e24c0b66-93b4-4352-a03d-87522b672f2f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1318766573-172.17.0.12-1595664752693:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38894,DS-cf65fbd9-3fbf-48ee-99b8-1efb5733cb20,DISK], DatanodeInfoWithStorage[127.0.0.1:42084,DS-e9ea4132-ce49-4e38-a128-9c91cca8b05b,DISK], DatanodeInfoWithStorage[127.0.0.1:40544,DS-f6969b80-1157-4da8-b4ba-d0fd9d48543f,DISK], DatanodeInfoWithStorage[127.0.0.1:37801,DS-67d66c74-7cbb-4e39-8834-f22ff6890edf,DISK], DatanodeInfoWithStorage[127.0.0.1:36539,DS-6889fffa-9d2f-462a-ac17-e6d9056c9c68,DISK], DatanodeInfoWithStorage[127.0.0.1:45575,DS-7c608730-adb6-4ef9-8ce2-0f59d382e8a2,DISK], DatanodeInfoWithStorage[127.0.0.1:33284,DS-a801665a-fe6d-470d-9f2e-c06bd6c73041,DISK], DatanodeInfoWithStorage[127.0.0.1:40655,DS-e24c0b66-93b4-4352-a03d-87522b672f2f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1062742254-172.17.0.12-1595664861283:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40401,DS-8aa09c92-62bd-46b0-8b30-eada1054b045,DISK], DatanodeInfoWithStorage[127.0.0.1:41847,DS-01d48ad3-d787-421f-947f-54b4976ec8a2,DISK], DatanodeInfoWithStorage[127.0.0.1:37999,DS-dad62ae4-7f38-45af-bef9-eac722853d50,DISK], DatanodeInfoWithStorage[127.0.0.1:38781,DS-690e278b-2dfd-4231-8c50-7b7e447b1e03,DISK], DatanodeInfoWithStorage[127.0.0.1:45396,DS-6460e9f7-574c-4540-90dc-8ec29d1c0608,DISK], DatanodeInfoWithStorage[127.0.0.1:42967,DS-67e57dd4-20f0-4f44-abe3-3a108c36ec21,DISK], DatanodeInfoWithStorage[127.0.0.1:45338,DS-c5ccd6ba-b1f7-491a-8dd0-9deb58455c5d,DISK], DatanodeInfoWithStorage[127.0.0.1:42483,DS-0035c6c5-a699-47dc-a7c9-7c557120a4ea,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1062742254-172.17.0.12-1595664861283:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40401,DS-8aa09c92-62bd-46b0-8b30-eada1054b045,DISK], DatanodeInfoWithStorage[127.0.0.1:41847,DS-01d48ad3-d787-421f-947f-54b4976ec8a2,DISK], DatanodeInfoWithStorage[127.0.0.1:37999,DS-dad62ae4-7f38-45af-bef9-eac722853d50,DISK], DatanodeInfoWithStorage[127.0.0.1:38781,DS-690e278b-2dfd-4231-8c50-7b7e447b1e03,DISK], DatanodeInfoWithStorage[127.0.0.1:45396,DS-6460e9f7-574c-4540-90dc-8ec29d1c0608,DISK], DatanodeInfoWithStorage[127.0.0.1:42967,DS-67e57dd4-20f0-4f44-abe3-3a108c36ec21,DISK], DatanodeInfoWithStorage[127.0.0.1:45338,DS-c5ccd6ba-b1f7-491a-8dd0-9deb58455c5d,DISK], DatanodeInfoWithStorage[127.0.0.1:42483,DS-0035c6c5-a699-47dc-a7c9-7c557120a4ea,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1629577952-172.17.0.12-1595664996723:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41402,DS-8d6ca56d-5630-42e9-90f2-21416041d84f,DISK], DatanodeInfoWithStorage[127.0.0.1:41084,DS-3d43eb85-ea98-46ae-a2b6-40ca0ef980f1,DISK], DatanodeInfoWithStorage[127.0.0.1:36258,DS-e5825aa8-8a89-42b1-ad1f-60926b525432,DISK], DatanodeInfoWithStorage[127.0.0.1:36418,DS-73151be9-1da5-4ed3-9b45-1e1c2a4f1636,DISK], DatanodeInfoWithStorage[127.0.0.1:40630,DS-e4bcf8fa-c241-4227-a090-3284a2621aa7,DISK], DatanodeInfoWithStorage[127.0.0.1:33369,DS-b84afee6-759d-4e5a-9cd7-b29deec35d5c,DISK], DatanodeInfoWithStorage[127.0.0.1:45708,DS-58c14cc9-d7e3-4ec7-828e-8844244114f3,DISK], DatanodeInfoWithStorage[127.0.0.1:44925,DS-e9925b5f-5859-4e3a-982a-a630a7543815,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1629577952-172.17.0.12-1595664996723:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41402,DS-8d6ca56d-5630-42e9-90f2-21416041d84f,DISK], DatanodeInfoWithStorage[127.0.0.1:41084,DS-3d43eb85-ea98-46ae-a2b6-40ca0ef980f1,DISK], DatanodeInfoWithStorage[127.0.0.1:36258,DS-e5825aa8-8a89-42b1-ad1f-60926b525432,DISK], DatanodeInfoWithStorage[127.0.0.1:36418,DS-73151be9-1da5-4ed3-9b45-1e1c2a4f1636,DISK], DatanodeInfoWithStorage[127.0.0.1:40630,DS-e4bcf8fa-c241-4227-a090-3284a2621aa7,DISK], DatanodeInfoWithStorage[127.0.0.1:33369,DS-b84afee6-759d-4e5a-9cd7-b29deec35d5c,DISK], DatanodeInfoWithStorage[127.0.0.1:45708,DS-58c14cc9-d7e3-4ec7-828e-8844244114f3,DISK], DatanodeInfoWithStorage[127.0.0.1:44925,DS-e9925b5f-5859-4e3a-982a-a630a7543815,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1571116661-172.17.0.12-1595665138539:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41340,DS-813503bf-b034-471e-8656-989846e19b7c,DISK], DatanodeInfoWithStorage[127.0.0.1:40581,DS-cb4411dd-1586-43f5-b6db-eaa87347635f,DISK], DatanodeInfoWithStorage[127.0.0.1:34693,DS-f83eb2f6-b6b9-408d-be3e-e05af47681b1,DISK], DatanodeInfoWithStorage[127.0.0.1:36139,DS-66d0ac9f-8a32-4086-9513-0265d5d75d3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46477,DS-4f28d77a-186c-452b-8a4b-73c35c0d1724,DISK], DatanodeInfoWithStorage[127.0.0.1:34890,DS-dc8b5aeb-bf5f-4d86-b8fa-c0070c8fa92b,DISK], DatanodeInfoWithStorage[127.0.0.1:34274,DS-5238fd8a-edc2-4029-a65f-4f25d55aa8aa,DISK], DatanodeInfoWithStorage[127.0.0.1:46401,DS-d3e77415-408e-4a43-9d7a-90fe1a82128b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1571116661-172.17.0.12-1595665138539:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41340,DS-813503bf-b034-471e-8656-989846e19b7c,DISK], DatanodeInfoWithStorage[127.0.0.1:40581,DS-cb4411dd-1586-43f5-b6db-eaa87347635f,DISK], DatanodeInfoWithStorage[127.0.0.1:34693,DS-f83eb2f6-b6b9-408d-be3e-e05af47681b1,DISK], DatanodeInfoWithStorage[127.0.0.1:36139,DS-66d0ac9f-8a32-4086-9513-0265d5d75d3c,DISK], DatanodeInfoWithStorage[127.0.0.1:46477,DS-4f28d77a-186c-452b-8a4b-73c35c0d1724,DISK], DatanodeInfoWithStorage[127.0.0.1:34890,DS-dc8b5aeb-bf5f-4d86-b8fa-c0070c8fa92b,DISK], DatanodeInfoWithStorage[127.0.0.1:34274,DS-5238fd8a-edc2-4029-a65f-4f25d55aa8aa,DISK], DatanodeInfoWithStorage[127.0.0.1:46401,DS-d3e77415-408e-4a43-9d7a-90fe1a82128b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1533754734-172.17.0.12-1595665210589:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44063,DS-15e77a95-eed1-40e1-8719-a16364f3c5ac,DISK], DatanodeInfoWithStorage[127.0.0.1:34507,DS-2b55b01a-eb73-4f47-9a0e-490265228116,DISK], DatanodeInfoWithStorage[127.0.0.1:40658,DS-9f734aea-ab1f-4f3c-8ff4-93906d53ab35,DISK], DatanodeInfoWithStorage[127.0.0.1:45582,DS-6f5002bd-c5e3-4653-921e-12c073831dab,DISK], DatanodeInfoWithStorage[127.0.0.1:34921,DS-35938554-1803-46cd-b307-6cac63d379e7,DISK], DatanodeInfoWithStorage[127.0.0.1:43786,DS-78d6ab56-4d83-4abf-9b9c-5a3d3002aa54,DISK], DatanodeInfoWithStorage[127.0.0.1:36966,DS-73c6e3f5-d022-48a7-8ee8-fda15ddfabbe,DISK], DatanodeInfoWithStorage[127.0.0.1:45947,DS-11e05cdf-9e00-4ab3-bdba-d5eb19eec139,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1533754734-172.17.0.12-1595665210589:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44063,DS-15e77a95-eed1-40e1-8719-a16364f3c5ac,DISK], DatanodeInfoWithStorage[127.0.0.1:34507,DS-2b55b01a-eb73-4f47-9a0e-490265228116,DISK], DatanodeInfoWithStorage[127.0.0.1:40658,DS-9f734aea-ab1f-4f3c-8ff4-93906d53ab35,DISK], DatanodeInfoWithStorage[127.0.0.1:45582,DS-6f5002bd-c5e3-4653-921e-12c073831dab,DISK], DatanodeInfoWithStorage[127.0.0.1:34921,DS-35938554-1803-46cd-b307-6cac63d379e7,DISK], DatanodeInfoWithStorage[127.0.0.1:43786,DS-78d6ab56-4d83-4abf-9b9c-5a3d3002aa54,DISK], DatanodeInfoWithStorage[127.0.0.1:36966,DS-73c6e3f5-d022-48a7-8ee8-fda15ddfabbe,DISK], DatanodeInfoWithStorage[127.0.0.1:45947,DS-11e05cdf-9e00-4ab3-bdba-d5eb19eec139,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1254295845-172.17.0.12-1595665356131:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46756,DS-e537256f-525d-43a1-9026-d55118cfe6ee,DISK], DatanodeInfoWithStorage[127.0.0.1:44100,DS-4a986cf0-1f16-4d14-aaf1-2a6ecb02e7c6,DISK], DatanodeInfoWithStorage[127.0.0.1:44058,DS-b27af081-bd76-43c4-a055-32643954ab6e,DISK], DatanodeInfoWithStorage[127.0.0.1:41785,DS-fbf00609-b08c-4edb-b2cd-e7891311a9be,DISK], DatanodeInfoWithStorage[127.0.0.1:35472,DS-cccb0af6-6667-4a8f-b022-137047aa2b5d,DISK], DatanodeInfoWithStorage[127.0.0.1:38374,DS-5ee90d54-7a89-47f6-b0c6-b722140f425a,DISK], DatanodeInfoWithStorage[127.0.0.1:38686,DS-a1c2aee4-d69e-4baa-9274-0177eea46457,DISK], DatanodeInfoWithStorage[127.0.0.1:42586,DS-20e159e5-8b0a-49f7-bc97-def5ce312b0f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1254295845-172.17.0.12-1595665356131:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46756,DS-e537256f-525d-43a1-9026-d55118cfe6ee,DISK], DatanodeInfoWithStorage[127.0.0.1:44100,DS-4a986cf0-1f16-4d14-aaf1-2a6ecb02e7c6,DISK], DatanodeInfoWithStorage[127.0.0.1:44058,DS-b27af081-bd76-43c4-a055-32643954ab6e,DISK], DatanodeInfoWithStorage[127.0.0.1:41785,DS-fbf00609-b08c-4edb-b2cd-e7891311a9be,DISK], DatanodeInfoWithStorage[127.0.0.1:35472,DS-cccb0af6-6667-4a8f-b022-137047aa2b5d,DISK], DatanodeInfoWithStorage[127.0.0.1:38374,DS-5ee90d54-7a89-47f6-b0c6-b722140f425a,DISK], DatanodeInfoWithStorage[127.0.0.1:38686,DS-a1c2aee4-d69e-4baa-9274-0177eea46457,DISK], DatanodeInfoWithStorage[127.0.0.1:42586,DS-20e159e5-8b0a-49f7-bc97-def5ce312b0f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-569405521-172.17.0.12-1595665610188:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46290,DS-b2af3042-8944-4795-898f-9c44d823e818,DISK], DatanodeInfoWithStorage[127.0.0.1:43305,DS-a2eef730-c82c-4b99-8b84-d3ab870932b2,DISK], DatanodeInfoWithStorage[127.0.0.1:40254,DS-7cf94afd-8464-42e1-82f6-c472e90bfd6d,DISK], DatanodeInfoWithStorage[127.0.0.1:39179,DS-a683bba3-2bd4-428f-9831-c41836aa6788,DISK], DatanodeInfoWithStorage[127.0.0.1:38159,DS-2e18fafd-2a88-4b13-9f43-43a0b3c4a78b,DISK], DatanodeInfoWithStorage[127.0.0.1:44653,DS-1bbcf19e-1045-40d0-bce8-ff3d5db55790,DISK], DatanodeInfoWithStorage[127.0.0.1:34024,DS-2c082329-4e79-4baa-b65c-53fe2f7dba74,DISK], DatanodeInfoWithStorage[127.0.0.1:40407,DS-71ff59a9-aaeb-48f9-ab08-17e39dd7f0a7,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-569405521-172.17.0.12-1595665610188:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46290,DS-b2af3042-8944-4795-898f-9c44d823e818,DISK], DatanodeInfoWithStorage[127.0.0.1:43305,DS-a2eef730-c82c-4b99-8b84-d3ab870932b2,DISK], DatanodeInfoWithStorage[127.0.0.1:40254,DS-7cf94afd-8464-42e1-82f6-c472e90bfd6d,DISK], DatanodeInfoWithStorage[127.0.0.1:39179,DS-a683bba3-2bd4-428f-9831-c41836aa6788,DISK], DatanodeInfoWithStorage[127.0.0.1:38159,DS-2e18fafd-2a88-4b13-9f43-43a0b3c4a78b,DISK], DatanodeInfoWithStorage[127.0.0.1:44653,DS-1bbcf19e-1045-40d0-bce8-ff3d5db55790,DISK], DatanodeInfoWithStorage[127.0.0.1:34024,DS-2c082329-4e79-4baa-b65c-53fe2f7dba74,DISK], DatanodeInfoWithStorage[127.0.0.1:40407,DS-71ff59a9-aaeb-48f9-ab08-17e39dd7f0a7,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1973304559-172.17.0.12-1595665791099:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35966,DS-256a2940-fce6-46f6-b542-5aa05b095560,DISK], DatanodeInfoWithStorage[127.0.0.1:46226,DS-ab39bcca-c174-471b-b96a-dc18886a6e84,DISK], DatanodeInfoWithStorage[127.0.0.1:41840,DS-1e166c65-b532-4d9a-9271-e7e65c90e44c,DISK], DatanodeInfoWithStorage[127.0.0.1:35541,DS-51bf5e77-c000-4f71-a5fa-ca53fb7ed367,DISK], DatanodeInfoWithStorage[127.0.0.1:38573,DS-b5d1687c-9da6-49b2-a362-9efab4bb29ff,DISK], DatanodeInfoWithStorage[127.0.0.1:34896,DS-d342192e-77a9-491e-9ca0-eb8223d0a32d,DISK], DatanodeInfoWithStorage[127.0.0.1:43246,DS-89bfd110-a364-4182-ac2a-048207d19cb8,DISK], DatanodeInfoWithStorage[127.0.0.1:35365,DS-f783e925-5e56-460c-92ae-ea8f175d2c47,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1973304559-172.17.0.12-1595665791099:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35966,DS-256a2940-fce6-46f6-b542-5aa05b095560,DISK], DatanodeInfoWithStorage[127.0.0.1:46226,DS-ab39bcca-c174-471b-b96a-dc18886a6e84,DISK], DatanodeInfoWithStorage[127.0.0.1:41840,DS-1e166c65-b532-4d9a-9271-e7e65c90e44c,DISK], DatanodeInfoWithStorage[127.0.0.1:35541,DS-51bf5e77-c000-4f71-a5fa-ca53fb7ed367,DISK], DatanodeInfoWithStorage[127.0.0.1:38573,DS-b5d1687c-9da6-49b2-a362-9efab4bb29ff,DISK], DatanodeInfoWithStorage[127.0.0.1:34896,DS-d342192e-77a9-491e-9ca0-eb8223d0a32d,DISK], DatanodeInfoWithStorage[127.0.0.1:43246,DS-89bfd110-a364-4182-ac2a-048207d19cb8,DISK], DatanodeInfoWithStorage[127.0.0.1:35365,DS-f783e925-5e56-460c-92ae-ea8f175d2c47,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1961215730-172.17.0.12-1595666291713:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36319,DS-b708dc45-daed-46ce-a6f7-f75896900de2,DISK], DatanodeInfoWithStorage[127.0.0.1:38480,DS-71bac8b9-fee6-4ad2-ac99-2d29061025b6,DISK], DatanodeInfoWithStorage[127.0.0.1:34290,DS-94a79db2-b5c8-4e5d-8dfc-fcd0f98bcfae,DISK], DatanodeInfoWithStorage[127.0.0.1:44840,DS-28aae0b7-71b5-4c18-997e-d66acc9f4973,DISK], DatanodeInfoWithStorage[127.0.0.1:41594,DS-90a7dc3e-81ef-4786-87ee-6bac1825df78,DISK], DatanodeInfoWithStorage[127.0.0.1:46083,DS-28135785-e840-4721-b1f8-229c2d5039f0,DISK], DatanodeInfoWithStorage[127.0.0.1:33370,DS-31a80ae8-1057-4a14-a0bd-3080a7b7a336,DISK], DatanodeInfoWithStorage[127.0.0.1:40093,DS-a3c9c515-7a8e-4eda-bd27-1be5d19584c9,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1961215730-172.17.0.12-1595666291713:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36319,DS-b708dc45-daed-46ce-a6f7-f75896900de2,DISK], DatanodeInfoWithStorage[127.0.0.1:38480,DS-71bac8b9-fee6-4ad2-ac99-2d29061025b6,DISK], DatanodeInfoWithStorage[127.0.0.1:34290,DS-94a79db2-b5c8-4e5d-8dfc-fcd0f98bcfae,DISK], DatanodeInfoWithStorage[127.0.0.1:44840,DS-28aae0b7-71b5-4c18-997e-d66acc9f4973,DISK], DatanodeInfoWithStorage[127.0.0.1:41594,DS-90a7dc3e-81ef-4786-87ee-6bac1825df78,DISK], DatanodeInfoWithStorage[127.0.0.1:46083,DS-28135785-e840-4721-b1f8-229c2d5039f0,DISK], DatanodeInfoWithStorage[127.0.0.1:33370,DS-31a80ae8-1057-4a14-a0bd-3080a7b7a336,DISK], DatanodeInfoWithStorage[127.0.0.1:40093,DS-a3c9c515-7a8e-4eda-bd27-1be5d19584c9,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-165383994-172.17.0.12-1595666445218:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43676,DS-99e2b367-5971-47cf-8814-be347a0a1edc,DISK], DatanodeInfoWithStorage[127.0.0.1:44576,DS-eaa46db0-7879-4223-a4ca-8b22737271a5,DISK], DatanodeInfoWithStorage[127.0.0.1:44756,DS-0f0e2523-15a0-40bd-87b5-48d040cdc6af,DISK], DatanodeInfoWithStorage[127.0.0.1:42637,DS-46aab9e6-6846-4494-8e54-9be844230020,DISK], DatanodeInfoWithStorage[127.0.0.1:43971,DS-1fdb28e3-84d3-4521-ae05-e43b22cf0ff2,DISK], DatanodeInfoWithStorage[127.0.0.1:44013,DS-9bf76295-53ba-4e8f-9046-53f3a4335d4a,DISK], DatanodeInfoWithStorage[127.0.0.1:41075,DS-81d252b3-0285-4292-ba86-ce6136a3fbed,DISK], DatanodeInfoWithStorage[127.0.0.1:34186,DS-1ac7572e-9def-4492-8243-6da7e20eb4c2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-165383994-172.17.0.12-1595666445218:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43676,DS-99e2b367-5971-47cf-8814-be347a0a1edc,DISK], DatanodeInfoWithStorage[127.0.0.1:44576,DS-eaa46db0-7879-4223-a4ca-8b22737271a5,DISK], DatanodeInfoWithStorage[127.0.0.1:44756,DS-0f0e2523-15a0-40bd-87b5-48d040cdc6af,DISK], DatanodeInfoWithStorage[127.0.0.1:42637,DS-46aab9e6-6846-4494-8e54-9be844230020,DISK], DatanodeInfoWithStorage[127.0.0.1:43971,DS-1fdb28e3-84d3-4521-ae05-e43b22cf0ff2,DISK], DatanodeInfoWithStorage[127.0.0.1:44013,DS-9bf76295-53ba-4e8f-9046-53f3a4335d4a,DISK], DatanodeInfoWithStorage[127.0.0.1:41075,DS-81d252b3-0285-4292-ba86-ce6136a3fbed,DISK], DatanodeInfoWithStorage[127.0.0.1:34186,DS-1ac7572e-9def-4492-8243-6da7e20eb4c2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-408562776-172.17.0.12-1595667023521:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38466,DS-1fe834ae-1779-45cb-909b-a1c6cb54c536,DISK], DatanodeInfoWithStorage[127.0.0.1:35509,DS-0048532f-3cc2-4fe2-979a-346b4229a563,DISK], DatanodeInfoWithStorage[127.0.0.1:40813,DS-52e914c4-3e89-4bb0-bd13-0f5e6765fc33,DISK], DatanodeInfoWithStorage[127.0.0.1:37337,DS-3397af7d-8c17-4344-b9b0-723acc3a395c,DISK], DatanodeInfoWithStorage[127.0.0.1:40765,DS-b20efd3c-f1d1-4b9f-b39a-03f76b312aa4,DISK], DatanodeInfoWithStorage[127.0.0.1:44072,DS-6b99eb77-a9a2-4fcf-94d9-87a0c18afa19,DISK], DatanodeInfoWithStorage[127.0.0.1:43809,DS-13cb1f07-58e6-4c8f-8df1-a35bfa096c16,DISK], DatanodeInfoWithStorage[127.0.0.1:40298,DS-5449cffb-a493-4ba5-ba42-4eb328e51fa9,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-408562776-172.17.0.12-1595667023521:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38466,DS-1fe834ae-1779-45cb-909b-a1c6cb54c536,DISK], DatanodeInfoWithStorage[127.0.0.1:35509,DS-0048532f-3cc2-4fe2-979a-346b4229a563,DISK], DatanodeInfoWithStorage[127.0.0.1:40813,DS-52e914c4-3e89-4bb0-bd13-0f5e6765fc33,DISK], DatanodeInfoWithStorage[127.0.0.1:37337,DS-3397af7d-8c17-4344-b9b0-723acc3a395c,DISK], DatanodeInfoWithStorage[127.0.0.1:40765,DS-b20efd3c-f1d1-4b9f-b39a-03f76b312aa4,DISK], DatanodeInfoWithStorage[127.0.0.1:44072,DS-6b99eb77-a9a2-4fcf-94d9-87a0c18afa19,DISK], DatanodeInfoWithStorage[127.0.0.1:43809,DS-13cb1f07-58e6-4c8f-8df1-a35bfa096c16,DISK], DatanodeInfoWithStorage[127.0.0.1:40298,DS-5449cffb-a493-4ba5-ba42-4eb328e51fa9,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1786802946-172.17.0.12-1595667404386:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40610,DS-99a5e198-e2b1-4275-bde9-f6f83292f081,DISK], DatanodeInfoWithStorage[127.0.0.1:33026,DS-cd73ccc9-92d9-4fc2-961c-56fc7f19a71a,DISK], DatanodeInfoWithStorage[127.0.0.1:43685,DS-9283b1eb-73dd-417f-b3ca-de4b60cf5b5f,DISK], DatanodeInfoWithStorage[127.0.0.1:34046,DS-398f9ecb-8b81-4924-ab43-52d22d10722e,DISK], DatanodeInfoWithStorage[127.0.0.1:45430,DS-ea74a1a8-c1c1-434c-897c-1fe5a995c0ae,DISK], DatanodeInfoWithStorage[127.0.0.1:34736,DS-0b2278f3-c80f-44b5-8e60-d1dd6ab9250f,DISK], DatanodeInfoWithStorage[127.0.0.1:37071,DS-72589a02-9e72-4f76-b150-bf2d573dd339,DISK], DatanodeInfoWithStorage[127.0.0.1:35968,DS-57eee52d-7c18-4b2c-8685-e0398b4e4985,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1786802946-172.17.0.12-1595667404386:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40610,DS-99a5e198-e2b1-4275-bde9-f6f83292f081,DISK], DatanodeInfoWithStorage[127.0.0.1:33026,DS-cd73ccc9-92d9-4fc2-961c-56fc7f19a71a,DISK], DatanodeInfoWithStorage[127.0.0.1:43685,DS-9283b1eb-73dd-417f-b3ca-de4b60cf5b5f,DISK], DatanodeInfoWithStorage[127.0.0.1:34046,DS-398f9ecb-8b81-4924-ab43-52d22d10722e,DISK], DatanodeInfoWithStorage[127.0.0.1:45430,DS-ea74a1a8-c1c1-434c-897c-1fe5a995c0ae,DISK], DatanodeInfoWithStorage[127.0.0.1:34736,DS-0b2278f3-c80f-44b5-8e60-d1dd6ab9250f,DISK], DatanodeInfoWithStorage[127.0.0.1:37071,DS-72589a02-9e72-4f76-b150-bf2d573dd339,DISK], DatanodeInfoWithStorage[127.0.0.1:35968,DS-57eee52d-7c18-4b2c-8685-e0398b4e4985,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1231282344-172.17.0.12-1595667581986:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42529,DS-89399d97-83ea-4672-b1bc-ecb38feb371e,DISK], DatanodeInfoWithStorage[127.0.0.1:45625,DS-5f0e6b39-e5e3-46bd-93dc-8bfca7e82eaf,DISK], DatanodeInfoWithStorage[127.0.0.1:40373,DS-9e1d3038-2995-4e07-ad6f-894f388a9cfc,DISK], DatanodeInfoWithStorage[127.0.0.1:41687,DS-58e43437-a034-4ea5-90a8-2108f1970c54,DISK], DatanodeInfoWithStorage[127.0.0.1:38188,DS-11a3eec9-9844-4724-9f7f-d98f576e3cdd,DISK], DatanodeInfoWithStorage[127.0.0.1:42984,DS-77fd5b7f-46bb-40e1-9a80-e5134cca5e28,DISK], DatanodeInfoWithStorage[127.0.0.1:40186,DS-71a2bff1-c307-4c03-99ee-a6ae6bb919f7,DISK], DatanodeInfoWithStorage[127.0.0.1:40460,DS-989cb466-c10a-4e62-b4a0-f011bf3fa3f2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1231282344-172.17.0.12-1595667581986:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42529,DS-89399d97-83ea-4672-b1bc-ecb38feb371e,DISK], DatanodeInfoWithStorage[127.0.0.1:45625,DS-5f0e6b39-e5e3-46bd-93dc-8bfca7e82eaf,DISK], DatanodeInfoWithStorage[127.0.0.1:40373,DS-9e1d3038-2995-4e07-ad6f-894f388a9cfc,DISK], DatanodeInfoWithStorage[127.0.0.1:41687,DS-58e43437-a034-4ea5-90a8-2108f1970c54,DISK], DatanodeInfoWithStorage[127.0.0.1:38188,DS-11a3eec9-9844-4724-9f7f-d98f576e3cdd,DISK], DatanodeInfoWithStorage[127.0.0.1:42984,DS-77fd5b7f-46bb-40e1-9a80-e5134cca5e28,DISK], DatanodeInfoWithStorage[127.0.0.1:40186,DS-71a2bff1-c307-4c03-99ee-a6ae6bb919f7,DISK], DatanodeInfoWithStorage[127.0.0.1:40460,DS-989cb466-c10a-4e62-b4a0-f011bf3fa3f2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1676936748-172.17.0.12-1595668463446:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40170,DS-91b74b2c-d3e6-480f-b48e-bdd2f04cb066,DISK], DatanodeInfoWithStorage[127.0.0.1:45181,DS-1d914315-ea44-4365-8320-9016f9bfd845,DISK], DatanodeInfoWithStorage[127.0.0.1:46154,DS-6d890fc0-5965-4fcb-852b-3076f5415180,DISK], DatanodeInfoWithStorage[127.0.0.1:36797,DS-04aab2f8-223f-4066-b988-22be4c8ecbee,DISK], DatanodeInfoWithStorage[127.0.0.1:40414,DS-e99b4504-b16e-417c-a178-b4603efa0918,DISK], DatanodeInfoWithStorage[127.0.0.1:41636,DS-49c0f505-b6e3-4605-b6db-9df8d83f57b9,DISK], DatanodeInfoWithStorage[127.0.0.1:35184,DS-12d523d1-abd6-4cac-9be5-821901e716be,DISK], DatanodeInfoWithStorage[127.0.0.1:41210,DS-89185c28-1f5d-4522-ac13-184b8e13e10a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1676936748-172.17.0.12-1595668463446:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40170,DS-91b74b2c-d3e6-480f-b48e-bdd2f04cb066,DISK], DatanodeInfoWithStorage[127.0.0.1:45181,DS-1d914315-ea44-4365-8320-9016f9bfd845,DISK], DatanodeInfoWithStorage[127.0.0.1:46154,DS-6d890fc0-5965-4fcb-852b-3076f5415180,DISK], DatanodeInfoWithStorage[127.0.0.1:36797,DS-04aab2f8-223f-4066-b988-22be4c8ecbee,DISK], DatanodeInfoWithStorage[127.0.0.1:40414,DS-e99b4504-b16e-417c-a178-b4603efa0918,DISK], DatanodeInfoWithStorage[127.0.0.1:41636,DS-49c0f505-b6e3-4605-b6db-9df8d83f57b9,DISK], DatanodeInfoWithStorage[127.0.0.1:35184,DS-12d523d1-abd6-4cac-9be5-821901e716be,DISK], DatanodeInfoWithStorage[127.0.0.1:41210,DS-89185c28-1f5d-4522-ac13-184b8e13e10a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-40809734-172.17.0.12-1595668818363:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34426,DS-dcce8d91-2f04-469d-82b6-a4186c018538,DISK], DatanodeInfoWithStorage[127.0.0.1:33002,DS-84f37838-00eb-4064-8996-68e20617c84b,DISK], DatanodeInfoWithStorage[127.0.0.1:45719,DS-b725eb1c-1a08-4e81-8207-3575ca4fdacb,DISK], DatanodeInfoWithStorage[127.0.0.1:38587,DS-6ada69be-3e1d-456d-ab27-f2b94389fb8e,DISK], DatanodeInfoWithStorage[127.0.0.1:38171,DS-0e7292f8-c014-45ef-9053-735069a6e5c1,DISK], DatanodeInfoWithStorage[127.0.0.1:43176,DS-afde6230-c721-4c10-9974-ff2d75f56584,DISK], DatanodeInfoWithStorage[127.0.0.1:45100,DS-5ce0b269-2894-4d24-8474-9b22f9eeb969,DISK], DatanodeInfoWithStorage[127.0.0.1:43659,DS-40c26a35-7510-4dcd-b7e7-4a42c5e575f7,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-40809734-172.17.0.12-1595668818363:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34426,DS-dcce8d91-2f04-469d-82b6-a4186c018538,DISK], DatanodeInfoWithStorage[127.0.0.1:33002,DS-84f37838-00eb-4064-8996-68e20617c84b,DISK], DatanodeInfoWithStorage[127.0.0.1:45719,DS-b725eb1c-1a08-4e81-8207-3575ca4fdacb,DISK], DatanodeInfoWithStorage[127.0.0.1:38587,DS-6ada69be-3e1d-456d-ab27-f2b94389fb8e,DISK], DatanodeInfoWithStorage[127.0.0.1:38171,DS-0e7292f8-c014-45ef-9053-735069a6e5c1,DISK], DatanodeInfoWithStorage[127.0.0.1:43176,DS-afde6230-c721-4c10-9974-ff2d75f56584,DISK], DatanodeInfoWithStorage[127.0.0.1:45100,DS-5ce0b269-2894-4d24-8474-9b22f9eeb969,DISK], DatanodeInfoWithStorage[127.0.0.1:43659,DS-40c26a35-7510-4dcd-b7e7-4a42c5e575f7,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1277157504-172.17.0.12-1595668880225:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37995,DS-08a994a5-0564-4784-9f87-60c637889382,DISK], DatanodeInfoWithStorage[127.0.0.1:41996,DS-8339ae68-9e09-4166-8f75-b02040ad1b83,DISK], DatanodeInfoWithStorage[127.0.0.1:39419,DS-aa38fd0a-55ec-4088-9236-aaba782d96df,DISK], DatanodeInfoWithStorage[127.0.0.1:46228,DS-9cbbeb5a-6745-40d9-b3c4-c693df3dd063,DISK], DatanodeInfoWithStorage[127.0.0.1:38376,DS-0746bd67-9b46-4c51-ba7a-b5ff3b52a562,DISK], DatanodeInfoWithStorage[127.0.0.1:33202,DS-d81890cd-19ad-41ad-8d48-d0780e6b765d,DISK], DatanodeInfoWithStorage[127.0.0.1:45285,DS-b8d6b50f-c368-40ba-b60f-d40320063472,DISK], DatanodeInfoWithStorage[127.0.0.1:34544,DS-2941791c-44ae-4fbe-9de9-f3c1450f7a58,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1277157504-172.17.0.12-1595668880225:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37995,DS-08a994a5-0564-4784-9f87-60c637889382,DISK], DatanodeInfoWithStorage[127.0.0.1:41996,DS-8339ae68-9e09-4166-8f75-b02040ad1b83,DISK], DatanodeInfoWithStorage[127.0.0.1:39419,DS-aa38fd0a-55ec-4088-9236-aaba782d96df,DISK], DatanodeInfoWithStorage[127.0.0.1:46228,DS-9cbbeb5a-6745-40d9-b3c4-c693df3dd063,DISK], DatanodeInfoWithStorage[127.0.0.1:38376,DS-0746bd67-9b46-4c51-ba7a-b5ff3b52a562,DISK], DatanodeInfoWithStorage[127.0.0.1:33202,DS-d81890cd-19ad-41ad-8d48-d0780e6b765d,DISK], DatanodeInfoWithStorage[127.0.0.1:45285,DS-b8d6b50f-c368-40ba-b60f-d40320063472,DISK], DatanodeInfoWithStorage[127.0.0.1:34544,DS-2941791c-44ae-4fbe-9de9-f3c1450f7a58,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: ipc.client.connect.retry.interval
component: hdfs:DataNode
v1: 1000
v2: 10
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1930323417-172.17.0.12-1595668950299:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42455,DS-8554e3d0-ef67-4a00-941a-07e00487171b,DISK], DatanodeInfoWithStorage[127.0.0.1:42509,DS-fa87cb9c-7b20-4a10-a14f-573f9b9a6919,DISK], DatanodeInfoWithStorage[127.0.0.1:42331,DS-31e2fbce-3bcf-450a-b73e-e43a4db5bf9f,DISK], DatanodeInfoWithStorage[127.0.0.1:39618,DS-067bd61c-381c-4352-85b6-c82b3c248e6f,DISK], DatanodeInfoWithStorage[127.0.0.1:34000,DS-bf3b0051-c4e1-4935-a46d-15401abdab3c,DISK], DatanodeInfoWithStorage[127.0.0.1:37438,DS-7cb47f32-3144-45b6-a646-dce4f6e35c9a,DISK], DatanodeInfoWithStorage[127.0.0.1:38164,DS-5dc43b70-ee4b-4a11-b0c2-165ab06b761d,DISK], DatanodeInfoWithStorage[127.0.0.1:34970,DS-60b7a1f7-170e-43bc-af6f-f09a7ae78f81,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1930323417-172.17.0.12-1595668950299:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42455,DS-8554e3d0-ef67-4a00-941a-07e00487171b,DISK], DatanodeInfoWithStorage[127.0.0.1:42509,DS-fa87cb9c-7b20-4a10-a14f-573f9b9a6919,DISK], DatanodeInfoWithStorage[127.0.0.1:42331,DS-31e2fbce-3bcf-450a-b73e-e43a4db5bf9f,DISK], DatanodeInfoWithStorage[127.0.0.1:39618,DS-067bd61c-381c-4352-85b6-c82b3c248e6f,DISK], DatanodeInfoWithStorage[127.0.0.1:34000,DS-bf3b0051-c4e1-4935-a46d-15401abdab3c,DISK], DatanodeInfoWithStorage[127.0.0.1:37438,DS-7cb47f32-3144-45b6-a646-dce4f6e35c9a,DISK], DatanodeInfoWithStorage[127.0.0.1:38164,DS-5dc43b70-ee4b-4a11-b0c2-165ab06b761d,DISK], DatanodeInfoWithStorage[127.0.0.1:34970,DS-60b7a1f7-170e-43bc-af6f-f09a7ae78f81,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


v1v2 failed with probability 7 out of 50
v1v1v2v2 failed with probability 13 out of 50
result: false positive !!!
Total execution time in seconds : 5377

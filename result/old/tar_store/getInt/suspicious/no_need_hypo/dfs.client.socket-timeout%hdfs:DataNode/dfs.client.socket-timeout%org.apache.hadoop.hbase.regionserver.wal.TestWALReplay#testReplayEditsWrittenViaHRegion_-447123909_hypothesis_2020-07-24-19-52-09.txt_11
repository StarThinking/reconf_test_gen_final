reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion
reconfPoint: -1
result: -1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44216,DS-cb95873a-78a2-46a3-a740-f279a33ae12e,DISK], DatanodeInfoWithStorage[127.0.0.1:34352,DS-4700a736-cad7-4f90-b790-e4f77ce3cfe6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34352,DS-4700a736-cad7-4f90-b790-e4f77ce3cfe6,DISK], DatanodeInfoWithStorage[127.0.0.1:44216,DS-cb95873a-78a2-46a3-a740-f279a33ae12e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44216,DS-cb95873a-78a2-46a3-a740-f279a33ae12e,DISK], DatanodeInfoWithStorage[127.0.0.1:34352,DS-4700a736-cad7-4f90-b790-e4f77ce3cfe6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34352,DS-4700a736-cad7-4f90-b790-e4f77ce3cfe6,DISK], DatanodeInfoWithStorage[127.0.0.1:44216,DS-cb95873a-78a2-46a3-a740-f279a33ae12e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38838,DS-11009743-7fff-43fd-92b5-66d8c5c388c5,DISK], DatanodeInfoWithStorage[127.0.0.1:38606,DS-f42360de-d98c-43d2-b26f-90c794ec71e6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38838,DS-11009743-7fff-43fd-92b5-66d8c5c388c5,DISK], DatanodeInfoWithStorage[127.0.0.1:38606,DS-f42360de-d98c-43d2-b26f-90c794ec71e6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38838,DS-11009743-7fff-43fd-92b5-66d8c5c388c5,DISK], DatanodeInfoWithStorage[127.0.0.1:38606,DS-f42360de-d98c-43d2-b26f-90c794ec71e6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38838,DS-11009743-7fff-43fd-92b5-66d8c5c388c5,DISK], DatanodeInfoWithStorage[127.0.0.1:38606,DS-f42360de-d98c-43d2-b26f-90c794ec71e6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35325,DS-56c1af55-14b9-4a12-af5c-03b75d5a0211,DISK], DatanodeInfoWithStorage[127.0.0.1:39999,DS-a6ec6d0e-50b9-4e5c-8261-777b1762359b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35325,DS-56c1af55-14b9-4a12-af5c-03b75d5a0211,DISK], DatanodeInfoWithStorage[127.0.0.1:39999,DS-a6ec6d0e-50b9-4e5c-8261-777b1762359b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35325,DS-56c1af55-14b9-4a12-af5c-03b75d5a0211,DISK], DatanodeInfoWithStorage[127.0.0.1:39999,DS-a6ec6d0e-50b9-4e5c-8261-777b1762359b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35325,DS-56c1af55-14b9-4a12-af5c-03b75d5a0211,DISK], DatanodeInfoWithStorage[127.0.0.1:39999,DS-a6ec6d0e-50b9-4e5c-8261-777b1762359b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion
reconfPoint: -1
result: -1
failureMessage: Append sequenceId=37, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=37, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43302,DS-bf377c2c-ff86-471c-8cc8-13a78fa90941,DISK], DatanodeInfoWithStorage[127.0.0.1:43147,DS-2be7fb3c-0116-49f9-84e9-31d7525650ea,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43302,DS-bf377c2c-ff86-471c-8cc8-13a78fa90941,DISK], DatanodeInfoWithStorage[127.0.0.1:43147,DS-2be7fb3c-0116-49f9-84e9-31d7525650ea,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testReplayEditsWrittenViaHRegion
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 3121

reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: test timed out after 300000 milliseconds
stackTrace: java.lang.Exception: test timed out after 300000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
	at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.doLock(FSNamesystemLock.java:316)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeLock(FSNamesystemLock.java:203)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock(FSNamesystem.java:1621)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1335)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1987)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1026)
	at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2131)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2071)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: test timed out after 300000 milliseconds
stackTrace: java.lang.Exception: test timed out after 300000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
	at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.doLock(FSNamesystemLock.java:316)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeLock(FSNamesystemLock.java:203)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock(FSNamesystem.java:1621)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1335)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1987)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1026)
	at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2131)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2071)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:120)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:120)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:120)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: GC overhead limit exceeded
stackTrace: java.lang.OutOfMemoryError: GC overhead limit exceeded



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: test timed out after 300000 milliseconds
stackTrace: java.lang.Exception: test timed out after 300000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
	at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.doLock(FSNamesystemLock.java:316)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeLock(FSNamesystemLock.java:203)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock(FSNamesystem.java:1621)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1335)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1987)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1026)
	at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2131)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2071)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: GC overhead limit exceeded
stackTrace: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at io.netty.channel.nio.SelectedSelectionKeySet.<init>(SelectedSelectionKeySet.java:29)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:183)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: GC overhead limit exceeded
stackTrace: java.lang.OutOfMemoryError: GC overhead limit exceeded



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: GC overhead limit exceeded
stackTrace: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:120)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: GC overhead limit exceeded
stackTrace: java.lang.OutOfMemoryError: GC overhead limit exceeded



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Test resulted in an unexpected exit
stackTrace: java.lang.AssertionError: Test resulted in an unexpected exit
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2055)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:120)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: test timed out after 300000 milliseconds
stackTrace: java.lang.Exception: test timed out after 300000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
	at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.doLock(FSNamesystemLock.java:316)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeLock(FSNamesystemLock.java:203)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock(FSNamesystem.java:1621)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1335)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1987)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1026)
	at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2131)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2071)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:120)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:120)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: test timed out after 300000 milliseconds
stackTrace: java.lang.Exception: test timed out after 300000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
	at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.doLock(FSNamesystemLock.java:316)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeLock(FSNamesystemLock.java:203)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock(FSNamesystem.java:1621)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1335)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1987)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1026)
	at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2131)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2071)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: GC overhead limit exceeded
stackTrace: java.lang.OutOfMemoryError: GC overhead limit exceeded



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:120)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:120)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:120)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: GC overhead limit exceeded
stackTrace: java.lang.OutOfMemoryError: GC overhead limit exceeded



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: test timed out after 300000 milliseconds
stackTrace: java.lang.Exception: test timed out after 300000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
	at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.doLock(FSNamesystemLock.java:316)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeLock(FSNamesystemLock.java:203)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock(FSNamesystem.java:1621)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1335)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1987)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1026)
	at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2131)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2071)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:120)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Test resulted in an unexpected exit
stackTrace: java.lang.AssertionError: Test resulted in an unexpected exit
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2055)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: GC overhead limit exceeded
stackTrace: java.lang.OutOfMemoryError: GC overhead limit exceeded



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Test resulted in an unexpected exit
stackTrace: java.lang.AssertionError: Test resulted in an unexpected exit
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2055)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: GC overhead limit exceeded
stackTrace: java.lang.OutOfMemoryError: GC overhead limit exceeded



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: GC overhead limit exceeded
stackTrace: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:125)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:147)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: test timed out after 300000 milliseconds
stackTrace: java.lang.Exception: test timed out after 300000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
	at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.doLock(FSNamesystemLock.java:316)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeLock(FSNamesystemLock.java:203)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock(FSNamesystem.java:1621)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1335)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1987)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1026)
	at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2131)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2071)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Java heap space
stackTrace: java.lang.OutOfMemoryError: Java heap space
	at sun.nio.ch.EPollArrayWrapper.<init>(EPollArrayWrapper.java:120)
	at sun.nio.ch.EPollSelectorImpl.<init>(EPollSelectorImpl.java:69)
	at sun.nio.ch.EPollSelectorProvider.openSelector(EPollSelectorProvider.java:36)
	at io.netty.channel.nio.NioEventLoop.openSelector(NioEventLoop.java:174)
	at io.netty.channel.nio.NioEventLoop.<init>(NioEventLoop.java:150)
	at io.netty.channel.nio.NioEventLoopGroup.newChild(NioEventLoopGroup.java:103)
	at io.netty.util.concurrent.MultithreadEventExecutorGroup.<init>(MultithreadEventExecutorGroup.java:64)
	at io.netty.channel.MultithreadEventLoopGroup.<init>(MultithreadEventLoopGroup.java:50)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:70)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:65)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:56)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:48)
	at io.netty.channel.nio.NioEventLoopGroup.<init>(NioEventLoopGroup.java:40)
	at org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer.<init>(DatanodeHttpServer.java:148)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startInfoServer(DataNode.java:964)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.startDataNode(DataNode.java:1424)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.<init>(DataNode.java:507)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2812)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2720)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:1695)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:911)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: test timed out after 300000 milliseconds
stackTrace: java.lang.Exception: test timed out after 300000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
	at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.doLock(FSNamesystemLock.java:316)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeLock(FSNamesystemLock.java:203)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock(FSNamesystem.java:1621)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1335)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1987)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1026)
	at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2131)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2071)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: test timed out after 300000 milliseconds
stackTrace: java.lang.Exception: test timed out after 300000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
	at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.doLock(FSNamesystemLock.java:316)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeLock(FSNamesystemLock.java:203)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock(FSNamesystem.java:1621)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1335)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1987)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1026)
	at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2131)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2071)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: test timed out after 300000 milliseconds
stackTrace: java.lang.Exception: test timed out after 300000 milliseconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(AbstractQueuedSynchronizer.java:870)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(AbstractQueuedSynchronizer.java:1199)
	at java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock(ReentrantReadWriteLock.java:943)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.doLock(FSNamesystemLock.java:316)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystemLock.writeLock(FSNamesystemLock.java:203)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.writeLock(FSNamesystem.java:1621)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopActiveServices(FSNamesystem.java:1335)
	at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.stopActiveServices(NameNode.java:1987)
	at org.apache.hadoop.hdfs.server.namenode.ha.ActiveState.exitState(ActiveState.java:70)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.stop(NameNode.java:1026)
	at org.apache.hadoop.hdfs.MiniDFSCluster.stopAndJoinNameNode(MiniDFSCluster.java:2131)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2071)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: Test resulted in an unexpected exit
stackTrace: java.lang.AssertionError: Test resulted in an unexpected exit
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2055)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2042)
	at org.apache.hadoop.hdfs.MiniDFSCluster.shutdown(MiniDFSCluster.java:2035)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:921)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:518)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:477)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.setup(TestDFSStripedOutputStreamWithFailureBase.java:208)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureBase.runTestWithMultipleFailure(TestDFSStripedOutputStreamWithFailureBase.java:263)
	at org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailure.testMultipleDatanodeFailure56(TestDFSStripedOutputStreamWithFailure.java:50)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: GC overhead limit exceeded
stackTrace: java.lang.OutOfMemoryError: GC overhead limit exceeded


reconf_parameter: dfs.namenode.max.op.size
component: hdfs:NameNode
v1: 52428800
v2: 104857600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestDFSStripedOutputStreamWithFailureWithRandomECPolicy#testMultipleDatanodeFailure56
reconfPoint: -1
result: -1
failureMessage: GC overhead limit exceeded
stackTrace: java.lang.OutOfMemoryError: GC overhead limit exceeded


v1v2 failed with probability 17 out of 50
v1v1v2v2 failed with probability 20 out of 50
result: false positive !!!
Total execution time in seconds : 15960

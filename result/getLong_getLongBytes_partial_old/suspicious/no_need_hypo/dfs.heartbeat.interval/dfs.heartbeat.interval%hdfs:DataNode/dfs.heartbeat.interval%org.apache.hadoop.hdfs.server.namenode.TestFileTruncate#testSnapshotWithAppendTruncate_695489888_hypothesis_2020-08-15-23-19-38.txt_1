reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37022,DS-29f1ab08-efc1-4dd2-ba5d-ee90dabac540,DISK], DatanodeInfoWithStorage[127.0.0.1:42074,DS-63fb3656-c04e-4560-9ec2-81d3b067930d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42074,DS-63fb3656-c04e-4560-9ec2-81d3b067930d,DISK], DatanodeInfoWithStorage[127.0.0.1:37022,DS-29f1ab08-efc1-4dd2-ba5d-ee90dabac540,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37022,DS-29f1ab08-efc1-4dd2-ba5d-ee90dabac540,DISK], DatanodeInfoWithStorage[127.0.0.1:42074,DS-63fb3656-c04e-4560-9ec2-81d3b067930d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42074,DS-63fb3656-c04e-4560-9ec2-81d3b067930d,DISK], DatanodeInfoWithStorage[127.0.0.1:37022,DS-29f1ab08-efc1-4dd2-ba5d-ee90dabac540,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39692,DS-8eb63439-1229-4b02-b0d8-da79e44b305d,DISK], DatanodeInfoWithStorage[127.0.0.1:45692,DS-5a1232f6-ac22-4875-b697-41c3b94b0b07,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39692,DS-8eb63439-1229-4b02-b0d8-da79e44b305d,DISK], DatanodeInfoWithStorage[127.0.0.1:45692,DS-5a1232f6-ac22-4875-b697-41c3b94b0b07,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39692,DS-8eb63439-1229-4b02-b0d8-da79e44b305d,DISK], DatanodeInfoWithStorage[127.0.0.1:45692,DS-5a1232f6-ac22-4875-b697-41c3b94b0b07,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39692,DS-8eb63439-1229-4b02-b0d8-da79e44b305d,DISK], DatanodeInfoWithStorage[127.0.0.1:45692,DS-5a1232f6-ac22-4875-b697-41c3b94b0b07,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: inode should complete in ~30000 ms.
Expected: is <true>
     but: was <false>
stackTrace: java.lang.AssertionError: inode should complete in ~30000 ms.
Expected: is <true>
     but: was <false>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:865)
	at org.apache.hadoop.hdfs.server.namenode.TestFileTruncate.checkBlockRecovery(TestFileTruncate.java:1229)
	at org.apache.hadoop.hdfs.server.namenode.TestFileTruncate.checkBlockRecovery(TestFileTruncate.java:1213)
	at org.apache.hadoop.hdfs.server.namenode.TestFileTruncate.checkBlockRecovery(TestFileTruncate.java:1208)
	at org.apache.hadoop.hdfs.server.namenode.TestFileTruncate.testSnapshotWithAppendTruncate(TestFileTruncate.java:343)
	at org.apache.hadoop.hdfs.server.namenode.TestFileTruncate.testSnapshotWithAppendTruncate(TestFileTruncate.java:261)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36811,DS-d016349f-1a38-42fd-996b-9ea94743bf23,DISK], DatanodeInfoWithStorage[127.0.0.1:46193,DS-07c81336-6f43-4d43-90b2-801f6de18c2b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36811,DS-d016349f-1a38-42fd-996b-9ea94743bf23,DISK], DatanodeInfoWithStorage[127.0.0.1:46193,DS-07c81336-6f43-4d43-90b2-801f6de18c2b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36811,DS-d016349f-1a38-42fd-996b-9ea94743bf23,DISK], DatanodeInfoWithStorage[127.0.0.1:46193,DS-07c81336-6f43-4d43-90b2-801f6de18c2b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36811,DS-d016349f-1a38-42fd-996b-9ea94743bf23,DISK], DatanodeInfoWithStorage[127.0.0.1:46193,DS-07c81336-6f43-4d43-90b2-801f6de18c2b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46359,DS-04e431ad-ffcc-464e-902f-196788657aab,DISK], DatanodeInfoWithStorage[127.0.0.1:33142,DS-fd65c999-88eb-46e6-b7e2-29f2b14635d9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46359,DS-04e431ad-ffcc-464e-902f-196788657aab,DISK], DatanodeInfoWithStorage[127.0.0.1:33142,DS-fd65c999-88eb-46e6-b7e2-29f2b14635d9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46359,DS-04e431ad-ffcc-464e-902f-196788657aab,DISK], DatanodeInfoWithStorage[127.0.0.1:33142,DS-fd65c999-88eb-46e6-b7e2-29f2b14635d9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46359,DS-04e431ad-ffcc-464e-902f-196788657aab,DISK], DatanodeInfoWithStorage[127.0.0.1:33142,DS-fd65c999-88eb-46e6-b7e2-29f2b14635d9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38089,DS-52e19ef7-5465-4c75-8578-ffc25f630e53,DISK], DatanodeInfoWithStorage[127.0.0.1:39662,DS-c6ada0a3-0466-46c9-b0e2-58dc885d094e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38089,DS-52e19ef7-5465-4c75-8578-ffc25f630e53,DISK], DatanodeInfoWithStorage[127.0.0.1:39662,DS-c6ada0a3-0466-46c9-b0e2-58dc885d094e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38089,DS-52e19ef7-5465-4c75-8578-ffc25f630e53,DISK], DatanodeInfoWithStorage[127.0.0.1:39662,DS-c6ada0a3-0466-46c9-b0e2-58dc885d094e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38089,DS-52e19ef7-5465-4c75-8578-ffc25f630e53,DISK], DatanodeInfoWithStorage[127.0.0.1:39662,DS-c6ada0a3-0466-46c9-b0e2-58dc885d094e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42527,DS-53403ce1-ecce-44df-b1e6-a563366f50fa,DISK], DatanodeInfoWithStorage[127.0.0.1:40255,DS-6ff766fd-e3f1-4966-b819-ed6022f10ceb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42527,DS-53403ce1-ecce-44df-b1e6-a563366f50fa,DISK], DatanodeInfoWithStorage[127.0.0.1:40255,DS-6ff766fd-e3f1-4966-b819-ed6022f10ceb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42527,DS-53403ce1-ecce-44df-b1e6-a563366f50fa,DISK], DatanodeInfoWithStorage[127.0.0.1:40255,DS-6ff766fd-e3f1-4966-b819-ed6022f10ceb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42527,DS-53403ce1-ecce-44df-b1e6-a563366f50fa,DISK], DatanodeInfoWithStorage[127.0.0.1:40255,DS-6ff766fd-e3f1-4966-b819-ed6022f10ceb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43164,DS-9f06eaac-622b-4a9d-ba54-8276542d2d8f,DISK], DatanodeInfoWithStorage[127.0.0.1:42727,DS-6dc8fc37-e509-4d17-a260-693e208ba104,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43164,DS-9f06eaac-622b-4a9d-ba54-8276542d2d8f,DISK], DatanodeInfoWithStorage[127.0.0.1:42727,DS-6dc8fc37-e509-4d17-a260-693e208ba104,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43164,DS-9f06eaac-622b-4a9d-ba54-8276542d2d8f,DISK], DatanodeInfoWithStorage[127.0.0.1:42727,DS-6dc8fc37-e509-4d17-a260-693e208ba104,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43164,DS-9f06eaac-622b-4a9d-ba54-8276542d2d8f,DISK], DatanodeInfoWithStorage[127.0.0.1:42727,DS-6dc8fc37-e509-4d17-a260-693e208ba104,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45045,DS-66db5ed9-1ddc-4acb-b6a2-2cde1b58be36,DISK], DatanodeInfoWithStorage[127.0.0.1:35873,DS-31e9f5c0-98fe-468b-a9e0-357558f3c3d4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35873,DS-31e9f5c0-98fe-468b-a9e0-357558f3c3d4,DISK], DatanodeInfoWithStorage[127.0.0.1:45045,DS-66db5ed9-1ddc-4acb-b6a2-2cde1b58be36,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45045,DS-66db5ed9-1ddc-4acb-b6a2-2cde1b58be36,DISK], DatanodeInfoWithStorage[127.0.0.1:35873,DS-31e9f5c0-98fe-468b-a9e0-357558f3c3d4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35873,DS-31e9f5c0-98fe-468b-a9e0-357558f3c3d4,DISK], DatanodeInfoWithStorage[127.0.0.1:45045,DS-66db5ed9-1ddc-4acb-b6a2-2cde1b58be36,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43923,DS-e59c3ebd-2b47-4533-b3fd-df50eb3189c8,DISK], DatanodeInfoWithStorage[127.0.0.1:35764,DS-bb1f0c60-ec34-4ce5-98ab-011f844fcd34,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43923,DS-e59c3ebd-2b47-4533-b3fd-df50eb3189c8,DISK], DatanodeInfoWithStorage[127.0.0.1:35764,DS-bb1f0c60-ec34-4ce5-98ab-011f844fcd34,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43923,DS-e59c3ebd-2b47-4533-b3fd-df50eb3189c8,DISK], DatanodeInfoWithStorage[127.0.0.1:35764,DS-bb1f0c60-ec34-4ce5-98ab-011f844fcd34,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43923,DS-e59c3ebd-2b47-4533-b3fd-df50eb3189c8,DISK], DatanodeInfoWithStorage[127.0.0.1:35764,DS-bb1f0c60-ec34-4ce5-98ab-011f844fcd34,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37025,DS-2f7e353a-a2d6-4068-b8df-c6ab6c924dc9,DISK], DatanodeInfoWithStorage[127.0.0.1:34920,DS-cdefc952-4307-4a1a-baf4-ad895ec00556,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34920,DS-cdefc952-4307-4a1a-baf4-ad895ec00556,DISK], DatanodeInfoWithStorage[127.0.0.1:37025,DS-2f7e353a-a2d6-4068-b8df-c6ab6c924dc9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37025,DS-2f7e353a-a2d6-4068-b8df-c6ab6c924dc9,DISK], DatanodeInfoWithStorage[127.0.0.1:34920,DS-cdefc952-4307-4a1a-baf4-ad895ec00556,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34920,DS-cdefc952-4307-4a1a-baf4-ad895ec00556,DISK], DatanodeInfoWithStorage[127.0.0.1:37025,DS-2f7e353a-a2d6-4068-b8df-c6ab6c924dc9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36811,DS-818c7ecd-5259-409b-b2e2-0380ed768ba6,DISK], DatanodeInfoWithStorage[127.0.0.1:39299,DS-f0839f2b-aeab-4c29-83fd-6d732ab201af,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36811,DS-818c7ecd-5259-409b-b2e2-0380ed768ba6,DISK], DatanodeInfoWithStorage[127.0.0.1:39299,DS-f0839f2b-aeab-4c29-83fd-6d732ab201af,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36811,DS-818c7ecd-5259-409b-b2e2-0380ed768ba6,DISK], DatanodeInfoWithStorage[127.0.0.1:39299,DS-f0839f2b-aeab-4c29-83fd-6d732ab201af,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36811,DS-818c7ecd-5259-409b-b2e2-0380ed768ba6,DISK], DatanodeInfoWithStorage[127.0.0.1:39299,DS-f0839f2b-aeab-4c29-83fd-6d732ab201af,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33247,DS-f1a541c2-b402-4a77-9718-f57b9680b6a5,DISK], DatanodeInfoWithStorage[127.0.0.1:36305,DS-e3a0c91b-2b65-45fe-9472-2ca7c50b883b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33247,DS-f1a541c2-b402-4a77-9718-f57b9680b6a5,DISK], DatanodeInfoWithStorage[127.0.0.1:36305,DS-e3a0c91b-2b65-45fe-9472-2ca7c50b883b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33247,DS-f1a541c2-b402-4a77-9718-f57b9680b6a5,DISK], DatanodeInfoWithStorage[127.0.0.1:36305,DS-e3a0c91b-2b65-45fe-9472-2ca7c50b883b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33247,DS-f1a541c2-b402-4a77-9718-f57b9680b6a5,DISK], DatanodeInfoWithStorage[127.0.0.1:36305,DS-e3a0c91b-2b65-45fe-9472-2ca7c50b883b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38264,DS-c6334148-e8bf-4221-b3de-5d773a925d27,DISK], DatanodeInfoWithStorage[127.0.0.1:39849,DS-4f590557-ea0a-4c50-8dbb-7287a18cab60,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39849,DS-4f590557-ea0a-4c50-8dbb-7287a18cab60,DISK], DatanodeInfoWithStorage[127.0.0.1:38264,DS-c6334148-e8bf-4221-b3de-5d773a925d27,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38264,DS-c6334148-e8bf-4221-b3de-5d773a925d27,DISK], DatanodeInfoWithStorage[127.0.0.1:39849,DS-4f590557-ea0a-4c50-8dbb-7287a18cab60,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39849,DS-4f590557-ea0a-4c50-8dbb-7287a18cab60,DISK], DatanodeInfoWithStorage[127.0.0.1:38264,DS-c6334148-e8bf-4221-b3de-5d773a925d27,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: inode should complete in ~30000 ms.
Expected: is <true>
     but: was <false>
stackTrace: java.lang.AssertionError: inode should complete in ~30000 ms.
Expected: is <true>
     but: was <false>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:865)
	at org.apache.hadoop.hdfs.server.namenode.TestFileTruncate.checkBlockRecovery(TestFileTruncate.java:1229)
	at org.apache.hadoop.hdfs.server.namenode.TestFileTruncate.checkBlockRecovery(TestFileTruncate.java:1213)
	at org.apache.hadoop.hdfs.server.namenode.TestFileTruncate.checkBlockRecovery(TestFileTruncate.java:1208)
	at org.apache.hadoop.hdfs.server.namenode.TestFileTruncate.testSnapshotWithAppendTruncate(TestFileTruncate.java:343)
	at org.apache.hadoop.hdfs.server.namenode.TestFileTruncate.testSnapshotWithAppendTruncate(TestFileTruncate.java:260)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159)
	at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:384)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:345)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:126)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:418)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34729,DS-dae25adb-6811-40a4-a032-a4fe1cb7232c,DISK], DatanodeInfoWithStorage[127.0.0.1:39295,DS-0b30e724-2f2c-473e-a4c5-f0e079e70c29,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34729,DS-dae25adb-6811-40a4-a032-a4fe1cb7232c,DISK], DatanodeInfoWithStorage[127.0.0.1:39295,DS-0b30e724-2f2c-473e-a4c5-f0e079e70c29,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34729,DS-dae25adb-6811-40a4-a032-a4fe1cb7232c,DISK], DatanodeInfoWithStorage[127.0.0.1:39295,DS-0b30e724-2f2c-473e-a4c5-f0e079e70c29,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34729,DS-dae25adb-6811-40a4-a032-a4fe1cb7232c,DISK], DatanodeInfoWithStorage[127.0.0.1:39295,DS-0b30e724-2f2c-473e-a4c5-f0e079e70c29,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42970,DS-f6f770ea-8272-405a-82b2-cd58935ff8df,DISK], DatanodeInfoWithStorage[127.0.0.1:44951,DS-0f5d2a95-ff11-4d1e-ba6d-4430f65cea83,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42970,DS-f6f770ea-8272-405a-82b2-cd58935ff8df,DISK], DatanodeInfoWithStorage[127.0.0.1:44951,DS-0f5d2a95-ff11-4d1e-ba6d-4430f65cea83,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42970,DS-f6f770ea-8272-405a-82b2-cd58935ff8df,DISK], DatanodeInfoWithStorage[127.0.0.1:44951,DS-0f5d2a95-ff11-4d1e-ba6d-4430f65cea83,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42970,DS-f6f770ea-8272-405a-82b2-cd58935ff8df,DISK], DatanodeInfoWithStorage[127.0.0.1:44951,DS-0f5d2a95-ff11-4d1e-ba6d-4430f65cea83,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43064,DS-36b63086-1865-415d-99af-13ec102146b1,DISK], DatanodeInfoWithStorage[127.0.0.1:44011,DS-7612b9b5-009f-43ee-802c-17b010431d3a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43064,DS-36b63086-1865-415d-99af-13ec102146b1,DISK], DatanodeInfoWithStorage[127.0.0.1:44011,DS-7612b9b5-009f-43ee-802c-17b010431d3a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43064,DS-36b63086-1865-415d-99af-13ec102146b1,DISK], DatanodeInfoWithStorage[127.0.0.1:44011,DS-7612b9b5-009f-43ee-802c-17b010431d3a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43064,DS-36b63086-1865-415d-99af-13ec102146b1,DISK], DatanodeInfoWithStorage[127.0.0.1:44011,DS-7612b9b5-009f-43ee-802c-17b010431d3a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34486,DS-987845ae-6071-4545-8f8e-228fd464f718,DISK], DatanodeInfoWithStorage[127.0.0.1:46662,DS-794c60c6-c373-432b-abb8-5cc973d573c6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34486,DS-987845ae-6071-4545-8f8e-228fd464f718,DISK], DatanodeInfoWithStorage[127.0.0.1:46662,DS-794c60c6-c373-432b-abb8-5cc973d573c6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34486,DS-987845ae-6071-4545-8f8e-228fd464f718,DISK], DatanodeInfoWithStorage[127.0.0.1:46662,DS-794c60c6-c373-432b-abb8-5cc973d573c6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34486,DS-987845ae-6071-4545-8f8e-228fd464f718,DISK], DatanodeInfoWithStorage[127.0.0.1:46662,DS-794c60c6-c373-432b-abb8-5cc973d573c6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36841,DS-79cfed75-dc3b-4e57-b7bf-813314bd7b11,DISK], DatanodeInfoWithStorage[127.0.0.1:33182,DS-eb286279-9c2a-41e7-976e-ef80fc740a1d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36841,DS-79cfed75-dc3b-4e57-b7bf-813314bd7b11,DISK], DatanodeInfoWithStorage[127.0.0.1:33182,DS-eb286279-9c2a-41e7-976e-ef80fc740a1d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36841,DS-79cfed75-dc3b-4e57-b7bf-813314bd7b11,DISK], DatanodeInfoWithStorage[127.0.0.1:33182,DS-eb286279-9c2a-41e7-976e-ef80fc740a1d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36841,DS-79cfed75-dc3b-4e57-b7bf-813314bd7b11,DISK], DatanodeInfoWithStorage[127.0.0.1:33182,DS-eb286279-9c2a-41e7-976e-ef80fc740a1d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40106,DS-147312c3-bfa2-4278-89f4-11e8d5ca3fcb,DISK], DatanodeInfoWithStorage[127.0.0.1:35211,DS-6be32d16-5c30-4e03-912f-4f687205e57c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40106,DS-147312c3-bfa2-4278-89f4-11e8d5ca3fcb,DISK], DatanodeInfoWithStorage[127.0.0.1:35211,DS-6be32d16-5c30-4e03-912f-4f687205e57c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40106,DS-147312c3-bfa2-4278-89f4-11e8d5ca3fcb,DISK], DatanodeInfoWithStorage[127.0.0.1:35211,DS-6be32d16-5c30-4e03-912f-4f687205e57c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40106,DS-147312c3-bfa2-4278-89f4-11e8d5ca3fcb,DISK], DatanodeInfoWithStorage[127.0.0.1:35211,DS-6be32d16-5c30-4e03-912f-4f687205e57c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38570,DS-8231740d-274d-4d88-94ce-281d2be177cb,DISK], DatanodeInfoWithStorage[127.0.0.1:35516,DS-0e9dfb2f-1cb2-43bd-9fcc-3671fda4baba,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35516,DS-0e9dfb2f-1cb2-43bd-9fcc-3671fda4baba,DISK], DatanodeInfoWithStorage[127.0.0.1:38570,DS-8231740d-274d-4d88-94ce-281d2be177cb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38570,DS-8231740d-274d-4d88-94ce-281d2be177cb,DISK], DatanodeInfoWithStorage[127.0.0.1:35516,DS-0e9dfb2f-1cb2-43bd-9fcc-3671fda4baba,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35516,DS-0e9dfb2f-1cb2-43bd-9fcc-3671fda4baba,DISK], DatanodeInfoWithStorage[127.0.0.1:38570,DS-8231740d-274d-4d88-94ce-281d2be177cb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46608,DS-e8b4ad01-d8d1-4fc7-9cad-14a23329594e,DISK], DatanodeInfoWithStorage[127.0.0.1:39347,DS-fb4c7f3d-838a-42f0-aeac-2e6de24b5305,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46608,DS-e8b4ad01-d8d1-4fc7-9cad-14a23329594e,DISK], DatanodeInfoWithStorage[127.0.0.1:39347,DS-fb4c7f3d-838a-42f0-aeac-2e6de24b5305,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46608,DS-e8b4ad01-d8d1-4fc7-9cad-14a23329594e,DISK], DatanodeInfoWithStorage[127.0.0.1:39347,DS-fb4c7f3d-838a-42f0-aeac-2e6de24b5305,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46608,DS-e8b4ad01-d8d1-4fc7-9cad-14a23329594e,DISK], DatanodeInfoWithStorage[127.0.0.1:39347,DS-fb4c7f3d-838a-42f0-aeac-2e6de24b5305,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38827,DS-6b74f76c-00a5-4605-b063-b20a725b79a5,DISK], DatanodeInfoWithStorage[127.0.0.1:45940,DS-35991188-7214-43c4-bcef-afeed774a6e0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38827,DS-6b74f76c-00a5-4605-b063-b20a725b79a5,DISK], DatanodeInfoWithStorage[127.0.0.1:45940,DS-35991188-7214-43c4-bcef-afeed774a6e0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38827,DS-6b74f76c-00a5-4605-b063-b20a725b79a5,DISK], DatanodeInfoWithStorage[127.0.0.1:45940,DS-35991188-7214-43c4-bcef-afeed774a6e0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38827,DS-6b74f76c-00a5-4605-b063-b20a725b79a5,DISK], DatanodeInfoWithStorage[127.0.0.1:45940,DS-35991188-7214-43c4-bcef-afeed774a6e0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32863,DS-868f306e-7f38-4151-a820-220588d58b5f,DISK], DatanodeInfoWithStorage[127.0.0.1:40161,DS-2af95f48-0796-46f9-965f-c3da5b0a2f1c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32863,DS-868f306e-7f38-4151-a820-220588d58b5f,DISK], DatanodeInfoWithStorage[127.0.0.1:40161,DS-2af95f48-0796-46f9-965f-c3da5b0a2f1c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32863,DS-868f306e-7f38-4151-a820-220588d58b5f,DISK], DatanodeInfoWithStorage[127.0.0.1:40161,DS-2af95f48-0796-46f9-965f-c3da5b0a2f1c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32863,DS-868f306e-7f38-4151-a820-220588d58b5f,DISK], DatanodeInfoWithStorage[127.0.0.1:40161,DS-2af95f48-0796-46f9-965f-c3da5b0a2f1c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35452,DS-cde42db4-37ae-4124-920d-aabc596d7cc1,DISK], DatanodeInfoWithStorage[127.0.0.1:34267,DS-b82c1a53-8b1b-4d35-9109-9830380f8250,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35452,DS-cde42db4-37ae-4124-920d-aabc596d7cc1,DISK], DatanodeInfoWithStorage[127.0.0.1:34267,DS-b82c1a53-8b1b-4d35-9109-9830380f8250,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35452,DS-cde42db4-37ae-4124-920d-aabc596d7cc1,DISK], DatanodeInfoWithStorage[127.0.0.1:34267,DS-b82c1a53-8b1b-4d35-9109-9830380f8250,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35452,DS-cde42db4-37ae-4124-920d-aabc596d7cc1,DISK], DatanodeInfoWithStorage[127.0.0.1:34267,DS-b82c1a53-8b1b-4d35-9109-9830380f8250,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36610,DS-a518576b-9a4e-40df-986a-ae57efd39226,DISK], DatanodeInfoWithStorage[127.0.0.1:35594,DS-4d417359-900f-4e55-8880-c355f4ac3ac2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36610,DS-a518576b-9a4e-40df-986a-ae57efd39226,DISK], DatanodeInfoWithStorage[127.0.0.1:35594,DS-4d417359-900f-4e55-8880-c355f4ac3ac2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36610,DS-a518576b-9a4e-40df-986a-ae57efd39226,DISK], DatanodeInfoWithStorage[127.0.0.1:35594,DS-4d417359-900f-4e55-8880-c355f4ac3ac2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36610,DS-a518576b-9a4e-40df-986a-ae57efd39226,DISK], DatanodeInfoWithStorage[127.0.0.1:35594,DS-4d417359-900f-4e55-8880-c355f4ac3ac2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44618,DS-7de01760-3546-4fdc-8b7f-4e0a5f90fccb,DISK], DatanodeInfoWithStorage[127.0.0.1:32968,DS-9d1f3279-b1cc-4f51-b3b7-ce99a3940f0c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44618,DS-7de01760-3546-4fdc-8b7f-4e0a5f90fccb,DISK], DatanodeInfoWithStorage[127.0.0.1:32968,DS-9d1f3279-b1cc-4f51-b3b7-ce99a3940f0c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44618,DS-7de01760-3546-4fdc-8b7f-4e0a5f90fccb,DISK], DatanodeInfoWithStorage[127.0.0.1:32968,DS-9d1f3279-b1cc-4f51-b3b7-ce99a3940f0c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44618,DS-7de01760-3546-4fdc-8b7f-4e0a5f90fccb,DISK], DatanodeInfoWithStorage[127.0.0.1:32968,DS-9d1f3279-b1cc-4f51-b3b7-ce99a3940f0c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40328,DS-a5a5ebe8-b1ba-47e4-b968-2c44254c8fa5,DISK], DatanodeInfoWithStorage[127.0.0.1:42247,DS-c58652e8-ffe2-42fd-82e0-08ef1bebe651,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40328,DS-a5a5ebe8-b1ba-47e4-b968-2c44254c8fa5,DISK], DatanodeInfoWithStorage[127.0.0.1:42247,DS-c58652e8-ffe2-42fd-82e0-08ef1bebe651,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40328,DS-a5a5ebe8-b1ba-47e4-b968-2c44254c8fa5,DISK], DatanodeInfoWithStorage[127.0.0.1:42247,DS-c58652e8-ffe2-42fd-82e0-08ef1bebe651,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40328,DS-a5a5ebe8-b1ba-47e4-b968-2c44254c8fa5,DISK], DatanodeInfoWithStorage[127.0.0.1:42247,DS-c58652e8-ffe2-42fd-82e0-08ef1bebe651,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.heartbeat.interval
component: hdfs:DataNode
v1: 1
v2: 30s
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestFileTruncate#testSnapshotWithAppendTruncate
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41143,DS-044ec1f7-d994-4290-aa8b-86e2c8ae007d,DISK], DatanodeInfoWithStorage[127.0.0.1:33480,DS-52e20ba1-6ee5-4b7b-8c76-af0b0d08ec46,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41143,DS-044ec1f7-d994-4290-aa8b-86e2c8ae007d,DISK], DatanodeInfoWithStorage[127.0.0.1:33480,DS-52e20ba1-6ee5-4b7b-8c76-af0b0d08ec46,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41143,DS-044ec1f7-d994-4290-aa8b-86e2c8ae007d,DISK], DatanodeInfoWithStorage[127.0.0.1:33480,DS-52e20ba1-6ee5-4b7b-8c76-af0b0d08ec46,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41143,DS-044ec1f7-d994-4290-aa8b-86e2c8ae007d,DISK], DatanodeInfoWithStorage[127.0.0.1:33480,DS-52e20ba1-6ee5-4b7b-8c76-af0b0d08ec46,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:720)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 28 out of 50
v1v1v2v2 failed with probability 2 out of 50
result: might be true error
Total execution time in seconds : 14907

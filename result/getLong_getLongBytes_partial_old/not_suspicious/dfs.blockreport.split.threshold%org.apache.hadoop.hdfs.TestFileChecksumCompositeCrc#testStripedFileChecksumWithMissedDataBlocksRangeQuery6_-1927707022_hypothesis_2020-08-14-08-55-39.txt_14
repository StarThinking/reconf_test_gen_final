reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-155363170-172.17.0.6-1597395704458:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43784,DS-68689bfe-2c72-4322-8107-0605d3cc045c,DISK], DatanodeInfoWithStorage[127.0.0.1:42636,DS-1ce2f1ea-f20e-49c0-a4dd-f9156abc73ca,DISK], DatanodeInfoWithStorage[127.0.0.1:42074,DS-14c5389d-1ec0-4610-93ee-9cade15ce0f1,DISK], DatanodeInfoWithStorage[127.0.0.1:42560,DS-f61c5465-6103-48cc-b903-f1a2ba180e92,DISK], DatanodeInfoWithStorage[127.0.0.1:41702,DS-0a75c0f6-203d-45bc-a6b6-06e8993e998d,DISK], DatanodeInfoWithStorage[127.0.0.1:41781,DS-310db09a-b25f-4db7-a698-37893d40bc08,DISK], DatanodeInfoWithStorage[127.0.0.1:36820,DS-99cecd9a-2247-416e-9fe8-036a4d3a20ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37867,DS-086d4778-0ec3-479e-9d5f-f5e72491df0b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-155363170-172.17.0.6-1597395704458:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43784,DS-68689bfe-2c72-4322-8107-0605d3cc045c,DISK], DatanodeInfoWithStorage[127.0.0.1:42636,DS-1ce2f1ea-f20e-49c0-a4dd-f9156abc73ca,DISK], DatanodeInfoWithStorage[127.0.0.1:42074,DS-14c5389d-1ec0-4610-93ee-9cade15ce0f1,DISK], DatanodeInfoWithStorage[127.0.0.1:42560,DS-f61c5465-6103-48cc-b903-f1a2ba180e92,DISK], DatanodeInfoWithStorage[127.0.0.1:41702,DS-0a75c0f6-203d-45bc-a6b6-06e8993e998d,DISK], DatanodeInfoWithStorage[127.0.0.1:41781,DS-310db09a-b25f-4db7-a698-37893d40bc08,DISK], DatanodeInfoWithStorage[127.0.0.1:36820,DS-99cecd9a-2247-416e-9fe8-036a4d3a20ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37867,DS-086d4778-0ec3-479e-9d5f-f5e72491df0b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1694163699-172.17.0.6-1597395781221:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45939,DS-14dc2608-7a11-489a-a978-5a431127fb77,DISK], DatanodeInfoWithStorage[127.0.0.1:45786,DS-1462cefa-d30a-43e6-ab74-c05eae092b7b,DISK], DatanodeInfoWithStorage[127.0.0.1:42988,DS-51824065-8f81-4aee-8e29-17486ca39e81,DISK], DatanodeInfoWithStorage[127.0.0.1:32911,DS-1a2274f2-1232-4f47-a7aa-84a03629be50,DISK], DatanodeInfoWithStorage[127.0.0.1:40692,DS-2cebe099-626b-4ef0-bbd7-c580a4c18d4c,DISK], DatanodeInfoWithStorage[127.0.0.1:36430,DS-d20bb2c2-7cdf-45cb-9129-2c8ec071a3a4,DISK], DatanodeInfoWithStorage[127.0.0.1:33420,DS-09e9359d-448b-44a9-b1b9-67c15069fbde,DISK], DatanodeInfoWithStorage[127.0.0.1:46050,DS-ea1ea4fa-6e74-47b7-90b3-e369cbcf8ec2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1694163699-172.17.0.6-1597395781221:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45939,DS-14dc2608-7a11-489a-a978-5a431127fb77,DISK], DatanodeInfoWithStorage[127.0.0.1:45786,DS-1462cefa-d30a-43e6-ab74-c05eae092b7b,DISK], DatanodeInfoWithStorage[127.0.0.1:42988,DS-51824065-8f81-4aee-8e29-17486ca39e81,DISK], DatanodeInfoWithStorage[127.0.0.1:32911,DS-1a2274f2-1232-4f47-a7aa-84a03629be50,DISK], DatanodeInfoWithStorage[127.0.0.1:40692,DS-2cebe099-626b-4ef0-bbd7-c580a4c18d4c,DISK], DatanodeInfoWithStorage[127.0.0.1:36430,DS-d20bb2c2-7cdf-45cb-9129-2c8ec071a3a4,DISK], DatanodeInfoWithStorage[127.0.0.1:33420,DS-09e9359d-448b-44a9-b1b9-67c15069fbde,DISK], DatanodeInfoWithStorage[127.0.0.1:46050,DS-ea1ea4fa-6e74-47b7-90b3-e369cbcf8ec2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-836590886-172.17.0.6-1597395817149:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38467,DS-057e0dc6-d34c-49bd-99bd-dd2e7dbbaa2b,DISK], DatanodeInfoWithStorage[127.0.0.1:34815,DS-18ae8d27-63c8-4213-bda6-f1aae140492d,DISK], DatanodeInfoWithStorage[127.0.0.1:36274,DS-2e8afaaf-20f8-49cb-ba84-a2e890c3f31a,DISK], DatanodeInfoWithStorage[127.0.0.1:42712,DS-916beff9-a37f-4c81-b848-efb3d323dcd6,DISK], DatanodeInfoWithStorage[127.0.0.1:32933,DS-f8527b00-9946-4e43-8a6f-d28c8b54e321,DISK], DatanodeInfoWithStorage[127.0.0.1:36721,DS-91cd9d1f-ac25-4752-9f55-f812db176b54,DISK], DatanodeInfoWithStorage[127.0.0.1:38232,DS-c7581638-85f5-4198-afcd-66b569216293,DISK], DatanodeInfoWithStorage[127.0.0.1:42605,DS-c9add19a-dbe4-41ab-93bb-1692e0a6b457,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-836590886-172.17.0.6-1597395817149:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38467,DS-057e0dc6-d34c-49bd-99bd-dd2e7dbbaa2b,DISK], DatanodeInfoWithStorage[127.0.0.1:34815,DS-18ae8d27-63c8-4213-bda6-f1aae140492d,DISK], DatanodeInfoWithStorage[127.0.0.1:36274,DS-2e8afaaf-20f8-49cb-ba84-a2e890c3f31a,DISK], DatanodeInfoWithStorage[127.0.0.1:42712,DS-916beff9-a37f-4c81-b848-efb3d323dcd6,DISK], DatanodeInfoWithStorage[127.0.0.1:32933,DS-f8527b00-9946-4e43-8a6f-d28c8b54e321,DISK], DatanodeInfoWithStorage[127.0.0.1:36721,DS-91cd9d1f-ac25-4752-9f55-f812db176b54,DISK], DatanodeInfoWithStorage[127.0.0.1:38232,DS-c7581638-85f5-4198-afcd-66b569216293,DISK], DatanodeInfoWithStorage[127.0.0.1:42605,DS-c9add19a-dbe4-41ab-93bb-1692e0a6b457,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1950806931-172.17.0.6-1597396079602:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36424,DS-1e5228aa-ac61-4307-b084-dcf004d54a8b,DISK], DatanodeInfoWithStorage[127.0.0.1:38322,DS-15a1bcb4-1036-472d-9f0a-a896609d3701,DISK], DatanodeInfoWithStorage[127.0.0.1:37426,DS-4382c394-03c1-49ac-b129-53abfab3fe0b,DISK], DatanodeInfoWithStorage[127.0.0.1:41626,DS-9eb5cdc2-7829-4663-b0b4-506eadc94b8f,DISK], DatanodeInfoWithStorage[127.0.0.1:45006,DS-f309f8e1-5729-49b2-9d1f-ca61e7d13596,DISK], DatanodeInfoWithStorage[127.0.0.1:39207,DS-37cee8f6-fc6e-4791-a04a-a0261937ca9a,DISK], DatanodeInfoWithStorage[127.0.0.1:33237,DS-a2f454b3-8aa2-48cd-90ee-7816f558496a,DISK], DatanodeInfoWithStorage[127.0.0.1:43228,DS-d1f4c318-218b-4958-91cd-cc051d6ed0f6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1950806931-172.17.0.6-1597396079602:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36424,DS-1e5228aa-ac61-4307-b084-dcf004d54a8b,DISK], DatanodeInfoWithStorage[127.0.0.1:38322,DS-15a1bcb4-1036-472d-9f0a-a896609d3701,DISK], DatanodeInfoWithStorage[127.0.0.1:37426,DS-4382c394-03c1-49ac-b129-53abfab3fe0b,DISK], DatanodeInfoWithStorage[127.0.0.1:41626,DS-9eb5cdc2-7829-4663-b0b4-506eadc94b8f,DISK], DatanodeInfoWithStorage[127.0.0.1:45006,DS-f309f8e1-5729-49b2-9d1f-ca61e7d13596,DISK], DatanodeInfoWithStorage[127.0.0.1:39207,DS-37cee8f6-fc6e-4791-a04a-a0261937ca9a,DISK], DatanodeInfoWithStorage[127.0.0.1:33237,DS-a2f454b3-8aa2-48cd-90ee-7816f558496a,DISK], DatanodeInfoWithStorage[127.0.0.1:43228,DS-d1f4c318-218b-4958-91cd-cc051d6ed0f6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-87653352-172.17.0.6-1597396192265:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43418,DS-1681ebd6-c602-467d-a0ed-30db5af38496,DISK], DatanodeInfoWithStorage[127.0.0.1:38804,DS-cf34c2c1-ca8a-494d-aca4-33cc41256b38,DISK], DatanodeInfoWithStorage[127.0.0.1:39808,DS-8f89acb8-0ec8-450a-b964-a5f80b395a8f,DISK], DatanodeInfoWithStorage[127.0.0.1:34681,DS-0b6bcada-49eb-4aae-a0e2-30d26895440b,DISK], DatanodeInfoWithStorage[127.0.0.1:40946,DS-91e36e1c-8805-4358-ba09-9d6a222669ba,DISK], DatanodeInfoWithStorage[127.0.0.1:39519,DS-182fba20-4b4c-4009-bead-f44adcf8662e,DISK], DatanodeInfoWithStorage[127.0.0.1:42914,DS-c2e33357-b088-43cb-b7fc-1ffcd91f46a0,DISK], DatanodeInfoWithStorage[127.0.0.1:34814,DS-f1d3bb32-d8a5-42d6-97ff-6e39c25d5f9e,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-87653352-172.17.0.6-1597396192265:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43418,DS-1681ebd6-c602-467d-a0ed-30db5af38496,DISK], DatanodeInfoWithStorage[127.0.0.1:38804,DS-cf34c2c1-ca8a-494d-aca4-33cc41256b38,DISK], DatanodeInfoWithStorage[127.0.0.1:39808,DS-8f89acb8-0ec8-450a-b964-a5f80b395a8f,DISK], DatanodeInfoWithStorage[127.0.0.1:34681,DS-0b6bcada-49eb-4aae-a0e2-30d26895440b,DISK], DatanodeInfoWithStorage[127.0.0.1:40946,DS-91e36e1c-8805-4358-ba09-9d6a222669ba,DISK], DatanodeInfoWithStorage[127.0.0.1:39519,DS-182fba20-4b4c-4009-bead-f44adcf8662e,DISK], DatanodeInfoWithStorage[127.0.0.1:42914,DS-c2e33357-b088-43cb-b7fc-1ffcd91f46a0,DISK], DatanodeInfoWithStorage[127.0.0.1:34814,DS-f1d3bb32-d8a5-42d6-97ff-6e39c25d5f9e,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1175870987-172.17.0.6-1597396232336:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38739,DS-78ad8fdf-da58-4b4d-8a9c-756f1a66a43c,DISK], DatanodeInfoWithStorage[127.0.0.1:40224,DS-39451692-c524-4540-804f-3f28ba44fda4,DISK], DatanodeInfoWithStorage[127.0.0.1:46595,DS-68c1df2e-ef18-4bf6-bfe2-d9de44b4c21a,DISK], DatanodeInfoWithStorage[127.0.0.1:40830,DS-7cb91200-b9d5-4aaa-8a27-cd5d9d85da75,DISK], DatanodeInfoWithStorage[127.0.0.1:44220,DS-d877073f-f2db-40c7-8bbb-cebbba17a3a7,DISK], DatanodeInfoWithStorage[127.0.0.1:46561,DS-531e0d44-36dd-4b35-9c23-0eba6dd6f2e9,DISK], DatanodeInfoWithStorage[127.0.0.1:46150,DS-2da04830-91ab-4627-92b7-9fc1d9f8e7a4,DISK], DatanodeInfoWithStorage[127.0.0.1:44295,DS-48f15270-38f5-4390-93c2-ac082ebb7b3b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1175870987-172.17.0.6-1597396232336:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38739,DS-78ad8fdf-da58-4b4d-8a9c-756f1a66a43c,DISK], DatanodeInfoWithStorage[127.0.0.1:40224,DS-39451692-c524-4540-804f-3f28ba44fda4,DISK], DatanodeInfoWithStorage[127.0.0.1:46595,DS-68c1df2e-ef18-4bf6-bfe2-d9de44b4c21a,DISK], DatanodeInfoWithStorage[127.0.0.1:40830,DS-7cb91200-b9d5-4aaa-8a27-cd5d9d85da75,DISK], DatanodeInfoWithStorage[127.0.0.1:44220,DS-d877073f-f2db-40c7-8bbb-cebbba17a3a7,DISK], DatanodeInfoWithStorage[127.0.0.1:46561,DS-531e0d44-36dd-4b35-9c23-0eba6dd6f2e9,DISK], DatanodeInfoWithStorage[127.0.0.1:46150,DS-2da04830-91ab-4627-92b7-9fc1d9f8e7a4,DISK], DatanodeInfoWithStorage[127.0.0.1:44295,DS-48f15270-38f5-4390-93c2-ac082ebb7b3b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2105418946-172.17.0.6-1597396724234:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33409,DS-0cb09f8e-6075-432d-9d2a-203f2a5d2a5d,DISK], DatanodeInfoWithStorage[127.0.0.1:34401,DS-e1df61fa-3be7-4eb2-86ff-ce578b682169,DISK], DatanodeInfoWithStorage[127.0.0.1:45554,DS-6b1efef2-479c-4075-a3b3-8c9f9e60bfbf,DISK], DatanodeInfoWithStorage[127.0.0.1:35923,DS-eb788349-bbe4-45b8-8a58-210bcb15e807,DISK], DatanodeInfoWithStorage[127.0.0.1:39504,DS-acf6d342-3035-4ffc-8510-dfc31d5f8514,DISK], DatanodeInfoWithStorage[127.0.0.1:45235,DS-67a65baa-00a5-4a77-90ef-db7c5cb1da9c,DISK], DatanodeInfoWithStorage[127.0.0.1:42805,DS-3c99f4cd-62d7-4ffe-9b10-0f5d045307fe,DISK], DatanodeInfoWithStorage[127.0.0.1:33492,DS-f50bf7b6-4f46-4827-b230-f9a232006cd5,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2105418946-172.17.0.6-1597396724234:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33409,DS-0cb09f8e-6075-432d-9d2a-203f2a5d2a5d,DISK], DatanodeInfoWithStorage[127.0.0.1:34401,DS-e1df61fa-3be7-4eb2-86ff-ce578b682169,DISK], DatanodeInfoWithStorage[127.0.0.1:45554,DS-6b1efef2-479c-4075-a3b3-8c9f9e60bfbf,DISK], DatanodeInfoWithStorage[127.0.0.1:35923,DS-eb788349-bbe4-45b8-8a58-210bcb15e807,DISK], DatanodeInfoWithStorage[127.0.0.1:39504,DS-acf6d342-3035-4ffc-8510-dfc31d5f8514,DISK], DatanodeInfoWithStorage[127.0.0.1:45235,DS-67a65baa-00a5-4a77-90ef-db7c5cb1da9c,DISK], DatanodeInfoWithStorage[127.0.0.1:42805,DS-3c99f4cd-62d7-4ffe-9b10-0f5d045307fe,DISK], DatanodeInfoWithStorage[127.0.0.1:33492,DS-f50bf7b6-4f46-4827-b230-f9a232006cd5,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1446086381-172.17.0.6-1597396761348:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37131,DS-1476f5a5-cbdd-47bb-bc8e-a6c28ff6197a,DISK], DatanodeInfoWithStorage[127.0.0.1:38922,DS-99643479-848f-4b67-9c0d-68c74185fa51,DISK], DatanodeInfoWithStorage[127.0.0.1:34670,DS-d703b48b-c4c1-41f2-a0ad-a518cbb697ad,DISK], DatanodeInfoWithStorage[127.0.0.1:46719,DS-84f20683-6828-482b-9274-51e841261857,DISK], DatanodeInfoWithStorage[127.0.0.1:46588,DS-d8f51d33-3b89-4d78-8c6e-38c27bac92ef,DISK], DatanodeInfoWithStorage[127.0.0.1:34839,DS-7d3f73ac-0daa-4eab-a886-7608ae3190e5,DISK], DatanodeInfoWithStorage[127.0.0.1:41236,DS-614549df-0032-480f-b76b-917ef735b203,DISK], DatanodeInfoWithStorage[127.0.0.1:38387,DS-43020b27-1236-4b02-84bd-ea6c1319fefb,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1446086381-172.17.0.6-1597396761348:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37131,DS-1476f5a5-cbdd-47bb-bc8e-a6c28ff6197a,DISK], DatanodeInfoWithStorage[127.0.0.1:38922,DS-99643479-848f-4b67-9c0d-68c74185fa51,DISK], DatanodeInfoWithStorage[127.0.0.1:34670,DS-d703b48b-c4c1-41f2-a0ad-a518cbb697ad,DISK], DatanodeInfoWithStorage[127.0.0.1:46719,DS-84f20683-6828-482b-9274-51e841261857,DISK], DatanodeInfoWithStorage[127.0.0.1:46588,DS-d8f51d33-3b89-4d78-8c6e-38c27bac92ef,DISK], DatanodeInfoWithStorage[127.0.0.1:34839,DS-7d3f73ac-0daa-4eab-a886-7608ae3190e5,DISK], DatanodeInfoWithStorage[127.0.0.1:41236,DS-614549df-0032-480f-b76b-917ef735b203,DISK], DatanodeInfoWithStorage[127.0.0.1:38387,DS-43020b27-1236-4b02-84bd-ea6c1319fefb,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-761366545-172.17.0.6-1597396798958:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38673,DS-bbe66cac-cd40-4dd1-8e2a-96e7cfa12eb4,DISK], DatanodeInfoWithStorage[127.0.0.1:38359,DS-38c08d3e-010e-4fc2-87d0-6915d663ed63,DISK], DatanodeInfoWithStorage[127.0.0.1:39363,DS-3997eba2-0a42-4305-bcc2-1d38a6bda3d8,DISK], DatanodeInfoWithStorage[127.0.0.1:35684,DS-0bc267c1-4091-4d96-99f8-e2668eb7d88b,DISK], DatanodeInfoWithStorage[127.0.0.1:35084,DS-e6d6fa85-b338-4daa-8ddf-12c648cff7ef,DISK], DatanodeInfoWithStorage[127.0.0.1:40949,DS-4876e0a5-99f4-4ee5-865b-afda971f0e8f,DISK], DatanodeInfoWithStorage[127.0.0.1:33394,DS-247fe116-5e9e-4cbc-8ae7-2aab79acb618,DISK], DatanodeInfoWithStorage[127.0.0.1:32961,DS-0fcbe59a-6497-4cc0-925e-1af69f50e548,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-761366545-172.17.0.6-1597396798958:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38673,DS-bbe66cac-cd40-4dd1-8e2a-96e7cfa12eb4,DISK], DatanodeInfoWithStorage[127.0.0.1:38359,DS-38c08d3e-010e-4fc2-87d0-6915d663ed63,DISK], DatanodeInfoWithStorage[127.0.0.1:39363,DS-3997eba2-0a42-4305-bcc2-1d38a6bda3d8,DISK], DatanodeInfoWithStorage[127.0.0.1:35684,DS-0bc267c1-4091-4d96-99f8-e2668eb7d88b,DISK], DatanodeInfoWithStorage[127.0.0.1:35084,DS-e6d6fa85-b338-4daa-8ddf-12c648cff7ef,DISK], DatanodeInfoWithStorage[127.0.0.1:40949,DS-4876e0a5-99f4-4ee5-865b-afda971f0e8f,DISK], DatanodeInfoWithStorage[127.0.0.1:33394,DS-247fe116-5e9e-4cbc-8ae7-2aab79acb618,DISK], DatanodeInfoWithStorage[127.0.0.1:32961,DS-0fcbe59a-6497-4cc0-925e-1af69f50e548,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-254663267-172.17.0.6-1597396831977:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36905,DS-34d23df3-b42d-4060-ac40-7d7b82bc5a14,DISK], DatanodeInfoWithStorage[127.0.0.1:43482,DS-b025fa20-69a3-441e-937f-67134dd3547d,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-0846b5e9-a591-49fc-83ea-56e2d02ce71e,DISK], DatanodeInfoWithStorage[127.0.0.1:41954,DS-3bb480a5-609c-43cf-95e0-2aa1447f686b,DISK], DatanodeInfoWithStorage[127.0.0.1:37501,DS-d90f3eaf-1215-4b2c-b238-83391ff463dc,DISK], DatanodeInfoWithStorage[127.0.0.1:46298,DS-216bb52b-f47e-4453-974a-e62b1d9b6a9b,DISK], DatanodeInfoWithStorage[127.0.0.1:34996,DS-f62ab6fc-cb56-49fd-b96c-0f40560b7757,DISK], DatanodeInfoWithStorage[127.0.0.1:38273,DS-0e0de2fd-6291-4329-9a7e-e9e2ffe7800b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-254663267-172.17.0.6-1597396831977:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36905,DS-34d23df3-b42d-4060-ac40-7d7b82bc5a14,DISK], DatanodeInfoWithStorage[127.0.0.1:43482,DS-b025fa20-69a3-441e-937f-67134dd3547d,DISK], DatanodeInfoWithStorage[127.0.0.1:40243,DS-0846b5e9-a591-49fc-83ea-56e2d02ce71e,DISK], DatanodeInfoWithStorage[127.0.0.1:41954,DS-3bb480a5-609c-43cf-95e0-2aa1447f686b,DISK], DatanodeInfoWithStorage[127.0.0.1:37501,DS-d90f3eaf-1215-4b2c-b238-83391ff463dc,DISK], DatanodeInfoWithStorage[127.0.0.1:46298,DS-216bb52b-f47e-4453-974a-e62b1d9b6a9b,DISK], DatanodeInfoWithStorage[127.0.0.1:34996,DS-f62ab6fc-cb56-49fd-b96c-0f40560b7757,DISK], DatanodeInfoWithStorage[127.0.0.1:38273,DS-0e0de2fd-6291-4329-9a7e-e9e2ffe7800b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1959656574-172.17.0.6-1597396910828:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43127,DS-12b14bc7-eee5-40d2-836e-19dec897993d,DISK], DatanodeInfoWithStorage[127.0.0.1:43039,DS-9c04d445-1e9b-47c9-b450-f2cb67b06f54,DISK], DatanodeInfoWithStorage[127.0.0.1:36729,DS-276f0d23-daa1-420a-8528-947ab42656aa,DISK], DatanodeInfoWithStorage[127.0.0.1:46164,DS-5b87d7c1-c38b-4f93-b4b7-d448748be65b,DISK], DatanodeInfoWithStorage[127.0.0.1:35937,DS-f7019fb5-f189-4c97-b73f-220f26032fad,DISK], DatanodeInfoWithStorage[127.0.0.1:33432,DS-08866988-93ef-414a-904a-33bd6f3b53b9,DISK], DatanodeInfoWithStorage[127.0.0.1:38883,DS-a15100a3-3e23-4af5-81df-b4806ba7ccee,DISK], DatanodeInfoWithStorage[127.0.0.1:34243,DS-b1264b99-d971-48c1-8247-5ce412b5325f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1959656574-172.17.0.6-1597396910828:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43127,DS-12b14bc7-eee5-40d2-836e-19dec897993d,DISK], DatanodeInfoWithStorage[127.0.0.1:43039,DS-9c04d445-1e9b-47c9-b450-f2cb67b06f54,DISK], DatanodeInfoWithStorage[127.0.0.1:36729,DS-276f0d23-daa1-420a-8528-947ab42656aa,DISK], DatanodeInfoWithStorage[127.0.0.1:46164,DS-5b87d7c1-c38b-4f93-b4b7-d448748be65b,DISK], DatanodeInfoWithStorage[127.0.0.1:35937,DS-f7019fb5-f189-4c97-b73f-220f26032fad,DISK], DatanodeInfoWithStorage[127.0.0.1:33432,DS-08866988-93ef-414a-904a-33bd6f3b53b9,DISK], DatanodeInfoWithStorage[127.0.0.1:38883,DS-a15100a3-3e23-4af5-81df-b4806ba7ccee,DISK], DatanodeInfoWithStorage[127.0.0.1:34243,DS-b1264b99-d971-48c1-8247-5ce412b5325f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-244508872-172.17.0.6-1597396996414:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41994,DS-20f48924-0489-426a-a73e-5310cf90a87c,DISK], DatanodeInfoWithStorage[127.0.0.1:34934,DS-6eccac01-8fd0-4765-aa6a-45807512a156,DISK], DatanodeInfoWithStorage[127.0.0.1:43229,DS-4c611f5b-16b9-4334-aa97-8a1fb9a827df,DISK], DatanodeInfoWithStorage[127.0.0.1:36656,DS-4cbbb7b9-130a-4b9a-b775-e294743666ba,DISK], DatanodeInfoWithStorage[127.0.0.1:37257,DS-285650e8-1517-48ed-b60d-593a58f68978,DISK], DatanodeInfoWithStorage[127.0.0.1:38756,DS-c2602512-6d9e-4baa-9c5c-5b9d839a1f9b,DISK], DatanodeInfoWithStorage[127.0.0.1:37748,DS-609a6287-a0b3-450f-a0db-e266c6223497,DISK], DatanodeInfoWithStorage[127.0.0.1:45875,DS-db8b15a6-faf7-478a-bade-c02a001be03c,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-244508872-172.17.0.6-1597396996414:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41994,DS-20f48924-0489-426a-a73e-5310cf90a87c,DISK], DatanodeInfoWithStorage[127.0.0.1:34934,DS-6eccac01-8fd0-4765-aa6a-45807512a156,DISK], DatanodeInfoWithStorage[127.0.0.1:43229,DS-4c611f5b-16b9-4334-aa97-8a1fb9a827df,DISK], DatanodeInfoWithStorage[127.0.0.1:36656,DS-4cbbb7b9-130a-4b9a-b775-e294743666ba,DISK], DatanodeInfoWithStorage[127.0.0.1:37257,DS-285650e8-1517-48ed-b60d-593a58f68978,DISK], DatanodeInfoWithStorage[127.0.0.1:38756,DS-c2602512-6d9e-4baa-9c5c-5b9d839a1f9b,DISK], DatanodeInfoWithStorage[127.0.0.1:37748,DS-609a6287-a0b3-450f-a0db-e266c6223497,DISK], DatanodeInfoWithStorage[127.0.0.1:45875,DS-db8b15a6-faf7-478a-bade-c02a001be03c,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-811379031-172.17.0.6-1597397033160:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32911,DS-a3abe968-d113-4e4e-9e9a-03de8f0e71f2,DISK], DatanodeInfoWithStorage[127.0.0.1:38908,DS-b84cda98-a8f9-4ad7-847c-befc33db7eac,DISK], DatanodeInfoWithStorage[127.0.0.1:45951,DS-403ac5aa-81b1-439d-a204-c528407b2b12,DISK], DatanodeInfoWithStorage[127.0.0.1:40687,DS-102a6c5b-ce03-4e3d-8864-acc53ce3f5f2,DISK], DatanodeInfoWithStorage[127.0.0.1:41253,DS-1762ca8f-74fc-4236-98b6-f5f8119bc580,DISK], DatanodeInfoWithStorage[127.0.0.1:35522,DS-4dab0e2f-f7b9-454f-99a9-44cd2123224f,DISK], DatanodeInfoWithStorage[127.0.0.1:33604,DS-e4da4bbb-8c16-457a-92c8-1f239f663c2d,DISK], DatanodeInfoWithStorage[127.0.0.1:46649,DS-88cca683-e38b-4d4a-b551-c6b3a91b5fe6,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-811379031-172.17.0.6-1597397033160:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32911,DS-a3abe968-d113-4e4e-9e9a-03de8f0e71f2,DISK], DatanodeInfoWithStorage[127.0.0.1:38908,DS-b84cda98-a8f9-4ad7-847c-befc33db7eac,DISK], DatanodeInfoWithStorage[127.0.0.1:45951,DS-403ac5aa-81b1-439d-a204-c528407b2b12,DISK], DatanodeInfoWithStorage[127.0.0.1:40687,DS-102a6c5b-ce03-4e3d-8864-acc53ce3f5f2,DISK], DatanodeInfoWithStorage[127.0.0.1:41253,DS-1762ca8f-74fc-4236-98b6-f5f8119bc580,DISK], DatanodeInfoWithStorage[127.0.0.1:35522,DS-4dab0e2f-f7b9-454f-99a9-44cd2123224f,DISK], DatanodeInfoWithStorage[127.0.0.1:33604,DS-e4da4bbb-8c16-457a-92c8-1f239f663c2d,DISK], DatanodeInfoWithStorage[127.0.0.1:46649,DS-88cca683-e38b-4d4a-b551-c6b3a91b5fe6,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1834451218-172.17.0.6-1597397075273:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42627,DS-b5b6f96a-4c75-403f-be75-fc512a2a60fb,DISK], DatanodeInfoWithStorage[127.0.0.1:46080,DS-f1ce05f9-a7b7-4b76-b894-71692d543735,DISK], DatanodeInfoWithStorage[127.0.0.1:39020,DS-d747385c-f270-4d1c-a3ae-24862a1703be,DISK], DatanodeInfoWithStorage[127.0.0.1:44085,DS-894b0376-b8b8-4fae-9d10-993f5fb2c668,DISK], DatanodeInfoWithStorage[127.0.0.1:42963,DS-141db943-85a7-40ae-b7c2-1e1309d5a5c4,DISK], DatanodeInfoWithStorage[127.0.0.1:38153,DS-90f827f3-0698-400c-bf3f-3846c1fc0572,DISK], DatanodeInfoWithStorage[127.0.0.1:39241,DS-bb240f6e-4b60-4a5f-b7f2-f8453ecafdfe,DISK], DatanodeInfoWithStorage[127.0.0.1:33391,DS-3471c60e-5ecf-4cb3-a184-1cae6ad88e16,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1834451218-172.17.0.6-1597397075273:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42627,DS-b5b6f96a-4c75-403f-be75-fc512a2a60fb,DISK], DatanodeInfoWithStorage[127.0.0.1:46080,DS-f1ce05f9-a7b7-4b76-b894-71692d543735,DISK], DatanodeInfoWithStorage[127.0.0.1:39020,DS-d747385c-f270-4d1c-a3ae-24862a1703be,DISK], DatanodeInfoWithStorage[127.0.0.1:44085,DS-894b0376-b8b8-4fae-9d10-993f5fb2c668,DISK], DatanodeInfoWithStorage[127.0.0.1:42963,DS-141db943-85a7-40ae-b7c2-1e1309d5a5c4,DISK], DatanodeInfoWithStorage[127.0.0.1:38153,DS-90f827f3-0698-400c-bf3f-3846c1fc0572,DISK], DatanodeInfoWithStorage[127.0.0.1:39241,DS-bb240f6e-4b60-4a5f-b7f2-f8453ecafdfe,DISK], DatanodeInfoWithStorage[127.0.0.1:33391,DS-3471c60e-5ecf-4cb3-a184-1cae6ad88e16,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2111974319-172.17.0.6-1597397265326:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34741,DS-4ff01631-edd3-4381-b0ff-170fdebde7d7,DISK], DatanodeInfoWithStorage[127.0.0.1:45897,DS-7483e821-1c50-46d7-b1a3-1ba81be291d0,DISK], DatanodeInfoWithStorage[127.0.0.1:38158,DS-1099b6f8-042d-4b4b-af2c-1613e191dbed,DISK], DatanodeInfoWithStorage[127.0.0.1:36652,DS-b8610ee3-f06d-46de-bb93-41d0c4724f52,DISK], DatanodeInfoWithStorage[127.0.0.1:34587,DS-4f6b81b5-089c-4e8d-adfe-f7299185f571,DISK], DatanodeInfoWithStorage[127.0.0.1:41961,DS-199ae058-7511-44df-bc9e-0e4e981e595b,DISK], DatanodeInfoWithStorage[127.0.0.1:41171,DS-f0bd0fdf-fe34-40eb-bea4-107f6a3cead3,DISK], DatanodeInfoWithStorage[127.0.0.1:44187,DS-40d26c9a-e421-4764-98b4-63c6418b3ff6,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2111974319-172.17.0.6-1597397265326:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34741,DS-4ff01631-edd3-4381-b0ff-170fdebde7d7,DISK], DatanodeInfoWithStorage[127.0.0.1:45897,DS-7483e821-1c50-46d7-b1a3-1ba81be291d0,DISK], DatanodeInfoWithStorage[127.0.0.1:38158,DS-1099b6f8-042d-4b4b-af2c-1613e191dbed,DISK], DatanodeInfoWithStorage[127.0.0.1:36652,DS-b8610ee3-f06d-46de-bb93-41d0c4724f52,DISK], DatanodeInfoWithStorage[127.0.0.1:34587,DS-4f6b81b5-089c-4e8d-adfe-f7299185f571,DISK], DatanodeInfoWithStorage[127.0.0.1:41961,DS-199ae058-7511-44df-bc9e-0e4e981e595b,DISK], DatanodeInfoWithStorage[127.0.0.1:41171,DS-f0bd0fdf-fe34-40eb-bea4-107f6a3cead3,DISK], DatanodeInfoWithStorage[127.0.0.1:44187,DS-40d26c9a-e421-4764-98b4-63c6418b3ff6,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-163103121-172.17.0.6-1597397518592:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35406,DS-aaf76e9b-4bcb-4189-9eed-893b47507bbf,DISK], DatanodeInfoWithStorage[127.0.0.1:41025,DS-ebc08b8b-53f1-482d-9c82-674e14cbc767,DISK], DatanodeInfoWithStorage[127.0.0.1:46212,DS-f8802f13-0c24-4c07-83e5-38e2db9bb1eb,DISK], DatanodeInfoWithStorage[127.0.0.1:45027,DS-c3887a79-3d30-4bf4-870b-df4e697778e5,DISK], DatanodeInfoWithStorage[127.0.0.1:38750,DS-c7509cde-7547-406f-bdfc-ab96b3a3bf4b,DISK], DatanodeInfoWithStorage[127.0.0.1:39312,DS-e1e61f6d-e55a-47fa-ab37-e7204074d4a0,DISK], DatanodeInfoWithStorage[127.0.0.1:35536,DS-ff4fe071-7858-45d3-ace6-382913a64c66,DISK], DatanodeInfoWithStorage[127.0.0.1:38616,DS-d465902b-efd2-4a2e-898b-870e8333c08a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-163103121-172.17.0.6-1597397518592:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35406,DS-aaf76e9b-4bcb-4189-9eed-893b47507bbf,DISK], DatanodeInfoWithStorage[127.0.0.1:41025,DS-ebc08b8b-53f1-482d-9c82-674e14cbc767,DISK], DatanodeInfoWithStorage[127.0.0.1:46212,DS-f8802f13-0c24-4c07-83e5-38e2db9bb1eb,DISK], DatanodeInfoWithStorage[127.0.0.1:45027,DS-c3887a79-3d30-4bf4-870b-df4e697778e5,DISK], DatanodeInfoWithStorage[127.0.0.1:38750,DS-c7509cde-7547-406f-bdfc-ab96b3a3bf4b,DISK], DatanodeInfoWithStorage[127.0.0.1:39312,DS-e1e61f6d-e55a-47fa-ab37-e7204074d4a0,DISK], DatanodeInfoWithStorage[127.0.0.1:35536,DS-ff4fe071-7858-45d3-ace6-382913a64c66,DISK], DatanodeInfoWithStorage[127.0.0.1:38616,DS-d465902b-efd2-4a2e-898b-870e8333c08a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1500702587-172.17.0.6-1597397553224:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36953,DS-61df7b8e-fc3e-4965-8293-31c8859a8908,DISK], DatanodeInfoWithStorage[127.0.0.1:34472,DS-9fba9344-ad8d-4d46-85b4-f6eb3d952b43,DISK], DatanodeInfoWithStorage[127.0.0.1:41463,DS-8819d327-a50e-48b3-b119-143f01e84d7c,DISK], DatanodeInfoWithStorage[127.0.0.1:36947,DS-dfe241c0-87dc-45df-8bab-21a1cd9ce966,DISK], DatanodeInfoWithStorage[127.0.0.1:46115,DS-fda740c2-4970-49e6-8650-ff2f051b2d56,DISK], DatanodeInfoWithStorage[127.0.0.1:44760,DS-1d922387-0cc8-4a80-a75b-848de68ba826,DISK], DatanodeInfoWithStorage[127.0.0.1:44624,DS-03896dc9-e460-4dcb-8e62-cca6c31e24b5,DISK], DatanodeInfoWithStorage[127.0.0.1:46636,DS-4cdff983-bbc5-4aec-b630-d6dd77794872,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1500702587-172.17.0.6-1597397553224:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36953,DS-61df7b8e-fc3e-4965-8293-31c8859a8908,DISK], DatanodeInfoWithStorage[127.0.0.1:34472,DS-9fba9344-ad8d-4d46-85b4-f6eb3d952b43,DISK], DatanodeInfoWithStorage[127.0.0.1:41463,DS-8819d327-a50e-48b3-b119-143f01e84d7c,DISK], DatanodeInfoWithStorage[127.0.0.1:36947,DS-dfe241c0-87dc-45df-8bab-21a1cd9ce966,DISK], DatanodeInfoWithStorage[127.0.0.1:46115,DS-fda740c2-4970-49e6-8650-ff2f051b2d56,DISK], DatanodeInfoWithStorage[127.0.0.1:44760,DS-1d922387-0cc8-4a80-a75b-848de68ba826,DISK], DatanodeInfoWithStorage[127.0.0.1:44624,DS-03896dc9-e460-4dcb-8e62-cca6c31e24b5,DISK], DatanodeInfoWithStorage[127.0.0.1:46636,DS-4cdff983-bbc5-4aec-b630-d6dd77794872,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1608359946-172.17.0.6-1597397589829:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35363,DS-a2c4fc69-89d4-4afb-a1f6-f18fc8a006dd,DISK], DatanodeInfoWithStorage[127.0.0.1:39989,DS-b28409f2-4761-49d4-a9f6-c02c045c8e7c,DISK], DatanodeInfoWithStorage[127.0.0.1:38960,DS-d31b4a00-eaa9-4f90-bdb9-d371e34396c5,DISK], DatanodeInfoWithStorage[127.0.0.1:46715,DS-7f40d5eb-94b9-4276-82c6-06eef2897526,DISK], DatanodeInfoWithStorage[127.0.0.1:45115,DS-9b6e59d1-7c45-4ff6-881b-3591ac226bb7,DISK], DatanodeInfoWithStorage[127.0.0.1:39925,DS-a69a536e-3fe7-40e6-a485-e8d5342a098b,DISK], DatanodeInfoWithStorage[127.0.0.1:42023,DS-cfea0820-f00b-4a57-bb55-6fe27d3bc25e,DISK], DatanodeInfoWithStorage[127.0.0.1:35491,DS-0b71b4c5-cce5-4abd-80eb-f6a80f4353b4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1608359946-172.17.0.6-1597397589829:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35363,DS-a2c4fc69-89d4-4afb-a1f6-f18fc8a006dd,DISK], DatanodeInfoWithStorage[127.0.0.1:39989,DS-b28409f2-4761-49d4-a9f6-c02c045c8e7c,DISK], DatanodeInfoWithStorage[127.0.0.1:38960,DS-d31b4a00-eaa9-4f90-bdb9-d371e34396c5,DISK], DatanodeInfoWithStorage[127.0.0.1:46715,DS-7f40d5eb-94b9-4276-82c6-06eef2897526,DISK], DatanodeInfoWithStorage[127.0.0.1:45115,DS-9b6e59d1-7c45-4ff6-881b-3591ac226bb7,DISK], DatanodeInfoWithStorage[127.0.0.1:39925,DS-a69a536e-3fe7-40e6-a485-e8d5342a098b,DISK], DatanodeInfoWithStorage[127.0.0.1:42023,DS-cfea0820-f00b-4a57-bb55-6fe27d3bc25e,DISK], DatanodeInfoWithStorage[127.0.0.1:35491,DS-0b71b4c5-cce5-4abd-80eb-f6a80f4353b4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-374518409-172.17.0.6-1597397621296:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40383,DS-3f1437b0-52c2-459c-868e-8afe904d3e0e,DISK], DatanodeInfoWithStorage[127.0.0.1:33347,DS-0865758d-d0ee-482d-be99-cb081d575ce8,DISK], DatanodeInfoWithStorage[127.0.0.1:35102,DS-7492b196-9947-4c93-b02f-1310a5994b54,DISK], DatanodeInfoWithStorage[127.0.0.1:40322,DS-bd182593-d646-4ffb-bcd0-1e19f45b4c27,DISK], DatanodeInfoWithStorage[127.0.0.1:43573,DS-bca87999-3986-4d7f-b05b-23f258c3c0e5,DISK], DatanodeInfoWithStorage[127.0.0.1:40610,DS-f4902da0-28ea-403f-83af-f462a6e48d2e,DISK], DatanodeInfoWithStorage[127.0.0.1:32879,DS-5951a581-c49a-4272-9474-394d93c0b236,DISK], DatanodeInfoWithStorage[127.0.0.1:41067,DS-c01996ab-15cb-4ec8-bc75-e0965b20dfa1,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-374518409-172.17.0.6-1597397621296:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40383,DS-3f1437b0-52c2-459c-868e-8afe904d3e0e,DISK], DatanodeInfoWithStorage[127.0.0.1:33347,DS-0865758d-d0ee-482d-be99-cb081d575ce8,DISK], DatanodeInfoWithStorage[127.0.0.1:35102,DS-7492b196-9947-4c93-b02f-1310a5994b54,DISK], DatanodeInfoWithStorage[127.0.0.1:40322,DS-bd182593-d646-4ffb-bcd0-1e19f45b4c27,DISK], DatanodeInfoWithStorage[127.0.0.1:43573,DS-bca87999-3986-4d7f-b05b-23f258c3c0e5,DISK], DatanodeInfoWithStorage[127.0.0.1:40610,DS-f4902da0-28ea-403f-83af-f462a6e48d2e,DISK], DatanodeInfoWithStorage[127.0.0.1:32879,DS-5951a581-c49a-4272-9474-394d93c0b236,DISK], DatanodeInfoWithStorage[127.0.0.1:41067,DS-c01996ab-15cb-4ec8-bc75-e0965b20dfa1,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1034388758-172.17.0.6-1597397706106:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36036,DS-bb977a0d-2e0e-45e7-9453-6f2a0fb9f43e,DISK], DatanodeInfoWithStorage[127.0.0.1:35641,DS-f2319d83-09ff-486d-a2d6-0b81a1d9d3aa,DISK], DatanodeInfoWithStorage[127.0.0.1:42979,DS-56c5cd70-7e2c-4518-a807-acba117f3a5c,DISK], DatanodeInfoWithStorage[127.0.0.1:35536,DS-9adbc477-91dd-463d-8d92-178d57bf18be,DISK], DatanodeInfoWithStorage[127.0.0.1:42816,DS-7d83f32c-57dc-428b-9d48-7175580ae438,DISK], DatanodeInfoWithStorage[127.0.0.1:43535,DS-02bacd82-f35e-4dc8-b573-8cdfa590a33b,DISK], DatanodeInfoWithStorage[127.0.0.1:38976,DS-db48b00b-9638-4f94-8642-c0ed34341f7d,DISK], DatanodeInfoWithStorage[127.0.0.1:44832,DS-925d94f1-e032-45c7-8518-4dbd6eea6bc6,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1034388758-172.17.0.6-1597397706106:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36036,DS-bb977a0d-2e0e-45e7-9453-6f2a0fb9f43e,DISK], DatanodeInfoWithStorage[127.0.0.1:35641,DS-f2319d83-09ff-486d-a2d6-0b81a1d9d3aa,DISK], DatanodeInfoWithStorage[127.0.0.1:42979,DS-56c5cd70-7e2c-4518-a807-acba117f3a5c,DISK], DatanodeInfoWithStorage[127.0.0.1:35536,DS-9adbc477-91dd-463d-8d92-178d57bf18be,DISK], DatanodeInfoWithStorage[127.0.0.1:42816,DS-7d83f32c-57dc-428b-9d48-7175580ae438,DISK], DatanodeInfoWithStorage[127.0.0.1:43535,DS-02bacd82-f35e-4dc8-b573-8cdfa590a33b,DISK], DatanodeInfoWithStorage[127.0.0.1:38976,DS-db48b00b-9638-4f94-8642-c0ed34341f7d,DISK], DatanodeInfoWithStorage[127.0.0.1:44832,DS-925d94f1-e032-45c7-8518-4dbd6eea6bc6,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-272731510-172.17.0.6-1597397969436:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42459,DS-6f5500b4-b74d-44d0-aa70-440a2e6f05aa,DISK], DatanodeInfoWithStorage[127.0.0.1:37779,DS-6f3e6751-2677-4e7b-9bc7-3ec9c9a9be84,DISK], DatanodeInfoWithStorage[127.0.0.1:36500,DS-778a92f2-ef1b-4e52-b1de-07c1804e32e5,DISK], DatanodeInfoWithStorage[127.0.0.1:41601,DS-138022bf-5743-4367-9a41-083d40571bf8,DISK], DatanodeInfoWithStorage[127.0.0.1:45878,DS-caff7b2d-b12a-422d-bcde-a851adbf3b39,DISK], DatanodeInfoWithStorage[127.0.0.1:41383,DS-eec3c24d-f8f1-4c93-9678-161c91baaf52,DISK], DatanodeInfoWithStorage[127.0.0.1:42062,DS-41db58b3-be50-49d7-9e9a-6c5615a11cd7,DISK], DatanodeInfoWithStorage[127.0.0.1:35891,DS-4529c293-f597-476c-91ad-69cde247db67,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-272731510-172.17.0.6-1597397969436:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42459,DS-6f5500b4-b74d-44d0-aa70-440a2e6f05aa,DISK], DatanodeInfoWithStorage[127.0.0.1:37779,DS-6f3e6751-2677-4e7b-9bc7-3ec9c9a9be84,DISK], DatanodeInfoWithStorage[127.0.0.1:36500,DS-778a92f2-ef1b-4e52-b1de-07c1804e32e5,DISK], DatanodeInfoWithStorage[127.0.0.1:41601,DS-138022bf-5743-4367-9a41-083d40571bf8,DISK], DatanodeInfoWithStorage[127.0.0.1:45878,DS-caff7b2d-b12a-422d-bcde-a851adbf3b39,DISK], DatanodeInfoWithStorage[127.0.0.1:41383,DS-eec3c24d-f8f1-4c93-9678-161c91baaf52,DISK], DatanodeInfoWithStorage[127.0.0.1:42062,DS-41db58b3-be50-49d7-9e9a-6c5615a11cd7,DISK], DatanodeInfoWithStorage[127.0.0.1:35891,DS-4529c293-f597-476c-91ad-69cde247db67,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-112854115-172.17.0.6-1597398076868:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46501,DS-ed45cfb0-be91-40ed-9967-2f773d4dece2,DISK], DatanodeInfoWithStorage[127.0.0.1:38144,DS-75664f87-a593-4a39-92be-26810f3bfc4c,DISK], DatanodeInfoWithStorage[127.0.0.1:43390,DS-a16004bb-c393-41a8-b646-e8c9f28c7365,DISK], DatanodeInfoWithStorage[127.0.0.1:35239,DS-59d5543e-1549-4b0e-ac20-e7b6f3965db0,DISK], DatanodeInfoWithStorage[127.0.0.1:45231,DS-d792971c-aa6b-494c-89dd-f8c1783a80b5,DISK], DatanodeInfoWithStorage[127.0.0.1:34353,DS-1d9650bd-7f60-4dc4-8162-5db1ee0a2372,DISK], DatanodeInfoWithStorage[127.0.0.1:36337,DS-6cc3cadd-ad9c-4113-869a-78b3eba07515,DISK], DatanodeInfoWithStorage[127.0.0.1:33278,DS-6e42254d-9dfd-4b5b-8752-7622ed93079c,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-112854115-172.17.0.6-1597398076868:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46501,DS-ed45cfb0-be91-40ed-9967-2f773d4dece2,DISK], DatanodeInfoWithStorage[127.0.0.1:38144,DS-75664f87-a593-4a39-92be-26810f3bfc4c,DISK], DatanodeInfoWithStorage[127.0.0.1:43390,DS-a16004bb-c393-41a8-b646-e8c9f28c7365,DISK], DatanodeInfoWithStorage[127.0.0.1:35239,DS-59d5543e-1549-4b0e-ac20-e7b6f3965db0,DISK], DatanodeInfoWithStorage[127.0.0.1:45231,DS-d792971c-aa6b-494c-89dd-f8c1783a80b5,DISK], DatanodeInfoWithStorage[127.0.0.1:34353,DS-1d9650bd-7f60-4dc4-8162-5db1ee0a2372,DISK], DatanodeInfoWithStorage[127.0.0.1:36337,DS-6cc3cadd-ad9c-4113-869a-78b3eba07515,DISK], DatanodeInfoWithStorage[127.0.0.1:33278,DS-6e42254d-9dfd-4b5b-8752-7622ed93079c,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1898351840-172.17.0.6-1597398254486:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43979,DS-146e5c6b-3529-478f-b869-2bfa0dc462ef,DISK], DatanodeInfoWithStorage[127.0.0.1:39698,DS-9b9253b0-35e4-491b-8ba9-37a28fb14de1,DISK], DatanodeInfoWithStorage[127.0.0.1:37966,DS-04a42b7c-801f-4522-88b2-c1c4b0cda4b9,DISK], DatanodeInfoWithStorage[127.0.0.1:43372,DS-3c1feaab-d65c-4a54-8767-a9ecaf94f66f,DISK], DatanodeInfoWithStorage[127.0.0.1:34882,DS-addae213-fad6-4249-bb26-ce67439dc76a,DISK], DatanodeInfoWithStorage[127.0.0.1:39428,DS-33ee8978-ec99-4cd2-9737-90e2c63269c0,DISK], DatanodeInfoWithStorage[127.0.0.1:45176,DS-9cc96d84-474e-466b-a9e2-bda819aab87e,DISK], DatanodeInfoWithStorage[127.0.0.1:39108,DS-879e7bbf-0e54-42ca-8b92-200edf7f4c41,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1898351840-172.17.0.6-1597398254486:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43979,DS-146e5c6b-3529-478f-b869-2bfa0dc462ef,DISK], DatanodeInfoWithStorage[127.0.0.1:39698,DS-9b9253b0-35e4-491b-8ba9-37a28fb14de1,DISK], DatanodeInfoWithStorage[127.0.0.1:37966,DS-04a42b7c-801f-4522-88b2-c1c4b0cda4b9,DISK], DatanodeInfoWithStorage[127.0.0.1:43372,DS-3c1feaab-d65c-4a54-8767-a9ecaf94f66f,DISK], DatanodeInfoWithStorage[127.0.0.1:34882,DS-addae213-fad6-4249-bb26-ce67439dc76a,DISK], DatanodeInfoWithStorage[127.0.0.1:39428,DS-33ee8978-ec99-4cd2-9737-90e2c63269c0,DISK], DatanodeInfoWithStorage[127.0.0.1:45176,DS-9cc96d84-474e-466b-a9e2-bda819aab87e,DISK], DatanodeInfoWithStorage[127.0.0.1:39108,DS-879e7bbf-0e54-42ca-8b92-200edf7f4c41,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1155464784-172.17.0.6-1597398327448:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40964,DS-0ac9c9f0-f78b-4ade-888e-a28796c4ed3b,DISK], DatanodeInfoWithStorage[127.0.0.1:40439,DS-06d8bc7d-039c-41a4-a700-6e4c746592e8,DISK], DatanodeInfoWithStorage[127.0.0.1:35518,DS-a305df0f-581b-43d6-925e-da3c8d4a515c,DISK], DatanodeInfoWithStorage[127.0.0.1:33854,DS-caf27046-e578-4d91-963b-3b4711bacbf4,DISK], DatanodeInfoWithStorage[127.0.0.1:39886,DS-fdde301e-8812-410b-a945-ac51a9a1f66b,DISK], DatanodeInfoWithStorage[127.0.0.1:36091,DS-6337389d-e9e9-42ae-af02-48d2c593aff6,DISK], DatanodeInfoWithStorage[127.0.0.1:33703,DS-6f4934ad-d85a-4132-836f-11fea6deb0a3,DISK], DatanodeInfoWithStorage[127.0.0.1:34721,DS-fdd780b0-18ad-4617-a217-fab6a178b642,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1155464784-172.17.0.6-1597398327448:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40964,DS-0ac9c9f0-f78b-4ade-888e-a28796c4ed3b,DISK], DatanodeInfoWithStorage[127.0.0.1:40439,DS-06d8bc7d-039c-41a4-a700-6e4c746592e8,DISK], DatanodeInfoWithStorage[127.0.0.1:35518,DS-a305df0f-581b-43d6-925e-da3c8d4a515c,DISK], DatanodeInfoWithStorage[127.0.0.1:33854,DS-caf27046-e578-4d91-963b-3b4711bacbf4,DISK], DatanodeInfoWithStorage[127.0.0.1:39886,DS-fdde301e-8812-410b-a945-ac51a9a1f66b,DISK], DatanodeInfoWithStorage[127.0.0.1:36091,DS-6337389d-e9e9-42ae-af02-48d2c593aff6,DISK], DatanodeInfoWithStorage[127.0.0.1:33703,DS-6f4934ad-d85a-4132-836f-11fea6deb0a3,DISK], DatanodeInfoWithStorage[127.0.0.1:34721,DS-fdd780b0-18ad-4617-a217-fab6a178b642,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2067866377-172.17.0.6-1597398409751:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35324,DS-21ab0862-1bf6-4b5d-82be-68eda24621ef,DISK], DatanodeInfoWithStorage[127.0.0.1:44029,DS-f4d36fe3-6aba-4c96-87d6-febe00125e00,DISK], DatanodeInfoWithStorage[127.0.0.1:39182,DS-7bca53d3-e89e-43b5-86f3-b43667390ef3,DISK], DatanodeInfoWithStorage[127.0.0.1:41488,DS-73eeb672-9acd-4fa6-a5db-1b21bd3f4948,DISK], DatanodeInfoWithStorage[127.0.0.1:40277,DS-eb86f7b6-f07c-4f6e-a181-d17883d96ef8,DISK], DatanodeInfoWithStorage[127.0.0.1:46325,DS-2f513228-3b64-4bd4-acb5-20a558412a1f,DISK], DatanodeInfoWithStorage[127.0.0.1:45974,DS-79e37e8a-c065-4678-83f5-aaef172e7f24,DISK], DatanodeInfoWithStorage[127.0.0.1:46668,DS-ca2c78c2-1c84-400b-9740-6ffe928e64a7,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2067866377-172.17.0.6-1597398409751:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35324,DS-21ab0862-1bf6-4b5d-82be-68eda24621ef,DISK], DatanodeInfoWithStorage[127.0.0.1:44029,DS-f4d36fe3-6aba-4c96-87d6-febe00125e00,DISK], DatanodeInfoWithStorage[127.0.0.1:39182,DS-7bca53d3-e89e-43b5-86f3-b43667390ef3,DISK], DatanodeInfoWithStorage[127.0.0.1:41488,DS-73eeb672-9acd-4fa6-a5db-1b21bd3f4948,DISK], DatanodeInfoWithStorage[127.0.0.1:40277,DS-eb86f7b6-f07c-4f6e-a181-d17883d96ef8,DISK], DatanodeInfoWithStorage[127.0.0.1:46325,DS-2f513228-3b64-4bd4-acb5-20a558412a1f,DISK], DatanodeInfoWithStorage[127.0.0.1:45974,DS-79e37e8a-c065-4678-83f5-aaef172e7f24,DISK], DatanodeInfoWithStorage[127.0.0.1:46668,DS-ca2c78c2-1c84-400b-9740-6ffe928e64a7,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1104203892-172.17.0.6-1597398559910:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44887,DS-d5290e18-1527-4f20-a434-bdf8c76d9ee7,DISK], DatanodeInfoWithStorage[127.0.0.1:32983,DS-e52f759b-8a2c-4f2e-b185-2939cc71f34b,DISK], DatanodeInfoWithStorage[127.0.0.1:37561,DS-6b211151-b373-4207-b32f-f11f0b676a67,DISK], DatanodeInfoWithStorage[127.0.0.1:35073,DS-7713725a-390d-4d4a-9ebe-fe85d2613059,DISK], DatanodeInfoWithStorage[127.0.0.1:38179,DS-a43f12a4-935c-47fb-b9e2-04f36c11d2cc,DISK], DatanodeInfoWithStorage[127.0.0.1:32911,DS-83301558-f44c-4370-b26c-954a1c4dbfa0,DISK], DatanodeInfoWithStorage[127.0.0.1:41968,DS-354af0fa-060c-4dae-956b-016a7882af1a,DISK], DatanodeInfoWithStorage[127.0.0.1:38267,DS-6017ccef-743d-4c4b-a4f5-a3a5afbe8347,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1104203892-172.17.0.6-1597398559910:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44887,DS-d5290e18-1527-4f20-a434-bdf8c76d9ee7,DISK], DatanodeInfoWithStorage[127.0.0.1:32983,DS-e52f759b-8a2c-4f2e-b185-2939cc71f34b,DISK], DatanodeInfoWithStorage[127.0.0.1:37561,DS-6b211151-b373-4207-b32f-f11f0b676a67,DISK], DatanodeInfoWithStorage[127.0.0.1:35073,DS-7713725a-390d-4d4a-9ebe-fe85d2613059,DISK], DatanodeInfoWithStorage[127.0.0.1:38179,DS-a43f12a4-935c-47fb-b9e2-04f36c11d2cc,DISK], DatanodeInfoWithStorage[127.0.0.1:32911,DS-83301558-f44c-4370-b26c-954a1c4dbfa0,DISK], DatanodeInfoWithStorage[127.0.0.1:41968,DS-354af0fa-060c-4dae-956b-016a7882af1a,DISK], DatanodeInfoWithStorage[127.0.0.1:38267,DS-6017ccef-743d-4c4b-a4f5-a3a5afbe8347,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-26183727-172.17.0.6-1597398673257:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40781,DS-3e2d8551-941f-482a-be82-3c638ea202f9,DISK], DatanodeInfoWithStorage[127.0.0.1:46669,DS-3f40daf4-9773-4de7-9e84-b55dec0b599a,DISK], DatanodeInfoWithStorage[127.0.0.1:44113,DS-0cbcec7f-df3a-48fc-87e4-ced0320aba96,DISK], DatanodeInfoWithStorage[127.0.0.1:37731,DS-c359dbd6-5b10-42d6-8a8c-0d00a53a7383,DISK], DatanodeInfoWithStorage[127.0.0.1:38166,DS-89230835-3b50-4cb7-81a8-5f1ef31308a7,DISK], DatanodeInfoWithStorage[127.0.0.1:41460,DS-e403e198-21e8-4392-b47f-1161078b41b0,DISK], DatanodeInfoWithStorage[127.0.0.1:33596,DS-df28b72b-7bb0-4ea5-9768-34a6f0652f24,DISK], DatanodeInfoWithStorage[127.0.0.1:35591,DS-d3c5e2e3-fac6-4631-977b-8974908b39fe,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-26183727-172.17.0.6-1597398673257:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40781,DS-3e2d8551-941f-482a-be82-3c638ea202f9,DISK], DatanodeInfoWithStorage[127.0.0.1:46669,DS-3f40daf4-9773-4de7-9e84-b55dec0b599a,DISK], DatanodeInfoWithStorage[127.0.0.1:44113,DS-0cbcec7f-df3a-48fc-87e4-ced0320aba96,DISK], DatanodeInfoWithStorage[127.0.0.1:37731,DS-c359dbd6-5b10-42d6-8a8c-0d00a53a7383,DISK], DatanodeInfoWithStorage[127.0.0.1:38166,DS-89230835-3b50-4cb7-81a8-5f1ef31308a7,DISK], DatanodeInfoWithStorage[127.0.0.1:41460,DS-e403e198-21e8-4392-b47f-1161078b41b0,DISK], DatanodeInfoWithStorage[127.0.0.1:33596,DS-df28b72b-7bb0-4ea5-9768-34a6f0652f24,DISK], DatanodeInfoWithStorage[127.0.0.1:35591,DS-d3c5e2e3-fac6-4631-977b-8974908b39fe,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-473423794-172.17.0.6-1597398888831:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40417,DS-ae50616e-ef66-45d5-acc9-ea9c8af4438d,DISK], DatanodeInfoWithStorage[127.0.0.1:36162,DS-865fe3cc-b7c0-4c9b-8c1a-d6ab1d3839f5,DISK], DatanodeInfoWithStorage[127.0.0.1:44692,DS-660f28e9-4a16-40f7-93db-fe69de3a7366,DISK], DatanodeInfoWithStorage[127.0.0.1:32872,DS-9f48b438-c8aa-47af-b30f-dea58a294e69,DISK], DatanodeInfoWithStorage[127.0.0.1:37286,DS-c5cf3b17-a04c-4df1-b62a-a38c21573d4a,DISK], DatanodeInfoWithStorage[127.0.0.1:46070,DS-4a431362-6151-49d0-97a5-f70f47b0ec68,DISK], DatanodeInfoWithStorage[127.0.0.1:38021,DS-2260f88d-057f-4662-bfc7-9c3935178e09,DISK], DatanodeInfoWithStorage[127.0.0.1:45470,DS-6373c1da-4adc-49e3-8630-4a6be9b19738,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-473423794-172.17.0.6-1597398888831:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40417,DS-ae50616e-ef66-45d5-acc9-ea9c8af4438d,DISK], DatanodeInfoWithStorage[127.0.0.1:36162,DS-865fe3cc-b7c0-4c9b-8c1a-d6ab1d3839f5,DISK], DatanodeInfoWithStorage[127.0.0.1:44692,DS-660f28e9-4a16-40f7-93db-fe69de3a7366,DISK], DatanodeInfoWithStorage[127.0.0.1:32872,DS-9f48b438-c8aa-47af-b30f-dea58a294e69,DISK], DatanodeInfoWithStorage[127.0.0.1:37286,DS-c5cf3b17-a04c-4df1-b62a-a38c21573d4a,DISK], DatanodeInfoWithStorage[127.0.0.1:46070,DS-4a431362-6151-49d0-97a5-f70f47b0ec68,DISK], DatanodeInfoWithStorage[127.0.0.1:38021,DS-2260f88d-057f-4662-bfc7-9c3935178e09,DISK], DatanodeInfoWithStorage[127.0.0.1:45470,DS-6373c1da-4adc-49e3-8630-4a6be9b19738,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-31845279-172.17.0.6-1597399315519:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35120,DS-145d80c8-815f-4aff-b77c-1d55ab41ff2e,DISK], DatanodeInfoWithStorage[127.0.0.1:43958,DS-0689a31c-683d-4026-befa-74c39c012b36,DISK], DatanodeInfoWithStorage[127.0.0.1:40818,DS-cafe2470-1b10-4668-9f2d-d8c9602f84bf,DISK], DatanodeInfoWithStorage[127.0.0.1:45294,DS-50c773e2-8d99-4d76-975c-de6f3e44ee27,DISK], DatanodeInfoWithStorage[127.0.0.1:40032,DS-a39772d2-71f7-4bf2-8d36-d1285951209a,DISK], DatanodeInfoWithStorage[127.0.0.1:34539,DS-675a7df8-ca7f-41e3-831e-30445d3ad8cc,DISK], DatanodeInfoWithStorage[127.0.0.1:38633,DS-e3a3f4ca-9fb5-4e13-85d3-8e2eb4c1f5ba,DISK], DatanodeInfoWithStorage[127.0.0.1:45403,DS-60fb03e8-8977-4afd-9a34-fc774802820f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-31845279-172.17.0.6-1597399315519:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35120,DS-145d80c8-815f-4aff-b77c-1d55ab41ff2e,DISK], DatanodeInfoWithStorage[127.0.0.1:43958,DS-0689a31c-683d-4026-befa-74c39c012b36,DISK], DatanodeInfoWithStorage[127.0.0.1:40818,DS-cafe2470-1b10-4668-9f2d-d8c9602f84bf,DISK], DatanodeInfoWithStorage[127.0.0.1:45294,DS-50c773e2-8d99-4d76-975c-de6f3e44ee27,DISK], DatanodeInfoWithStorage[127.0.0.1:40032,DS-a39772d2-71f7-4bf2-8d36-d1285951209a,DISK], DatanodeInfoWithStorage[127.0.0.1:34539,DS-675a7df8-ca7f-41e3-831e-30445d3ad8cc,DISK], DatanodeInfoWithStorage[127.0.0.1:38633,DS-e3a3f4ca-9fb5-4e13-85d3-8e2eb4c1f5ba,DISK], DatanodeInfoWithStorage[127.0.0.1:45403,DS-60fb03e8-8977-4afd-9a34-fc774802820f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1174648259-172.17.0.6-1597399431275:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40661,DS-85ac6543-321b-4ccd-ab85-e903dec2dcef,DISK], DatanodeInfoWithStorage[127.0.0.1:44800,DS-6ce5235f-2624-42a5-a52a-108006812926,DISK], DatanodeInfoWithStorage[127.0.0.1:40955,DS-739cc9cd-b3a0-4823-a2e8-5a5a5f85dfbb,DISK], DatanodeInfoWithStorage[127.0.0.1:38390,DS-3defa9bc-6397-4811-9b2e-0b9c452bd914,DISK], DatanodeInfoWithStorage[127.0.0.1:34623,DS-b5bed3f1-6bda-4e33-a520-2f1bc0d8409c,DISK], DatanodeInfoWithStorage[127.0.0.1:42938,DS-a27e1455-8711-4888-aa50-67c9feb41618,DISK], DatanodeInfoWithStorage[127.0.0.1:40500,DS-a262efa1-6b32-4631-a0af-a7c80b6acfc6,DISK], DatanodeInfoWithStorage[127.0.0.1:40334,DS-0c21cfd6-dbac-4f47-baef-b3953609266a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1174648259-172.17.0.6-1597399431275:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40661,DS-85ac6543-321b-4ccd-ab85-e903dec2dcef,DISK], DatanodeInfoWithStorage[127.0.0.1:44800,DS-6ce5235f-2624-42a5-a52a-108006812926,DISK], DatanodeInfoWithStorage[127.0.0.1:40955,DS-739cc9cd-b3a0-4823-a2e8-5a5a5f85dfbb,DISK], DatanodeInfoWithStorage[127.0.0.1:38390,DS-3defa9bc-6397-4811-9b2e-0b9c452bd914,DISK], DatanodeInfoWithStorage[127.0.0.1:34623,DS-b5bed3f1-6bda-4e33-a520-2f1bc0d8409c,DISK], DatanodeInfoWithStorage[127.0.0.1:42938,DS-a27e1455-8711-4888-aa50-67c9feb41618,DISK], DatanodeInfoWithStorage[127.0.0.1:40500,DS-a262efa1-6b32-4631-a0af-a7c80b6acfc6,DISK], DatanodeInfoWithStorage[127.0.0.1:40334,DS-0c21cfd6-dbac-4f47-baef-b3953609266a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-815877847-172.17.0.6-1597399467101:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44549,DS-5efd40ea-f778-4661-96d2-024335daf958,DISK], DatanodeInfoWithStorage[127.0.0.1:34182,DS-ce8bc052-b3b9-43ed-b910-1e1a5a7d2155,DISK], DatanodeInfoWithStorage[127.0.0.1:33706,DS-7c429dd4-d791-47b7-b735-ff88341ab8a4,DISK], DatanodeInfoWithStorage[127.0.0.1:40984,DS-db09a953-b2d6-4088-ac51-62948d114949,DISK], DatanodeInfoWithStorage[127.0.0.1:46190,DS-d466f387-4246-4ddb-83f1-38679d5f54e6,DISK], DatanodeInfoWithStorage[127.0.0.1:34181,DS-602000ad-fd10-4b50-8db8-b9b8ad84d99d,DISK], DatanodeInfoWithStorage[127.0.0.1:37212,DS-da7fe95e-c970-4a5b-8e9f-b9b7ecbec033,DISK], DatanodeInfoWithStorage[127.0.0.1:38077,DS-33f200d0-59a1-4e80-bcf5-5559726faba8,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-815877847-172.17.0.6-1597399467101:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44549,DS-5efd40ea-f778-4661-96d2-024335daf958,DISK], DatanodeInfoWithStorage[127.0.0.1:34182,DS-ce8bc052-b3b9-43ed-b910-1e1a5a7d2155,DISK], DatanodeInfoWithStorage[127.0.0.1:33706,DS-7c429dd4-d791-47b7-b735-ff88341ab8a4,DISK], DatanodeInfoWithStorage[127.0.0.1:40984,DS-db09a953-b2d6-4088-ac51-62948d114949,DISK], DatanodeInfoWithStorage[127.0.0.1:46190,DS-d466f387-4246-4ddb-83f1-38679d5f54e6,DISK], DatanodeInfoWithStorage[127.0.0.1:34181,DS-602000ad-fd10-4b50-8db8-b9b8ad84d99d,DISK], DatanodeInfoWithStorage[127.0.0.1:37212,DS-da7fe95e-c970-4a5b-8e9f-b9b7ecbec033,DISK], DatanodeInfoWithStorage[127.0.0.1:38077,DS-33f200d0-59a1-4e80-bcf5-5559726faba8,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-131820864-172.17.0.6-1597399578232:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36826,DS-f015d850-d417-4058-b256-341ab5090f2d,DISK], DatanodeInfoWithStorage[127.0.0.1:40401,DS-f210daab-7108-49ea-95d3-2d7c6989adab,DISK], DatanodeInfoWithStorage[127.0.0.1:44118,DS-df3c13bd-f75a-4e46-a28b-b85b61961690,DISK], DatanodeInfoWithStorage[127.0.0.1:43420,DS-dd316af0-7f6f-446d-97ad-09f72a47057c,DISK], DatanodeInfoWithStorage[127.0.0.1:45529,DS-941c8c48-89af-4efd-8878-af03e98f4599,DISK], DatanodeInfoWithStorage[127.0.0.1:35349,DS-f8bb483f-284c-4c29-94f2-e948abc0d0d0,DISK], DatanodeInfoWithStorage[127.0.0.1:40996,DS-8043276b-0251-45c2-8190-cfdf2cc30dc9,DISK], DatanodeInfoWithStorage[127.0.0.1:42975,DS-670c318e-2871-4f65-92c6-a1ea23e6a4be,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-131820864-172.17.0.6-1597399578232:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36826,DS-f015d850-d417-4058-b256-341ab5090f2d,DISK], DatanodeInfoWithStorage[127.0.0.1:40401,DS-f210daab-7108-49ea-95d3-2d7c6989adab,DISK], DatanodeInfoWithStorage[127.0.0.1:44118,DS-df3c13bd-f75a-4e46-a28b-b85b61961690,DISK], DatanodeInfoWithStorage[127.0.0.1:43420,DS-dd316af0-7f6f-446d-97ad-09f72a47057c,DISK], DatanodeInfoWithStorage[127.0.0.1:45529,DS-941c8c48-89af-4efd-8878-af03e98f4599,DISK], DatanodeInfoWithStorage[127.0.0.1:35349,DS-f8bb483f-284c-4c29-94f2-e948abc0d0d0,DISK], DatanodeInfoWithStorage[127.0.0.1:40996,DS-8043276b-0251-45c2-8190-cfdf2cc30dc9,DISK], DatanodeInfoWithStorage[127.0.0.1:42975,DS-670c318e-2871-4f65-92c6-a1ea23e6a4be,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-6154653-172.17.0.6-1597399733055:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45288,DS-e8e84e18-7b41-4ebf-80fd-531ef1acbc1f,DISK], DatanodeInfoWithStorage[127.0.0.1:34723,DS-43017a7b-5ee4-4ddf-a094-9003e6c510e0,DISK], DatanodeInfoWithStorage[127.0.0.1:42950,DS-28dabb69-b401-4c08-ae9e-0a03def37b13,DISK], DatanodeInfoWithStorage[127.0.0.1:46532,DS-01d631f6-a33b-48ad-99e0-57dfc78182c3,DISK], DatanodeInfoWithStorage[127.0.0.1:37965,DS-a6ba382c-ac4d-494b-b67a-b6a5ad85e45d,DISK], DatanodeInfoWithStorage[127.0.0.1:36082,DS-f845409f-ca00-4d7d-9b10-2ad0032945a1,DISK], DatanodeInfoWithStorage[127.0.0.1:35567,DS-3837a5f7-232b-4f23-8466-333d2402c53f,DISK], DatanodeInfoWithStorage[127.0.0.1:44169,DS-97678a5a-82ae-4218-afc3-0451f5dd2f00,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-6154653-172.17.0.6-1597399733055:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45288,DS-e8e84e18-7b41-4ebf-80fd-531ef1acbc1f,DISK], DatanodeInfoWithStorage[127.0.0.1:34723,DS-43017a7b-5ee4-4ddf-a094-9003e6c510e0,DISK], DatanodeInfoWithStorage[127.0.0.1:42950,DS-28dabb69-b401-4c08-ae9e-0a03def37b13,DISK], DatanodeInfoWithStorage[127.0.0.1:46532,DS-01d631f6-a33b-48ad-99e0-57dfc78182c3,DISK], DatanodeInfoWithStorage[127.0.0.1:37965,DS-a6ba382c-ac4d-494b-b67a-b6a5ad85e45d,DISK], DatanodeInfoWithStorage[127.0.0.1:36082,DS-f845409f-ca00-4d7d-9b10-2ad0032945a1,DISK], DatanodeInfoWithStorage[127.0.0.1:35567,DS-3837a5f7-232b-4f23-8466-333d2402c53f,DISK], DatanodeInfoWithStorage[127.0.0.1:44169,DS-97678a5a-82ae-4218-afc3-0451f5dd2f00,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1885315833-172.17.0.6-1597399773169:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39353,DS-5cfcb74e-6f50-4cdc-ba3e-3d7d2e64a1be,DISK], DatanodeInfoWithStorage[127.0.0.1:36410,DS-20ec331a-9ad6-402b-88a4-4abacdd030f0,DISK], DatanodeInfoWithStorage[127.0.0.1:32788,DS-47f5edaa-ec85-4b10-ac1a-01ff34609fe6,DISK], DatanodeInfoWithStorage[127.0.0.1:44085,DS-26a3bf5d-94aa-46ca-9c02-d1759c505ca8,DISK], DatanodeInfoWithStorage[127.0.0.1:37098,DS-69c29f60-15dd-4d2b-bd8a-0d8ac63685e2,DISK], DatanodeInfoWithStorage[127.0.0.1:44282,DS-e0e6d2f9-0fcc-4aa1-b4a7-85c8634af465,DISK], DatanodeInfoWithStorage[127.0.0.1:43933,DS-611cfec0-c96a-45bc-bdc0-3f5ec82ec596,DISK], DatanodeInfoWithStorage[127.0.0.1:44099,DS-6a981db3-ba2a-4cd3-a1b4-8f2b3b778297,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1885315833-172.17.0.6-1597399773169:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39353,DS-5cfcb74e-6f50-4cdc-ba3e-3d7d2e64a1be,DISK], DatanodeInfoWithStorage[127.0.0.1:36410,DS-20ec331a-9ad6-402b-88a4-4abacdd030f0,DISK], DatanodeInfoWithStorage[127.0.0.1:32788,DS-47f5edaa-ec85-4b10-ac1a-01ff34609fe6,DISK], DatanodeInfoWithStorage[127.0.0.1:44085,DS-26a3bf5d-94aa-46ca-9c02-d1759c505ca8,DISK], DatanodeInfoWithStorage[127.0.0.1:37098,DS-69c29f60-15dd-4d2b-bd8a-0d8ac63685e2,DISK], DatanodeInfoWithStorage[127.0.0.1:44282,DS-e0e6d2f9-0fcc-4aa1-b4a7-85c8634af465,DISK], DatanodeInfoWithStorage[127.0.0.1:43933,DS-611cfec0-c96a-45bc-bdc0-3f5ec82ec596,DISK], DatanodeInfoWithStorage[127.0.0.1:44099,DS-6a981db3-ba2a-4cd3-a1b4-8f2b3b778297,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1375337632-172.17.0.6-1597399813209:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41860,DS-b8840655-ff31-4a02-9db3-a3da539fd3bc,DISK], DatanodeInfoWithStorage[127.0.0.1:34552,DS-6b584365-1bd6-4ef2-81d8-796da7adbc2a,DISK], DatanodeInfoWithStorage[127.0.0.1:39524,DS-40373f6b-fe65-422f-ba1d-54915eff4db9,DISK], DatanodeInfoWithStorage[127.0.0.1:33692,DS-67b192c1-4894-4ce6-930f-d4ffa9dca020,DISK], DatanodeInfoWithStorage[127.0.0.1:34506,DS-75ea188f-93a2-431c-a044-6186afe11bb7,DISK], DatanodeInfoWithStorage[127.0.0.1:46762,DS-c49643bd-2b20-4314-a4cc-ef31ac9cc303,DISK], DatanodeInfoWithStorage[127.0.0.1:41541,DS-9b95c4d2-54ce-4c33-8f71-d876fe688ec6,DISK], DatanodeInfoWithStorage[127.0.0.1:38622,DS-9296f483-f819-4ff2-900f-6858868e575f,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1375337632-172.17.0.6-1597399813209:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41860,DS-b8840655-ff31-4a02-9db3-a3da539fd3bc,DISK], DatanodeInfoWithStorage[127.0.0.1:34552,DS-6b584365-1bd6-4ef2-81d8-796da7adbc2a,DISK], DatanodeInfoWithStorage[127.0.0.1:39524,DS-40373f6b-fe65-422f-ba1d-54915eff4db9,DISK], DatanodeInfoWithStorage[127.0.0.1:33692,DS-67b192c1-4894-4ce6-930f-d4ffa9dca020,DISK], DatanodeInfoWithStorage[127.0.0.1:34506,DS-75ea188f-93a2-431c-a044-6186afe11bb7,DISK], DatanodeInfoWithStorage[127.0.0.1:46762,DS-c49643bd-2b20-4314-a4cc-ef31ac9cc303,DISK], DatanodeInfoWithStorage[127.0.0.1:41541,DS-9b95c4d2-54ce-4c33-8f71-d876fe688ec6,DISK], DatanodeInfoWithStorage[127.0.0.1:38622,DS-9296f483-f819-4ff2-900f-6858868e575f,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1320265170-172.17.0.6-1597399890507:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46642,DS-c0aa97a6-6ed9-4c38-b86f-05e9dac63ed0,DISK], DatanodeInfoWithStorage[127.0.0.1:46558,DS-99615685-d379-4fb2-bf58-3198ec30df17,DISK], DatanodeInfoWithStorage[127.0.0.1:41106,DS-8c30f6eb-1d5b-497e-8f33-690754e196ef,DISK], DatanodeInfoWithStorage[127.0.0.1:43422,DS-97689fa9-1151-433e-aa0b-789867c3e6a4,DISK], DatanodeInfoWithStorage[127.0.0.1:33918,DS-800992e5-eb2b-4f7f-8a01-a452a09ad8a2,DISK], DatanodeInfoWithStorage[127.0.0.1:35997,DS-bdf79375-56d3-4fe7-84a9-0074263958a5,DISK], DatanodeInfoWithStorage[127.0.0.1:44924,DS-c1be5ee1-e8ba-4539-b3f6-0007089039e6,DISK], DatanodeInfoWithStorage[127.0.0.1:40577,DS-63648408-8805-4360-b1b4-09009c363559,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1320265170-172.17.0.6-1597399890507:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46642,DS-c0aa97a6-6ed9-4c38-b86f-05e9dac63ed0,DISK], DatanodeInfoWithStorage[127.0.0.1:46558,DS-99615685-d379-4fb2-bf58-3198ec30df17,DISK], DatanodeInfoWithStorage[127.0.0.1:41106,DS-8c30f6eb-1d5b-497e-8f33-690754e196ef,DISK], DatanodeInfoWithStorage[127.0.0.1:43422,DS-97689fa9-1151-433e-aa0b-789867c3e6a4,DISK], DatanodeInfoWithStorage[127.0.0.1:33918,DS-800992e5-eb2b-4f7f-8a01-a452a09ad8a2,DISK], DatanodeInfoWithStorage[127.0.0.1:35997,DS-bdf79375-56d3-4fe7-84a9-0074263958a5,DISK], DatanodeInfoWithStorage[127.0.0.1:44924,DS-c1be5ee1-e8ba-4539-b3f6-0007089039e6,DISK], DatanodeInfoWithStorage[127.0.0.1:40577,DS-63648408-8805-4360-b1b4-09009c363559,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1158344305-172.17.0.6-1597400091696:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41657,DS-942b1d7a-bc49-42ef-9662-beaad48e47fe,DISK], DatanodeInfoWithStorage[127.0.0.1:38705,DS-6b45a582-c391-4f7c-8794-b9b5be2deb3f,DISK], DatanodeInfoWithStorage[127.0.0.1:34750,DS-72aa9f17-1f6c-4ba9-b8a5-974af1e9df0e,DISK], DatanodeInfoWithStorage[127.0.0.1:37647,DS-3d147bec-16ac-4e64-ab1a-21db759371c1,DISK], DatanodeInfoWithStorage[127.0.0.1:35697,DS-571b462f-a1a9-4946-84a0-483a3f684fb4,DISK], DatanodeInfoWithStorage[127.0.0.1:35204,DS-06746d4c-693a-42e3-8a64-dce3c134fbc4,DISK], DatanodeInfoWithStorage[127.0.0.1:45810,DS-3d97dd41-5e28-4055-9ca3-188863b081a8,DISK], DatanodeInfoWithStorage[127.0.0.1:43630,DS-79bb9d14-7902-4e58-9f86-8d36a415f63d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1158344305-172.17.0.6-1597400091696:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41657,DS-942b1d7a-bc49-42ef-9662-beaad48e47fe,DISK], DatanodeInfoWithStorage[127.0.0.1:38705,DS-6b45a582-c391-4f7c-8794-b9b5be2deb3f,DISK], DatanodeInfoWithStorage[127.0.0.1:34750,DS-72aa9f17-1f6c-4ba9-b8a5-974af1e9df0e,DISK], DatanodeInfoWithStorage[127.0.0.1:37647,DS-3d147bec-16ac-4e64-ab1a-21db759371c1,DISK], DatanodeInfoWithStorage[127.0.0.1:35697,DS-571b462f-a1a9-4946-84a0-483a3f684fb4,DISK], DatanodeInfoWithStorage[127.0.0.1:35204,DS-06746d4c-693a-42e3-8a64-dce3c134fbc4,DISK], DatanodeInfoWithStorage[127.0.0.1:45810,DS-3d97dd41-5e28-4055-9ca3-188863b081a8,DISK], DatanodeInfoWithStorage[127.0.0.1:43630,DS-79bb9d14-7902-4e58-9f86-8d36a415f63d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-306924996-172.17.0.6-1597400220010:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35089,DS-6ff38ad1-b3a0-4387-8eeb-383d8c662b2d,DISK], DatanodeInfoWithStorage[127.0.0.1:44626,DS-aef6eef3-3951-4dc3-bf1a-e0fea4088002,DISK], DatanodeInfoWithStorage[127.0.0.1:45950,DS-931ebbc5-1e5d-4d83-8401-df26e76f2d8f,DISK], DatanodeInfoWithStorage[127.0.0.1:35217,DS-8614da5f-1c11-48b9-a49d-d6793533c010,DISK], DatanodeInfoWithStorage[127.0.0.1:43998,DS-7ff17560-d6a1-471d-b296-d9ffac717248,DISK], DatanodeInfoWithStorage[127.0.0.1:41065,DS-b166c6b3-557f-4921-9468-663fde9e7b10,DISK], DatanodeInfoWithStorage[127.0.0.1:37194,DS-80e8ab46-bbc4-4124-a3cc-a6dfa200a98b,DISK], DatanodeInfoWithStorage[127.0.0.1:36635,DS-82137b30-d524-4460-8418-5ae8119cf442,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-306924996-172.17.0.6-1597400220010:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35089,DS-6ff38ad1-b3a0-4387-8eeb-383d8c662b2d,DISK], DatanodeInfoWithStorage[127.0.0.1:44626,DS-aef6eef3-3951-4dc3-bf1a-e0fea4088002,DISK], DatanodeInfoWithStorage[127.0.0.1:45950,DS-931ebbc5-1e5d-4d83-8401-df26e76f2d8f,DISK], DatanodeInfoWithStorage[127.0.0.1:35217,DS-8614da5f-1c11-48b9-a49d-d6793533c010,DISK], DatanodeInfoWithStorage[127.0.0.1:43998,DS-7ff17560-d6a1-471d-b296-d9ffac717248,DISK], DatanodeInfoWithStorage[127.0.0.1:41065,DS-b166c6b3-557f-4921-9468-663fde9e7b10,DISK], DatanodeInfoWithStorage[127.0.0.1:37194,DS-80e8ab46-bbc4-4124-a3cc-a6dfa200a98b,DISK], DatanodeInfoWithStorage[127.0.0.1:36635,DS-82137b30-d524-4460-8418-5ae8119cf442,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 1000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1644464764-172.17.0.6-1597400523022:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34837,DS-ac058185-5cd1-4e7a-a035-18a2826eccf7,DISK], DatanodeInfoWithStorage[127.0.0.1:33559,DS-fad08875-489c-44e8-8457-f5303235ee2b,DISK], DatanodeInfoWithStorage[127.0.0.1:39866,DS-87da83cd-6890-4cad-9724-e1874d6728d2,DISK], DatanodeInfoWithStorage[127.0.0.1:35459,DS-287def17-f169-42a0-8de8-59c6d9564b97,DISK], DatanodeInfoWithStorage[127.0.0.1:34477,DS-94eb89e5-6d29-4918-8eb4-b7bf957ab486,DISK], DatanodeInfoWithStorage[127.0.0.1:35388,DS-2805be80-48f4-40d4-ab52-318259774b11,DISK], DatanodeInfoWithStorage[127.0.0.1:34258,DS-8eaea023-e6c4-4618-a876-323111a7c05a,DISK], DatanodeInfoWithStorage[127.0.0.1:41880,DS-dde7063c-0d82-4b22-81de-ec1058eada56,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1644464764-172.17.0.6-1597400523022:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34837,DS-ac058185-5cd1-4e7a-a035-18a2826eccf7,DISK], DatanodeInfoWithStorage[127.0.0.1:33559,DS-fad08875-489c-44e8-8457-f5303235ee2b,DISK], DatanodeInfoWithStorage[127.0.0.1:39866,DS-87da83cd-6890-4cad-9724-e1874d6728d2,DISK], DatanodeInfoWithStorage[127.0.0.1:35459,DS-287def17-f169-42a0-8de8-59c6d9564b97,DISK], DatanodeInfoWithStorage[127.0.0.1:34477,DS-94eb89e5-6d29-4918-8eb4-b7bf957ab486,DISK], DatanodeInfoWithStorage[127.0.0.1:35388,DS-2805be80-48f4-40d4-ab52-318259774b11,DISK], DatanodeInfoWithStorage[127.0.0.1:34258,DS-8eaea023-e6c4-4618-a876-323111a7c05a,DISK], DatanodeInfoWithStorage[127.0.0.1:41880,DS-dde7063c-0d82-4b22-81de-ec1058eada56,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 14 out of 50
v1v1v2v2 failed with probability 21 out of 50
result: false positive !!!
Total execution time in seconds : 5707

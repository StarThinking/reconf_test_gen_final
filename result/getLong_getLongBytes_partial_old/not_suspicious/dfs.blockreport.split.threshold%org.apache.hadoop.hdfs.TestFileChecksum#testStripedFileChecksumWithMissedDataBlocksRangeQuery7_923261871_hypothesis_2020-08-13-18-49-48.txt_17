reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-333330273-172.17.0.10-1597344606723:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36945,DS-9186ec5f-4290-496f-8233-e40a1811cf3f,DISK], DatanodeInfoWithStorage[127.0.0.1:43105,DS-cc2595ba-ce6c-4b5c-aa11-181d54049a9b,DISK], DatanodeInfoWithStorage[127.0.0.1:44447,DS-a7efe97d-83ef-42f8-8bb0-b9f2505fb502,DISK], DatanodeInfoWithStorage[127.0.0.1:40399,DS-d93ddb0a-49ce-4138-add2-802f8f51e1d3,DISK], DatanodeInfoWithStorage[127.0.0.1:42893,DS-5d1073dd-f27a-4924-a01e-bdabdc7ac08d,DISK], DatanodeInfoWithStorage[127.0.0.1:45213,DS-3549024a-8a9a-489f-b08b-a496bcbbdcd9,DISK], DatanodeInfoWithStorage[127.0.0.1:41942,DS-50bc9741-567c-4256-a9ba-cbc1d429a932,DISK], DatanodeInfoWithStorage[127.0.0.1:41934,DS-44e2258c-ef1e-4247-a1c4-891bd7dce90e,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-333330273-172.17.0.10-1597344606723:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36945,DS-9186ec5f-4290-496f-8233-e40a1811cf3f,DISK], DatanodeInfoWithStorage[127.0.0.1:43105,DS-cc2595ba-ce6c-4b5c-aa11-181d54049a9b,DISK], DatanodeInfoWithStorage[127.0.0.1:44447,DS-a7efe97d-83ef-42f8-8bb0-b9f2505fb502,DISK], DatanodeInfoWithStorage[127.0.0.1:40399,DS-d93ddb0a-49ce-4138-add2-802f8f51e1d3,DISK], DatanodeInfoWithStorage[127.0.0.1:42893,DS-5d1073dd-f27a-4924-a01e-bdabdc7ac08d,DISK], DatanodeInfoWithStorage[127.0.0.1:45213,DS-3549024a-8a9a-489f-b08b-a496bcbbdcd9,DISK], DatanodeInfoWithStorage[127.0.0.1:41942,DS-50bc9741-567c-4256-a9ba-cbc1d429a932,DISK], DatanodeInfoWithStorage[127.0.0.1:41934,DS-44e2258c-ef1e-4247-a1c4-891bd7dce90e,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1663675502-172.17.0.10-1597345033712:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40949,DS-5f11ea0d-ff41-4bc1-ad51-3949c2b5c2cc,DISK], DatanodeInfoWithStorage[127.0.0.1:36426,DS-53e6ed1b-a379-4c53-9f4f-947d84a9c8b8,DISK], DatanodeInfoWithStorage[127.0.0.1:33665,DS-13260818-3549-48d2-b60b-5d14009d7a94,DISK], DatanodeInfoWithStorage[127.0.0.1:41729,DS-b58a1344-14a6-4f51-8897-caadcd30df03,DISK], DatanodeInfoWithStorage[127.0.0.1:39726,DS-a7992db6-f6ad-4636-9bb7-0e24cd06a2d5,DISK], DatanodeInfoWithStorage[127.0.0.1:45577,DS-0811bf06-285c-4c4b-8551-ab5bbf4cf0bb,DISK], DatanodeInfoWithStorage[127.0.0.1:42258,DS-6bbb312a-67d3-41b1-a03a-a839b69ac7b1,DISK], DatanodeInfoWithStorage[127.0.0.1:36560,DS-3f21e15e-baad-43b2-88c4-87e2fab69af8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1663675502-172.17.0.10-1597345033712:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40949,DS-5f11ea0d-ff41-4bc1-ad51-3949c2b5c2cc,DISK], DatanodeInfoWithStorage[127.0.0.1:36426,DS-53e6ed1b-a379-4c53-9f4f-947d84a9c8b8,DISK], DatanodeInfoWithStorage[127.0.0.1:33665,DS-13260818-3549-48d2-b60b-5d14009d7a94,DISK], DatanodeInfoWithStorage[127.0.0.1:41729,DS-b58a1344-14a6-4f51-8897-caadcd30df03,DISK], DatanodeInfoWithStorage[127.0.0.1:39726,DS-a7992db6-f6ad-4636-9bb7-0e24cd06a2d5,DISK], DatanodeInfoWithStorage[127.0.0.1:45577,DS-0811bf06-285c-4c4b-8551-ab5bbf4cf0bb,DISK], DatanodeInfoWithStorage[127.0.0.1:42258,DS-6bbb312a-67d3-41b1-a03a-a839b69ac7b1,DISK], DatanodeInfoWithStorage[127.0.0.1:36560,DS-3f21e15e-baad-43b2-88c4-87e2fab69af8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-543404396-172.17.0.10-1597345145927:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45207,DS-fbaae1d4-1387-411c-92aa-6ac4fe560f62,DISK], DatanodeInfoWithStorage[127.0.0.1:35454,DS-b24cb652-c2fb-4ab7-8ce5-078518fa82fc,DISK], DatanodeInfoWithStorage[127.0.0.1:35293,DS-b82438c2-e3ef-4acb-b22b-89a4408a2f0c,DISK], DatanodeInfoWithStorage[127.0.0.1:44571,DS-6c0b0021-9b1f-40bc-8772-c137bd0dc9f9,DISK], DatanodeInfoWithStorage[127.0.0.1:36760,DS-ceb0a778-efc9-4f1e-9146-a26124761238,DISK], DatanodeInfoWithStorage[127.0.0.1:40960,DS-b4800ff1-d8d4-4d4f-a465-995edf277eb6,DISK], DatanodeInfoWithStorage[127.0.0.1:39804,DS-a83bcd8d-bb19-42fb-b9cb-53f140bd6974,DISK], DatanodeInfoWithStorage[127.0.0.1:33472,DS-5093098d-e30d-48ef-9790-26502f108793,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-543404396-172.17.0.10-1597345145927:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45207,DS-fbaae1d4-1387-411c-92aa-6ac4fe560f62,DISK], DatanodeInfoWithStorage[127.0.0.1:35454,DS-b24cb652-c2fb-4ab7-8ce5-078518fa82fc,DISK], DatanodeInfoWithStorage[127.0.0.1:35293,DS-b82438c2-e3ef-4acb-b22b-89a4408a2f0c,DISK], DatanodeInfoWithStorage[127.0.0.1:44571,DS-6c0b0021-9b1f-40bc-8772-c137bd0dc9f9,DISK], DatanodeInfoWithStorage[127.0.0.1:36760,DS-ceb0a778-efc9-4f1e-9146-a26124761238,DISK], DatanodeInfoWithStorage[127.0.0.1:40960,DS-b4800ff1-d8d4-4d4f-a465-995edf277eb6,DISK], DatanodeInfoWithStorage[127.0.0.1:39804,DS-a83bcd8d-bb19-42fb-b9cb-53f140bd6974,DISK], DatanodeInfoWithStorage[127.0.0.1:33472,DS-5093098d-e30d-48ef-9790-26502f108793,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-204005457-172.17.0.10-1597345188714:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39216,DS-29d19c30-5740-410c-ae08-afb8fe77ca57,DISK], DatanodeInfoWithStorage[127.0.0.1:36362,DS-12a43a43-e068-4174-815f-40674ffd80d2,DISK], DatanodeInfoWithStorage[127.0.0.1:36678,DS-23c84072-9aa6-4efa-9826-7b21d0105b9c,DISK], DatanodeInfoWithStorage[127.0.0.1:43933,DS-efb5c84f-3003-4a4d-b233-737c17073256,DISK], DatanodeInfoWithStorage[127.0.0.1:42911,DS-0ca7dd33-ddec-48c6-877e-7650ced1ee40,DISK], DatanodeInfoWithStorage[127.0.0.1:39925,DS-cfdfa3d6-397b-4c07-a208-1f899c3c969d,DISK], DatanodeInfoWithStorage[127.0.0.1:33443,DS-0114178e-1e36-409e-a560-f4015ccac2ee,DISK], DatanodeInfoWithStorage[127.0.0.1:34735,DS-e99657e9-578b-403c-89c5-f9b1b606ac38,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-204005457-172.17.0.10-1597345188714:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39216,DS-29d19c30-5740-410c-ae08-afb8fe77ca57,DISK], DatanodeInfoWithStorage[127.0.0.1:36362,DS-12a43a43-e068-4174-815f-40674ffd80d2,DISK], DatanodeInfoWithStorage[127.0.0.1:36678,DS-23c84072-9aa6-4efa-9826-7b21d0105b9c,DISK], DatanodeInfoWithStorage[127.0.0.1:43933,DS-efb5c84f-3003-4a4d-b233-737c17073256,DISK], DatanodeInfoWithStorage[127.0.0.1:42911,DS-0ca7dd33-ddec-48c6-877e-7650ced1ee40,DISK], DatanodeInfoWithStorage[127.0.0.1:39925,DS-cfdfa3d6-397b-4c07-a208-1f899c3c969d,DISK], DatanodeInfoWithStorage[127.0.0.1:33443,DS-0114178e-1e36-409e-a560-f4015ccac2ee,DISK], DatanodeInfoWithStorage[127.0.0.1:34735,DS-e99657e9-578b-403c-89c5-f9b1b606ac38,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1540208674-172.17.0.10-1597345346683:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36015,DS-4148efce-73a9-4ec2-a41c-55bc370ed08a,DISK], DatanodeInfoWithStorage[127.0.0.1:43203,DS-bc80364d-99ff-44c7-b57b-8b6d206a1796,DISK], DatanodeInfoWithStorage[127.0.0.1:46662,DS-80f638a3-f301-4119-b5cf-df7a8718188c,DISK], DatanodeInfoWithStorage[127.0.0.1:46734,DS-2b4e084f-103f-4a9d-bd13-d7b086308d03,DISK], DatanodeInfoWithStorage[127.0.0.1:41372,DS-6afa4856-9c77-4f77-8c50-c980ba7f9bef,DISK], DatanodeInfoWithStorage[127.0.0.1:39145,DS-2b5775cc-9b69-4a0e-8f88-90f73056e222,DISK], DatanodeInfoWithStorage[127.0.0.1:45035,DS-242f6979-0bef-4545-bb74-8a6edd5efdad,DISK], DatanodeInfoWithStorage[127.0.0.1:45732,DS-0569bb68-5076-4fb5-8d9a-90f0064e2b18,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1540208674-172.17.0.10-1597345346683:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36015,DS-4148efce-73a9-4ec2-a41c-55bc370ed08a,DISK], DatanodeInfoWithStorage[127.0.0.1:43203,DS-bc80364d-99ff-44c7-b57b-8b6d206a1796,DISK], DatanodeInfoWithStorage[127.0.0.1:46662,DS-80f638a3-f301-4119-b5cf-df7a8718188c,DISK], DatanodeInfoWithStorage[127.0.0.1:46734,DS-2b4e084f-103f-4a9d-bd13-d7b086308d03,DISK], DatanodeInfoWithStorage[127.0.0.1:41372,DS-6afa4856-9c77-4f77-8c50-c980ba7f9bef,DISK], DatanodeInfoWithStorage[127.0.0.1:39145,DS-2b5775cc-9b69-4a0e-8f88-90f73056e222,DISK], DatanodeInfoWithStorage[127.0.0.1:45035,DS-242f6979-0bef-4545-bb74-8a6edd5efdad,DISK], DatanodeInfoWithStorage[127.0.0.1:45732,DS-0569bb68-5076-4fb5-8d9a-90f0064e2b18,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-296052597-172.17.0.10-1597345726555:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39979,DS-28e81fda-081f-4582-8ba9-9142ed93b1cb,DISK], DatanodeInfoWithStorage[127.0.0.1:41385,DS-fe7ef796-cb42-4189-a8bd-c06198696b47,DISK], DatanodeInfoWithStorage[127.0.0.1:39442,DS-aacdc6de-1bb7-440b-a8cf-1f436beb829b,DISK], DatanodeInfoWithStorage[127.0.0.1:36369,DS-cb7beb42-4478-41ec-9f4a-c3a3d7d2c845,DISK], DatanodeInfoWithStorage[127.0.0.1:40410,DS-30b34ade-02b3-477b-a412-8acf8f594e46,DISK], DatanodeInfoWithStorage[127.0.0.1:41623,DS-dcadaee8-e585-4ccb-987c-2c2b136e6898,DISK], DatanodeInfoWithStorage[127.0.0.1:46513,DS-022f9665-6c89-43b1-b5bc-dad2b0e83107,DISK], DatanodeInfoWithStorage[127.0.0.1:38298,DS-c166395c-bde5-46e0-9912-ea99b22885de,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-296052597-172.17.0.10-1597345726555:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39979,DS-28e81fda-081f-4582-8ba9-9142ed93b1cb,DISK], DatanodeInfoWithStorage[127.0.0.1:41385,DS-fe7ef796-cb42-4189-a8bd-c06198696b47,DISK], DatanodeInfoWithStorage[127.0.0.1:39442,DS-aacdc6de-1bb7-440b-a8cf-1f436beb829b,DISK], DatanodeInfoWithStorage[127.0.0.1:36369,DS-cb7beb42-4478-41ec-9f4a-c3a3d7d2c845,DISK], DatanodeInfoWithStorage[127.0.0.1:40410,DS-30b34ade-02b3-477b-a412-8acf8f594e46,DISK], DatanodeInfoWithStorage[127.0.0.1:41623,DS-dcadaee8-e585-4ccb-987c-2c2b136e6898,DISK], DatanodeInfoWithStorage[127.0.0.1:46513,DS-022f9665-6c89-43b1-b5bc-dad2b0e83107,DISK], DatanodeInfoWithStorage[127.0.0.1:38298,DS-c166395c-bde5-46e0-9912-ea99b22885de,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-531993948-172.17.0.10-1597345990755:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46508,DS-ef97346d-ac29-43f2-b8f0-c31bd13aa75e,DISK], DatanodeInfoWithStorage[127.0.0.1:44780,DS-faa0258b-4899-4424-bbd8-a6c8dd7f95bb,DISK], DatanodeInfoWithStorage[127.0.0.1:35896,DS-af087473-dfdb-421c-a156-b86e4a8c5760,DISK], DatanodeInfoWithStorage[127.0.0.1:44973,DS-1e30a278-b6e7-4877-9a1b-3a5400b0fb19,DISK], DatanodeInfoWithStorage[127.0.0.1:40474,DS-dc8e7ce2-9164-4c26-b3e2-2bbb684821c5,DISK], DatanodeInfoWithStorage[127.0.0.1:37981,DS-ff3ff410-f3aa-4228-b6e6-3702f8c199a9,DISK], DatanodeInfoWithStorage[127.0.0.1:40644,DS-4f546cd2-4036-49c7-8ecc-7fdd413cea62,DISK], DatanodeInfoWithStorage[127.0.0.1:34526,DS-dfc9dc3d-cfbf-43c2-8a64-c246581b23bb,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-531993948-172.17.0.10-1597345990755:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46508,DS-ef97346d-ac29-43f2-b8f0-c31bd13aa75e,DISK], DatanodeInfoWithStorage[127.0.0.1:44780,DS-faa0258b-4899-4424-bbd8-a6c8dd7f95bb,DISK], DatanodeInfoWithStorage[127.0.0.1:35896,DS-af087473-dfdb-421c-a156-b86e4a8c5760,DISK], DatanodeInfoWithStorage[127.0.0.1:44973,DS-1e30a278-b6e7-4877-9a1b-3a5400b0fb19,DISK], DatanodeInfoWithStorage[127.0.0.1:40474,DS-dc8e7ce2-9164-4c26-b3e2-2bbb684821c5,DISK], DatanodeInfoWithStorage[127.0.0.1:37981,DS-ff3ff410-f3aa-4228-b6e6-3702f8c199a9,DISK], DatanodeInfoWithStorage[127.0.0.1:40644,DS-4f546cd2-4036-49c7-8ecc-7fdd413cea62,DISK], DatanodeInfoWithStorage[127.0.0.1:34526,DS-dfc9dc3d-cfbf-43c2-8a64-c246581b23bb,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-928490277-172.17.0.10-1597346027608:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46613,DS-66f8e59a-4b1e-4b3d-8e1e-401b88122899,DISK], DatanodeInfoWithStorage[127.0.0.1:40769,DS-cff4b00a-f975-4d7b-a0f4-9c4a53834269,DISK], DatanodeInfoWithStorage[127.0.0.1:35762,DS-1b3bee3c-86f7-4c03-8a37-be14c8dab248,DISK], DatanodeInfoWithStorage[127.0.0.1:33494,DS-63f36c3b-0fda-4bb2-80da-4897fa187b97,DISK], DatanodeInfoWithStorage[127.0.0.1:36824,DS-7367d4d2-9ab6-4ceb-b741-03c6750e0f33,DISK], DatanodeInfoWithStorage[127.0.0.1:39726,DS-50f5dd48-5907-4106-bd8b-8b651cc672f3,DISK], DatanodeInfoWithStorage[127.0.0.1:46123,DS-f6acb8a6-1c18-4098-9445-f5b72f3adaef,DISK], DatanodeInfoWithStorage[127.0.0.1:39549,DS-93e7cd91-0bb0-4328-ba84-6782405e7c4e,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-928490277-172.17.0.10-1597346027608:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46613,DS-66f8e59a-4b1e-4b3d-8e1e-401b88122899,DISK], DatanodeInfoWithStorage[127.0.0.1:40769,DS-cff4b00a-f975-4d7b-a0f4-9c4a53834269,DISK], DatanodeInfoWithStorage[127.0.0.1:35762,DS-1b3bee3c-86f7-4c03-8a37-be14c8dab248,DISK], DatanodeInfoWithStorage[127.0.0.1:33494,DS-63f36c3b-0fda-4bb2-80da-4897fa187b97,DISK], DatanodeInfoWithStorage[127.0.0.1:36824,DS-7367d4d2-9ab6-4ceb-b741-03c6750e0f33,DISK], DatanodeInfoWithStorage[127.0.0.1:39726,DS-50f5dd48-5907-4106-bd8b-8b651cc672f3,DISK], DatanodeInfoWithStorage[127.0.0.1:46123,DS-f6acb8a6-1c18-4098-9445-f5b72f3adaef,DISK], DatanodeInfoWithStorage[127.0.0.1:39549,DS-93e7cd91-0bb0-4328-ba84-6782405e7c4e,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1953821521-172.17.0.10-1597347002224:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34162,DS-c00ff3a9-baaf-4fb8-9e54-98109198f61e,DISK], DatanodeInfoWithStorage[127.0.0.1:45679,DS-cb6364d3-d6b7-4c25-9340-56f9c65a80ce,DISK], DatanodeInfoWithStorage[127.0.0.1:43422,DS-a9e3cc9b-0dad-41c1-b151-968d011a0ed1,DISK], DatanodeInfoWithStorage[127.0.0.1:42740,DS-f1c43596-e961-4f36-a9e5-04c6b8f14d3e,DISK], DatanodeInfoWithStorage[127.0.0.1:46017,DS-ffabc447-febc-47ba-9c83-4df816e6bb89,DISK], DatanodeInfoWithStorage[127.0.0.1:40264,DS-91bd5d0a-c0f7-4c10-b67c-3cb8e932e17d,DISK], DatanodeInfoWithStorage[127.0.0.1:34099,DS-325dabf7-392b-4699-9fbe-2e6d02100c9d,DISK], DatanodeInfoWithStorage[127.0.0.1:43898,DS-1dc4ca5c-48cd-41a7-8280-467c4110d79b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1953821521-172.17.0.10-1597347002224:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34162,DS-c00ff3a9-baaf-4fb8-9e54-98109198f61e,DISK], DatanodeInfoWithStorage[127.0.0.1:45679,DS-cb6364d3-d6b7-4c25-9340-56f9c65a80ce,DISK], DatanodeInfoWithStorage[127.0.0.1:43422,DS-a9e3cc9b-0dad-41c1-b151-968d011a0ed1,DISK], DatanodeInfoWithStorage[127.0.0.1:42740,DS-f1c43596-e961-4f36-a9e5-04c6b8f14d3e,DISK], DatanodeInfoWithStorage[127.0.0.1:46017,DS-ffabc447-febc-47ba-9c83-4df816e6bb89,DISK], DatanodeInfoWithStorage[127.0.0.1:40264,DS-91bd5d0a-c0f7-4c10-b67c-3cb8e932e17d,DISK], DatanodeInfoWithStorage[127.0.0.1:34099,DS-325dabf7-392b-4699-9fbe-2e6d02100c9d,DISK], DatanodeInfoWithStorage[127.0.0.1:43898,DS-1dc4ca5c-48cd-41a7-8280-467c4110d79b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1626000554-172.17.0.10-1597347071355:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34206,DS-c46338ce-4d26-460d-ae3a-70ae3bd20185,DISK], DatanodeInfoWithStorage[127.0.0.1:46747,DS-f50eca45-4a05-4da2-b146-a1c5742bf1d6,DISK], DatanodeInfoWithStorage[127.0.0.1:44515,DS-0437d605-995d-4e12-ac64-8f1d78eaf94a,DISK], DatanodeInfoWithStorage[127.0.0.1:43792,DS-2e84cb58-22f9-478b-84e0-cc0fa262c54a,DISK], DatanodeInfoWithStorage[127.0.0.1:39501,DS-9697fa0f-d26a-4c3f-84a0-085b48553eb1,DISK], DatanodeInfoWithStorage[127.0.0.1:45834,DS-5d200b2b-8b62-4a0a-beaf-758101a1368a,DISK], DatanodeInfoWithStorage[127.0.0.1:42550,DS-acec6d67-f176-4ebc-be84-3769607d7cc8,DISK], DatanodeInfoWithStorage[127.0.0.1:44962,DS-8ac67ac9-78bb-40d4-a441-1c0786b732d1,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1626000554-172.17.0.10-1597347071355:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34206,DS-c46338ce-4d26-460d-ae3a-70ae3bd20185,DISK], DatanodeInfoWithStorage[127.0.0.1:46747,DS-f50eca45-4a05-4da2-b146-a1c5742bf1d6,DISK], DatanodeInfoWithStorage[127.0.0.1:44515,DS-0437d605-995d-4e12-ac64-8f1d78eaf94a,DISK], DatanodeInfoWithStorage[127.0.0.1:43792,DS-2e84cb58-22f9-478b-84e0-cc0fa262c54a,DISK], DatanodeInfoWithStorage[127.0.0.1:39501,DS-9697fa0f-d26a-4c3f-84a0-085b48553eb1,DISK], DatanodeInfoWithStorage[127.0.0.1:45834,DS-5d200b2b-8b62-4a0a-beaf-758101a1368a,DISK], DatanodeInfoWithStorage[127.0.0.1:42550,DS-acec6d67-f176-4ebc-be84-3769607d7cc8,DISK], DatanodeInfoWithStorage[127.0.0.1:44962,DS-8ac67ac9-78bb-40d4-a441-1c0786b732d1,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-329977058-172.17.0.10-1597347160174:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34260,DS-400406fc-5b85-4c50-b615-65ed25882e02,DISK], DatanodeInfoWithStorage[127.0.0.1:34729,DS-3590d878-150f-4c2f-b13c-439cb8354c53,DISK], DatanodeInfoWithStorage[127.0.0.1:44572,DS-4457e2eb-7c10-4250-8625-824d3e8f29f6,DISK], DatanodeInfoWithStorage[127.0.0.1:40965,DS-65a92f93-5e76-456a-ac8a-df0d9c57b3ac,DISK], DatanodeInfoWithStorage[127.0.0.1:39221,DS-1d836dfa-2e11-4668-a700-98b2fec85bb9,DISK], DatanodeInfoWithStorage[127.0.0.1:33350,DS-d71cc9f5-1506-4664-bb66-9f3137535589,DISK], DatanodeInfoWithStorage[127.0.0.1:46208,DS-2b7cca21-0a34-4a9e-bb3d-199a329ecdc7,DISK], DatanodeInfoWithStorage[127.0.0.1:45260,DS-59b4ef14-9990-4334-b5b7-1fe2a71686b0,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-329977058-172.17.0.10-1597347160174:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34260,DS-400406fc-5b85-4c50-b615-65ed25882e02,DISK], DatanodeInfoWithStorage[127.0.0.1:34729,DS-3590d878-150f-4c2f-b13c-439cb8354c53,DISK], DatanodeInfoWithStorage[127.0.0.1:44572,DS-4457e2eb-7c10-4250-8625-824d3e8f29f6,DISK], DatanodeInfoWithStorage[127.0.0.1:40965,DS-65a92f93-5e76-456a-ac8a-df0d9c57b3ac,DISK], DatanodeInfoWithStorage[127.0.0.1:39221,DS-1d836dfa-2e11-4668-a700-98b2fec85bb9,DISK], DatanodeInfoWithStorage[127.0.0.1:33350,DS-d71cc9f5-1506-4664-bb66-9f3137535589,DISK], DatanodeInfoWithStorage[127.0.0.1:46208,DS-2b7cca21-0a34-4a9e-bb3d-199a329ecdc7,DISK], DatanodeInfoWithStorage[127.0.0.1:45260,DS-59b4ef14-9990-4334-b5b7-1fe2a71686b0,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-945366295-172.17.0.10-1597347330239:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45326,DS-755a161d-e7d1-4519-9ad8-f38b0f2b2567,DISK], DatanodeInfoWithStorage[127.0.0.1:45778,DS-51905180-cf28-439b-bfaa-aa12dfdb3f12,DISK], DatanodeInfoWithStorage[127.0.0.1:38621,DS-f1961275-2c44-47c8-890c-788fec8514cc,DISK], DatanodeInfoWithStorage[127.0.0.1:34390,DS-8bf32a9b-be27-41a7-987f-33453eaa46ad,DISK], DatanodeInfoWithStorage[127.0.0.1:42518,DS-d54ddc1e-dbfb-4422-91e2-756dad690508,DISK], DatanodeInfoWithStorage[127.0.0.1:45378,DS-a140bd8d-a145-4660-a990-3a5db3cfad97,DISK], DatanodeInfoWithStorage[127.0.0.1:44593,DS-42840b13-eb66-408b-9801-403e25473107,DISK], DatanodeInfoWithStorage[127.0.0.1:41476,DS-3aa735f0-ff34-4397-978d-967559b35864,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-945366295-172.17.0.10-1597347330239:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45326,DS-755a161d-e7d1-4519-9ad8-f38b0f2b2567,DISK], DatanodeInfoWithStorage[127.0.0.1:45778,DS-51905180-cf28-439b-bfaa-aa12dfdb3f12,DISK], DatanodeInfoWithStorage[127.0.0.1:38621,DS-f1961275-2c44-47c8-890c-788fec8514cc,DISK], DatanodeInfoWithStorage[127.0.0.1:34390,DS-8bf32a9b-be27-41a7-987f-33453eaa46ad,DISK], DatanodeInfoWithStorage[127.0.0.1:42518,DS-d54ddc1e-dbfb-4422-91e2-756dad690508,DISK], DatanodeInfoWithStorage[127.0.0.1:45378,DS-a140bd8d-a145-4660-a990-3a5db3cfad97,DISK], DatanodeInfoWithStorage[127.0.0.1:44593,DS-42840b13-eb66-408b-9801-403e25473107,DISK], DatanodeInfoWithStorage[127.0.0.1:41476,DS-3aa735f0-ff34-4397-978d-967559b35864,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-626976377-172.17.0.10-1597347737979:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44310,DS-0c6370a8-b0c5-4fb6-871a-bb2262ed63dc,DISK], DatanodeInfoWithStorage[127.0.0.1:34198,DS-ae5c7bec-1d11-4fd2-bc79-a2416cdd0750,DISK], DatanodeInfoWithStorage[127.0.0.1:42577,DS-4f5d3094-9ee1-4a29-8f26-d287ce603536,DISK], DatanodeInfoWithStorage[127.0.0.1:35453,DS-1dc212c6-a1e8-417a-b48b-43f14d357647,DISK], DatanodeInfoWithStorage[127.0.0.1:39215,DS-afeaf6db-f76a-497f-8d0f-d5593a0493e3,DISK], DatanodeInfoWithStorage[127.0.0.1:38060,DS-c57a49df-118b-4578-85d4-fb0869b27aae,DISK], DatanodeInfoWithStorage[127.0.0.1:36062,DS-32a40ce9-0ad4-48ab-b9a6-9c90d50cba3e,DISK], DatanodeInfoWithStorage[127.0.0.1:39607,DS-90e4e552-0db5-491d-b664-76bad1d13b38,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-626976377-172.17.0.10-1597347737979:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44310,DS-0c6370a8-b0c5-4fb6-871a-bb2262ed63dc,DISK], DatanodeInfoWithStorage[127.0.0.1:34198,DS-ae5c7bec-1d11-4fd2-bc79-a2416cdd0750,DISK], DatanodeInfoWithStorage[127.0.0.1:42577,DS-4f5d3094-9ee1-4a29-8f26-d287ce603536,DISK], DatanodeInfoWithStorage[127.0.0.1:35453,DS-1dc212c6-a1e8-417a-b48b-43f14d357647,DISK], DatanodeInfoWithStorage[127.0.0.1:39215,DS-afeaf6db-f76a-497f-8d0f-d5593a0493e3,DISK], DatanodeInfoWithStorage[127.0.0.1:38060,DS-c57a49df-118b-4578-85d4-fb0869b27aae,DISK], DatanodeInfoWithStorage[127.0.0.1:36062,DS-32a40ce9-0ad4-48ab-b9a6-9c90d50cba3e,DISK], DatanodeInfoWithStorage[127.0.0.1:39607,DS-90e4e552-0db5-491d-b664-76bad1d13b38,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1701694053-172.17.0.10-1597347786167:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42665,DS-34e6e39d-829a-4046-978e-89d96cbc785e,DISK], DatanodeInfoWithStorage[127.0.0.1:43881,DS-63bcbf94-4a20-4505-b5a4-8e5641fd5dfc,DISK], DatanodeInfoWithStorage[127.0.0.1:46819,DS-dace432b-4a43-4638-a961-2af818b1bc58,DISK], DatanodeInfoWithStorage[127.0.0.1:46633,DS-ffcbdc72-0055-4378-92f6-8b08fbe8d8d8,DISK], DatanodeInfoWithStorage[127.0.0.1:45061,DS-a71fd869-7c5c-4da3-bb12-de517cacd44e,DISK], DatanodeInfoWithStorage[127.0.0.1:44407,DS-79e74752-af9d-44e7-9f49-6d91f9950bed,DISK], DatanodeInfoWithStorage[127.0.0.1:39214,DS-32b872da-d34d-4119-9d36-3b8a754260f2,DISK], DatanodeInfoWithStorage[127.0.0.1:32829,DS-2eb588d2-2e92-4c43-afad-27ccb496ef92,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1701694053-172.17.0.10-1597347786167:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42665,DS-34e6e39d-829a-4046-978e-89d96cbc785e,DISK], DatanodeInfoWithStorage[127.0.0.1:43881,DS-63bcbf94-4a20-4505-b5a4-8e5641fd5dfc,DISK], DatanodeInfoWithStorage[127.0.0.1:46819,DS-dace432b-4a43-4638-a961-2af818b1bc58,DISK], DatanodeInfoWithStorage[127.0.0.1:46633,DS-ffcbdc72-0055-4378-92f6-8b08fbe8d8d8,DISK], DatanodeInfoWithStorage[127.0.0.1:45061,DS-a71fd869-7c5c-4da3-bb12-de517cacd44e,DISK], DatanodeInfoWithStorage[127.0.0.1:44407,DS-79e74752-af9d-44e7-9f49-6d91f9950bed,DISK], DatanodeInfoWithStorage[127.0.0.1:39214,DS-32b872da-d34d-4119-9d36-3b8a754260f2,DISK], DatanodeInfoWithStorage[127.0.0.1:32829,DS-2eb588d2-2e92-4c43-afad-27ccb496ef92,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-528772132-172.17.0.10-1597347903365:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40974,DS-78f85eab-be21-46ab-8fd2-f39b4c0e7c4a,DISK], DatanodeInfoWithStorage[127.0.0.1:35096,DS-904c18f2-1014-491e-8b51-b242308f56e0,DISK], DatanodeInfoWithStorage[127.0.0.1:35095,DS-a13f81f4-63bf-4a06-aeae-4a8c6e952ad8,DISK], DatanodeInfoWithStorage[127.0.0.1:39746,DS-19f1b0f0-92a7-418c-aa4a-b450e7ee32e3,DISK], DatanodeInfoWithStorage[127.0.0.1:37284,DS-6fd8707b-8dc2-4f92-8155-d62da5a7e560,DISK], DatanodeInfoWithStorage[127.0.0.1:45519,DS-966c01cd-9b4b-4be5-b10f-1b691c8cb1f5,DISK], DatanodeInfoWithStorage[127.0.0.1:40236,DS-bc93f42a-4070-4344-a161-4c003d969384,DISK], DatanodeInfoWithStorage[127.0.0.1:41064,DS-580f8bdc-70c6-4f0d-9d84-5c4499354adc,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-528772132-172.17.0.10-1597347903365:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40974,DS-78f85eab-be21-46ab-8fd2-f39b4c0e7c4a,DISK], DatanodeInfoWithStorage[127.0.0.1:35096,DS-904c18f2-1014-491e-8b51-b242308f56e0,DISK], DatanodeInfoWithStorage[127.0.0.1:35095,DS-a13f81f4-63bf-4a06-aeae-4a8c6e952ad8,DISK], DatanodeInfoWithStorage[127.0.0.1:39746,DS-19f1b0f0-92a7-418c-aa4a-b450e7ee32e3,DISK], DatanodeInfoWithStorage[127.0.0.1:37284,DS-6fd8707b-8dc2-4f92-8155-d62da5a7e560,DISK], DatanodeInfoWithStorage[127.0.0.1:45519,DS-966c01cd-9b4b-4be5-b10f-1b691c8cb1f5,DISK], DatanodeInfoWithStorage[127.0.0.1:40236,DS-bc93f42a-4070-4344-a161-4c003d969384,DISK], DatanodeInfoWithStorage[127.0.0.1:41064,DS-580f8bdc-70c6-4f0d-9d84-5c4499354adc,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-524716437-172.17.0.10-1597348226870:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45085,DS-86dadec5-2e0b-4feb-b14c-8137b237af09,DISK], DatanodeInfoWithStorage[127.0.0.1:45123,DS-ee7be3e9-6ab8-47d2-99f9-8011af0f6677,DISK], DatanodeInfoWithStorage[127.0.0.1:45689,DS-784b234d-b288-4b38-a1f9-3a84e9048f52,DISK], DatanodeInfoWithStorage[127.0.0.1:44011,DS-e546799a-04b4-47a2-8288-420e27893eac,DISK], DatanodeInfoWithStorage[127.0.0.1:43141,DS-3a276d44-ce87-40bb-9c5a-f6913c8bd2c6,DISK], DatanodeInfoWithStorage[127.0.0.1:39257,DS-be493594-5840-491d-8511-31c301bb827f,DISK], DatanodeInfoWithStorage[127.0.0.1:40179,DS-935e43b8-959f-479e-8951-5395a8e05342,DISK], DatanodeInfoWithStorage[127.0.0.1:34535,DS-34e113a6-0f5b-474b-a8c2-20aae41c837c,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-524716437-172.17.0.10-1597348226870:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45085,DS-86dadec5-2e0b-4feb-b14c-8137b237af09,DISK], DatanodeInfoWithStorage[127.0.0.1:45123,DS-ee7be3e9-6ab8-47d2-99f9-8011af0f6677,DISK], DatanodeInfoWithStorage[127.0.0.1:45689,DS-784b234d-b288-4b38-a1f9-3a84e9048f52,DISK], DatanodeInfoWithStorage[127.0.0.1:44011,DS-e546799a-04b4-47a2-8288-420e27893eac,DISK], DatanodeInfoWithStorage[127.0.0.1:43141,DS-3a276d44-ce87-40bb-9c5a-f6913c8bd2c6,DISK], DatanodeInfoWithStorage[127.0.0.1:39257,DS-be493594-5840-491d-8511-31c301bb827f,DISK], DatanodeInfoWithStorage[127.0.0.1:40179,DS-935e43b8-959f-479e-8951-5395a8e05342,DISK], DatanodeInfoWithStorage[127.0.0.1:34535,DS-34e113a6-0f5b-474b-a8c2-20aae41c837c,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-567723775-172.17.0.10-1597348310571:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42594,DS-90da9711-530b-4f16-ab0f-1e70c0573bc1,DISK], DatanodeInfoWithStorage[127.0.0.1:38013,DS-a340d360-9e51-44c0-9a63-54a61eb1326c,DISK], DatanodeInfoWithStorage[127.0.0.1:40690,DS-11f07806-9213-43f3-b84d-c248f27baf43,DISK], DatanodeInfoWithStorage[127.0.0.1:42962,DS-077cd791-972f-4ad2-8b00-6906f02bc76d,DISK], DatanodeInfoWithStorage[127.0.0.1:42548,DS-68117e8a-4425-474b-8dd1-1d415c719a7c,DISK], DatanodeInfoWithStorage[127.0.0.1:32908,DS-1f1f05f9-e779-4101-bce4-5becc40f27f5,DISK], DatanodeInfoWithStorage[127.0.0.1:39748,DS-601fe9be-6c3a-41b6-bc99-64baa9215f4f,DISK], DatanodeInfoWithStorage[127.0.0.1:44464,DS-70a0750e-678c-4c8a-9a31-6ea7dbe5ddfb,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-567723775-172.17.0.10-1597348310571:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42594,DS-90da9711-530b-4f16-ab0f-1e70c0573bc1,DISK], DatanodeInfoWithStorage[127.0.0.1:38013,DS-a340d360-9e51-44c0-9a63-54a61eb1326c,DISK], DatanodeInfoWithStorage[127.0.0.1:40690,DS-11f07806-9213-43f3-b84d-c248f27baf43,DISK], DatanodeInfoWithStorage[127.0.0.1:42962,DS-077cd791-972f-4ad2-8b00-6906f02bc76d,DISK], DatanodeInfoWithStorage[127.0.0.1:42548,DS-68117e8a-4425-474b-8dd1-1d415c719a7c,DISK], DatanodeInfoWithStorage[127.0.0.1:32908,DS-1f1f05f9-e779-4101-bce4-5becc40f27f5,DISK], DatanodeInfoWithStorage[127.0.0.1:39748,DS-601fe9be-6c3a-41b6-bc99-64baa9215f4f,DISK], DatanodeInfoWithStorage[127.0.0.1:44464,DS-70a0750e-678c-4c8a-9a31-6ea7dbe5ddfb,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-812228391-172.17.0.10-1597348393045:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35765,DS-86a004e8-0006-4f5e-a2c5-2f06accd476e,DISK], DatanodeInfoWithStorage[127.0.0.1:32893,DS-0f0a080f-a0f9-4c3e-93d9-f3386cfc92a1,DISK], DatanodeInfoWithStorage[127.0.0.1:43879,DS-60a233c0-a45b-486b-9d30-cd62a4f36356,DISK], DatanodeInfoWithStorage[127.0.0.1:34394,DS-31726d33-2af0-4fcf-b305-ff4111adb891,DISK], DatanodeInfoWithStorage[127.0.0.1:41416,DS-7e979a8a-5b3f-47cf-b37e-3fc4f35918c4,DISK], DatanodeInfoWithStorage[127.0.0.1:41041,DS-1f33fc5c-737f-4793-bd61-3d44c8d4d7ee,DISK], DatanodeInfoWithStorage[127.0.0.1:36768,DS-fdbd5394-4ffb-430a-a8a9-2bb040311f17,DISK], DatanodeInfoWithStorage[127.0.0.1:36824,DS-3d3776ca-2bea-4923-aea0-89fb4f570ea5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-812228391-172.17.0.10-1597348393045:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35765,DS-86a004e8-0006-4f5e-a2c5-2f06accd476e,DISK], DatanodeInfoWithStorage[127.0.0.1:32893,DS-0f0a080f-a0f9-4c3e-93d9-f3386cfc92a1,DISK], DatanodeInfoWithStorage[127.0.0.1:43879,DS-60a233c0-a45b-486b-9d30-cd62a4f36356,DISK], DatanodeInfoWithStorage[127.0.0.1:34394,DS-31726d33-2af0-4fcf-b305-ff4111adb891,DISK], DatanodeInfoWithStorage[127.0.0.1:41416,DS-7e979a8a-5b3f-47cf-b37e-3fc4f35918c4,DISK], DatanodeInfoWithStorage[127.0.0.1:41041,DS-1f33fc5c-737f-4793-bd61-3d44c8d4d7ee,DISK], DatanodeInfoWithStorage[127.0.0.1:36768,DS-fdbd5394-4ffb-430a-a8a9-2bb040311f17,DISK], DatanodeInfoWithStorage[127.0.0.1:36824,DS-3d3776ca-2bea-4923-aea0-89fb4f570ea5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1964726738-172.17.0.10-1597348810887:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39942,DS-86d369fe-0fbf-4d73-ba5f-448b94e26551,DISK], DatanodeInfoWithStorage[127.0.0.1:33469,DS-9e38b80d-c4e8-4416-877b-61a67f4557a3,DISK], DatanodeInfoWithStorage[127.0.0.1:41224,DS-ed2ed730-3c61-4f62-9185-9c625f51da5e,DISK], DatanodeInfoWithStorage[127.0.0.1:41879,DS-9b845fa8-5d3e-4deb-9049-3cbfe6834d47,DISK], DatanodeInfoWithStorage[127.0.0.1:42155,DS-0bb225fb-03c1-4545-a55b-5f9e05b6460b,DISK], DatanodeInfoWithStorage[127.0.0.1:43393,DS-c3564c2d-e328-469b-a6a0-3b9089277493,DISK], DatanodeInfoWithStorage[127.0.0.1:40009,DS-77d170f7-9035-4dd3-95ed-6b0a458b39c1,DISK], DatanodeInfoWithStorage[127.0.0.1:39216,DS-955b7c17-f85d-4fe3-aea9-80445c30a9d7,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1964726738-172.17.0.10-1597348810887:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39942,DS-86d369fe-0fbf-4d73-ba5f-448b94e26551,DISK], DatanodeInfoWithStorage[127.0.0.1:33469,DS-9e38b80d-c4e8-4416-877b-61a67f4557a3,DISK], DatanodeInfoWithStorage[127.0.0.1:41224,DS-ed2ed730-3c61-4f62-9185-9c625f51da5e,DISK], DatanodeInfoWithStorage[127.0.0.1:41879,DS-9b845fa8-5d3e-4deb-9049-3cbfe6834d47,DISK], DatanodeInfoWithStorage[127.0.0.1:42155,DS-0bb225fb-03c1-4545-a55b-5f9e05b6460b,DISK], DatanodeInfoWithStorage[127.0.0.1:43393,DS-c3564c2d-e328-469b-a6a0-3b9089277493,DISK], DatanodeInfoWithStorage[127.0.0.1:40009,DS-77d170f7-9035-4dd3-95ed-6b0a458b39c1,DISK], DatanodeInfoWithStorage[127.0.0.1:39216,DS-955b7c17-f85d-4fe3-aea9-80445c30a9d7,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1786202402-172.17.0.10-1597348968827:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46109,DS-da3a50e9-5023-43b2-8afb-76446b3cd80b,DISK], DatanodeInfoWithStorage[127.0.0.1:39563,DS-eaff391d-8459-44c0-9845-b30a5afec413,DISK], DatanodeInfoWithStorage[127.0.0.1:37373,DS-33b106d4-7e90-4fc6-8360-afe7d5579718,DISK], DatanodeInfoWithStorage[127.0.0.1:38082,DS-cbd63370-2bcb-4272-b968-e50ab03cdcb4,DISK], DatanodeInfoWithStorage[127.0.0.1:44744,DS-5b1a700f-a925-4df9-ba9c-a481cc5c4340,DISK], DatanodeInfoWithStorage[127.0.0.1:36586,DS-bcf460f7-921d-4ebd-abad-d2b52743af57,DISK], DatanodeInfoWithStorage[127.0.0.1:37609,DS-a6a1a4be-7cbb-42bb-9817-def56e3e3e1d,DISK], DatanodeInfoWithStorage[127.0.0.1:33211,DS-f95ac5e8-f16f-42c6-b5c6-37505b9a7440,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1786202402-172.17.0.10-1597348968827:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46109,DS-da3a50e9-5023-43b2-8afb-76446b3cd80b,DISK], DatanodeInfoWithStorage[127.0.0.1:39563,DS-eaff391d-8459-44c0-9845-b30a5afec413,DISK], DatanodeInfoWithStorage[127.0.0.1:37373,DS-33b106d4-7e90-4fc6-8360-afe7d5579718,DISK], DatanodeInfoWithStorage[127.0.0.1:38082,DS-cbd63370-2bcb-4272-b968-e50ab03cdcb4,DISK], DatanodeInfoWithStorage[127.0.0.1:44744,DS-5b1a700f-a925-4df9-ba9c-a481cc5c4340,DISK], DatanodeInfoWithStorage[127.0.0.1:36586,DS-bcf460f7-921d-4ebd-abad-d2b52743af57,DISK], DatanodeInfoWithStorage[127.0.0.1:37609,DS-a6a1a4be-7cbb-42bb-9817-def56e3e3e1d,DISK], DatanodeInfoWithStorage[127.0.0.1:33211,DS-f95ac5e8-f16f-42c6-b5c6-37505b9a7440,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-632690339-172.17.0.10-1597349278027:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41055,DS-af1ea3a2-fc6b-4e10-90fa-819d6a23cdfa,DISK], DatanodeInfoWithStorage[127.0.0.1:39283,DS-23c3dd8a-e0c0-4c53-a5e1-d0f5f0deb923,DISK], DatanodeInfoWithStorage[127.0.0.1:42534,DS-a747f40a-f123-43ac-a281-8482046ffbac,DISK], DatanodeInfoWithStorage[127.0.0.1:42584,DS-f509fdc0-8d4c-47d1-be19-23c4b72cd8e0,DISK], DatanodeInfoWithStorage[127.0.0.1:42648,DS-d671a60b-96b2-49d2-b286-3e328104bbe2,DISK], DatanodeInfoWithStorage[127.0.0.1:42191,DS-57cab64c-b8d8-4237-bed2-1d0bd6ea2226,DISK], DatanodeInfoWithStorage[127.0.0.1:43446,DS-0da4eebc-6de0-4642-9ba1-3ccfa03e0cd0,DISK], DatanodeInfoWithStorage[127.0.0.1:43633,DS-a6add026-0777-47a5-9c66-77d42e8d1e94,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-632690339-172.17.0.10-1597349278027:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41055,DS-af1ea3a2-fc6b-4e10-90fa-819d6a23cdfa,DISK], DatanodeInfoWithStorage[127.0.0.1:39283,DS-23c3dd8a-e0c0-4c53-a5e1-d0f5f0deb923,DISK], DatanodeInfoWithStorage[127.0.0.1:42534,DS-a747f40a-f123-43ac-a281-8482046ffbac,DISK], DatanodeInfoWithStorage[127.0.0.1:42584,DS-f509fdc0-8d4c-47d1-be19-23c4b72cd8e0,DISK], DatanodeInfoWithStorage[127.0.0.1:42648,DS-d671a60b-96b2-49d2-b286-3e328104bbe2,DISK], DatanodeInfoWithStorage[127.0.0.1:42191,DS-57cab64c-b8d8-4237-bed2-1d0bd6ea2226,DISK], DatanodeInfoWithStorage[127.0.0.1:43446,DS-0da4eebc-6de0-4642-9ba1-3ccfa03e0cd0,DISK], DatanodeInfoWithStorage[127.0.0.1:43633,DS-a6add026-0777-47a5-9c66-77d42e8d1e94,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1445754237-172.17.0.10-1597349313887:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36306,DS-047b471a-30ca-476a-a97d-d1f11e7c0c4e,DISK], DatanodeInfoWithStorage[127.0.0.1:41189,DS-b290722d-50c2-4ef9-950c-ffe02197cf62,DISK], DatanodeInfoWithStorage[127.0.0.1:36073,DS-1d0681f5-4860-43fc-813a-ebe8907e6144,DISK], DatanodeInfoWithStorage[127.0.0.1:40572,DS-1ec491e5-a20f-4510-89aa-fd931e41be58,DISK], DatanodeInfoWithStorage[127.0.0.1:42655,DS-ab6065ed-5ab8-4187-a163-f297e5ace619,DISK], DatanodeInfoWithStorage[127.0.0.1:40936,DS-ae54d08f-f1fe-4777-a287-2a7c721881e1,DISK], DatanodeInfoWithStorage[127.0.0.1:40078,DS-d5d41c00-2e4f-4bc5-a3ef-48b9ce38642b,DISK], DatanodeInfoWithStorage[127.0.0.1:45873,DS-16077b99-3ff1-4041-adeb-04effdeeb3fb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1445754237-172.17.0.10-1597349313887:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36306,DS-047b471a-30ca-476a-a97d-d1f11e7c0c4e,DISK], DatanodeInfoWithStorage[127.0.0.1:41189,DS-b290722d-50c2-4ef9-950c-ffe02197cf62,DISK], DatanodeInfoWithStorage[127.0.0.1:36073,DS-1d0681f5-4860-43fc-813a-ebe8907e6144,DISK], DatanodeInfoWithStorage[127.0.0.1:40572,DS-1ec491e5-a20f-4510-89aa-fd931e41be58,DISK], DatanodeInfoWithStorage[127.0.0.1:42655,DS-ab6065ed-5ab8-4187-a163-f297e5ace619,DISK], DatanodeInfoWithStorage[127.0.0.1:40936,DS-ae54d08f-f1fe-4777-a287-2a7c721881e1,DISK], DatanodeInfoWithStorage[127.0.0.1:40078,DS-d5d41c00-2e4f-4bc5-a3ef-48b9ce38642b,DISK], DatanodeInfoWithStorage[127.0.0.1:45873,DS-16077b99-3ff1-4041-adeb-04effdeeb3fb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1532484975-172.17.0.10-1597349677265:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46816,DS-e8426387-01c8-4077-83fb-212d2edf34e7,DISK], DatanodeInfoWithStorage[127.0.0.1:41941,DS-b7cbf384-c290-4529-b52d-3a8db5aa7a1a,DISK], DatanodeInfoWithStorage[127.0.0.1:37837,DS-96ad38dd-3f91-4754-a58c-c818c1cc2d0c,DISK], DatanodeInfoWithStorage[127.0.0.1:35264,DS-ced88de5-588f-411a-82e3-cda4d1320be3,DISK], DatanodeInfoWithStorage[127.0.0.1:33290,DS-b241aec7-2af7-4b0a-96c2-5a9ad59f4ecd,DISK], DatanodeInfoWithStorage[127.0.0.1:40342,DS-b6f501ed-8983-44a7-9336-560762f793f9,DISK], DatanodeInfoWithStorage[127.0.0.1:46747,DS-4bb8de1c-e7f3-42a0-8036-8bf3f3bc0a79,DISK], DatanodeInfoWithStorage[127.0.0.1:43955,DS-f619b40b-ed11-4f4f-8524-f5ffaccd8ce4,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1532484975-172.17.0.10-1597349677265:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46816,DS-e8426387-01c8-4077-83fb-212d2edf34e7,DISK], DatanodeInfoWithStorage[127.0.0.1:41941,DS-b7cbf384-c290-4529-b52d-3a8db5aa7a1a,DISK], DatanodeInfoWithStorage[127.0.0.1:37837,DS-96ad38dd-3f91-4754-a58c-c818c1cc2d0c,DISK], DatanodeInfoWithStorage[127.0.0.1:35264,DS-ced88de5-588f-411a-82e3-cda4d1320be3,DISK], DatanodeInfoWithStorage[127.0.0.1:33290,DS-b241aec7-2af7-4b0a-96c2-5a9ad59f4ecd,DISK], DatanodeInfoWithStorage[127.0.0.1:40342,DS-b6f501ed-8983-44a7-9336-560762f793f9,DISK], DatanodeInfoWithStorage[127.0.0.1:46747,DS-4bb8de1c-e7f3-42a0-8036-8bf3f3bc0a79,DISK], DatanodeInfoWithStorage[127.0.0.1:43955,DS-f619b40b-ed11-4f4f-8524-f5ffaccd8ce4,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-94568308-172.17.0.10-1597349930688:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39576,DS-3d8c3ca1-1154-489a-a217-b7cd5aa61c18,DISK], DatanodeInfoWithStorage[127.0.0.1:41374,DS-0c83542b-ac35-4aa6-b993-24787fcfc14c,DISK], DatanodeInfoWithStorage[127.0.0.1:38484,DS-46bc045f-2548-4580-9418-7cbf21e47c67,DISK], DatanodeInfoWithStorage[127.0.0.1:33189,DS-b7bbfd2e-1a49-4c48-924d-2bdcd8a30a68,DISK], DatanodeInfoWithStorage[127.0.0.1:45109,DS-1d717d88-9617-4f73-98ef-560ebdad716e,DISK], DatanodeInfoWithStorage[127.0.0.1:44024,DS-8c395529-e7a9-413e-bf12-7e96a50a822c,DISK], DatanodeInfoWithStorage[127.0.0.1:33061,DS-bd9229e7-a393-4c47-bca3-812767902e54,DISK], DatanodeInfoWithStorage[127.0.0.1:36599,DS-ae990e20-0399-4632-82c2-633aea2c77e8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-94568308-172.17.0.10-1597349930688:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39576,DS-3d8c3ca1-1154-489a-a217-b7cd5aa61c18,DISK], DatanodeInfoWithStorage[127.0.0.1:41374,DS-0c83542b-ac35-4aa6-b993-24787fcfc14c,DISK], DatanodeInfoWithStorage[127.0.0.1:38484,DS-46bc045f-2548-4580-9418-7cbf21e47c67,DISK], DatanodeInfoWithStorage[127.0.0.1:33189,DS-b7bbfd2e-1a49-4c48-924d-2bdcd8a30a68,DISK], DatanodeInfoWithStorage[127.0.0.1:45109,DS-1d717d88-9617-4f73-98ef-560ebdad716e,DISK], DatanodeInfoWithStorage[127.0.0.1:44024,DS-8c395529-e7a9-413e-bf12-7e96a50a822c,DISK], DatanodeInfoWithStorage[127.0.0.1:33061,DS-bd9229e7-a393-4c47-bca3-812767902e54,DISK], DatanodeInfoWithStorage[127.0.0.1:36599,DS-ae990e20-0399-4632-82c2-633aea2c77e8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2033351168-172.17.0.10-1597350017794:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43799,DS-74b24879-9c93-4044-8889-37285439132c,DISK], DatanodeInfoWithStorage[127.0.0.1:36749,DS-a26c0a9c-ddb1-41de-8581-1acc2c7d83ef,DISK], DatanodeInfoWithStorage[127.0.0.1:38374,DS-55ba7c4b-3363-4a49-919d-e79b132b5228,DISK], DatanodeInfoWithStorage[127.0.0.1:39625,DS-d7a28999-6751-43b2-a408-98a3113d956e,DISK], DatanodeInfoWithStorage[127.0.0.1:35168,DS-749b5b28-b95f-4511-be06-cedcf0d1fcc7,DISK], DatanodeInfoWithStorage[127.0.0.1:41364,DS-1f063dfb-317d-4259-8f9f-fe58496011b2,DISK], DatanodeInfoWithStorage[127.0.0.1:37934,DS-82ad9314-113d-42eb-ac05-fcc27129a425,DISK], DatanodeInfoWithStorage[127.0.0.1:35245,DS-a5e162c4-39c8-46d6-a329-b6c14356a127,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2033351168-172.17.0.10-1597350017794:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43799,DS-74b24879-9c93-4044-8889-37285439132c,DISK], DatanodeInfoWithStorage[127.0.0.1:36749,DS-a26c0a9c-ddb1-41de-8581-1acc2c7d83ef,DISK], DatanodeInfoWithStorage[127.0.0.1:38374,DS-55ba7c4b-3363-4a49-919d-e79b132b5228,DISK], DatanodeInfoWithStorage[127.0.0.1:39625,DS-d7a28999-6751-43b2-a408-98a3113d956e,DISK], DatanodeInfoWithStorage[127.0.0.1:35168,DS-749b5b28-b95f-4511-be06-cedcf0d1fcc7,DISK], DatanodeInfoWithStorage[127.0.0.1:41364,DS-1f063dfb-317d-4259-8f9f-fe58496011b2,DISK], DatanodeInfoWithStorage[127.0.0.1:37934,DS-82ad9314-113d-42eb-ac05-fcc27129a425,DISK], DatanodeInfoWithStorage[127.0.0.1:35245,DS-a5e162c4-39c8-46d6-a329-b6c14356a127,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1214952899-172.17.0.10-1597350127796:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36060,DS-7de69288-ffd2-4900-a780-35ebae7f6618,DISK], DatanodeInfoWithStorage[127.0.0.1:39859,DS-64ab5cef-4d48-434f-aee5-e86cae40795f,DISK], DatanodeInfoWithStorage[127.0.0.1:35178,DS-d3cb79ce-4625-49a5-a5e7-e2d1916cf9cb,DISK], DatanodeInfoWithStorage[127.0.0.1:34608,DS-35e27a80-97fc-453d-8017-d9e42f751f8f,DISK], DatanodeInfoWithStorage[127.0.0.1:46674,DS-50c36572-a5e1-4165-af87-c86c4ed81b8d,DISK], DatanodeInfoWithStorage[127.0.0.1:45734,DS-cf6ffd16-3d56-41a3-9a16-136f16c65d65,DISK], DatanodeInfoWithStorage[127.0.0.1:41225,DS-2cb2d581-2734-4bab-a21c-1fe2044603bd,DISK], DatanodeInfoWithStorage[127.0.0.1:41810,DS-ae4bee55-19df-4e90-8fd5-629e67602e60,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1214952899-172.17.0.10-1597350127796:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36060,DS-7de69288-ffd2-4900-a780-35ebae7f6618,DISK], DatanodeInfoWithStorage[127.0.0.1:39859,DS-64ab5cef-4d48-434f-aee5-e86cae40795f,DISK], DatanodeInfoWithStorage[127.0.0.1:35178,DS-d3cb79ce-4625-49a5-a5e7-e2d1916cf9cb,DISK], DatanodeInfoWithStorage[127.0.0.1:34608,DS-35e27a80-97fc-453d-8017-d9e42f751f8f,DISK], DatanodeInfoWithStorage[127.0.0.1:46674,DS-50c36572-a5e1-4165-af87-c86c4ed81b8d,DISK], DatanodeInfoWithStorage[127.0.0.1:45734,DS-cf6ffd16-3d56-41a3-9a16-136f16c65d65,DISK], DatanodeInfoWithStorage[127.0.0.1:41225,DS-2cb2d581-2734-4bab-a21c-1fe2044603bd,DISK], DatanodeInfoWithStorage[127.0.0.1:41810,DS-ae4bee55-19df-4e90-8fd5-629e67602e60,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1542814263-172.17.0.10-1597350160654:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38706,DS-bf66b15f-5d31-4193-8f7e-6fd320cdc6a3,DISK], DatanodeInfoWithStorage[127.0.0.1:39377,DS-ddd47332-61e2-42ae-bded-70dc96e5ff0a,DISK], DatanodeInfoWithStorage[127.0.0.1:39070,DS-df18cf16-a784-427e-923f-33560fa165be,DISK], DatanodeInfoWithStorage[127.0.0.1:35912,DS-a4c608c8-5484-4a2c-8402-f6afbb84104a,DISK], DatanodeInfoWithStorage[127.0.0.1:34354,DS-b6d3c4ab-fa93-4929-aea0-b8db266a78fc,DISK], DatanodeInfoWithStorage[127.0.0.1:41820,DS-55036e06-9261-4ef7-b485-8086e273418d,DISK], DatanodeInfoWithStorage[127.0.0.1:44046,DS-733fdd7e-eaa7-41cb-9d20-260bcaa2ad24,DISK], DatanodeInfoWithStorage[127.0.0.1:38491,DS-b7c518c0-990d-4e2a-bcae-f55eddc7f907,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1542814263-172.17.0.10-1597350160654:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38706,DS-bf66b15f-5d31-4193-8f7e-6fd320cdc6a3,DISK], DatanodeInfoWithStorage[127.0.0.1:39377,DS-ddd47332-61e2-42ae-bded-70dc96e5ff0a,DISK], DatanodeInfoWithStorage[127.0.0.1:39070,DS-df18cf16-a784-427e-923f-33560fa165be,DISK], DatanodeInfoWithStorage[127.0.0.1:35912,DS-a4c608c8-5484-4a2c-8402-f6afbb84104a,DISK], DatanodeInfoWithStorage[127.0.0.1:34354,DS-b6d3c4ab-fa93-4929-aea0-b8db266a78fc,DISK], DatanodeInfoWithStorage[127.0.0.1:41820,DS-55036e06-9261-4ef7-b485-8086e273418d,DISK], DatanodeInfoWithStorage[127.0.0.1:44046,DS-733fdd7e-eaa7-41cb-9d20-260bcaa2ad24,DISK], DatanodeInfoWithStorage[127.0.0.1:38491,DS-b7c518c0-990d-4e2a-bcae-f55eddc7f907,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.blockreport.split.threshold
component: hdfs:DataNode
v1: 1000000
v2: 100
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-114266915-172.17.0.10-1597350362861:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46858,DS-239e10ee-086f-4b70-8194-e8256ed124c3,DISK], DatanodeInfoWithStorage[127.0.0.1:42898,DS-3984be6a-24ea-4f21-aff3-bd0c1ba19e6c,DISK], DatanodeInfoWithStorage[127.0.0.1:40062,DS-3fae2b10-02df-48f4-adc4-64141b3cedfa,DISK], DatanodeInfoWithStorage[127.0.0.1:44437,DS-89472e9f-048a-4b73-9639-69ac89d66ba9,DISK], DatanodeInfoWithStorage[127.0.0.1:43750,DS-6984c399-ef96-400c-816d-8cd0bf230bad,DISK], DatanodeInfoWithStorage[127.0.0.1:37003,DS-f4bf84d4-c4a6-4b39-87e5-ad80d669446f,DISK], DatanodeInfoWithStorage[127.0.0.1:36253,DS-35952cd4-969c-4dae-b4e0-0f902e93ee20,DISK], DatanodeInfoWithStorage[127.0.0.1:45652,DS-b6b8f17a-7624-40aa-b4d9-ce5e670df010,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-114266915-172.17.0.10-1597350362861:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46858,DS-239e10ee-086f-4b70-8194-e8256ed124c3,DISK], DatanodeInfoWithStorage[127.0.0.1:42898,DS-3984be6a-24ea-4f21-aff3-bd0c1ba19e6c,DISK], DatanodeInfoWithStorage[127.0.0.1:40062,DS-3fae2b10-02df-48f4-adc4-64141b3cedfa,DISK], DatanodeInfoWithStorage[127.0.0.1:44437,DS-89472e9f-048a-4b73-9639-69ac89d66ba9,DISK], DatanodeInfoWithStorage[127.0.0.1:43750,DS-6984c399-ef96-400c-816d-8cd0bf230bad,DISK], DatanodeInfoWithStorage[127.0.0.1:37003,DS-f4bf84d4-c4a6-4b39-87e5-ad80d669446f,DISK], DatanodeInfoWithStorage[127.0.0.1:36253,DS-35952cd4-969c-4dae-b4e0-0f902e93ee20,DISK], DatanodeInfoWithStorage[127.0.0.1:45652,DS-b6b8f17a-7624-40aa-b4d9-ce5e670df010,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 12 out of 50
v1v1v2v2 failed with probability 14 out of 50
result: false positive !!!
Total execution time in seconds : 5914

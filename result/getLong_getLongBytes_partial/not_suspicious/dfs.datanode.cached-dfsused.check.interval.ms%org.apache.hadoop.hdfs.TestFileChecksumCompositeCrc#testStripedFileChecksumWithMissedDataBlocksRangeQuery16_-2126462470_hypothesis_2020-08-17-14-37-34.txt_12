reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-868551933-172.17.0.10-1597675069421:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46848,DS-498896cf-3f16-4a2e-86e1-2974099927ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37070,DS-a56d1825-3f3f-4932-bd15-bdfec5770caf,DISK], DatanodeInfoWithStorage[127.0.0.1:42005,DS-fbd7782c-104e-45c8-b102-e7ab9de59227,DISK], DatanodeInfoWithStorage[127.0.0.1:40303,DS-d5575e65-1e91-44bc-9b3a-7d9b7f881fb4,DISK], DatanodeInfoWithStorage[127.0.0.1:40170,DS-c4a13c73-2424-4aa2-9375-4a8517256190,DISK], DatanodeInfoWithStorage[127.0.0.1:39933,DS-059c565a-7073-4948-a41a-6fffc5adda82,DISK], DatanodeInfoWithStorage[127.0.0.1:41734,DS-a9482bd3-434c-40cf-bc2a-af71a61d70a6,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-12800e5e-37d8-4a22-903f-858a2bdf87d9,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-868551933-172.17.0.10-1597675069421:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46848,DS-498896cf-3f16-4a2e-86e1-2974099927ef,DISK], DatanodeInfoWithStorage[127.0.0.1:37070,DS-a56d1825-3f3f-4932-bd15-bdfec5770caf,DISK], DatanodeInfoWithStorage[127.0.0.1:42005,DS-fbd7782c-104e-45c8-b102-e7ab9de59227,DISK], DatanodeInfoWithStorage[127.0.0.1:40303,DS-d5575e65-1e91-44bc-9b3a-7d9b7f881fb4,DISK], DatanodeInfoWithStorage[127.0.0.1:40170,DS-c4a13c73-2424-4aa2-9375-4a8517256190,DISK], DatanodeInfoWithStorage[127.0.0.1:39933,DS-059c565a-7073-4948-a41a-6fffc5adda82,DISK], DatanodeInfoWithStorage[127.0.0.1:41734,DS-a9482bd3-434c-40cf-bc2a-af71a61d70a6,DISK], DatanodeInfoWithStorage[127.0.0.1:40990,DS-12800e5e-37d8-4a22-903f-858a2bdf87d9,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-797904333-172.17.0.10-1597675611676:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33584,DS-feee024c-6b37-4786-b931-ae7f833c3e2e,DISK], DatanodeInfoWithStorage[127.0.0.1:38960,DS-802f41a2-268a-4939-abbf-62fe7f5a41a6,DISK], DatanodeInfoWithStorage[127.0.0.1:39848,DS-a4171e4a-544d-4b89-a59c-108e6cd16c24,DISK], DatanodeInfoWithStorage[127.0.0.1:44812,DS-62c37c00-3e2b-4da5-ae3a-b84de14d5b1f,DISK], DatanodeInfoWithStorage[127.0.0.1:42222,DS-579ee705-9153-408f-a8a3-8ae016ddcc84,DISK], DatanodeInfoWithStorage[127.0.0.1:40251,DS-b6f68334-a639-4af3-a51c-c962d5d48412,DISK], DatanodeInfoWithStorage[127.0.0.1:41252,DS-2c08c280-f726-47fa-8f3b-69906a541a14,DISK], DatanodeInfoWithStorage[127.0.0.1:33271,DS-e3f8e1ce-7fd2-4caa-9e9c-d13dd619a156,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-797904333-172.17.0.10-1597675611676:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33584,DS-feee024c-6b37-4786-b931-ae7f833c3e2e,DISK], DatanodeInfoWithStorage[127.0.0.1:38960,DS-802f41a2-268a-4939-abbf-62fe7f5a41a6,DISK], DatanodeInfoWithStorage[127.0.0.1:39848,DS-a4171e4a-544d-4b89-a59c-108e6cd16c24,DISK], DatanodeInfoWithStorage[127.0.0.1:44812,DS-62c37c00-3e2b-4da5-ae3a-b84de14d5b1f,DISK], DatanodeInfoWithStorage[127.0.0.1:42222,DS-579ee705-9153-408f-a8a3-8ae016ddcc84,DISK], DatanodeInfoWithStorage[127.0.0.1:40251,DS-b6f68334-a639-4af3-a51c-c962d5d48412,DISK], DatanodeInfoWithStorage[127.0.0.1:41252,DS-2c08c280-f726-47fa-8f3b-69906a541a14,DISK], DatanodeInfoWithStorage[127.0.0.1:33271,DS-e3f8e1ce-7fd2-4caa-9e9c-d13dd619a156,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-483912497-172.17.0.10-1597676030670:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39990,DS-0c5ae055-8c7e-48ac-816c-dd683d6c5a37,DISK], DatanodeInfoWithStorage[127.0.0.1:46846,DS-0b6e7251-1e0a-473f-9dbe-e71a5cc8584f,DISK], DatanodeInfoWithStorage[127.0.0.1:35358,DS-2c76d895-a408-4577-b8e2-6e429f87ac79,DISK], DatanodeInfoWithStorage[127.0.0.1:39836,DS-51ca8629-949c-4e76-939b-b0e539a2de97,DISK], DatanodeInfoWithStorage[127.0.0.1:34676,DS-7f46110f-b75f-4632-99f5-0102e942b2c4,DISK], DatanodeInfoWithStorage[127.0.0.1:37694,DS-f96b04dd-6ea6-43ad-aed3-3adf02744329,DISK], DatanodeInfoWithStorage[127.0.0.1:40278,DS-d3c71075-0b27-4438-99fc-b489c84848af,DISK], DatanodeInfoWithStorage[127.0.0.1:38620,DS-324d5038-aaa7-496c-ac8b-d8fe5a00e69e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-483912497-172.17.0.10-1597676030670:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39990,DS-0c5ae055-8c7e-48ac-816c-dd683d6c5a37,DISK], DatanodeInfoWithStorage[127.0.0.1:46846,DS-0b6e7251-1e0a-473f-9dbe-e71a5cc8584f,DISK], DatanodeInfoWithStorage[127.0.0.1:35358,DS-2c76d895-a408-4577-b8e2-6e429f87ac79,DISK], DatanodeInfoWithStorage[127.0.0.1:39836,DS-51ca8629-949c-4e76-939b-b0e539a2de97,DISK], DatanodeInfoWithStorage[127.0.0.1:34676,DS-7f46110f-b75f-4632-99f5-0102e942b2c4,DISK], DatanodeInfoWithStorage[127.0.0.1:37694,DS-f96b04dd-6ea6-43ad-aed3-3adf02744329,DISK], DatanodeInfoWithStorage[127.0.0.1:40278,DS-d3c71075-0b27-4438-99fc-b489c84848af,DISK], DatanodeInfoWithStorage[127.0.0.1:38620,DS-324d5038-aaa7-496c-ac8b-d8fe5a00e69e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1156815962-172.17.0.10-1597676073010:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34271,DS-33c4d714-a8d5-4068-b512-0909598bb4bb,DISK], DatanodeInfoWithStorage[127.0.0.1:39933,DS-fc953f83-d292-4528-a22e-233a22a8cb11,DISK], DatanodeInfoWithStorage[127.0.0.1:32768,DS-8bb51bae-efea-47bb-93a4-a87c89028ed2,DISK], DatanodeInfoWithStorage[127.0.0.1:43532,DS-4e1c18c7-65ac-4ea2-8c8e-c5fd0939cf97,DISK], DatanodeInfoWithStorage[127.0.0.1:37220,DS-47183034-c282-4737-b022-c6a1017ae190,DISK], DatanodeInfoWithStorage[127.0.0.1:39233,DS-6aaac63e-771f-4c19-889a-66f913d19f0d,DISK], DatanodeInfoWithStorage[127.0.0.1:43548,DS-26d7b282-0b43-4945-ab42-0634dfb5fe53,DISK], DatanodeInfoWithStorage[127.0.0.1:39040,DS-33f3d386-7d1e-4a28-b7a7-4499155d8b50,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1156815962-172.17.0.10-1597676073010:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34271,DS-33c4d714-a8d5-4068-b512-0909598bb4bb,DISK], DatanodeInfoWithStorage[127.0.0.1:39933,DS-fc953f83-d292-4528-a22e-233a22a8cb11,DISK], DatanodeInfoWithStorage[127.0.0.1:32768,DS-8bb51bae-efea-47bb-93a4-a87c89028ed2,DISK], DatanodeInfoWithStorage[127.0.0.1:43532,DS-4e1c18c7-65ac-4ea2-8c8e-c5fd0939cf97,DISK], DatanodeInfoWithStorage[127.0.0.1:37220,DS-47183034-c282-4737-b022-c6a1017ae190,DISK], DatanodeInfoWithStorage[127.0.0.1:39233,DS-6aaac63e-771f-4c19-889a-66f913d19f0d,DISK], DatanodeInfoWithStorage[127.0.0.1:43548,DS-26d7b282-0b43-4945-ab42-0634dfb5fe53,DISK], DatanodeInfoWithStorage[127.0.0.1:39040,DS-33f3d386-7d1e-4a28-b7a7-4499155d8b50,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1780124856-172.17.0.10-1597676436390:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37217,DS-4b0edb1b-1c6b-4cf0-8207-63d37eebe985,DISK], DatanodeInfoWithStorage[127.0.0.1:34849,DS-56dac240-b8c9-45d4-ae82-a12af86314b1,DISK], DatanodeInfoWithStorage[127.0.0.1:32991,DS-79db0ddf-cd88-447f-aad9-e4c238b5b3b2,DISK], DatanodeInfoWithStorage[127.0.0.1:45806,DS-b12668c1-6884-450d-8d33-b444030ea940,DISK], DatanodeInfoWithStorage[127.0.0.1:40282,DS-f2c24131-8d0d-4d9a-b2b5-b3a98cec555b,DISK], DatanodeInfoWithStorage[127.0.0.1:44512,DS-f5a55e72-f5ae-44c2-aeed-37973a99e39f,DISK], DatanodeInfoWithStorage[127.0.0.1:44460,DS-ae0f4273-71a8-4bcf-8647-d1ad06878818,DISK], DatanodeInfoWithStorage[127.0.0.1:37093,DS-166beb74-b399-4f1e-b5a0-185a88243379,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1780124856-172.17.0.10-1597676436390:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37217,DS-4b0edb1b-1c6b-4cf0-8207-63d37eebe985,DISK], DatanodeInfoWithStorage[127.0.0.1:34849,DS-56dac240-b8c9-45d4-ae82-a12af86314b1,DISK], DatanodeInfoWithStorage[127.0.0.1:32991,DS-79db0ddf-cd88-447f-aad9-e4c238b5b3b2,DISK], DatanodeInfoWithStorage[127.0.0.1:45806,DS-b12668c1-6884-450d-8d33-b444030ea940,DISK], DatanodeInfoWithStorage[127.0.0.1:40282,DS-f2c24131-8d0d-4d9a-b2b5-b3a98cec555b,DISK], DatanodeInfoWithStorage[127.0.0.1:44512,DS-f5a55e72-f5ae-44c2-aeed-37973a99e39f,DISK], DatanodeInfoWithStorage[127.0.0.1:44460,DS-ae0f4273-71a8-4bcf-8647-d1ad06878818,DISK], DatanodeInfoWithStorage[127.0.0.1:37093,DS-166beb74-b399-4f1e-b5a0-185a88243379,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1289829388-172.17.0.10-1597676526371:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33286,DS-63ba9671-59a8-4dcb-8819-eb7021e55df4,DISK], DatanodeInfoWithStorage[127.0.0.1:37245,DS-0f158a92-5f01-421b-ba05-a2c7d54d24f4,DISK], DatanodeInfoWithStorage[127.0.0.1:44788,DS-1dd81a67-e28e-4549-b549-d7e67fb0c924,DISK], DatanodeInfoWithStorage[127.0.0.1:46168,DS-ea2a359e-5f10-4325-a538-315a6d134fc8,DISK], DatanodeInfoWithStorage[127.0.0.1:46621,DS-232eac50-b6ec-44ef-b56f-651700ca52a2,DISK], DatanodeInfoWithStorage[127.0.0.1:45584,DS-08f0d9a8-7709-4c75-b676-58c9ce4ac7a9,DISK], DatanodeInfoWithStorage[127.0.0.1:34983,DS-ef25e47a-3b13-4bc2-b7fe-318c9fb1d773,DISK], DatanodeInfoWithStorage[127.0.0.1:44452,DS-13d780b0-ec13-4ee1-ab1a-4671d7eb2652,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1289829388-172.17.0.10-1597676526371:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33286,DS-63ba9671-59a8-4dcb-8819-eb7021e55df4,DISK], DatanodeInfoWithStorage[127.0.0.1:37245,DS-0f158a92-5f01-421b-ba05-a2c7d54d24f4,DISK], DatanodeInfoWithStorage[127.0.0.1:44788,DS-1dd81a67-e28e-4549-b549-d7e67fb0c924,DISK], DatanodeInfoWithStorage[127.0.0.1:46168,DS-ea2a359e-5f10-4325-a538-315a6d134fc8,DISK], DatanodeInfoWithStorage[127.0.0.1:46621,DS-232eac50-b6ec-44ef-b56f-651700ca52a2,DISK], DatanodeInfoWithStorage[127.0.0.1:45584,DS-08f0d9a8-7709-4c75-b676-58c9ce4ac7a9,DISK], DatanodeInfoWithStorage[127.0.0.1:34983,DS-ef25e47a-3b13-4bc2-b7fe-318c9fb1d773,DISK], DatanodeInfoWithStorage[127.0.0.1:44452,DS-13d780b0-ec13-4ee1-ab1a-4671d7eb2652,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-632139804-172.17.0.10-1597676657932:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35474,DS-4ee37844-00cb-4430-94dc-78128dbd5e49,DISK], DatanodeInfoWithStorage[127.0.0.1:42639,DS-5dd892ff-397d-4fc6-ac80-506cc343f055,DISK], DatanodeInfoWithStorage[127.0.0.1:44788,DS-01b59c95-cf1a-4308-b7e9-023278f9156d,DISK], DatanodeInfoWithStorage[127.0.0.1:46517,DS-dd0f69fc-fd83-47db-8d76-af95ad86a008,DISK], DatanodeInfoWithStorage[127.0.0.1:38962,DS-bec69d90-ac58-4a48-ba80-dcc56c7c321f,DISK], DatanodeInfoWithStorage[127.0.0.1:41249,DS-0c02bb0f-cfa8-45f8-bd0e-b01fbabdbd09,DISK], DatanodeInfoWithStorage[127.0.0.1:36016,DS-a5e560d6-b00a-4f9d-941f-93cc9af71882,DISK], DatanodeInfoWithStorage[127.0.0.1:44025,DS-25c2db2d-f5de-4518-99c6-65f989be905c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-632139804-172.17.0.10-1597676657932:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35474,DS-4ee37844-00cb-4430-94dc-78128dbd5e49,DISK], DatanodeInfoWithStorage[127.0.0.1:42639,DS-5dd892ff-397d-4fc6-ac80-506cc343f055,DISK], DatanodeInfoWithStorage[127.0.0.1:44788,DS-01b59c95-cf1a-4308-b7e9-023278f9156d,DISK], DatanodeInfoWithStorage[127.0.0.1:46517,DS-dd0f69fc-fd83-47db-8d76-af95ad86a008,DISK], DatanodeInfoWithStorage[127.0.0.1:38962,DS-bec69d90-ac58-4a48-ba80-dcc56c7c321f,DISK], DatanodeInfoWithStorage[127.0.0.1:41249,DS-0c02bb0f-cfa8-45f8-bd0e-b01fbabdbd09,DISK], DatanodeInfoWithStorage[127.0.0.1:36016,DS-a5e560d6-b00a-4f9d-941f-93cc9af71882,DISK], DatanodeInfoWithStorage[127.0.0.1:44025,DS-25c2db2d-f5de-4518-99c6-65f989be905c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-367782265-172.17.0.10-1597676890059:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32987,DS-64e87dcb-c66c-4221-93c0-bf2ff695f5bf,DISK], DatanodeInfoWithStorage[127.0.0.1:33984,DS-4551ed1c-9147-46c6-b755-8d5929fd2c9c,DISK], DatanodeInfoWithStorage[127.0.0.1:46871,DS-ee092dad-dc9c-4725-b132-e8d58712a1a9,DISK], DatanodeInfoWithStorage[127.0.0.1:41292,DS-dc7bd72c-c617-4208-8fd2-ce94b44e5886,DISK], DatanodeInfoWithStorage[127.0.0.1:44234,DS-df1ac8de-67a6-4e25-8b43-a1346dc60119,DISK], DatanodeInfoWithStorage[127.0.0.1:35913,DS-7abfb118-771e-4e23-8cd0-b9cca24fc4b4,DISK], DatanodeInfoWithStorage[127.0.0.1:36285,DS-a428b50d-b244-4222-b1e2-3a512b94d630,DISK], DatanodeInfoWithStorage[127.0.0.1:41605,DS-fe8b7224-7e89-4e43-910e-9e3f1263acf7,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-367782265-172.17.0.10-1597676890059:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32987,DS-64e87dcb-c66c-4221-93c0-bf2ff695f5bf,DISK], DatanodeInfoWithStorage[127.0.0.1:33984,DS-4551ed1c-9147-46c6-b755-8d5929fd2c9c,DISK], DatanodeInfoWithStorage[127.0.0.1:46871,DS-ee092dad-dc9c-4725-b132-e8d58712a1a9,DISK], DatanodeInfoWithStorage[127.0.0.1:41292,DS-dc7bd72c-c617-4208-8fd2-ce94b44e5886,DISK], DatanodeInfoWithStorage[127.0.0.1:44234,DS-df1ac8de-67a6-4e25-8b43-a1346dc60119,DISK], DatanodeInfoWithStorage[127.0.0.1:35913,DS-7abfb118-771e-4e23-8cd0-b9cca24fc4b4,DISK], DatanodeInfoWithStorage[127.0.0.1:36285,DS-a428b50d-b244-4222-b1e2-3a512b94d630,DISK], DatanodeInfoWithStorage[127.0.0.1:41605,DS-fe8b7224-7e89-4e43-910e-9e3f1263acf7,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-925950176-172.17.0.10-1597676942364:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41904,DS-6ce7c930-c5d2-44e6-81d2-6e9e2bb0801c,DISK], DatanodeInfoWithStorage[127.0.0.1:38918,DS-4000ff4d-1fb9-4d90-a6d5-c4da4b1a7b5a,DISK], DatanodeInfoWithStorage[127.0.0.1:32888,DS-42a54149-4dbd-4572-9ae4-67d9272b9eff,DISK], DatanodeInfoWithStorage[127.0.0.1:45971,DS-1c43aaa4-616f-4068-8075-5f281217d15d,DISK], DatanodeInfoWithStorage[127.0.0.1:39071,DS-7800c715-f4fe-4304-ae89-113f5a54e58b,DISK], DatanodeInfoWithStorage[127.0.0.1:34606,DS-f57c9bdb-a660-4e08-b64d-f1483ab21d82,DISK], DatanodeInfoWithStorage[127.0.0.1:39557,DS-c92486a0-438d-4250-9084-ab69b3e1c7a2,DISK], DatanodeInfoWithStorage[127.0.0.1:35898,DS-a087f576-6f0f-4411-b486-e17054dc796f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-925950176-172.17.0.10-1597676942364:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41904,DS-6ce7c930-c5d2-44e6-81d2-6e9e2bb0801c,DISK], DatanodeInfoWithStorage[127.0.0.1:38918,DS-4000ff4d-1fb9-4d90-a6d5-c4da4b1a7b5a,DISK], DatanodeInfoWithStorage[127.0.0.1:32888,DS-42a54149-4dbd-4572-9ae4-67d9272b9eff,DISK], DatanodeInfoWithStorage[127.0.0.1:45971,DS-1c43aaa4-616f-4068-8075-5f281217d15d,DISK], DatanodeInfoWithStorage[127.0.0.1:39071,DS-7800c715-f4fe-4304-ae89-113f5a54e58b,DISK], DatanodeInfoWithStorage[127.0.0.1:34606,DS-f57c9bdb-a660-4e08-b64d-f1483ab21d82,DISK], DatanodeInfoWithStorage[127.0.0.1:39557,DS-c92486a0-438d-4250-9084-ab69b3e1c7a2,DISK], DatanodeInfoWithStorage[127.0.0.1:35898,DS-a087f576-6f0f-4411-b486-e17054dc796f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-778963089-172.17.0.10-1597678313032:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37723,DS-99622388-6479-4c1b-9fa7-c13eff4a1263,DISK], DatanodeInfoWithStorage[127.0.0.1:42488,DS-7b7fbbd1-3a58-417f-8370-cfe367aea19b,DISK], DatanodeInfoWithStorage[127.0.0.1:35943,DS-7001a15f-a866-4e8e-8e8c-41ac0a7467b1,DISK], DatanodeInfoWithStorage[127.0.0.1:38016,DS-608ec96f-0fb9-49d7-865d-8a83cc9d68fc,DISK], DatanodeInfoWithStorage[127.0.0.1:36009,DS-9b8a942e-ef96-4cf4-a396-e5268215b8ea,DISK], DatanodeInfoWithStorage[127.0.0.1:45631,DS-a1073cf4-1548-4101-bac6-d43b6899f95d,DISK], DatanodeInfoWithStorage[127.0.0.1:35855,DS-f6817ae7-3c14-49c7-8af3-8fd3ae844684,DISK], DatanodeInfoWithStorage[127.0.0.1:40968,DS-89a5f979-73d4-4d13-8cac-a80ac321b659,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-778963089-172.17.0.10-1597678313032:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37723,DS-99622388-6479-4c1b-9fa7-c13eff4a1263,DISK], DatanodeInfoWithStorage[127.0.0.1:42488,DS-7b7fbbd1-3a58-417f-8370-cfe367aea19b,DISK], DatanodeInfoWithStorage[127.0.0.1:35943,DS-7001a15f-a866-4e8e-8e8c-41ac0a7467b1,DISK], DatanodeInfoWithStorage[127.0.0.1:38016,DS-608ec96f-0fb9-49d7-865d-8a83cc9d68fc,DISK], DatanodeInfoWithStorage[127.0.0.1:36009,DS-9b8a942e-ef96-4cf4-a396-e5268215b8ea,DISK], DatanodeInfoWithStorage[127.0.0.1:45631,DS-a1073cf4-1548-4101-bac6-d43b6899f95d,DISK], DatanodeInfoWithStorage[127.0.0.1:35855,DS-f6817ae7-3c14-49c7-8af3-8fd3ae844684,DISK], DatanodeInfoWithStorage[127.0.0.1:40968,DS-89a5f979-73d4-4d13-8cac-a80ac321b659,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1791721123-172.17.0.10-1597679171185:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42514,DS-db735f71-ad04-4ef6-abab-a6cc56141776,DISK], DatanodeInfoWithStorage[127.0.0.1:37456,DS-0013ffef-e0b5-495a-9150-55f8d762137a,DISK], DatanodeInfoWithStorage[127.0.0.1:34761,DS-fbeab7d1-744a-496e-a180-fb760ab8fd41,DISK], DatanodeInfoWithStorage[127.0.0.1:33215,DS-019feb4f-23e4-41da-88a8-c13f178e6d8b,DISK], DatanodeInfoWithStorage[127.0.0.1:41599,DS-f9003d39-7b77-4fc2-b448-09acad308643,DISK], DatanodeInfoWithStorage[127.0.0.1:38948,DS-5352bb26-6b8a-415c-b54e-0bf436140d09,DISK], DatanodeInfoWithStorage[127.0.0.1:37747,DS-0ec3c533-6f52-421d-9bc3-80e8b8051389,DISK], DatanodeInfoWithStorage[127.0.0.1:36688,DS-b6d0b33f-26d8-4f8b-a26b-0d94f0a42dcf,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1791721123-172.17.0.10-1597679171185:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42514,DS-db735f71-ad04-4ef6-abab-a6cc56141776,DISK], DatanodeInfoWithStorage[127.0.0.1:37456,DS-0013ffef-e0b5-495a-9150-55f8d762137a,DISK], DatanodeInfoWithStorage[127.0.0.1:34761,DS-fbeab7d1-744a-496e-a180-fb760ab8fd41,DISK], DatanodeInfoWithStorage[127.0.0.1:33215,DS-019feb4f-23e4-41da-88a8-c13f178e6d8b,DISK], DatanodeInfoWithStorage[127.0.0.1:41599,DS-f9003d39-7b77-4fc2-b448-09acad308643,DISK], DatanodeInfoWithStorage[127.0.0.1:38948,DS-5352bb26-6b8a-415c-b54e-0bf436140d09,DISK], DatanodeInfoWithStorage[127.0.0.1:37747,DS-0ec3c533-6f52-421d-9bc3-80e8b8051389,DISK], DatanodeInfoWithStorage[127.0.0.1:36688,DS-b6d0b33f-26d8-4f8b-a26b-0d94f0a42dcf,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1267593249-172.17.0.10-1597679894074:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44401,DS-4f0dfbb1-5cfc-4ee5-838b-31d557b2269e,DISK], DatanodeInfoWithStorage[127.0.0.1:46666,DS-db3b29d2-7844-460f-83d5-3410f9be1c22,DISK], DatanodeInfoWithStorage[127.0.0.1:46044,DS-5e2ca75c-c077-47e8-ae2b-9dea35f28fb9,DISK], DatanodeInfoWithStorage[127.0.0.1:36831,DS-861feed7-1c1b-4ff2-8879-6bba9ba68332,DISK], DatanodeInfoWithStorage[127.0.0.1:38757,DS-a2ed8e4d-29ac-47bf-a456-a4bf4e6a4560,DISK], DatanodeInfoWithStorage[127.0.0.1:34970,DS-33ed04f5-7104-407f-95b2-53a86090b82d,DISK], DatanodeInfoWithStorage[127.0.0.1:37078,DS-7f7288fc-4ec6-40da-8592-6f7b1ee52d3b,DISK], DatanodeInfoWithStorage[127.0.0.1:44485,DS-90cd481b-204a-42c4-93ab-2312272aae9c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1267593249-172.17.0.10-1597679894074:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44401,DS-4f0dfbb1-5cfc-4ee5-838b-31d557b2269e,DISK], DatanodeInfoWithStorage[127.0.0.1:46666,DS-db3b29d2-7844-460f-83d5-3410f9be1c22,DISK], DatanodeInfoWithStorage[127.0.0.1:46044,DS-5e2ca75c-c077-47e8-ae2b-9dea35f28fb9,DISK], DatanodeInfoWithStorage[127.0.0.1:36831,DS-861feed7-1c1b-4ff2-8879-6bba9ba68332,DISK], DatanodeInfoWithStorage[127.0.0.1:38757,DS-a2ed8e4d-29ac-47bf-a456-a4bf4e6a4560,DISK], DatanodeInfoWithStorage[127.0.0.1:34970,DS-33ed04f5-7104-407f-95b2-53a86090b82d,DISK], DatanodeInfoWithStorage[127.0.0.1:37078,DS-7f7288fc-4ec6-40da-8592-6f7b1ee52d3b,DISK], DatanodeInfoWithStorage[127.0.0.1:44485,DS-90cd481b-204a-42c4-93ab-2312272aae9c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.cached-dfsused.check.interval.ms
component: hdfs:DataNode
v1: 60
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery16
reconfPoint: -2
result: -1
failureMessage: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1759310994-172.17.0.10-1597681413145:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39588,DS-9e61a247-d0e5-4911-ac5e-f5194a61e89c,DISK], DatanodeInfoWithStorage[127.0.0.1:43908,DS-11727838-90d6-4733-a4f1-c3303b9de980,DISK], DatanodeInfoWithStorage[127.0.0.1:34002,DS-d52f570e-8fa0-47e7-93d4-62c003e2af43,DISK], DatanodeInfoWithStorage[127.0.0.1:39251,DS-72d651b5-9764-4ede-8b46-fba7326ac74f,DISK], DatanodeInfoWithStorage[127.0.0.1:43483,DS-49d59574-d6d9-4548-8df9-df0a07e38ddc,DISK], DatanodeInfoWithStorage[127.0.0.1:40780,DS-448c05dd-4f3f-4f2a-9c64-7359877f8393,DISK], DatanodeInfoWithStorage[127.0.0.1:37375,DS-52377a13-5bde-42c4-8050-b902c6fc8362,DISK], DatanodeInfoWithStorage[127.0.0.1:40717,DS-73809316-1280-4d0d-a296-ac98519e97b1,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum3': Fail to get block checksum for LocatedStripedBlock{BP-1759310994-172.17.0.10-1597681413145:blk_-9223372036854775776_1002; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39588,DS-9e61a247-d0e5-4911-ac5e-f5194a61e89c,DISK], DatanodeInfoWithStorage[127.0.0.1:43908,DS-11727838-90d6-4733-a4f1-c3303b9de980,DISK], DatanodeInfoWithStorage[127.0.0.1:34002,DS-d52f570e-8fa0-47e7-93d4-62c003e2af43,DISK], DatanodeInfoWithStorage[127.0.0.1:39251,DS-72d651b5-9764-4ede-8b46-fba7326ac74f,DISK], DatanodeInfoWithStorage[127.0.0.1:43483,DS-49d59574-d6d9-4548-8df9-df0a07e38ddc,DISK], DatanodeInfoWithStorage[127.0.0.1:40780,DS-448c05dd-4f3f-4f2a-9c64-7359877f8393,DISK], DatanodeInfoWithStorage[127.0.0.1:37375,DS-52377a13-5bde-42c4-8050-b902c6fc8362,DISK], DatanodeInfoWithStorage[127.0.0.1:40717,DS-73809316-1280-4d0d-a296-ac98519e97b1,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery16(TestFileChecksum.java:479)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 5 out of 50
v1v1v2v2 failed with probability 7 out of 50
result: false positive !!!
Total execution time in seconds : 7050

reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1265651383-172.17.0.19-1597641769534:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39455,DS-47b2e9d9-2eb1-4db3-b003-20bc544b941b,DISK], DatanodeInfoWithStorage[127.0.0.1:43277,DS-02339a08-e1e1-48d7-81a9-1563a4c47396,DISK], DatanodeInfoWithStorage[127.0.0.1:37349,DS-6c494e8e-0d79-4774-9287-cc8c1cdb3698,DISK], DatanodeInfoWithStorage[127.0.0.1:41852,DS-55802256-7a51-4f39-93ae-e85aa85d5bbb,DISK], DatanodeInfoWithStorage[127.0.0.1:39710,DS-f29b4953-8767-4bc0-8409-f4c0538d28ac,DISK], DatanodeInfoWithStorage[127.0.0.1:36016,DS-05b5c464-3ac0-4969-85ed-d355e4b7c763,DISK], DatanodeInfoWithStorage[127.0.0.1:33978,DS-36ba8a5e-b49e-48aa-8938-f44939981fc6,DISK], DatanodeInfoWithStorage[127.0.0.1:43227,DS-dbb6d90a-cfdc-4bb9-a6e9-ba496599f29f,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1265651383-172.17.0.19-1597641769534:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39455,DS-47b2e9d9-2eb1-4db3-b003-20bc544b941b,DISK], DatanodeInfoWithStorage[127.0.0.1:43277,DS-02339a08-e1e1-48d7-81a9-1563a4c47396,DISK], DatanodeInfoWithStorage[127.0.0.1:37349,DS-6c494e8e-0d79-4774-9287-cc8c1cdb3698,DISK], DatanodeInfoWithStorage[127.0.0.1:41852,DS-55802256-7a51-4f39-93ae-e85aa85d5bbb,DISK], DatanodeInfoWithStorage[127.0.0.1:39710,DS-f29b4953-8767-4bc0-8409-f4c0538d28ac,DISK], DatanodeInfoWithStorage[127.0.0.1:36016,DS-05b5c464-3ac0-4969-85ed-d355e4b7c763,DISK], DatanodeInfoWithStorage[127.0.0.1:33978,DS-36ba8a5e-b49e-48aa-8938-f44939981fc6,DISK], DatanodeInfoWithStorage[127.0.0.1:43227,DS-dbb6d90a-cfdc-4bb9-a6e9-ba496599f29f,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-117867420-172.17.0.19-1597641856756:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43718,DS-b2b5926d-795e-4ab2-8996-524fe847e4fd,DISK], DatanodeInfoWithStorage[127.0.0.1:40814,DS-c71145bd-98fe-44e1-b8cb-b80674cd84c3,DISK], DatanodeInfoWithStorage[127.0.0.1:46048,DS-8f034535-99b4-4993-9cf8-130c73a6f4cf,DISK], DatanodeInfoWithStorage[127.0.0.1:34488,DS-56d9b100-ca67-43be-ae44-07fcc3ca0329,DISK], DatanodeInfoWithStorage[127.0.0.1:44142,DS-10eb1595-d703-4f3a-a584-6117cbc76890,DISK], DatanodeInfoWithStorage[127.0.0.1:39946,DS-3e1ea1df-2606-4b32-9fb0-b69a5a67b9e7,DISK], DatanodeInfoWithStorage[127.0.0.1:38983,DS-26b58fcf-7ccd-4472-a36d-ed3fb96a683e,DISK], DatanodeInfoWithStorage[127.0.0.1:37728,DS-3cd3f3cf-9608-4a88-922b-bc664279bdda,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-117867420-172.17.0.19-1597641856756:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43718,DS-b2b5926d-795e-4ab2-8996-524fe847e4fd,DISK], DatanodeInfoWithStorage[127.0.0.1:40814,DS-c71145bd-98fe-44e1-b8cb-b80674cd84c3,DISK], DatanodeInfoWithStorage[127.0.0.1:46048,DS-8f034535-99b4-4993-9cf8-130c73a6f4cf,DISK], DatanodeInfoWithStorage[127.0.0.1:34488,DS-56d9b100-ca67-43be-ae44-07fcc3ca0329,DISK], DatanodeInfoWithStorage[127.0.0.1:44142,DS-10eb1595-d703-4f3a-a584-6117cbc76890,DISK], DatanodeInfoWithStorage[127.0.0.1:39946,DS-3e1ea1df-2606-4b32-9fb0-b69a5a67b9e7,DISK], DatanodeInfoWithStorage[127.0.0.1:38983,DS-26b58fcf-7ccd-4472-a36d-ed3fb96a683e,DISK], DatanodeInfoWithStorage[127.0.0.1:37728,DS-3cd3f3cf-9608-4a88-922b-bc664279bdda,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-792726009-172.17.0.19-1597641931064:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43907,DS-6931452f-b6cc-4b9c-b844-0481f3885f03,DISK], DatanodeInfoWithStorage[127.0.0.1:39497,DS-262dbda4-7cd6-453a-9a3e-07da2dc45a58,DISK], DatanodeInfoWithStorage[127.0.0.1:38059,DS-280f7033-b0a7-4a60-b2b5-ffa54420090e,DISK], DatanodeInfoWithStorage[127.0.0.1:43321,DS-a2c35603-9d93-49bc-bac3-ecd01086fc6f,DISK], DatanodeInfoWithStorage[127.0.0.1:45558,DS-c12bdbca-558e-4048-8382-03761001fd19,DISK], DatanodeInfoWithStorage[127.0.0.1:45269,DS-6571eb00-104a-4628-90df-34519d6f6ae8,DISK], DatanodeInfoWithStorage[127.0.0.1:44857,DS-5a94f7d0-454e-41f9-94be-3b1a813b5497,DISK], DatanodeInfoWithStorage[127.0.0.1:38536,DS-aabf497a-1402-4abb-bb6f-7c646df1ccfd,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-792726009-172.17.0.19-1597641931064:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43907,DS-6931452f-b6cc-4b9c-b844-0481f3885f03,DISK], DatanodeInfoWithStorage[127.0.0.1:39497,DS-262dbda4-7cd6-453a-9a3e-07da2dc45a58,DISK], DatanodeInfoWithStorage[127.0.0.1:38059,DS-280f7033-b0a7-4a60-b2b5-ffa54420090e,DISK], DatanodeInfoWithStorage[127.0.0.1:43321,DS-a2c35603-9d93-49bc-bac3-ecd01086fc6f,DISK], DatanodeInfoWithStorage[127.0.0.1:45558,DS-c12bdbca-558e-4048-8382-03761001fd19,DISK], DatanodeInfoWithStorage[127.0.0.1:45269,DS-6571eb00-104a-4628-90df-34519d6f6ae8,DISK], DatanodeInfoWithStorage[127.0.0.1:44857,DS-5a94f7d0-454e-41f9-94be-3b1a813b5497,DISK], DatanodeInfoWithStorage[127.0.0.1:38536,DS-aabf497a-1402-4abb-bb6f-7c646df1ccfd,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2013714103-172.17.0.19-1597642083099:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34225,DS-63e6eb5a-4215-4446-b6dc-6a4dfc9c3c25,DISK], DatanodeInfoWithStorage[127.0.0.1:37856,DS-e174e035-2c86-458f-8062-0f5d5273c3ec,DISK], DatanodeInfoWithStorage[127.0.0.1:39990,DS-1a064a35-4691-46b9-a4f6-b87c6446d13e,DISK], DatanodeInfoWithStorage[127.0.0.1:46172,DS-1abc5ef7-b8e0-4ed5-ab71-793ef302ce1b,DISK], DatanodeInfoWithStorage[127.0.0.1:46395,DS-992b1d6a-9f95-446e-89f4-03b2b9e4b8f3,DISK], DatanodeInfoWithStorage[127.0.0.1:45905,DS-a673e252-85b6-4d44-965a-05666c1037b5,DISK], DatanodeInfoWithStorage[127.0.0.1:38752,DS-c43893a1-7e21-4440-800d-c31ea6442d9c,DISK], DatanodeInfoWithStorage[127.0.0.1:42604,DS-dae0b5eb-0db7-491d-9d91-f727c9745d05,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2013714103-172.17.0.19-1597642083099:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34225,DS-63e6eb5a-4215-4446-b6dc-6a4dfc9c3c25,DISK], DatanodeInfoWithStorage[127.0.0.1:37856,DS-e174e035-2c86-458f-8062-0f5d5273c3ec,DISK], DatanodeInfoWithStorage[127.0.0.1:39990,DS-1a064a35-4691-46b9-a4f6-b87c6446d13e,DISK], DatanodeInfoWithStorage[127.0.0.1:46172,DS-1abc5ef7-b8e0-4ed5-ab71-793ef302ce1b,DISK], DatanodeInfoWithStorage[127.0.0.1:46395,DS-992b1d6a-9f95-446e-89f4-03b2b9e4b8f3,DISK], DatanodeInfoWithStorage[127.0.0.1:45905,DS-a673e252-85b6-4d44-965a-05666c1037b5,DISK], DatanodeInfoWithStorage[127.0.0.1:38752,DS-c43893a1-7e21-4440-800d-c31ea6442d9c,DISK], DatanodeInfoWithStorage[127.0.0.1:42604,DS-dae0b5eb-0db7-491d-9d91-f727c9745d05,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1868351739-172.17.0.19-1597642166637:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38427,DS-5fe83661-c26e-4593-a6e7-9ba17d9d9eea,DISK], DatanodeInfoWithStorage[127.0.0.1:38641,DS-b253fc1e-27ef-4410-9b4f-1a7acad5c34e,DISK], DatanodeInfoWithStorage[127.0.0.1:44692,DS-02df4c05-dad3-4f10-b775-682f12920f0d,DISK], DatanodeInfoWithStorage[127.0.0.1:39716,DS-b3a2e9aa-ad43-4782-8d88-be8ed97bb485,DISK], DatanodeInfoWithStorage[127.0.0.1:45754,DS-e72fe021-730d-4722-aef0-2bdb3b9eb94a,DISK], DatanodeInfoWithStorage[127.0.0.1:40245,DS-f3b1de11-76d1-4e28-8bd2-4da06f6c2371,DISK], DatanodeInfoWithStorage[127.0.0.1:43544,DS-563f2644-a888-4a45-8c4c-ef2b9087fea2,DISK], DatanodeInfoWithStorage[127.0.0.1:39701,DS-1b976c21-4bae-47b6-9018-54e9954c050f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1868351739-172.17.0.19-1597642166637:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38427,DS-5fe83661-c26e-4593-a6e7-9ba17d9d9eea,DISK], DatanodeInfoWithStorage[127.0.0.1:38641,DS-b253fc1e-27ef-4410-9b4f-1a7acad5c34e,DISK], DatanodeInfoWithStorage[127.0.0.1:44692,DS-02df4c05-dad3-4f10-b775-682f12920f0d,DISK], DatanodeInfoWithStorage[127.0.0.1:39716,DS-b3a2e9aa-ad43-4782-8d88-be8ed97bb485,DISK], DatanodeInfoWithStorage[127.0.0.1:45754,DS-e72fe021-730d-4722-aef0-2bdb3b9eb94a,DISK], DatanodeInfoWithStorage[127.0.0.1:40245,DS-f3b1de11-76d1-4e28-8bd2-4da06f6c2371,DISK], DatanodeInfoWithStorage[127.0.0.1:43544,DS-563f2644-a888-4a45-8c4c-ef2b9087fea2,DISK], DatanodeInfoWithStorage[127.0.0.1:39701,DS-1b976c21-4bae-47b6-9018-54e9954c050f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1375416210-172.17.0.19-1597642342879:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42607,DS-11294458-b81a-41d9-be26-32efa62d1e68,DISK], DatanodeInfoWithStorage[127.0.0.1:43677,DS-37de4b94-5232-41db-83c0-02fdf7afbc80,DISK], DatanodeInfoWithStorage[127.0.0.1:35331,DS-9d19fd1e-8679-473d-872e-bb5cb36cde2b,DISK], DatanodeInfoWithStorage[127.0.0.1:45781,DS-7434a203-cc0b-4368-97a7-b07eeedbfda4,DISK], DatanodeInfoWithStorage[127.0.0.1:34957,DS-b52d2f91-8396-4c1f-803b-aaa8159f5744,DISK], DatanodeInfoWithStorage[127.0.0.1:36002,DS-b3ab3a1f-9065-4d70-bef1-dcb652b865f9,DISK], DatanodeInfoWithStorage[127.0.0.1:42678,DS-8b50fc8b-398a-411c-a9a0-1d0c439bb4d6,DISK], DatanodeInfoWithStorage[127.0.0.1:34961,DS-5641e68e-c497-48d9-8fb2-19907337056a,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1375416210-172.17.0.19-1597642342879:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42607,DS-11294458-b81a-41d9-be26-32efa62d1e68,DISK], DatanodeInfoWithStorage[127.0.0.1:43677,DS-37de4b94-5232-41db-83c0-02fdf7afbc80,DISK], DatanodeInfoWithStorage[127.0.0.1:35331,DS-9d19fd1e-8679-473d-872e-bb5cb36cde2b,DISK], DatanodeInfoWithStorage[127.0.0.1:45781,DS-7434a203-cc0b-4368-97a7-b07eeedbfda4,DISK], DatanodeInfoWithStorage[127.0.0.1:34957,DS-b52d2f91-8396-4c1f-803b-aaa8159f5744,DISK], DatanodeInfoWithStorage[127.0.0.1:36002,DS-b3ab3a1f-9065-4d70-bef1-dcb652b865f9,DISK], DatanodeInfoWithStorage[127.0.0.1:42678,DS-8b50fc8b-398a-411c-a9a0-1d0c439bb4d6,DISK], DatanodeInfoWithStorage[127.0.0.1:34961,DS-5641e68e-c497-48d9-8fb2-19907337056a,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-913895974-172.17.0.19-1597642521344:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46541,DS-664a39ba-51fe-4b59-8b73-8cf3280c2f44,DISK], DatanodeInfoWithStorage[127.0.0.1:33842,DS-7a110e85-058b-4339-bb51-1413f112983c,DISK], DatanodeInfoWithStorage[127.0.0.1:41496,DS-87b39593-4fb5-450c-83ac-e6e1c37337c0,DISK], DatanodeInfoWithStorage[127.0.0.1:40732,DS-05208325-ac62-464c-a9c6-219ae580dc39,DISK], DatanodeInfoWithStorage[127.0.0.1:39549,DS-680197b2-7cc9-4258-accd-f76f8c5b0cd6,DISK], DatanodeInfoWithStorage[127.0.0.1:37554,DS-a00ff207-c8d3-4315-83af-ed1bfb8beba5,DISK], DatanodeInfoWithStorage[127.0.0.1:38745,DS-e68d0552-29ca-4e6c-841c-e6d6e3887644,DISK], DatanodeInfoWithStorage[127.0.0.1:42608,DS-60dcb7de-ba01-4d09-bfe0-1bd953d85919,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-913895974-172.17.0.19-1597642521344:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46541,DS-664a39ba-51fe-4b59-8b73-8cf3280c2f44,DISK], DatanodeInfoWithStorage[127.0.0.1:33842,DS-7a110e85-058b-4339-bb51-1413f112983c,DISK], DatanodeInfoWithStorage[127.0.0.1:41496,DS-87b39593-4fb5-450c-83ac-e6e1c37337c0,DISK], DatanodeInfoWithStorage[127.0.0.1:40732,DS-05208325-ac62-464c-a9c6-219ae580dc39,DISK], DatanodeInfoWithStorage[127.0.0.1:39549,DS-680197b2-7cc9-4258-accd-f76f8c5b0cd6,DISK], DatanodeInfoWithStorage[127.0.0.1:37554,DS-a00ff207-c8d3-4315-83af-ed1bfb8beba5,DISK], DatanodeInfoWithStorage[127.0.0.1:38745,DS-e68d0552-29ca-4e6c-841c-e6d6e3887644,DISK], DatanodeInfoWithStorage[127.0.0.1:42608,DS-60dcb7de-ba01-4d09-bfe0-1bd953d85919,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1011807617-172.17.0.19-1597642783838:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39198,DS-da5c4372-3363-4ab1-bcf2-b2e27f49f559,DISK], DatanodeInfoWithStorage[127.0.0.1:37043,DS-b318da92-42e5-45cd-811d-1dd6d3e23f2a,DISK], DatanodeInfoWithStorage[127.0.0.1:36828,DS-a998d581-460d-4139-97a8-522c61dff24a,DISK], DatanodeInfoWithStorage[127.0.0.1:41964,DS-7fc4d881-6db8-415d-bdac-65965d4be6a3,DISK], DatanodeInfoWithStorage[127.0.0.1:44442,DS-8a4e1961-c354-423d-9eb6-ff31db695a25,DISK], DatanodeInfoWithStorage[127.0.0.1:38400,DS-363d4f50-8bde-4797-a8fe-2a09407c3c77,DISK], DatanodeInfoWithStorage[127.0.0.1:34895,DS-a7225c09-b0db-476c-902e-5e2e7d08f629,DISK], DatanodeInfoWithStorage[127.0.0.1:42081,DS-b806b82e-4c03-4be8-82ad-9af496dec07b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1011807617-172.17.0.19-1597642783838:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39198,DS-da5c4372-3363-4ab1-bcf2-b2e27f49f559,DISK], DatanodeInfoWithStorage[127.0.0.1:37043,DS-b318da92-42e5-45cd-811d-1dd6d3e23f2a,DISK], DatanodeInfoWithStorage[127.0.0.1:36828,DS-a998d581-460d-4139-97a8-522c61dff24a,DISK], DatanodeInfoWithStorage[127.0.0.1:41964,DS-7fc4d881-6db8-415d-bdac-65965d4be6a3,DISK], DatanodeInfoWithStorage[127.0.0.1:44442,DS-8a4e1961-c354-423d-9eb6-ff31db695a25,DISK], DatanodeInfoWithStorage[127.0.0.1:38400,DS-363d4f50-8bde-4797-a8fe-2a09407c3c77,DISK], DatanodeInfoWithStorage[127.0.0.1:34895,DS-a7225c09-b0db-476c-902e-5e2e7d08f629,DISK], DatanodeInfoWithStorage[127.0.0.1:42081,DS-b806b82e-4c03-4be8-82ad-9af496dec07b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1497830528-172.17.0.19-1597642856537:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35126,DS-25b13e7b-c441-4065-a244-878312d3979a,DISK], DatanodeInfoWithStorage[127.0.0.1:43332,DS-fa2fbb36-2d50-4e93-b992-188ff7ae7307,DISK], DatanodeInfoWithStorage[127.0.0.1:39074,DS-e2c8dd18-2d9c-4e01-b97d-f3cee7633fc3,DISK], DatanodeInfoWithStorage[127.0.0.1:46138,DS-52f29756-0870-472c-b1e7-65eaebcaa815,DISK], DatanodeInfoWithStorage[127.0.0.1:44967,DS-2404bb36-6ed3-4f97-8815-964fc1e72bf6,DISK], DatanodeInfoWithStorage[127.0.0.1:45870,DS-e2931e3f-b446-4eaf-a1b5-f55374ef0379,DISK], DatanodeInfoWithStorage[127.0.0.1:42899,DS-5539a1ba-c054-41b3-b29b-7b8ff8b6124c,DISK], DatanodeInfoWithStorage[127.0.0.1:38118,DS-9123caf5-4ecc-4550-8b4f-ded23aebe00d,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1497830528-172.17.0.19-1597642856537:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35126,DS-25b13e7b-c441-4065-a244-878312d3979a,DISK], DatanodeInfoWithStorage[127.0.0.1:43332,DS-fa2fbb36-2d50-4e93-b992-188ff7ae7307,DISK], DatanodeInfoWithStorage[127.0.0.1:39074,DS-e2c8dd18-2d9c-4e01-b97d-f3cee7633fc3,DISK], DatanodeInfoWithStorage[127.0.0.1:46138,DS-52f29756-0870-472c-b1e7-65eaebcaa815,DISK], DatanodeInfoWithStorage[127.0.0.1:44967,DS-2404bb36-6ed3-4f97-8815-964fc1e72bf6,DISK], DatanodeInfoWithStorage[127.0.0.1:45870,DS-e2931e3f-b446-4eaf-a1b5-f55374ef0379,DISK], DatanodeInfoWithStorage[127.0.0.1:42899,DS-5539a1ba-c054-41b3-b29b-7b8ff8b6124c,DISK], DatanodeInfoWithStorage[127.0.0.1:38118,DS-9123caf5-4ecc-4550-8b4f-ded23aebe00d,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-546720730-172.17.0.19-1597642897946:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39744,DS-5f3aafa1-fb0b-43d5-8e88-e0c7fc363790,DISK], DatanodeInfoWithStorage[127.0.0.1:46667,DS-3efe6cf9-36c5-40d2-b090-9e3ffef8296b,DISK], DatanodeInfoWithStorage[127.0.0.1:35395,DS-1e997a03-bcd8-470f-b9b6-6e10540cc53f,DISK], DatanodeInfoWithStorage[127.0.0.1:46688,DS-70547993-7f88-4850-b0c5-e98f9a515d10,DISK], DatanodeInfoWithStorage[127.0.0.1:43266,DS-1e49a73a-b4ab-4bf2-abb5-6bfde654ef65,DISK], DatanodeInfoWithStorage[127.0.0.1:37635,DS-c6093169-6811-4385-bd94-8066e8f5c0ed,DISK], DatanodeInfoWithStorage[127.0.0.1:35750,DS-b875ba3f-9846-40f4-bcaf-8f748f6a82f6,DISK], DatanodeInfoWithStorage[127.0.0.1:46034,DS-dc1b5686-6586-4584-94c8-43c767d8af47,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-546720730-172.17.0.19-1597642897946:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39744,DS-5f3aafa1-fb0b-43d5-8e88-e0c7fc363790,DISK], DatanodeInfoWithStorage[127.0.0.1:46667,DS-3efe6cf9-36c5-40d2-b090-9e3ffef8296b,DISK], DatanodeInfoWithStorage[127.0.0.1:35395,DS-1e997a03-bcd8-470f-b9b6-6e10540cc53f,DISK], DatanodeInfoWithStorage[127.0.0.1:46688,DS-70547993-7f88-4850-b0c5-e98f9a515d10,DISK], DatanodeInfoWithStorage[127.0.0.1:43266,DS-1e49a73a-b4ab-4bf2-abb5-6bfde654ef65,DISK], DatanodeInfoWithStorage[127.0.0.1:37635,DS-c6093169-6811-4385-bd94-8066e8f5c0ed,DISK], DatanodeInfoWithStorage[127.0.0.1:35750,DS-b875ba3f-9846-40f4-bcaf-8f748f6a82f6,DISK], DatanodeInfoWithStorage[127.0.0.1:46034,DS-dc1b5686-6586-4584-94c8-43c767d8af47,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1643771490-172.17.0.19-1597643038538:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33474,DS-cf813363-ad97-4799-b974-ea1128c76068,DISK], DatanodeInfoWithStorage[127.0.0.1:33621,DS-9fbe5c99-9c46-4553-9806-92fabec13449,DISK], DatanodeInfoWithStorage[127.0.0.1:37960,DS-b6f59ae6-9fd3-4531-9269-1708a0fb65ea,DISK], DatanodeInfoWithStorage[127.0.0.1:42877,DS-de44ef79-9b7e-46ab-a685-d76a2dd7657b,DISK], DatanodeInfoWithStorage[127.0.0.1:34676,DS-db455013-d886-4cb1-923a-35cdb4a82c5d,DISK], DatanodeInfoWithStorage[127.0.0.1:35154,DS-d75935aa-8c86-40ec-9873-edaa74c773a0,DISK], DatanodeInfoWithStorage[127.0.0.1:40853,DS-74c4e126-4203-4483-9dfe-9596493efa95,DISK], DatanodeInfoWithStorage[127.0.0.1:46563,DS-b835003c-7320-47ee-b708-0e5b45fa2bb6,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1643771490-172.17.0.19-1597643038538:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33474,DS-cf813363-ad97-4799-b974-ea1128c76068,DISK], DatanodeInfoWithStorage[127.0.0.1:33621,DS-9fbe5c99-9c46-4553-9806-92fabec13449,DISK], DatanodeInfoWithStorage[127.0.0.1:37960,DS-b6f59ae6-9fd3-4531-9269-1708a0fb65ea,DISK], DatanodeInfoWithStorage[127.0.0.1:42877,DS-de44ef79-9b7e-46ab-a685-d76a2dd7657b,DISK], DatanodeInfoWithStorage[127.0.0.1:34676,DS-db455013-d886-4cb1-923a-35cdb4a82c5d,DISK], DatanodeInfoWithStorage[127.0.0.1:35154,DS-d75935aa-8c86-40ec-9873-edaa74c773a0,DISK], DatanodeInfoWithStorage[127.0.0.1:40853,DS-74c4e126-4203-4483-9dfe-9596493efa95,DISK], DatanodeInfoWithStorage[127.0.0.1:46563,DS-b835003c-7320-47ee-b708-0e5b45fa2bb6,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1289152094-172.17.0.19-1597643221416:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36998,DS-66b6e255-8899-4540-b2ff-f840570d5591,DISK], DatanodeInfoWithStorage[127.0.0.1:39610,DS-7e460898-5dee-4414-8f71-69389676e4b8,DISK], DatanodeInfoWithStorage[127.0.0.1:36098,DS-ff0e5360-a609-45b7-b342-8aaf925bf726,DISK], DatanodeInfoWithStorage[127.0.0.1:43635,DS-b4200a09-f8e0-4948-b829-1a809935ea6f,DISK], DatanodeInfoWithStorage[127.0.0.1:45704,DS-a3f258d1-be95-45eb-8786-ceb432cd692d,DISK], DatanodeInfoWithStorage[127.0.0.1:38607,DS-cfb2ba7d-5843-4029-96a9-0da1fb843a62,DISK], DatanodeInfoWithStorage[127.0.0.1:39042,DS-fdcbada0-9ba8-49c3-8a90-fd66aa1673e0,DISK], DatanodeInfoWithStorage[127.0.0.1:42391,DS-88ee5c83-a93c-49e1-b450-926a4084ffd4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1289152094-172.17.0.19-1597643221416:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36998,DS-66b6e255-8899-4540-b2ff-f840570d5591,DISK], DatanodeInfoWithStorage[127.0.0.1:39610,DS-7e460898-5dee-4414-8f71-69389676e4b8,DISK], DatanodeInfoWithStorage[127.0.0.1:36098,DS-ff0e5360-a609-45b7-b342-8aaf925bf726,DISK], DatanodeInfoWithStorage[127.0.0.1:43635,DS-b4200a09-f8e0-4948-b829-1a809935ea6f,DISK], DatanodeInfoWithStorage[127.0.0.1:45704,DS-a3f258d1-be95-45eb-8786-ceb432cd692d,DISK], DatanodeInfoWithStorage[127.0.0.1:38607,DS-cfb2ba7d-5843-4029-96a9-0da1fb843a62,DISK], DatanodeInfoWithStorage[127.0.0.1:39042,DS-fdcbada0-9ba8-49c3-8a90-fd66aa1673e0,DISK], DatanodeInfoWithStorage[127.0.0.1:42391,DS-88ee5c83-a93c-49e1-b450-926a4084ffd4,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2001458754-172.17.0.19-1597643287950:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45859,DS-d781a0b2-fbc2-452b-bf9c-aaa9bcea9ce5,DISK], DatanodeInfoWithStorage[127.0.0.1:39736,DS-913f8059-60e7-4947-9608-3d50c0aa1ae5,DISK], DatanodeInfoWithStorage[127.0.0.1:33295,DS-6008e753-ce8a-443f-b012-d11bc3f6046e,DISK], DatanodeInfoWithStorage[127.0.0.1:36102,DS-b5e90304-af71-4120-b30e-1ca907039084,DISK], DatanodeInfoWithStorage[127.0.0.1:42680,DS-943d7e00-64f6-4c54-978b-1a74a02163b5,DISK], DatanodeInfoWithStorage[127.0.0.1:45413,DS-142f225f-5d95-480f-b496-b850aba6579f,DISK], DatanodeInfoWithStorage[127.0.0.1:44109,DS-654b24da-6e5e-4a24-a397-775e1f29572f,DISK], DatanodeInfoWithStorage[127.0.0.1:39702,DS-4b0700ac-3dc0-4a4e-849a-e9089a60d9ab,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2001458754-172.17.0.19-1597643287950:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45859,DS-d781a0b2-fbc2-452b-bf9c-aaa9bcea9ce5,DISK], DatanodeInfoWithStorage[127.0.0.1:39736,DS-913f8059-60e7-4947-9608-3d50c0aa1ae5,DISK], DatanodeInfoWithStorage[127.0.0.1:33295,DS-6008e753-ce8a-443f-b012-d11bc3f6046e,DISK], DatanodeInfoWithStorage[127.0.0.1:36102,DS-b5e90304-af71-4120-b30e-1ca907039084,DISK], DatanodeInfoWithStorage[127.0.0.1:42680,DS-943d7e00-64f6-4c54-978b-1a74a02163b5,DISK], DatanodeInfoWithStorage[127.0.0.1:45413,DS-142f225f-5d95-480f-b496-b850aba6579f,DISK], DatanodeInfoWithStorage[127.0.0.1:44109,DS-654b24da-6e5e-4a24-a397-775e1f29572f,DISK], DatanodeInfoWithStorage[127.0.0.1:39702,DS-4b0700ac-3dc0-4a4e-849a-e9089a60d9ab,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-398172446-172.17.0.19-1597643323679:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37545,DS-210359ad-7b21-46d4-9e0c-91f862a985d1,DISK], DatanodeInfoWithStorage[127.0.0.1:46730,DS-39584837-381b-464e-b900-c198dca37c7f,DISK], DatanodeInfoWithStorage[127.0.0.1:40957,DS-a41163a7-33db-4e62-8f26-84159c5d7ec5,DISK], DatanodeInfoWithStorage[127.0.0.1:41376,DS-8ea4546a-0af8-438c-8ae1-62a6986bee95,DISK], DatanodeInfoWithStorage[127.0.0.1:33850,DS-5e387f9f-32a5-417c-83ed-9c7712de5bcf,DISK], DatanodeInfoWithStorage[127.0.0.1:37956,DS-424e2262-29d2-4dc9-9b3c-319f30cd5a98,DISK], DatanodeInfoWithStorage[127.0.0.1:33782,DS-afda621d-059c-4639-a28c-62f8914fccd9,DISK], DatanodeInfoWithStorage[127.0.0.1:37431,DS-5d64118e-4e3e-4659-8fc4-ebeb8d621a98,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-398172446-172.17.0.19-1597643323679:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37545,DS-210359ad-7b21-46d4-9e0c-91f862a985d1,DISK], DatanodeInfoWithStorage[127.0.0.1:46730,DS-39584837-381b-464e-b900-c198dca37c7f,DISK], DatanodeInfoWithStorage[127.0.0.1:40957,DS-a41163a7-33db-4e62-8f26-84159c5d7ec5,DISK], DatanodeInfoWithStorage[127.0.0.1:41376,DS-8ea4546a-0af8-438c-8ae1-62a6986bee95,DISK], DatanodeInfoWithStorage[127.0.0.1:33850,DS-5e387f9f-32a5-417c-83ed-9c7712de5bcf,DISK], DatanodeInfoWithStorage[127.0.0.1:37956,DS-424e2262-29d2-4dc9-9b3c-319f30cd5a98,DISK], DatanodeInfoWithStorage[127.0.0.1:33782,DS-afda621d-059c-4639-a28c-62f8914fccd9,DISK], DatanodeInfoWithStorage[127.0.0.1:37431,DS-5d64118e-4e3e-4659-8fc4-ebeb8d621a98,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-270468416-172.17.0.19-1597643477376:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43941,DS-938448a9-5ea5-4a5c-ac5e-43afcbf270ba,DISK], DatanodeInfoWithStorage[127.0.0.1:42125,DS-28df94f0-4944-446a-982f-bc0050228158,DISK], DatanodeInfoWithStorage[127.0.0.1:33523,DS-c8e61ffc-1dca-45d2-a797-e57c1353127d,DISK], DatanodeInfoWithStorage[127.0.0.1:33219,DS-c584bb42-21c8-4341-af56-02a1eb9e7e26,DISK], DatanodeInfoWithStorage[127.0.0.1:42288,DS-145668a6-f7de-4c80-9f2e-f18b81cacc3d,DISK], DatanodeInfoWithStorage[127.0.0.1:43314,DS-c38b655f-d6ab-4753-8928-f57e5899a5f1,DISK], DatanodeInfoWithStorage[127.0.0.1:43486,DS-e8063d52-2010-46dc-99ce-1251b70becf9,DISK], DatanodeInfoWithStorage[127.0.0.1:44053,DS-e0f715bd-822b-44a2-9177-c1c4a3b5d235,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-270468416-172.17.0.19-1597643477376:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43941,DS-938448a9-5ea5-4a5c-ac5e-43afcbf270ba,DISK], DatanodeInfoWithStorage[127.0.0.1:42125,DS-28df94f0-4944-446a-982f-bc0050228158,DISK], DatanodeInfoWithStorage[127.0.0.1:33523,DS-c8e61ffc-1dca-45d2-a797-e57c1353127d,DISK], DatanodeInfoWithStorage[127.0.0.1:33219,DS-c584bb42-21c8-4341-af56-02a1eb9e7e26,DISK], DatanodeInfoWithStorage[127.0.0.1:42288,DS-145668a6-f7de-4c80-9f2e-f18b81cacc3d,DISK], DatanodeInfoWithStorage[127.0.0.1:43314,DS-c38b655f-d6ab-4753-8928-f57e5899a5f1,DISK], DatanodeInfoWithStorage[127.0.0.1:43486,DS-e8063d52-2010-46dc-99ce-1251b70becf9,DISK], DatanodeInfoWithStorage[127.0.0.1:44053,DS-e0f715bd-822b-44a2-9177-c1c4a3b5d235,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-920639427-172.17.0.19-1597643582178:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37345,DS-b9e8f31d-17e8-4102-aa7c-8bfe01441148,DISK], DatanodeInfoWithStorage[127.0.0.1:33131,DS-e73b44a3-28a6-4907-9e13-4f15b0b3938f,DISK], DatanodeInfoWithStorage[127.0.0.1:42719,DS-81deba17-f52a-4fee-9ee4-cee45b132519,DISK], DatanodeInfoWithStorage[127.0.0.1:45623,DS-7e66968b-8298-485f-988c-7e08dab6e40b,DISK], DatanodeInfoWithStorage[127.0.0.1:33173,DS-87896aeb-6bb0-4381-8105-6a3af143290e,DISK], DatanodeInfoWithStorage[127.0.0.1:39753,DS-54aebbd4-fcff-4b8e-b7fd-4dfe68e442cf,DISK], DatanodeInfoWithStorage[127.0.0.1:35472,DS-6b167e4d-5fdc-420e-983e-d209302e1566,DISK], DatanodeInfoWithStorage[127.0.0.1:41373,DS-a70cc38f-4f66-49f8-88df-57be4a4863bb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-920639427-172.17.0.19-1597643582178:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37345,DS-b9e8f31d-17e8-4102-aa7c-8bfe01441148,DISK], DatanodeInfoWithStorage[127.0.0.1:33131,DS-e73b44a3-28a6-4907-9e13-4f15b0b3938f,DISK], DatanodeInfoWithStorage[127.0.0.1:42719,DS-81deba17-f52a-4fee-9ee4-cee45b132519,DISK], DatanodeInfoWithStorage[127.0.0.1:45623,DS-7e66968b-8298-485f-988c-7e08dab6e40b,DISK], DatanodeInfoWithStorage[127.0.0.1:33173,DS-87896aeb-6bb0-4381-8105-6a3af143290e,DISK], DatanodeInfoWithStorage[127.0.0.1:39753,DS-54aebbd4-fcff-4b8e-b7fd-4dfe68e442cf,DISK], DatanodeInfoWithStorage[127.0.0.1:35472,DS-6b167e4d-5fdc-420e-983e-d209302e1566,DISK], DatanodeInfoWithStorage[127.0.0.1:41373,DS-a70cc38f-4f66-49f8-88df-57be4a4863bb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-216643432-172.17.0.19-1597643836227:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42845,DS-d4700cf1-0ded-4824-b930-d5d4b2f895ea,DISK], DatanodeInfoWithStorage[127.0.0.1:32801,DS-f5f8fea0-58d3-4579-923a-21bf07abdb61,DISK], DatanodeInfoWithStorage[127.0.0.1:45126,DS-fcda764d-209f-4ba6-a36a-6b2b035c5ed0,DISK], DatanodeInfoWithStorage[127.0.0.1:41083,DS-805d5b90-f6b6-4a8f-8c8f-cfe7a4413c33,DISK], DatanodeInfoWithStorage[127.0.0.1:34710,DS-fe5db6b8-a765-4796-acac-d9e7aedd078c,DISK], DatanodeInfoWithStorage[127.0.0.1:35721,DS-4049b25e-e1ec-441b-bfe2-b1a896094e50,DISK], DatanodeInfoWithStorage[127.0.0.1:35765,DS-fd55fa1c-23e8-4509-939b-f44a86996a8b,DISK], DatanodeInfoWithStorage[127.0.0.1:38074,DS-fb694126-e5f5-4975-b2ee-93cc75561eb6,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-216643432-172.17.0.19-1597643836227:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42845,DS-d4700cf1-0ded-4824-b930-d5d4b2f895ea,DISK], DatanodeInfoWithStorage[127.0.0.1:32801,DS-f5f8fea0-58d3-4579-923a-21bf07abdb61,DISK], DatanodeInfoWithStorage[127.0.0.1:45126,DS-fcda764d-209f-4ba6-a36a-6b2b035c5ed0,DISK], DatanodeInfoWithStorage[127.0.0.1:41083,DS-805d5b90-f6b6-4a8f-8c8f-cfe7a4413c33,DISK], DatanodeInfoWithStorage[127.0.0.1:34710,DS-fe5db6b8-a765-4796-acac-d9e7aedd078c,DISK], DatanodeInfoWithStorage[127.0.0.1:35721,DS-4049b25e-e1ec-441b-bfe2-b1a896094e50,DISK], DatanodeInfoWithStorage[127.0.0.1:35765,DS-fd55fa1c-23e8-4509-939b-f44a86996a8b,DISK], DatanodeInfoWithStorage[127.0.0.1:38074,DS-fb694126-e5f5-4975-b2ee-93cc75561eb6,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1017722575-172.17.0.19-1597644497828:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46344,DS-f92ec12c-cdfc-423b-953f-357d402007cb,DISK], DatanodeInfoWithStorage[127.0.0.1:45106,DS-8091bde9-e29b-4c42-bf0d-eb1b8c39cb02,DISK], DatanodeInfoWithStorage[127.0.0.1:37222,DS-f306f3cf-9b26-4a70-99bb-eb537bc13632,DISK], DatanodeInfoWithStorage[127.0.0.1:38870,DS-dae85cb6-a371-4d60-8505-8b5ae4ce07e0,DISK], DatanodeInfoWithStorage[127.0.0.1:45670,DS-d69c1af9-ad2f-456d-a21f-de3b8642858b,DISK], DatanodeInfoWithStorage[127.0.0.1:42873,DS-22dc9fdf-bf87-4fbf-a28b-f5e1fd4b61e9,DISK], DatanodeInfoWithStorage[127.0.0.1:39217,DS-2ab12957-ab0b-40ae-9325-39b5841946d6,DISK], DatanodeInfoWithStorage[127.0.0.1:35079,DS-0386d9fd-f394-4b3a-ab00-59b8d1037a9f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1017722575-172.17.0.19-1597644497828:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46344,DS-f92ec12c-cdfc-423b-953f-357d402007cb,DISK], DatanodeInfoWithStorage[127.0.0.1:45106,DS-8091bde9-e29b-4c42-bf0d-eb1b8c39cb02,DISK], DatanodeInfoWithStorage[127.0.0.1:37222,DS-f306f3cf-9b26-4a70-99bb-eb537bc13632,DISK], DatanodeInfoWithStorage[127.0.0.1:38870,DS-dae85cb6-a371-4d60-8505-8b5ae4ce07e0,DISK], DatanodeInfoWithStorage[127.0.0.1:45670,DS-d69c1af9-ad2f-456d-a21f-de3b8642858b,DISK], DatanodeInfoWithStorage[127.0.0.1:42873,DS-22dc9fdf-bf87-4fbf-a28b-f5e1fd4b61e9,DISK], DatanodeInfoWithStorage[127.0.0.1:39217,DS-2ab12957-ab0b-40ae-9325-39b5841946d6,DISK], DatanodeInfoWithStorage[127.0.0.1:35079,DS-0386d9fd-f394-4b3a-ab00-59b8d1037a9f,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-109755090-172.17.0.19-1597644614618:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36225,DS-8053dd46-6a83-459c-a79a-94248a9cdb1a,DISK], DatanodeInfoWithStorage[127.0.0.1:38288,DS-5d9e9578-3fa2-4ad4-b184-154f8695b22b,DISK], DatanodeInfoWithStorage[127.0.0.1:36415,DS-71300728-a971-4ad3-be0d-033ea510c9f9,DISK], DatanodeInfoWithStorage[127.0.0.1:46580,DS-2d023d3c-8bad-4ecf-9d39-a57bfab1f77c,DISK], DatanodeInfoWithStorage[127.0.0.1:34734,DS-e07252fa-42a4-4d47-bf60-decfe2bd1397,DISK], DatanodeInfoWithStorage[127.0.0.1:45336,DS-f13b23a9-ff1b-4359-a120-04a3609726da,DISK], DatanodeInfoWithStorage[127.0.0.1:36635,DS-4a375a7f-ba9c-476f-9431-d13be4869bcf,DISK], DatanodeInfoWithStorage[127.0.0.1:44631,DS-bfd2d89c-f5d9-40ec-8eb3-3012a17c5d7b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-109755090-172.17.0.19-1597644614618:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36225,DS-8053dd46-6a83-459c-a79a-94248a9cdb1a,DISK], DatanodeInfoWithStorage[127.0.0.1:38288,DS-5d9e9578-3fa2-4ad4-b184-154f8695b22b,DISK], DatanodeInfoWithStorage[127.0.0.1:36415,DS-71300728-a971-4ad3-be0d-033ea510c9f9,DISK], DatanodeInfoWithStorage[127.0.0.1:46580,DS-2d023d3c-8bad-4ecf-9d39-a57bfab1f77c,DISK], DatanodeInfoWithStorage[127.0.0.1:34734,DS-e07252fa-42a4-4d47-bf60-decfe2bd1397,DISK], DatanodeInfoWithStorage[127.0.0.1:45336,DS-f13b23a9-ff1b-4359-a120-04a3609726da,DISK], DatanodeInfoWithStorage[127.0.0.1:36635,DS-4a375a7f-ba9c-476f-9431-d13be4869bcf,DISK], DatanodeInfoWithStorage[127.0.0.1:44631,DS-bfd2d89c-f5d9-40ec-8eb3-3012a17c5d7b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1946507067-172.17.0.19-1597644688656:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41235,DS-5c312db7-46aa-4e09-a5bf-b8e8b730fd69,DISK], DatanodeInfoWithStorage[127.0.0.1:39200,DS-0f4c16ae-3e9d-4204-bba4-7ff117f2d5ae,DISK], DatanodeInfoWithStorage[127.0.0.1:35624,DS-04c3ad14-ba0e-4ac1-a69f-617fa9f4f966,DISK], DatanodeInfoWithStorage[127.0.0.1:44811,DS-5787358c-da38-4ec0-a602-3692941fc481,DISK], DatanodeInfoWithStorage[127.0.0.1:43786,DS-c64b3efc-09da-4753-b4d8-49dd4337b62e,DISK], DatanodeInfoWithStorage[127.0.0.1:34457,DS-0448e37e-f76d-4561-95e1-b519e7e2a72d,DISK], DatanodeInfoWithStorage[127.0.0.1:34351,DS-372dd83c-9f61-4e45-96cc-7dabfb1582b8,DISK], DatanodeInfoWithStorage[127.0.0.1:35376,DS-1f6f3a6b-aa90-4753-bdc3-91cbcb6f41ff,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1946507067-172.17.0.19-1597644688656:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41235,DS-5c312db7-46aa-4e09-a5bf-b8e8b730fd69,DISK], DatanodeInfoWithStorage[127.0.0.1:39200,DS-0f4c16ae-3e9d-4204-bba4-7ff117f2d5ae,DISK], DatanodeInfoWithStorage[127.0.0.1:35624,DS-04c3ad14-ba0e-4ac1-a69f-617fa9f4f966,DISK], DatanodeInfoWithStorage[127.0.0.1:44811,DS-5787358c-da38-4ec0-a602-3692941fc481,DISK], DatanodeInfoWithStorage[127.0.0.1:43786,DS-c64b3efc-09da-4753-b4d8-49dd4337b62e,DISK], DatanodeInfoWithStorage[127.0.0.1:34457,DS-0448e37e-f76d-4561-95e1-b519e7e2a72d,DISK], DatanodeInfoWithStorage[127.0.0.1:34351,DS-372dd83c-9f61-4e45-96cc-7dabfb1582b8,DISK], DatanodeInfoWithStorage[127.0.0.1:35376,DS-1f6f3a6b-aa90-4753-bdc3-91cbcb6f41ff,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-369863149-172.17.0.19-1597644968183:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38108,DS-f9b4ed83-02a7-4973-ac12-6d5defc3e693,DISK], DatanodeInfoWithStorage[127.0.0.1:40855,DS-e1691d1e-3908-42fe-a0d6-117a6e056916,DISK], DatanodeInfoWithStorage[127.0.0.1:43977,DS-b5173b6d-ebb2-4ac5-ba07-13bc8e41134d,DISK], DatanodeInfoWithStorage[127.0.0.1:43068,DS-c3549d17-caa4-46a6-a28b-8263aab7c3d3,DISK], DatanodeInfoWithStorage[127.0.0.1:45300,DS-08f5ca2d-24dc-4749-b7fe-255ee1513c35,DISK], DatanodeInfoWithStorage[127.0.0.1:44361,DS-1c7c6f7e-480d-41e1-a640-86d37e7e1f81,DISK], DatanodeInfoWithStorage[127.0.0.1:33012,DS-4fe1091c-00d0-4517-aaae-4036a3b8b173,DISK], DatanodeInfoWithStorage[127.0.0.1:43825,DS-99e865d3-93a7-4bc4-a3a2-ab301ec9560f,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-369863149-172.17.0.19-1597644968183:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38108,DS-f9b4ed83-02a7-4973-ac12-6d5defc3e693,DISK], DatanodeInfoWithStorage[127.0.0.1:40855,DS-e1691d1e-3908-42fe-a0d6-117a6e056916,DISK], DatanodeInfoWithStorage[127.0.0.1:43977,DS-b5173b6d-ebb2-4ac5-ba07-13bc8e41134d,DISK], DatanodeInfoWithStorage[127.0.0.1:43068,DS-c3549d17-caa4-46a6-a28b-8263aab7c3d3,DISK], DatanodeInfoWithStorage[127.0.0.1:45300,DS-08f5ca2d-24dc-4749-b7fe-255ee1513c35,DISK], DatanodeInfoWithStorage[127.0.0.1:44361,DS-1c7c6f7e-480d-41e1-a640-86d37e7e1f81,DISK], DatanodeInfoWithStorage[127.0.0.1:33012,DS-4fe1091c-00d0-4517-aaae-4036a3b8b173,DISK], DatanodeInfoWithStorage[127.0.0.1:43825,DS-99e865d3-93a7-4bc4-a3a2-ab301ec9560f,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-812826305-172.17.0.19-1597645075904:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36970,DS-9f483c3d-d991-41bb-a425-3f7af3bbb4bc,DISK], DatanodeInfoWithStorage[127.0.0.1:39915,DS-c54cc6b4-496c-4b4f-8820-a666f2a371ff,DISK], DatanodeInfoWithStorage[127.0.0.1:35162,DS-e3510333-d323-4211-beee-a6d6651c2d2c,DISK], DatanodeInfoWithStorage[127.0.0.1:36916,DS-b08a143c-a0b7-47a3-bf82-cd762192e92d,DISK], DatanodeInfoWithStorage[127.0.0.1:39362,DS-30d89b95-509c-4c19-b451-4c53329fd543,DISK], DatanodeInfoWithStorage[127.0.0.1:42965,DS-8290b061-1dcd-4833-ba44-e141d9635307,DISK], DatanodeInfoWithStorage[127.0.0.1:44159,DS-1c8688d9-09b6-4a65-afc8-223312fc9f5a,DISK], DatanodeInfoWithStorage[127.0.0.1:45693,DS-c53821f4-aabe-4966-b565-5cb493f3235a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-812826305-172.17.0.19-1597645075904:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36970,DS-9f483c3d-d991-41bb-a425-3f7af3bbb4bc,DISK], DatanodeInfoWithStorage[127.0.0.1:39915,DS-c54cc6b4-496c-4b4f-8820-a666f2a371ff,DISK], DatanodeInfoWithStorage[127.0.0.1:35162,DS-e3510333-d323-4211-beee-a6d6651c2d2c,DISK], DatanodeInfoWithStorage[127.0.0.1:36916,DS-b08a143c-a0b7-47a3-bf82-cd762192e92d,DISK], DatanodeInfoWithStorage[127.0.0.1:39362,DS-30d89b95-509c-4c19-b451-4c53329fd543,DISK], DatanodeInfoWithStorage[127.0.0.1:42965,DS-8290b061-1dcd-4833-ba44-e141d9635307,DISK], DatanodeInfoWithStorage[127.0.0.1:44159,DS-1c8688d9-09b6-4a65-afc8-223312fc9f5a,DISK], DatanodeInfoWithStorage[127.0.0.1:45693,DS-c53821f4-aabe-4966-b565-5cb493f3235a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1604849118-172.17.0.19-1597645230900:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36757,DS-509bb376-9f05-4958-9b9a-b9f2516e376d,DISK], DatanodeInfoWithStorage[127.0.0.1:37943,DS-33d41f1e-0449-4558-9477-6ce1f8a5caa2,DISK], DatanodeInfoWithStorage[127.0.0.1:44817,DS-4273a9af-f1f7-49b4-9465-376276cfd78e,DISK], DatanodeInfoWithStorage[127.0.0.1:33017,DS-7ecdfcd6-9b38-4817-bd0a-0e79045b4334,DISK], DatanodeInfoWithStorage[127.0.0.1:33802,DS-5a04df7b-e92e-4eea-905f-5146b155c446,DISK], DatanodeInfoWithStorage[127.0.0.1:38244,DS-4ff782ba-7900-4c88-ae17-48f5bdd72cb2,DISK], DatanodeInfoWithStorage[127.0.0.1:38631,DS-25d831c3-b76f-41f0-b01c-da52cf4e5902,DISK], DatanodeInfoWithStorage[127.0.0.1:46652,DS-c7b9e8e5-cb9c-4c97-8c6e-66910c8e0820,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1604849118-172.17.0.19-1597645230900:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36757,DS-509bb376-9f05-4958-9b9a-b9f2516e376d,DISK], DatanodeInfoWithStorage[127.0.0.1:37943,DS-33d41f1e-0449-4558-9477-6ce1f8a5caa2,DISK], DatanodeInfoWithStorage[127.0.0.1:44817,DS-4273a9af-f1f7-49b4-9465-376276cfd78e,DISK], DatanodeInfoWithStorage[127.0.0.1:33017,DS-7ecdfcd6-9b38-4817-bd0a-0e79045b4334,DISK], DatanodeInfoWithStorage[127.0.0.1:33802,DS-5a04df7b-e92e-4eea-905f-5146b155c446,DISK], DatanodeInfoWithStorage[127.0.0.1:38244,DS-4ff782ba-7900-4c88-ae17-48f5bdd72cb2,DISK], DatanodeInfoWithStorage[127.0.0.1:38631,DS-25d831c3-b76f-41f0-b01c-da52cf4e5902,DISK], DatanodeInfoWithStorage[127.0.0.1:46652,DS-c7b9e8e5-cb9c-4c97-8c6e-66910c8e0820,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1307697925-172.17.0.19-1597645274123:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43333,DS-78306855-91c5-4f29-85b2-4a0f1985116b,DISK], DatanodeInfoWithStorage[127.0.0.1:35801,DS-a1951469-b28f-42f3-945b-3616181d41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:37506,DS-9361be93-cd90-4038-aa76-901955e859e4,DISK], DatanodeInfoWithStorage[127.0.0.1:32952,DS-7073cc78-a9e2-4912-8d81-ea8d7d681b09,DISK], DatanodeInfoWithStorage[127.0.0.1:35473,DS-0d644a24-c8a1-407c-9191-5665e739e6c7,DISK], DatanodeInfoWithStorage[127.0.0.1:34371,DS-e6236054-210b-44bb-be1b-a6a478453839,DISK], DatanodeInfoWithStorage[127.0.0.1:33117,DS-917ab07d-0af4-4c7a-857c-fb84e7948eb0,DISK], DatanodeInfoWithStorage[127.0.0.1:37363,DS-577e420b-4bb9-445a-a405-dd3185ce49fa,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1307697925-172.17.0.19-1597645274123:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43333,DS-78306855-91c5-4f29-85b2-4a0f1985116b,DISK], DatanodeInfoWithStorage[127.0.0.1:35801,DS-a1951469-b28f-42f3-945b-3616181d41ae,DISK], DatanodeInfoWithStorage[127.0.0.1:37506,DS-9361be93-cd90-4038-aa76-901955e859e4,DISK], DatanodeInfoWithStorage[127.0.0.1:32952,DS-7073cc78-a9e2-4912-8d81-ea8d7d681b09,DISK], DatanodeInfoWithStorage[127.0.0.1:35473,DS-0d644a24-c8a1-407c-9191-5665e739e6c7,DISK], DatanodeInfoWithStorage[127.0.0.1:34371,DS-e6236054-210b-44bb-be1b-a6a478453839,DISK], DatanodeInfoWithStorage[127.0.0.1:33117,DS-917ab07d-0af4-4c7a-857c-fb84e7948eb0,DISK], DatanodeInfoWithStorage[127.0.0.1:37363,DS-577e420b-4bb9-445a-a405-dd3185ce49fa,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-302709870-172.17.0.19-1597645690289:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33744,DS-4c63c7dc-e044-4faf-a4d1-7c537140657f,DISK], DatanodeInfoWithStorage[127.0.0.1:36718,DS-a11a575e-3cbc-4972-a01e-5e6c462f2361,DISK], DatanodeInfoWithStorage[127.0.0.1:37349,DS-97cfd969-a1a9-4a0e-bf70-e50294a7860b,DISK], DatanodeInfoWithStorage[127.0.0.1:44259,DS-8b801133-17ea-4d89-8c06-0bfd995b3d34,DISK], DatanodeInfoWithStorage[127.0.0.1:43369,DS-9754f8cd-c097-4291-b8e0-698528a7a40e,DISK], DatanodeInfoWithStorage[127.0.0.1:39284,DS-113f85df-b9dc-443e-a0e1-c818639024a5,DISK], DatanodeInfoWithStorage[127.0.0.1:34445,DS-151e26f2-9bd1-4a64-a65c-f5222cd4232f,DISK], DatanodeInfoWithStorage[127.0.0.1:40867,DS-576368d2-50d4-4f01-97d7-8682354a0685,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-302709870-172.17.0.19-1597645690289:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33744,DS-4c63c7dc-e044-4faf-a4d1-7c537140657f,DISK], DatanodeInfoWithStorage[127.0.0.1:36718,DS-a11a575e-3cbc-4972-a01e-5e6c462f2361,DISK], DatanodeInfoWithStorage[127.0.0.1:37349,DS-97cfd969-a1a9-4a0e-bf70-e50294a7860b,DISK], DatanodeInfoWithStorage[127.0.0.1:44259,DS-8b801133-17ea-4d89-8c06-0bfd995b3d34,DISK], DatanodeInfoWithStorage[127.0.0.1:43369,DS-9754f8cd-c097-4291-b8e0-698528a7a40e,DISK], DatanodeInfoWithStorage[127.0.0.1:39284,DS-113f85df-b9dc-443e-a0e1-c818639024a5,DISK], DatanodeInfoWithStorage[127.0.0.1:34445,DS-151e26f2-9bd1-4a64-a65c-f5222cd4232f,DISK], DatanodeInfoWithStorage[127.0.0.1:40867,DS-576368d2-50d4-4f01-97d7-8682354a0685,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1047686277-172.17.0.19-1597645916907:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46546,DS-f34a7379-d932-47ac-87f9-0667ae76fe72,DISK], DatanodeInfoWithStorage[127.0.0.1:40390,DS-19d1664a-432b-473b-9323-bea6c3463ea8,DISK], DatanodeInfoWithStorage[127.0.0.1:36782,DS-a973b045-33c2-4e38-a9e4-b994bf150986,DISK], DatanodeInfoWithStorage[127.0.0.1:41392,DS-48a2292b-0761-4bf6-98d0-efa65bb56a7a,DISK], DatanodeInfoWithStorage[127.0.0.1:35980,DS-a8fab69a-cbfe-42f5-8399-441acbcc959a,DISK], DatanodeInfoWithStorage[127.0.0.1:45681,DS-5d222a92-8632-4f05-af7e-610ad1690db5,DISK], DatanodeInfoWithStorage[127.0.0.1:33292,DS-0b686ae8-beeb-4190-b9e6-6a7c2ebc4e67,DISK], DatanodeInfoWithStorage[127.0.0.1:43610,DS-30c01f61-864c-4a4d-ac04-16acc05790a8,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1047686277-172.17.0.19-1597645916907:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46546,DS-f34a7379-d932-47ac-87f9-0667ae76fe72,DISK], DatanodeInfoWithStorage[127.0.0.1:40390,DS-19d1664a-432b-473b-9323-bea6c3463ea8,DISK], DatanodeInfoWithStorage[127.0.0.1:36782,DS-a973b045-33c2-4e38-a9e4-b994bf150986,DISK], DatanodeInfoWithStorage[127.0.0.1:41392,DS-48a2292b-0761-4bf6-98d0-efa65bb56a7a,DISK], DatanodeInfoWithStorage[127.0.0.1:35980,DS-a8fab69a-cbfe-42f5-8399-441acbcc959a,DISK], DatanodeInfoWithStorage[127.0.0.1:45681,DS-5d222a92-8632-4f05-af7e-610ad1690db5,DISK], DatanodeInfoWithStorage[127.0.0.1:33292,DS-0b686ae8-beeb-4190-b9e6-6a7c2ebc4e67,DISK], DatanodeInfoWithStorage[127.0.0.1:43610,DS-30c01f61-864c-4a4d-ac04-16acc05790a8,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1939532607-172.17.0.19-1597645953933:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34173,DS-19b52ad3-ebe0-4f43-a5d6-9cc032162664,DISK], DatanodeInfoWithStorage[127.0.0.1:40317,DS-25fd8707-b1d5-4302-8ebb-41a3ab7b1d49,DISK], DatanodeInfoWithStorage[127.0.0.1:41035,DS-f018119e-0beb-44ec-a483-7efb979311ca,DISK], DatanodeInfoWithStorage[127.0.0.1:34732,DS-13733e46-add1-41ee-9105-3cb8773788ff,DISK], DatanodeInfoWithStorage[127.0.0.1:41618,DS-5b6abf1b-c057-4819-a4a0-7f86b15adba9,DISK], DatanodeInfoWithStorage[127.0.0.1:42322,DS-422e968d-db28-438c-8cd2-1f4cce308656,DISK], DatanodeInfoWithStorage[127.0.0.1:42359,DS-49b8ec77-9694-4911-90a9-d0507ab25599,DISK], DatanodeInfoWithStorage[127.0.0.1:42903,DS-5fc1a0c2-a049-4332-91ed-d667f3ce3fc0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1939532607-172.17.0.19-1597645953933:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34173,DS-19b52ad3-ebe0-4f43-a5d6-9cc032162664,DISK], DatanodeInfoWithStorage[127.0.0.1:40317,DS-25fd8707-b1d5-4302-8ebb-41a3ab7b1d49,DISK], DatanodeInfoWithStorage[127.0.0.1:41035,DS-f018119e-0beb-44ec-a483-7efb979311ca,DISK], DatanodeInfoWithStorage[127.0.0.1:34732,DS-13733e46-add1-41ee-9105-3cb8773788ff,DISK], DatanodeInfoWithStorage[127.0.0.1:41618,DS-5b6abf1b-c057-4819-a4a0-7f86b15adba9,DISK], DatanodeInfoWithStorage[127.0.0.1:42322,DS-422e968d-db28-438c-8cd2-1f4cce308656,DISK], DatanodeInfoWithStorage[127.0.0.1:42359,DS-49b8ec77-9694-4911-90a9-d0507ab25599,DISK], DatanodeInfoWithStorage[127.0.0.1:42903,DS-5fc1a0c2-a049-4332-91ed-d667f3ce3fc0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1531925520-172.17.0.19-1597646281215:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46874,DS-40567ca7-528e-480e-a7b8-b1b6dbda142f,DISK], DatanodeInfoWithStorage[127.0.0.1:44684,DS-d6c5d9c2-5865-448d-b3a8-94ede30ca032,DISK], DatanodeInfoWithStorage[127.0.0.1:36241,DS-4947ae26-b9d5-44ea-af04-1070606aad6e,DISK], DatanodeInfoWithStorage[127.0.0.1:42136,DS-16c6cea5-9d64-46db-bd9d-5bd387ca0019,DISK], DatanodeInfoWithStorage[127.0.0.1:46166,DS-1b78e72b-9513-48f3-b722-2d36d63d8f15,DISK], DatanodeInfoWithStorage[127.0.0.1:37488,DS-7da970de-1254-4e38-9336-774d09fee81f,DISK], DatanodeInfoWithStorage[127.0.0.1:44167,DS-f8f6d953-558f-4ac9-829c-dbbeba7e0745,DISK], DatanodeInfoWithStorage[127.0.0.1:35797,DS-5d3cc875-9184-4bc2-82d6-7a6f69cccb91,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1531925520-172.17.0.19-1597646281215:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46874,DS-40567ca7-528e-480e-a7b8-b1b6dbda142f,DISK], DatanodeInfoWithStorage[127.0.0.1:44684,DS-d6c5d9c2-5865-448d-b3a8-94ede30ca032,DISK], DatanodeInfoWithStorage[127.0.0.1:36241,DS-4947ae26-b9d5-44ea-af04-1070606aad6e,DISK], DatanodeInfoWithStorage[127.0.0.1:42136,DS-16c6cea5-9d64-46db-bd9d-5bd387ca0019,DISK], DatanodeInfoWithStorage[127.0.0.1:46166,DS-1b78e72b-9513-48f3-b722-2d36d63d8f15,DISK], DatanodeInfoWithStorage[127.0.0.1:37488,DS-7da970de-1254-4e38-9336-774d09fee81f,DISK], DatanodeInfoWithStorage[127.0.0.1:44167,DS-f8f6d953-558f-4ac9-829c-dbbeba7e0745,DISK], DatanodeInfoWithStorage[127.0.0.1:35797,DS-5d3cc875-9184-4bc2-82d6-7a6f69cccb91,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1727435859-172.17.0.19-1597646350482:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41460,DS-2ee24f40-c58d-4efb-8ff6-bfee6c036769,DISK], DatanodeInfoWithStorage[127.0.0.1:34242,DS-3db21e18-7f14-4e9b-bae4-c2fdd12011d1,DISK], DatanodeInfoWithStorage[127.0.0.1:39434,DS-4ed206e7-2dfc-455b-b204-fb6a4886367e,DISK], DatanodeInfoWithStorage[127.0.0.1:34780,DS-45e45c6a-edb6-41da-8cc7-c6694c254803,DISK], DatanodeInfoWithStorage[127.0.0.1:44982,DS-ebce4a06-9820-4841-9f19-3050228f05ee,DISK], DatanodeInfoWithStorage[127.0.0.1:45879,DS-543ed3ff-5e9f-4c1d-877c-31df329e1ab3,DISK], DatanodeInfoWithStorage[127.0.0.1:46869,DS-239610b5-4c74-4b4e-aae9-18bb5803e339,DISK], DatanodeInfoWithStorage[127.0.0.1:39777,DS-a96e9950-acd4-40ac-8837-8cab3de9bde5,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1727435859-172.17.0.19-1597646350482:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41460,DS-2ee24f40-c58d-4efb-8ff6-bfee6c036769,DISK], DatanodeInfoWithStorage[127.0.0.1:34242,DS-3db21e18-7f14-4e9b-bae4-c2fdd12011d1,DISK], DatanodeInfoWithStorage[127.0.0.1:39434,DS-4ed206e7-2dfc-455b-b204-fb6a4886367e,DISK], DatanodeInfoWithStorage[127.0.0.1:34780,DS-45e45c6a-edb6-41da-8cc7-c6694c254803,DISK], DatanodeInfoWithStorage[127.0.0.1:44982,DS-ebce4a06-9820-4841-9f19-3050228f05ee,DISK], DatanodeInfoWithStorage[127.0.0.1:45879,DS-543ed3ff-5e9f-4c1d-877c-31df329e1ab3,DISK], DatanodeInfoWithStorage[127.0.0.1:46869,DS-239610b5-4c74-4b4e-aae9-18bb5803e339,DISK], DatanodeInfoWithStorage[127.0.0.1:39777,DS-a96e9950-acd4-40ac-8837-8cab3de9bde5,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-215810870-172.17.0.19-1597646387559:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45562,DS-4323ce15-f9d2-45ca-a06d-6fd48f167636,DISK], DatanodeInfoWithStorage[127.0.0.1:43457,DS-96bdf08d-4015-473e-b1b8-9c74e6febb81,DISK], DatanodeInfoWithStorage[127.0.0.1:43034,DS-9fb9eaf9-4d11-446f-b3d1-98a933ea1c2a,DISK], DatanodeInfoWithStorage[127.0.0.1:44425,DS-eb0e3018-bec3-4a07-b5a0-146ff5b3bae3,DISK], DatanodeInfoWithStorage[127.0.0.1:40058,DS-ef56b5d8-46ef-4570-ae4b-0798f12ba44f,DISK], DatanodeInfoWithStorage[127.0.0.1:32877,DS-bbc13306-fc64-4435-bdf7-210586d2a656,DISK], DatanodeInfoWithStorage[127.0.0.1:41863,DS-650145b2-b5b2-411b-9533-2f6786a4345b,DISK], DatanodeInfoWithStorage[127.0.0.1:40203,DS-8346e693-49e2-4199-891c-0448890a1e06,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-215810870-172.17.0.19-1597646387559:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45562,DS-4323ce15-f9d2-45ca-a06d-6fd48f167636,DISK], DatanodeInfoWithStorage[127.0.0.1:43457,DS-96bdf08d-4015-473e-b1b8-9c74e6febb81,DISK], DatanodeInfoWithStorage[127.0.0.1:43034,DS-9fb9eaf9-4d11-446f-b3d1-98a933ea1c2a,DISK], DatanodeInfoWithStorage[127.0.0.1:44425,DS-eb0e3018-bec3-4a07-b5a0-146ff5b3bae3,DISK], DatanodeInfoWithStorage[127.0.0.1:40058,DS-ef56b5d8-46ef-4570-ae4b-0798f12ba44f,DISK], DatanodeInfoWithStorage[127.0.0.1:32877,DS-bbc13306-fc64-4435-bdf7-210586d2a656,DISK], DatanodeInfoWithStorage[127.0.0.1:41863,DS-650145b2-b5b2-411b-9533-2f6786a4345b,DISK], DatanodeInfoWithStorage[127.0.0.1:40203,DS-8346e693-49e2-4199-891c-0448890a1e06,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1236448825-172.17.0.19-1597646425628:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42780,DS-4c2406f3-5ba4-4666-bac3-76c053644fb9,DISK], DatanodeInfoWithStorage[127.0.0.1:41197,DS-99e47605-5fa0-48ab-9d27-a56f3ee3ab8b,DISK], DatanodeInfoWithStorage[127.0.0.1:40989,DS-d68298c3-4cdd-4081-a93e-05590562b502,DISK], DatanodeInfoWithStorage[127.0.0.1:42460,DS-c57b289a-adb0-4c83-9ddb-dcc1b6332419,DISK], DatanodeInfoWithStorage[127.0.0.1:36565,DS-71299b55-9f5e-4008-9365-deaf20cf086b,DISK], DatanodeInfoWithStorage[127.0.0.1:44861,DS-0030815f-caf5-459e-87fe-16cab3ce22d7,DISK], DatanodeInfoWithStorage[127.0.0.1:40399,DS-71a8c9b2-4683-4500-b190-2006ad482e68,DISK], DatanodeInfoWithStorage[127.0.0.1:43287,DS-2a0d3843-97d8-4f59-8b06-bccd0d4119ce,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1236448825-172.17.0.19-1597646425628:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42780,DS-4c2406f3-5ba4-4666-bac3-76c053644fb9,DISK], DatanodeInfoWithStorage[127.0.0.1:41197,DS-99e47605-5fa0-48ab-9d27-a56f3ee3ab8b,DISK], DatanodeInfoWithStorage[127.0.0.1:40989,DS-d68298c3-4cdd-4081-a93e-05590562b502,DISK], DatanodeInfoWithStorage[127.0.0.1:42460,DS-c57b289a-adb0-4c83-9ddb-dcc1b6332419,DISK], DatanodeInfoWithStorage[127.0.0.1:36565,DS-71299b55-9f5e-4008-9365-deaf20cf086b,DISK], DatanodeInfoWithStorage[127.0.0.1:44861,DS-0030815f-caf5-459e-87fe-16cab3ce22d7,DISK], DatanodeInfoWithStorage[127.0.0.1:40399,DS-71a8c9b2-4683-4500-b190-2006ad482e68,DISK], DatanodeInfoWithStorage[127.0.0.1:43287,DS-2a0d3843-97d8-4f59-8b06-bccd0d4119ce,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-794613781-172.17.0.19-1597646503618:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41163,DS-917e67ea-d432-46e8-a99c-128814e4bdf0,DISK], DatanodeInfoWithStorage[127.0.0.1:45386,DS-a5387d05-f990-4673-9022-7647bdf7b709,DISK], DatanodeInfoWithStorage[127.0.0.1:42067,DS-13b2f32f-552b-44eb-8d8d-44bd7fff722c,DISK], DatanodeInfoWithStorage[127.0.0.1:40049,DS-60c8928d-1e06-483f-aced-63e4644eb6a2,DISK], DatanodeInfoWithStorage[127.0.0.1:40881,DS-999d5989-43d5-4134-9e0f-c0400f780e46,DISK], DatanodeInfoWithStorage[127.0.0.1:39703,DS-5fa4692f-fa97-41cc-ae86-57c6a487f223,DISK], DatanodeInfoWithStorage[127.0.0.1:35389,DS-63c0f561-ea85-4615-9b7e-b4fdfeca526b,DISK], DatanodeInfoWithStorage[127.0.0.1:44239,DS-5d79aaa1-8ef8-4ce5-9610-cbac68538aa0,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-794613781-172.17.0.19-1597646503618:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41163,DS-917e67ea-d432-46e8-a99c-128814e4bdf0,DISK], DatanodeInfoWithStorage[127.0.0.1:45386,DS-a5387d05-f990-4673-9022-7647bdf7b709,DISK], DatanodeInfoWithStorage[127.0.0.1:42067,DS-13b2f32f-552b-44eb-8d8d-44bd7fff722c,DISK], DatanodeInfoWithStorage[127.0.0.1:40049,DS-60c8928d-1e06-483f-aced-63e4644eb6a2,DISK], DatanodeInfoWithStorage[127.0.0.1:40881,DS-999d5989-43d5-4134-9e0f-c0400f780e46,DISK], DatanodeInfoWithStorage[127.0.0.1:39703,DS-5fa4692f-fa97-41cc-ae86-57c6a487f223,DISK], DatanodeInfoWithStorage[127.0.0.1:35389,DS-63c0f561-ea85-4615-9b7e-b4fdfeca526b,DISK], DatanodeInfoWithStorage[127.0.0.1:44239,DS-5d79aaa1-8ef8-4ce5-9610-cbac68538aa0,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1487131779-172.17.0.19-1597647000387:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41327,DS-12c39a06-90a8-4476-808b-8ed7b4a7dd78,DISK], DatanodeInfoWithStorage[127.0.0.1:43145,DS-83637f5f-751e-4e93-b80f-d0f69a033bf9,DISK], DatanodeInfoWithStorage[127.0.0.1:39164,DS-9ff89979-4227-47bf-a0dd-d39e969a3a17,DISK], DatanodeInfoWithStorage[127.0.0.1:34349,DS-6dce4d08-0317-4757-a076-c1ed314ff49b,DISK], DatanodeInfoWithStorage[127.0.0.1:41503,DS-5d83c68e-c01f-48ee-a752-64b007b672c8,DISK], DatanodeInfoWithStorage[127.0.0.1:36081,DS-514bbee5-20a6-4f3b-b642-86c6983665d4,DISK], DatanodeInfoWithStorage[127.0.0.1:45907,DS-f0b98860-6db7-43f0-b7f8-86526b388756,DISK], DatanodeInfoWithStorage[127.0.0.1:43634,DS-50849d7a-2c8b-4ebb-85a7-76deaebe4e93,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1487131779-172.17.0.19-1597647000387:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41327,DS-12c39a06-90a8-4476-808b-8ed7b4a7dd78,DISK], DatanodeInfoWithStorage[127.0.0.1:43145,DS-83637f5f-751e-4e93-b80f-d0f69a033bf9,DISK], DatanodeInfoWithStorage[127.0.0.1:39164,DS-9ff89979-4227-47bf-a0dd-d39e969a3a17,DISK], DatanodeInfoWithStorage[127.0.0.1:34349,DS-6dce4d08-0317-4757-a076-c1ed314ff49b,DISK], DatanodeInfoWithStorage[127.0.0.1:41503,DS-5d83c68e-c01f-48ee-a752-64b007b672c8,DISK], DatanodeInfoWithStorage[127.0.0.1:36081,DS-514bbee5-20a6-4f3b-b642-86c6983665d4,DISK], DatanodeInfoWithStorage[127.0.0.1:45907,DS-f0b98860-6db7-43f0-b7f8-86526b388756,DISK], DatanodeInfoWithStorage[127.0.0.1:43634,DS-50849d7a-2c8b-4ebb-85a7-76deaebe4e93,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 600000
v2: 6000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery6
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1809305378-172.17.0.19-1597647095146:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33586,DS-8d39e475-af08-43d3-aec3-3876367f05a2,DISK], DatanodeInfoWithStorage[127.0.0.1:41014,DS-997c75c0-fcd4-43c7-b4bd-4ca30e9f227d,DISK], DatanodeInfoWithStorage[127.0.0.1:45307,DS-eed09766-b8ad-41fd-baa7-abce7fbb6766,DISK], DatanodeInfoWithStorage[127.0.0.1:36229,DS-7c23d29c-5e40-4995-a38d-7ec4ef59c3aa,DISK], DatanodeInfoWithStorage[127.0.0.1:36169,DS-7300924e-eed1-45ad-bb88-a240f069adad,DISK], DatanodeInfoWithStorage[127.0.0.1:36697,DS-31cb1fcb-fbd1-4193-981a-5f61f6564176,DISK], DatanodeInfoWithStorage[127.0.0.1:36314,DS-e757cf3b-018a-4d76-82f7-e3567ba8ce98,DISK], DatanodeInfoWithStorage[127.0.0.1:42965,DS-971f1f3c-7e44-48a7-8df5-2ae90dc7fb7d,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1809305378-172.17.0.19-1597647095146:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33586,DS-8d39e475-af08-43d3-aec3-3876367f05a2,DISK], DatanodeInfoWithStorage[127.0.0.1:41014,DS-997c75c0-fcd4-43c7-b4bd-4ca30e9f227d,DISK], DatanodeInfoWithStorage[127.0.0.1:45307,DS-eed09766-b8ad-41fd-baa7-abce7fbb6766,DISK], DatanodeInfoWithStorage[127.0.0.1:36229,DS-7c23d29c-5e40-4995-a38d-7ec4ef59c3aa,DISK], DatanodeInfoWithStorage[127.0.0.1:36169,DS-7300924e-eed1-45ad-bb88-a240f069adad,DISK], DatanodeInfoWithStorage[127.0.0.1:36697,DS-31cb1fcb-fbd1-4193-981a-5f61f6564176,DISK], DatanodeInfoWithStorage[127.0.0.1:36314,DS-e757cf3b-018a-4d76-82f7-e3567ba8ce98,DISK], DatanodeInfoWithStorage[127.0.0.1:42965,DS-971f1f3c-7e44-48a7-8df5-2ae90dc7fb7d,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery6(TestFileChecksum.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


v1v2 failed with probability 10 out of 50
v1v1v2v2 failed with probability 23 out of 50
result: false positive !!!
Total execution time in seconds : 5676

reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1622442407-172.17.0.9-1597636285526:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39862,DS-42af9aa8-ef0c-4b70-bf47-984a2e262fda,DISK], DatanodeInfoWithStorage[127.0.0.1:37802,DS-23f211b0-a9ed-49fa-9523-1b563ce85e89,DISK], DatanodeInfoWithStorage[127.0.0.1:34580,DS-45776fea-b508-4766-a4a2-ad291a71180d,DISK], DatanodeInfoWithStorage[127.0.0.1:35085,DS-0b8d10a3-c740-4ca2-ab10-e88116f13c5b,DISK], DatanodeInfoWithStorage[127.0.0.1:36197,DS-86bb71f8-7a49-4a84-9663-244e30e0b9c4,DISK], DatanodeInfoWithStorage[127.0.0.1:33524,DS-468628a8-9f1d-433e-a3a3-09c1add8b2cb,DISK], DatanodeInfoWithStorage[127.0.0.1:41023,DS-ff047fcd-d4d4-4a07-abba-0474254b80a2,DISK], DatanodeInfoWithStorage[127.0.0.1:35713,DS-2b84b630-e45a-4b5c-9f4e-7cc3572272cb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1622442407-172.17.0.9-1597636285526:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39862,DS-42af9aa8-ef0c-4b70-bf47-984a2e262fda,DISK], DatanodeInfoWithStorage[127.0.0.1:37802,DS-23f211b0-a9ed-49fa-9523-1b563ce85e89,DISK], DatanodeInfoWithStorage[127.0.0.1:34580,DS-45776fea-b508-4766-a4a2-ad291a71180d,DISK], DatanodeInfoWithStorage[127.0.0.1:35085,DS-0b8d10a3-c740-4ca2-ab10-e88116f13c5b,DISK], DatanodeInfoWithStorage[127.0.0.1:36197,DS-86bb71f8-7a49-4a84-9663-244e30e0b9c4,DISK], DatanodeInfoWithStorage[127.0.0.1:33524,DS-468628a8-9f1d-433e-a3a3-09c1add8b2cb,DISK], DatanodeInfoWithStorage[127.0.0.1:41023,DS-ff047fcd-d4d4-4a07-abba-0474254b80a2,DISK], DatanodeInfoWithStorage[127.0.0.1:35713,DS-2b84b630-e45a-4b5c-9f4e-7cc3572272cb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-91202657-172.17.0.9-1597636361134:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42215,DS-eebdd252-4a21-4b3f-88fd-ac96fd1b7777,DISK], DatanodeInfoWithStorage[127.0.0.1:45069,DS-6ed6f68d-37b4-4a64-8645-4edef793579e,DISK], DatanodeInfoWithStorage[127.0.0.1:37765,DS-9f60ec30-e935-452d-9f86-d342a39cf2bd,DISK], DatanodeInfoWithStorage[127.0.0.1:34561,DS-3b0eb295-e283-415e-a8e6-3bacbc2b3713,DISK], DatanodeInfoWithStorage[127.0.0.1:37049,DS-6bc2c0be-6791-44ae-a140-f8162e350306,DISK], DatanodeInfoWithStorage[127.0.0.1:40301,DS-b4b914ab-7378-498a-9d28-2e10b0f19a69,DISK], DatanodeInfoWithStorage[127.0.0.1:41668,DS-de491be6-db1d-4ac3-934c-919dc0ae4c33,DISK], DatanodeInfoWithStorage[127.0.0.1:45792,DS-572dd036-e036-472f-b12d-f92ccedadac7,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-91202657-172.17.0.9-1597636361134:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42215,DS-eebdd252-4a21-4b3f-88fd-ac96fd1b7777,DISK], DatanodeInfoWithStorage[127.0.0.1:45069,DS-6ed6f68d-37b4-4a64-8645-4edef793579e,DISK], DatanodeInfoWithStorage[127.0.0.1:37765,DS-9f60ec30-e935-452d-9f86-d342a39cf2bd,DISK], DatanodeInfoWithStorage[127.0.0.1:34561,DS-3b0eb295-e283-415e-a8e6-3bacbc2b3713,DISK], DatanodeInfoWithStorage[127.0.0.1:37049,DS-6bc2c0be-6791-44ae-a140-f8162e350306,DISK], DatanodeInfoWithStorage[127.0.0.1:40301,DS-b4b914ab-7378-498a-9d28-2e10b0f19a69,DISK], DatanodeInfoWithStorage[127.0.0.1:41668,DS-de491be6-db1d-4ac3-934c-919dc0ae4c33,DISK], DatanodeInfoWithStorage[127.0.0.1:45792,DS-572dd036-e036-472f-b12d-f92ccedadac7,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1987384641-172.17.0.9-1597636399538:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34672,DS-cfbab1ab-e6c4-4d4e-93e4-6686e14283dd,DISK], DatanodeInfoWithStorage[127.0.0.1:44142,DS-4d8bddcf-78c5-47a2-bfee-73cd228fcbf7,DISK], DatanodeInfoWithStorage[127.0.0.1:45317,DS-ef61a8c4-647e-4ed8-a685-0b20ba745415,DISK], DatanodeInfoWithStorage[127.0.0.1:45923,DS-8bca3607-e527-4dc6-a57a-44467eed6257,DISK], DatanodeInfoWithStorage[127.0.0.1:36962,DS-0eef7e3e-7372-4ce1-9b1a-0a15a04b8f9b,DISK], DatanodeInfoWithStorage[127.0.0.1:41646,DS-e5f4e8e7-9694-4066-b7b7-cc0c96ef5b34,DISK], DatanodeInfoWithStorage[127.0.0.1:36937,DS-4161f833-fc54-49f9-a7f9-2b3f25188829,DISK], DatanodeInfoWithStorage[127.0.0.1:42430,DS-c86413c6-043f-4c2f-a241-c1496bb76c8d,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1987384641-172.17.0.9-1597636399538:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34672,DS-cfbab1ab-e6c4-4d4e-93e4-6686e14283dd,DISK], DatanodeInfoWithStorage[127.0.0.1:44142,DS-4d8bddcf-78c5-47a2-bfee-73cd228fcbf7,DISK], DatanodeInfoWithStorage[127.0.0.1:45317,DS-ef61a8c4-647e-4ed8-a685-0b20ba745415,DISK], DatanodeInfoWithStorage[127.0.0.1:45923,DS-8bca3607-e527-4dc6-a57a-44467eed6257,DISK], DatanodeInfoWithStorage[127.0.0.1:36962,DS-0eef7e3e-7372-4ce1-9b1a-0a15a04b8f9b,DISK], DatanodeInfoWithStorage[127.0.0.1:41646,DS-e5f4e8e7-9694-4066-b7b7-cc0c96ef5b34,DISK], DatanodeInfoWithStorage[127.0.0.1:36937,DS-4161f833-fc54-49f9-a7f9-2b3f25188829,DISK], DatanodeInfoWithStorage[127.0.0.1:42430,DS-c86413c6-043f-4c2f-a241-c1496bb76c8d,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1164549044-172.17.0.9-1597636434194:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43094,DS-3e00c025-919e-42bb-b71c-66f019c5d463,DISK], DatanodeInfoWithStorage[127.0.0.1:46523,DS-2f9ea76b-ce35-438c-b28a-842fd4fa1163,DISK], DatanodeInfoWithStorage[127.0.0.1:41251,DS-e0bc31da-2348-4e1c-9a1d-5501ba500783,DISK], DatanodeInfoWithStorage[127.0.0.1:44213,DS-cca03b20-b211-4afb-a21a-de2d99f2903c,DISK], DatanodeInfoWithStorage[127.0.0.1:40546,DS-64c34d01-b422-4683-b71a-75bf1533b2d4,DISK], DatanodeInfoWithStorage[127.0.0.1:40845,DS-f78814b1-552a-462e-bb5b-e1ca1567adae,DISK], DatanodeInfoWithStorage[127.0.0.1:40619,DS-b68bd20f-15f9-48b2-84d9-0f52fa1563bf,DISK], DatanodeInfoWithStorage[127.0.0.1:37867,DS-a0d901d0-337e-4158-ac7b-38ca152951e6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1164549044-172.17.0.9-1597636434194:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43094,DS-3e00c025-919e-42bb-b71c-66f019c5d463,DISK], DatanodeInfoWithStorage[127.0.0.1:46523,DS-2f9ea76b-ce35-438c-b28a-842fd4fa1163,DISK], DatanodeInfoWithStorage[127.0.0.1:41251,DS-e0bc31da-2348-4e1c-9a1d-5501ba500783,DISK], DatanodeInfoWithStorage[127.0.0.1:44213,DS-cca03b20-b211-4afb-a21a-de2d99f2903c,DISK], DatanodeInfoWithStorage[127.0.0.1:40546,DS-64c34d01-b422-4683-b71a-75bf1533b2d4,DISK], DatanodeInfoWithStorage[127.0.0.1:40845,DS-f78814b1-552a-462e-bb5b-e1ca1567adae,DISK], DatanodeInfoWithStorage[127.0.0.1:40619,DS-b68bd20f-15f9-48b2-84d9-0f52fa1563bf,DISK], DatanodeInfoWithStorage[127.0.0.1:37867,DS-a0d901d0-337e-4158-ac7b-38ca152951e6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1605992144-172.17.0.9-1597636471461:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32827,DS-7db65a20-732a-4cee-b77a-c4365b31d714,DISK], DatanodeInfoWithStorage[127.0.0.1:41717,DS-e151875b-99bf-4186-b653-931067802ce2,DISK], DatanodeInfoWithStorage[127.0.0.1:37496,DS-ee44903e-988d-42b5-9a22-f4e0ec3ea19b,DISK], DatanodeInfoWithStorage[127.0.0.1:36032,DS-f0111f31-48b0-43ad-91ac-e392520e6d82,DISK], DatanodeInfoWithStorage[127.0.0.1:33447,DS-e16be1ce-c1b9-42c2-862e-e25b09026a2d,DISK], DatanodeInfoWithStorage[127.0.0.1:38736,DS-1295e289-da04-425c-82ce-161a8467dee0,DISK], DatanodeInfoWithStorage[127.0.0.1:44004,DS-7b7e9164-0482-41d6-b6d0-dba18d7371ec,DISK], DatanodeInfoWithStorage[127.0.0.1:36742,DS-89bbf69c-d6ec-4761-b5a5-ebd3dc8de9e2,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1605992144-172.17.0.9-1597636471461:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32827,DS-7db65a20-732a-4cee-b77a-c4365b31d714,DISK], DatanodeInfoWithStorage[127.0.0.1:41717,DS-e151875b-99bf-4186-b653-931067802ce2,DISK], DatanodeInfoWithStorage[127.0.0.1:37496,DS-ee44903e-988d-42b5-9a22-f4e0ec3ea19b,DISK], DatanodeInfoWithStorage[127.0.0.1:36032,DS-f0111f31-48b0-43ad-91ac-e392520e6d82,DISK], DatanodeInfoWithStorage[127.0.0.1:33447,DS-e16be1ce-c1b9-42c2-862e-e25b09026a2d,DISK], DatanodeInfoWithStorage[127.0.0.1:38736,DS-1295e289-da04-425c-82ce-161a8467dee0,DISK], DatanodeInfoWithStorage[127.0.0.1:44004,DS-7b7e9164-0482-41d6-b6d0-dba18d7371ec,DISK], DatanodeInfoWithStorage[127.0.0.1:36742,DS-89bbf69c-d6ec-4761-b5a5-ebd3dc8de9e2,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-742637719-172.17.0.9-1597636512601:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46608,DS-f32d757d-6bf6-4660-acaf-a659fa97452d,DISK], DatanodeInfoWithStorage[127.0.0.1:46708,DS-1cd0a919-2e95-4761-b94e-21d8af9cb2c1,DISK], DatanodeInfoWithStorage[127.0.0.1:40770,DS-71dd932a-11f4-48a9-a297-a06dafa9b87b,DISK], DatanodeInfoWithStorage[127.0.0.1:44294,DS-d3b9bdec-d24a-427a-9fd3-016faf884a26,DISK], DatanodeInfoWithStorage[127.0.0.1:36775,DS-54c31823-b538-40f0-a19d-73fcb5ac67cb,DISK], DatanodeInfoWithStorage[127.0.0.1:37423,DS-3d2f8f2b-5cf2-4a3a-aa0d-fe8994b1372c,DISK], DatanodeInfoWithStorage[127.0.0.1:33545,DS-ec5dc445-7f00-4940-b2bc-bb3aa76d827f,DISK], DatanodeInfoWithStorage[127.0.0.1:34377,DS-1b71b979-4ac6-4dd3-89b3-531080bbbf9a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-742637719-172.17.0.9-1597636512601:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46608,DS-f32d757d-6bf6-4660-acaf-a659fa97452d,DISK], DatanodeInfoWithStorage[127.0.0.1:46708,DS-1cd0a919-2e95-4761-b94e-21d8af9cb2c1,DISK], DatanodeInfoWithStorage[127.0.0.1:40770,DS-71dd932a-11f4-48a9-a297-a06dafa9b87b,DISK], DatanodeInfoWithStorage[127.0.0.1:44294,DS-d3b9bdec-d24a-427a-9fd3-016faf884a26,DISK], DatanodeInfoWithStorage[127.0.0.1:36775,DS-54c31823-b538-40f0-a19d-73fcb5ac67cb,DISK], DatanodeInfoWithStorage[127.0.0.1:37423,DS-3d2f8f2b-5cf2-4a3a-aa0d-fe8994b1372c,DISK], DatanodeInfoWithStorage[127.0.0.1:33545,DS-ec5dc445-7f00-4940-b2bc-bb3aa76d827f,DISK], DatanodeInfoWithStorage[127.0.0.1:34377,DS-1b71b979-4ac6-4dd3-89b3-531080bbbf9a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1526679908-172.17.0.9-1597636738858:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43185,DS-58bfae47-e8b6-45ae-94bc-a2e0de9d80ef,DISK], DatanodeInfoWithStorage[127.0.0.1:46167,DS-70fc09d4-971e-4ac7-a0f8-c2a743c2349d,DISK], DatanodeInfoWithStorage[127.0.0.1:46047,DS-30ab09cc-e3a7-40aa-8e3f-aa2bd481a1f2,DISK], DatanodeInfoWithStorage[127.0.0.1:35212,DS-e3a78893-6e11-4c20-b077-30d6cf608192,DISK], DatanodeInfoWithStorage[127.0.0.1:34853,DS-6c2cea8a-3bf2-4371-b0dc-4ddecf22424b,DISK], DatanodeInfoWithStorage[127.0.0.1:43441,DS-bc6b3284-3087-431f-a10a-6b04e62cb3cc,DISK], DatanodeInfoWithStorage[127.0.0.1:44248,DS-3b594f2c-7445-4992-9301-065a4d027dd7,DISK], DatanodeInfoWithStorage[127.0.0.1:46273,DS-8a74afb8-c973-49c8-895f-7fc7b3f4fdbe,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1526679908-172.17.0.9-1597636738858:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43185,DS-58bfae47-e8b6-45ae-94bc-a2e0de9d80ef,DISK], DatanodeInfoWithStorage[127.0.0.1:46167,DS-70fc09d4-971e-4ac7-a0f8-c2a743c2349d,DISK], DatanodeInfoWithStorage[127.0.0.1:46047,DS-30ab09cc-e3a7-40aa-8e3f-aa2bd481a1f2,DISK], DatanodeInfoWithStorage[127.0.0.1:35212,DS-e3a78893-6e11-4c20-b077-30d6cf608192,DISK], DatanodeInfoWithStorage[127.0.0.1:34853,DS-6c2cea8a-3bf2-4371-b0dc-4ddecf22424b,DISK], DatanodeInfoWithStorage[127.0.0.1:43441,DS-bc6b3284-3087-431f-a10a-6b04e62cb3cc,DISK], DatanodeInfoWithStorage[127.0.0.1:44248,DS-3b594f2c-7445-4992-9301-065a4d027dd7,DISK], DatanodeInfoWithStorage[127.0.0.1:46273,DS-8a74afb8-c973-49c8-895f-7fc7b3f4fdbe,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-769536866-172.17.0.9-1597636781249:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36533,DS-4875a8b0-63cc-433a-90b4-001814352fbd,DISK], DatanodeInfoWithStorage[127.0.0.1:40388,DS-4e624e7f-bbd2-4a33-8758-4b280b7ec5c5,DISK], DatanodeInfoWithStorage[127.0.0.1:44174,DS-d848891b-f805-4f2f-a8a6-ed7f9a10666d,DISK], DatanodeInfoWithStorage[127.0.0.1:45908,DS-9244f258-3006-4df8-a305-ccf7df84a0ca,DISK], DatanodeInfoWithStorage[127.0.0.1:38231,DS-182db61d-9eb1-4817-8bad-1c967703dc4a,DISK], DatanodeInfoWithStorage[127.0.0.1:35432,DS-a4b21b1c-622f-44aa-8ce9-10bb9108b307,DISK], DatanodeInfoWithStorage[127.0.0.1:38079,DS-c9af1bd2-bc96-4590-8111-67bdd629f43d,DISK], DatanodeInfoWithStorage[127.0.0.1:46180,DS-6dd36eaf-cfcb-416b-92bb-8a870a091ac0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-769536866-172.17.0.9-1597636781249:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36533,DS-4875a8b0-63cc-433a-90b4-001814352fbd,DISK], DatanodeInfoWithStorage[127.0.0.1:40388,DS-4e624e7f-bbd2-4a33-8758-4b280b7ec5c5,DISK], DatanodeInfoWithStorage[127.0.0.1:44174,DS-d848891b-f805-4f2f-a8a6-ed7f9a10666d,DISK], DatanodeInfoWithStorage[127.0.0.1:45908,DS-9244f258-3006-4df8-a305-ccf7df84a0ca,DISK], DatanodeInfoWithStorage[127.0.0.1:38231,DS-182db61d-9eb1-4817-8bad-1c967703dc4a,DISK], DatanodeInfoWithStorage[127.0.0.1:35432,DS-a4b21b1c-622f-44aa-8ce9-10bb9108b307,DISK], DatanodeInfoWithStorage[127.0.0.1:38079,DS-c9af1bd2-bc96-4590-8111-67bdd629f43d,DISK], DatanodeInfoWithStorage[127.0.0.1:46180,DS-6dd36eaf-cfcb-416b-92bb-8a870a091ac0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1343356194-172.17.0.9-1597636971286:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40707,DS-6d6c0acc-c2a8-4262-90fe-641d24df1dd0,DISK], DatanodeInfoWithStorage[127.0.0.1:45742,DS-7b19d345-57cb-4579-9fce-21f459e07564,DISK], DatanodeInfoWithStorage[127.0.0.1:44032,DS-21abc830-eacd-4fc0-bcfe-12a70952248b,DISK], DatanodeInfoWithStorage[127.0.0.1:33125,DS-46e67c1c-043e-4205-b016-ea7403164dbf,DISK], DatanodeInfoWithStorage[127.0.0.1:34181,DS-b04a92f0-28e1-4e1a-ad23-e8be20b12ec6,DISK], DatanodeInfoWithStorage[127.0.0.1:44041,DS-89204ade-212e-4e36-88c5-2c7dab54a993,DISK], DatanodeInfoWithStorage[127.0.0.1:32935,DS-145c4fbe-dd7c-4135-8dc3-748589ac499e,DISK], DatanodeInfoWithStorage[127.0.0.1:42221,DS-25c7f809-fac7-4c38-bb01-cc8c65d54e14,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1343356194-172.17.0.9-1597636971286:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40707,DS-6d6c0acc-c2a8-4262-90fe-641d24df1dd0,DISK], DatanodeInfoWithStorage[127.0.0.1:45742,DS-7b19d345-57cb-4579-9fce-21f459e07564,DISK], DatanodeInfoWithStorage[127.0.0.1:44032,DS-21abc830-eacd-4fc0-bcfe-12a70952248b,DISK], DatanodeInfoWithStorage[127.0.0.1:33125,DS-46e67c1c-043e-4205-b016-ea7403164dbf,DISK], DatanodeInfoWithStorage[127.0.0.1:34181,DS-b04a92f0-28e1-4e1a-ad23-e8be20b12ec6,DISK], DatanodeInfoWithStorage[127.0.0.1:44041,DS-89204ade-212e-4e36-88c5-2c7dab54a993,DISK], DatanodeInfoWithStorage[127.0.0.1:32935,DS-145c4fbe-dd7c-4135-8dc3-748589ac499e,DISK], DatanodeInfoWithStorage[127.0.0.1:42221,DS-25c7f809-fac7-4c38-bb01-cc8c65d54e14,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2112130791-172.17.0.9-1597637206605:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45587,DS-d047f12b-4d09-4b9b-8285-aa49250e2bf1,DISK], DatanodeInfoWithStorage[127.0.0.1:39412,DS-61567954-9995-4cfc-a0f4-977da9ecc642,DISK], DatanodeInfoWithStorage[127.0.0.1:33205,DS-770e7f94-a973-4642-b5da-8818cebedc92,DISK], DatanodeInfoWithStorage[127.0.0.1:38452,DS-84bcbd5b-910a-46ba-91e2-029c336d40d1,DISK], DatanodeInfoWithStorage[127.0.0.1:36196,DS-5d1fadda-7fde-4b01-80d6-5f8bfa248d8d,DISK], DatanodeInfoWithStorage[127.0.0.1:37402,DS-09c684c2-8ae6-473e-8dee-e08bce4e2716,DISK], DatanodeInfoWithStorage[127.0.0.1:35422,DS-0726522d-aaf9-4759-96c8-f8c3bffcffda,DISK], DatanodeInfoWithStorage[127.0.0.1:43975,DS-4f02c964-53cf-45c2-9abf-7262b6b57849,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2112130791-172.17.0.9-1597637206605:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45587,DS-d047f12b-4d09-4b9b-8285-aa49250e2bf1,DISK], DatanodeInfoWithStorage[127.0.0.1:39412,DS-61567954-9995-4cfc-a0f4-977da9ecc642,DISK], DatanodeInfoWithStorage[127.0.0.1:33205,DS-770e7f94-a973-4642-b5da-8818cebedc92,DISK], DatanodeInfoWithStorage[127.0.0.1:38452,DS-84bcbd5b-910a-46ba-91e2-029c336d40d1,DISK], DatanodeInfoWithStorage[127.0.0.1:36196,DS-5d1fadda-7fde-4b01-80d6-5f8bfa248d8d,DISK], DatanodeInfoWithStorage[127.0.0.1:37402,DS-09c684c2-8ae6-473e-8dee-e08bce4e2716,DISK], DatanodeInfoWithStorage[127.0.0.1:35422,DS-0726522d-aaf9-4759-96c8-f8c3bffcffda,DISK], DatanodeInfoWithStorage[127.0.0.1:43975,DS-4f02c964-53cf-45c2-9abf-7262b6b57849,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-970015869-172.17.0.9-1597637248582:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45953,DS-77fd9120-aa87-4b07-8340-a898622f3e36,DISK], DatanodeInfoWithStorage[127.0.0.1:40779,DS-b91f5eb3-698b-4d23-804e-61b1ffe4865e,DISK], DatanodeInfoWithStorage[127.0.0.1:43307,DS-0ecdceae-351a-462b-a320-a50bc8b22c7a,DISK], DatanodeInfoWithStorage[127.0.0.1:45843,DS-2ef8c3be-45f3-4a3f-acab-5deb333f938d,DISK], DatanodeInfoWithStorage[127.0.0.1:44257,DS-08512be3-e20f-4918-a692-18b243f544f8,DISK], DatanodeInfoWithStorage[127.0.0.1:40110,DS-2a22fcf4-7441-4b9b-9ad0-2eb80152de12,DISK], DatanodeInfoWithStorage[127.0.0.1:40335,DS-eae4c513-2acc-4275-a27a-b77dbd9a8d52,DISK], DatanodeInfoWithStorage[127.0.0.1:44821,DS-b6094baa-af15-4a96-bcc4-9dac04d862af,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-970015869-172.17.0.9-1597637248582:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45953,DS-77fd9120-aa87-4b07-8340-a898622f3e36,DISK], DatanodeInfoWithStorage[127.0.0.1:40779,DS-b91f5eb3-698b-4d23-804e-61b1ffe4865e,DISK], DatanodeInfoWithStorage[127.0.0.1:43307,DS-0ecdceae-351a-462b-a320-a50bc8b22c7a,DISK], DatanodeInfoWithStorage[127.0.0.1:45843,DS-2ef8c3be-45f3-4a3f-acab-5deb333f938d,DISK], DatanodeInfoWithStorage[127.0.0.1:44257,DS-08512be3-e20f-4918-a692-18b243f544f8,DISK], DatanodeInfoWithStorage[127.0.0.1:40110,DS-2a22fcf4-7441-4b9b-9ad0-2eb80152de12,DISK], DatanodeInfoWithStorage[127.0.0.1:40335,DS-eae4c513-2acc-4275-a27a-b77dbd9a8d52,DISK], DatanodeInfoWithStorage[127.0.0.1:44821,DS-b6094baa-af15-4a96-bcc4-9dac04d862af,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-488881073-172.17.0.9-1597637453777:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34618,DS-71d206a9-2289-4f14-9f2e-16f028207b5d,DISK], DatanodeInfoWithStorage[127.0.0.1:45689,DS-ddc52156-2728-4c3d-9c53-01fe795fbccc,DISK], DatanodeInfoWithStorage[127.0.0.1:42850,DS-620c326f-d2c7-4a05-abc5-94da1a43159d,DISK], DatanodeInfoWithStorage[127.0.0.1:45787,DS-cd9c7f96-7119-4604-90d7-f27a44efe46d,DISK], DatanodeInfoWithStorage[127.0.0.1:36135,DS-3306c582-198c-495e-a1ea-dcd6a250ed46,DISK], DatanodeInfoWithStorage[127.0.0.1:36218,DS-6fc6f062-9e81-492a-8b99-a84ad21ce491,DISK], DatanodeInfoWithStorage[127.0.0.1:46870,DS-a984a389-5899-4369-8b6b-d4304ccb095d,DISK], DatanodeInfoWithStorage[127.0.0.1:43448,DS-7f4d71aa-70c8-45b7-ad35-d611fe8937af,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-488881073-172.17.0.9-1597637453777:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34618,DS-71d206a9-2289-4f14-9f2e-16f028207b5d,DISK], DatanodeInfoWithStorage[127.0.0.1:45689,DS-ddc52156-2728-4c3d-9c53-01fe795fbccc,DISK], DatanodeInfoWithStorage[127.0.0.1:42850,DS-620c326f-d2c7-4a05-abc5-94da1a43159d,DISK], DatanodeInfoWithStorage[127.0.0.1:45787,DS-cd9c7f96-7119-4604-90d7-f27a44efe46d,DISK], DatanodeInfoWithStorage[127.0.0.1:36135,DS-3306c582-198c-495e-a1ea-dcd6a250ed46,DISK], DatanodeInfoWithStorage[127.0.0.1:36218,DS-6fc6f062-9e81-492a-8b99-a84ad21ce491,DISK], DatanodeInfoWithStorage[127.0.0.1:46870,DS-a984a389-5899-4369-8b6b-d4304ccb095d,DISK], DatanodeInfoWithStorage[127.0.0.1:43448,DS-7f4d71aa-70c8-45b7-ad35-d611fe8937af,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1780326686-172.17.0.9-1597637530812:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33738,DS-3705c15d-7e0f-4274-8146-b2c4b1cf1896,DISK], DatanodeInfoWithStorage[127.0.0.1:43922,DS-9c90699b-422c-4c98-8e27-bb5161fc0c1a,DISK], DatanodeInfoWithStorage[127.0.0.1:38210,DS-1926e22a-15c0-43c3-bb04-d1b2240f3742,DISK], DatanodeInfoWithStorage[127.0.0.1:36633,DS-52a121f0-4d02-4b8a-95ba-27a35fadfc09,DISK], DatanodeInfoWithStorage[127.0.0.1:43744,DS-91fa1778-50ef-41ec-9a65-bdadb530a0fc,DISK], DatanodeInfoWithStorage[127.0.0.1:35047,DS-b052b1d4-7265-45f9-a590-6df87b916b2b,DISK], DatanodeInfoWithStorage[127.0.0.1:45643,DS-407c7b4d-0565-404a-8f6d-3798244d3fc4,DISK], DatanodeInfoWithStorage[127.0.0.1:45041,DS-9d682c51-58b1-4a60-8c8c-6575ee76fbb0,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1780326686-172.17.0.9-1597637530812:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33738,DS-3705c15d-7e0f-4274-8146-b2c4b1cf1896,DISK], DatanodeInfoWithStorage[127.0.0.1:43922,DS-9c90699b-422c-4c98-8e27-bb5161fc0c1a,DISK], DatanodeInfoWithStorage[127.0.0.1:38210,DS-1926e22a-15c0-43c3-bb04-d1b2240f3742,DISK], DatanodeInfoWithStorage[127.0.0.1:36633,DS-52a121f0-4d02-4b8a-95ba-27a35fadfc09,DISK], DatanodeInfoWithStorage[127.0.0.1:43744,DS-91fa1778-50ef-41ec-9a65-bdadb530a0fc,DISK], DatanodeInfoWithStorage[127.0.0.1:35047,DS-b052b1d4-7265-45f9-a590-6df87b916b2b,DISK], DatanodeInfoWithStorage[127.0.0.1:45643,DS-407c7b4d-0565-404a-8f6d-3798244d3fc4,DISK], DatanodeInfoWithStorage[127.0.0.1:45041,DS-9d682c51-58b1-4a60-8c8c-6575ee76fbb0,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-536080169-172.17.0.9-1597637642139:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41419,DS-2a62f4be-85c7-4fb3-904f-33744a8bd7ec,DISK], DatanodeInfoWithStorage[127.0.0.1:35411,DS-147a9035-5531-4bd3-b45d-eee3ff34de6d,DISK], DatanodeInfoWithStorage[127.0.0.1:34067,DS-aa680c69-4651-4d68-824f-b09b952cd815,DISK], DatanodeInfoWithStorage[127.0.0.1:42687,DS-f1258a85-7eac-4653-8789-291cc4595f12,DISK], DatanodeInfoWithStorage[127.0.0.1:36735,DS-ff4aa139-ab92-4b22-b08d-e28e5672491e,DISK], DatanodeInfoWithStorage[127.0.0.1:42885,DS-cadd16d4-eb4b-4021-8097-11809b69548e,DISK], DatanodeInfoWithStorage[127.0.0.1:43139,DS-bd01dcaf-bd61-43f0-9dd4-544b7adcee6f,DISK], DatanodeInfoWithStorage[127.0.0.1:45953,DS-f0067316-2ed9-4002-a84a-69d69474091d,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-536080169-172.17.0.9-1597637642139:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41419,DS-2a62f4be-85c7-4fb3-904f-33744a8bd7ec,DISK], DatanodeInfoWithStorage[127.0.0.1:35411,DS-147a9035-5531-4bd3-b45d-eee3ff34de6d,DISK], DatanodeInfoWithStorage[127.0.0.1:34067,DS-aa680c69-4651-4d68-824f-b09b952cd815,DISK], DatanodeInfoWithStorage[127.0.0.1:42687,DS-f1258a85-7eac-4653-8789-291cc4595f12,DISK], DatanodeInfoWithStorage[127.0.0.1:36735,DS-ff4aa139-ab92-4b22-b08d-e28e5672491e,DISK], DatanodeInfoWithStorage[127.0.0.1:42885,DS-cadd16d4-eb4b-4021-8097-11809b69548e,DISK], DatanodeInfoWithStorage[127.0.0.1:43139,DS-bd01dcaf-bd61-43f0-9dd4-544b7adcee6f,DISK], DatanodeInfoWithStorage[127.0.0.1:45953,DS-f0067316-2ed9-4002-a84a-69d69474091d,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1443939548-172.17.0.9-1597638019459:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35063,DS-d8ebe761-d914-40b3-b959-bdd3c9d0a294,DISK], DatanodeInfoWithStorage[127.0.0.1:44412,DS-d6e22152-f51a-4c89-ae34-712ec6aee8d5,DISK], DatanodeInfoWithStorage[127.0.0.1:42939,DS-ef9a9b11-596e-4105-8d10-ff09133092d4,DISK], DatanodeInfoWithStorage[127.0.0.1:36047,DS-03125131-147f-4847-80d7-a979e40d0db6,DISK], DatanodeInfoWithStorage[127.0.0.1:42757,DS-77ceaf08-5a59-48a9-b456-cfa9aa41f87c,DISK], DatanodeInfoWithStorage[127.0.0.1:35261,DS-9662b6d9-c81c-4ec9-8ced-6100d4dd8c32,DISK], DatanodeInfoWithStorage[127.0.0.1:35874,DS-82051b26-cbae-47f2-81c0-5728ef5bbd87,DISK], DatanodeInfoWithStorage[127.0.0.1:40949,DS-17d526ac-b7f0-461c-be2b-f4c9a56c38f0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1443939548-172.17.0.9-1597638019459:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35063,DS-d8ebe761-d914-40b3-b959-bdd3c9d0a294,DISK], DatanodeInfoWithStorage[127.0.0.1:44412,DS-d6e22152-f51a-4c89-ae34-712ec6aee8d5,DISK], DatanodeInfoWithStorage[127.0.0.1:42939,DS-ef9a9b11-596e-4105-8d10-ff09133092d4,DISK], DatanodeInfoWithStorage[127.0.0.1:36047,DS-03125131-147f-4847-80d7-a979e40d0db6,DISK], DatanodeInfoWithStorage[127.0.0.1:42757,DS-77ceaf08-5a59-48a9-b456-cfa9aa41f87c,DISK], DatanodeInfoWithStorage[127.0.0.1:35261,DS-9662b6d9-c81c-4ec9-8ced-6100d4dd8c32,DISK], DatanodeInfoWithStorage[127.0.0.1:35874,DS-82051b26-cbae-47f2-81c0-5728ef5bbd87,DISK], DatanodeInfoWithStorage[127.0.0.1:40949,DS-17d526ac-b7f0-461c-be2b-f4c9a56c38f0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1286310413-172.17.0.9-1597638092278:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40351,DS-76645589-0e39-4ed7-97cd-50557aea8da7,DISK], DatanodeInfoWithStorage[127.0.0.1:37788,DS-b618b44a-6416-4f1d-bd63-5a989e027c5b,DISK], DatanodeInfoWithStorage[127.0.0.1:33855,DS-da674794-4fe5-4e2f-ab5c-fbb22874e3b1,DISK], DatanodeInfoWithStorage[127.0.0.1:37230,DS-fef10016-a841-406c-a96d-3fd3d3f8ef2e,DISK], DatanodeInfoWithStorage[127.0.0.1:37580,DS-c114c6c6-2c71-4cd0-953e-eb3af429df66,DISK], DatanodeInfoWithStorage[127.0.0.1:34005,DS-f47301ba-0343-4590-80b6-4431cc0a132d,DISK], DatanodeInfoWithStorage[127.0.0.1:33754,DS-8c0261e4-d629-40bd-b2b9-b68e1f14fe67,DISK], DatanodeInfoWithStorage[127.0.0.1:33026,DS-f69edc39-1237-4f7c-8c42-55603815852f,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1286310413-172.17.0.9-1597638092278:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40351,DS-76645589-0e39-4ed7-97cd-50557aea8da7,DISK], DatanodeInfoWithStorage[127.0.0.1:37788,DS-b618b44a-6416-4f1d-bd63-5a989e027c5b,DISK], DatanodeInfoWithStorage[127.0.0.1:33855,DS-da674794-4fe5-4e2f-ab5c-fbb22874e3b1,DISK], DatanodeInfoWithStorage[127.0.0.1:37230,DS-fef10016-a841-406c-a96d-3fd3d3f8ef2e,DISK], DatanodeInfoWithStorage[127.0.0.1:37580,DS-c114c6c6-2c71-4cd0-953e-eb3af429df66,DISK], DatanodeInfoWithStorage[127.0.0.1:34005,DS-f47301ba-0343-4590-80b6-4431cc0a132d,DISK], DatanodeInfoWithStorage[127.0.0.1:33754,DS-8c0261e4-d629-40bd-b2b9-b68e1f14fe67,DISK], DatanodeInfoWithStorage[127.0.0.1:33026,DS-f69edc39-1237-4f7c-8c42-55603815852f,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1538917808-172.17.0.9-1597638133096:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41347,DS-74fd4754-d81d-4e1c-b838-209d61c9bd8e,DISK], DatanodeInfoWithStorage[127.0.0.1:42398,DS-10a8e583-7929-4cb2-9802-578472e6220c,DISK], DatanodeInfoWithStorage[127.0.0.1:38840,DS-2d45700b-988e-4cc7-8a7d-e71e6358296d,DISK], DatanodeInfoWithStorage[127.0.0.1:38395,DS-9d2c95cd-5d2f-4ba2-b3e8-b9c2da67f5de,DISK], DatanodeInfoWithStorage[127.0.0.1:39242,DS-c232cddd-8ef9-44e9-8146-a7b48b3cf41b,DISK], DatanodeInfoWithStorage[127.0.0.1:33011,DS-1d26086b-1fef-4bdd-bb41-f395b8b6fbd5,DISK], DatanodeInfoWithStorage[127.0.0.1:44297,DS-38001b2b-1b9e-49a9-96e6-7ea48c82044d,DISK], DatanodeInfoWithStorage[127.0.0.1:40499,DS-14f5ad88-8c3a-4638-a840-3be8d4052586,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1538917808-172.17.0.9-1597638133096:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41347,DS-74fd4754-d81d-4e1c-b838-209d61c9bd8e,DISK], DatanodeInfoWithStorage[127.0.0.1:42398,DS-10a8e583-7929-4cb2-9802-578472e6220c,DISK], DatanodeInfoWithStorage[127.0.0.1:38840,DS-2d45700b-988e-4cc7-8a7d-e71e6358296d,DISK], DatanodeInfoWithStorage[127.0.0.1:38395,DS-9d2c95cd-5d2f-4ba2-b3e8-b9c2da67f5de,DISK], DatanodeInfoWithStorage[127.0.0.1:39242,DS-c232cddd-8ef9-44e9-8146-a7b48b3cf41b,DISK], DatanodeInfoWithStorage[127.0.0.1:33011,DS-1d26086b-1fef-4bdd-bb41-f395b8b6fbd5,DISK], DatanodeInfoWithStorage[127.0.0.1:44297,DS-38001b2b-1b9e-49a9-96e6-7ea48c82044d,DISK], DatanodeInfoWithStorage[127.0.0.1:40499,DS-14f5ad88-8c3a-4638-a840-3be8d4052586,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-310995068-172.17.0.9-1597638397321:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37435,DS-99cc8db9-593f-444d-b174-84f45c08808d,DISK], DatanodeInfoWithStorage[127.0.0.1:34087,DS-1eed2328-61cd-4fac-9c3b-1e027ad8159a,DISK], DatanodeInfoWithStorage[127.0.0.1:44021,DS-26520692-d4be-45e0-8dcc-c7f9c0285cbf,DISK], DatanodeInfoWithStorage[127.0.0.1:44439,DS-662e8a14-fee6-4837-94bf-e76fd46d9c06,DISK], DatanodeInfoWithStorage[127.0.0.1:40525,DS-1f53535b-9d62-4a28-8908-dc458c16a878,DISK], DatanodeInfoWithStorage[127.0.0.1:37430,DS-b8c33500-5a23-4cab-b1d9-f721946e99a3,DISK], DatanodeInfoWithStorage[127.0.0.1:33933,DS-336b24bc-fa6f-4673-9263-5175ca752065,DISK], DatanodeInfoWithStorage[127.0.0.1:43102,DS-d55250ac-40ba-47eb-802c-1659a43b4699,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-310995068-172.17.0.9-1597638397321:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37435,DS-99cc8db9-593f-444d-b174-84f45c08808d,DISK], DatanodeInfoWithStorage[127.0.0.1:34087,DS-1eed2328-61cd-4fac-9c3b-1e027ad8159a,DISK], DatanodeInfoWithStorage[127.0.0.1:44021,DS-26520692-d4be-45e0-8dcc-c7f9c0285cbf,DISK], DatanodeInfoWithStorage[127.0.0.1:44439,DS-662e8a14-fee6-4837-94bf-e76fd46d9c06,DISK], DatanodeInfoWithStorage[127.0.0.1:40525,DS-1f53535b-9d62-4a28-8908-dc458c16a878,DISK], DatanodeInfoWithStorage[127.0.0.1:37430,DS-b8c33500-5a23-4cab-b1d9-f721946e99a3,DISK], DatanodeInfoWithStorage[127.0.0.1:33933,DS-336b24bc-fa6f-4673-9263-5175ca752065,DISK], DatanodeInfoWithStorage[127.0.0.1:43102,DS-d55250ac-40ba-47eb-802c-1659a43b4699,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1368052417-172.17.0.9-1597638477685:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35483,DS-b42a84aa-2652-4d3d-a0ff-a3a8893ad5b3,DISK], DatanodeInfoWithStorage[127.0.0.1:39022,DS-3f9d9bf7-1e69-4ab1-aa58-586f66c0178b,DISK], DatanodeInfoWithStorage[127.0.0.1:32807,DS-6cc21a0a-da94-4003-8892-3faf190fd98a,DISK], DatanodeInfoWithStorage[127.0.0.1:40606,DS-11a95fa0-065d-4016-a2de-8537bbf35a2a,DISK], DatanodeInfoWithStorage[127.0.0.1:34491,DS-6ed1ec1c-8648-4f4d-8379-3853cdbe913e,DISK], DatanodeInfoWithStorage[127.0.0.1:35726,DS-b368c7be-fbc1-4af4-898e-ee174b6a677e,DISK], DatanodeInfoWithStorage[127.0.0.1:34884,DS-4c0fe5ed-5c84-4b25-a368-af26e3f6da73,DISK], DatanodeInfoWithStorage[127.0.0.1:44453,DS-d86506b0-2985-40da-b404-afd02cafbfa7,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1368052417-172.17.0.9-1597638477685:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35483,DS-b42a84aa-2652-4d3d-a0ff-a3a8893ad5b3,DISK], DatanodeInfoWithStorage[127.0.0.1:39022,DS-3f9d9bf7-1e69-4ab1-aa58-586f66c0178b,DISK], DatanodeInfoWithStorage[127.0.0.1:32807,DS-6cc21a0a-da94-4003-8892-3faf190fd98a,DISK], DatanodeInfoWithStorage[127.0.0.1:40606,DS-11a95fa0-065d-4016-a2de-8537bbf35a2a,DISK], DatanodeInfoWithStorage[127.0.0.1:34491,DS-6ed1ec1c-8648-4f4d-8379-3853cdbe913e,DISK], DatanodeInfoWithStorage[127.0.0.1:35726,DS-b368c7be-fbc1-4af4-898e-ee174b6a677e,DISK], DatanodeInfoWithStorage[127.0.0.1:34884,DS-4c0fe5ed-5c84-4b25-a368-af26e3f6da73,DISK], DatanodeInfoWithStorage[127.0.0.1:44453,DS-d86506b0-2985-40da-b404-afd02cafbfa7,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1404707314-172.17.0.9-1597638597326:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36643,DS-a6906034-a6c4-484b-9c7c-9d7bf38aeba9,DISK], DatanodeInfoWithStorage[127.0.0.1:44787,DS-43f2c02a-5a77-4c19-996e-aa2983b8347f,DISK], DatanodeInfoWithStorage[127.0.0.1:34102,DS-c24eed48-43a3-40b1-be1e-dc45fd6b21b2,DISK], DatanodeInfoWithStorage[127.0.0.1:44423,DS-271a0eb8-5ee1-48e3-88de-7f0d6c5681e2,DISK], DatanodeInfoWithStorage[127.0.0.1:38462,DS-9a8c588f-6c4a-4b20-ba58-9c6cd10fd318,DISK], DatanodeInfoWithStorage[127.0.0.1:45467,DS-f4ac0e3d-a71f-4863-95b6-cfecf7e40734,DISK], DatanodeInfoWithStorage[127.0.0.1:40386,DS-800b2c21-7ff8-4782-807e-2c68aae20c0b,DISK], DatanodeInfoWithStorage[127.0.0.1:44854,DS-63767a22-204f-4945-ab98-65c0146acce9,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1404707314-172.17.0.9-1597638597326:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36643,DS-a6906034-a6c4-484b-9c7c-9d7bf38aeba9,DISK], DatanodeInfoWithStorage[127.0.0.1:44787,DS-43f2c02a-5a77-4c19-996e-aa2983b8347f,DISK], DatanodeInfoWithStorage[127.0.0.1:34102,DS-c24eed48-43a3-40b1-be1e-dc45fd6b21b2,DISK], DatanodeInfoWithStorage[127.0.0.1:44423,DS-271a0eb8-5ee1-48e3-88de-7f0d6c5681e2,DISK], DatanodeInfoWithStorage[127.0.0.1:38462,DS-9a8c588f-6c4a-4b20-ba58-9c6cd10fd318,DISK], DatanodeInfoWithStorage[127.0.0.1:45467,DS-f4ac0e3d-a71f-4863-95b6-cfecf7e40734,DISK], DatanodeInfoWithStorage[127.0.0.1:40386,DS-800b2c21-7ff8-4782-807e-2c68aae20c0b,DISK], DatanodeInfoWithStorage[127.0.0.1:44854,DS-63767a22-204f-4945-ab98-65c0146acce9,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1050106353-172.17.0.9-1597638675940:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46046,DS-5b1fdf10-7172-4964-9439-714cc595adf2,DISK], DatanodeInfoWithStorage[127.0.0.1:38720,DS-397efea1-7ca1-4eb2-b2a9-66454392a690,DISK], DatanodeInfoWithStorage[127.0.0.1:43395,DS-3787c31d-df11-46f3-92b0-8ff97bea4e56,DISK], DatanodeInfoWithStorage[127.0.0.1:43673,DS-dd01d2e5-6ac2-4893-a0bc-d706d78fb08e,DISK], DatanodeInfoWithStorage[127.0.0.1:34151,DS-f46291f5-5b9a-4db2-9c3e-c2799c2c731a,DISK], DatanodeInfoWithStorage[127.0.0.1:44262,DS-9838e19d-03fd-4402-bc46-798730586556,DISK], DatanodeInfoWithStorage[127.0.0.1:45250,DS-d4a8601a-89a7-4375-912f-dd20ea1c1919,DISK], DatanodeInfoWithStorage[127.0.0.1:34120,DS-6374c755-bc2e-42a2-aad7-f4f7e9de57bd,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1050106353-172.17.0.9-1597638675940:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46046,DS-5b1fdf10-7172-4964-9439-714cc595adf2,DISK], DatanodeInfoWithStorage[127.0.0.1:38720,DS-397efea1-7ca1-4eb2-b2a9-66454392a690,DISK], DatanodeInfoWithStorage[127.0.0.1:43395,DS-3787c31d-df11-46f3-92b0-8ff97bea4e56,DISK], DatanodeInfoWithStorage[127.0.0.1:43673,DS-dd01d2e5-6ac2-4893-a0bc-d706d78fb08e,DISK], DatanodeInfoWithStorage[127.0.0.1:34151,DS-f46291f5-5b9a-4db2-9c3e-c2799c2c731a,DISK], DatanodeInfoWithStorage[127.0.0.1:44262,DS-9838e19d-03fd-4402-bc46-798730586556,DISK], DatanodeInfoWithStorage[127.0.0.1:45250,DS-d4a8601a-89a7-4375-912f-dd20ea1c1919,DISK], DatanodeInfoWithStorage[127.0.0.1:34120,DS-6374c755-bc2e-42a2-aad7-f4f7e9de57bd,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-848500311-172.17.0.9-1597638796882:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46329,DS-9ca675ce-15f7-4d33-8bd6-10be945da218,DISK], DatanodeInfoWithStorage[127.0.0.1:44271,DS-a9131ebe-96fa-4648-ba4f-00d73f58fd1c,DISK], DatanodeInfoWithStorage[127.0.0.1:34415,DS-6fbc7a71-f317-4500-8d3b-b0ee19169f61,DISK], DatanodeInfoWithStorage[127.0.0.1:44168,DS-6537041b-f50a-4607-b703-0ec3ad1339cd,DISK], DatanodeInfoWithStorage[127.0.0.1:35406,DS-93e51191-5f31-4df0-9ce7-1a44bef3fc76,DISK], DatanodeInfoWithStorage[127.0.0.1:39422,DS-b90281bd-9517-42a0-a1b7-9c4e7831cf7d,DISK], DatanodeInfoWithStorage[127.0.0.1:35826,DS-0a404dfd-a111-4079-a777-cd7caf6a0c69,DISK], DatanodeInfoWithStorage[127.0.0.1:41799,DS-0e89fc27-ae45-414f-aa88-284c2e2f50da,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-848500311-172.17.0.9-1597638796882:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46329,DS-9ca675ce-15f7-4d33-8bd6-10be945da218,DISK], DatanodeInfoWithStorage[127.0.0.1:44271,DS-a9131ebe-96fa-4648-ba4f-00d73f58fd1c,DISK], DatanodeInfoWithStorage[127.0.0.1:34415,DS-6fbc7a71-f317-4500-8d3b-b0ee19169f61,DISK], DatanodeInfoWithStorage[127.0.0.1:44168,DS-6537041b-f50a-4607-b703-0ec3ad1339cd,DISK], DatanodeInfoWithStorage[127.0.0.1:35406,DS-93e51191-5f31-4df0-9ce7-1a44bef3fc76,DISK], DatanodeInfoWithStorage[127.0.0.1:39422,DS-b90281bd-9517-42a0-a1b7-9c4e7831cf7d,DISK], DatanodeInfoWithStorage[127.0.0.1:35826,DS-0a404dfd-a111-4079-a777-cd7caf6a0c69,DISK], DatanodeInfoWithStorage[127.0.0.1:41799,DS-0e89fc27-ae45-414f-aa88-284c2e2f50da,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1054741488-172.17.0.9-1597638949716:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35605,DS-d9d6f588-d19c-472d-9d4e-d206adbb9f43,DISK], DatanodeInfoWithStorage[127.0.0.1:34373,DS-c9ce05e6-454f-45be-87b5-f40fe8e2b50c,DISK], DatanodeInfoWithStorage[127.0.0.1:39532,DS-5a64923b-db3d-45b1-85e3-2f52f9544ba3,DISK], DatanodeInfoWithStorage[127.0.0.1:45850,DS-c0106335-3297-4c4e-be98-a0b2a75568de,DISK], DatanodeInfoWithStorage[127.0.0.1:34413,DS-e2e0c80f-8ea1-48c5-824d-c6027f96f14f,DISK], DatanodeInfoWithStorage[127.0.0.1:43167,DS-c6818eb0-f78a-4255-b8f8-a6a5cc1a125d,DISK], DatanodeInfoWithStorage[127.0.0.1:39047,DS-94a867b1-d775-4228-a3b2-85053a64e1e5,DISK], DatanodeInfoWithStorage[127.0.0.1:39740,DS-4c9bef19-b19d-4a04-858d-aa30f4f4ad29,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1054741488-172.17.0.9-1597638949716:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35605,DS-d9d6f588-d19c-472d-9d4e-d206adbb9f43,DISK], DatanodeInfoWithStorage[127.0.0.1:34373,DS-c9ce05e6-454f-45be-87b5-f40fe8e2b50c,DISK], DatanodeInfoWithStorage[127.0.0.1:39532,DS-5a64923b-db3d-45b1-85e3-2f52f9544ba3,DISK], DatanodeInfoWithStorage[127.0.0.1:45850,DS-c0106335-3297-4c4e-be98-a0b2a75568de,DISK], DatanodeInfoWithStorage[127.0.0.1:34413,DS-e2e0c80f-8ea1-48c5-824d-c6027f96f14f,DISK], DatanodeInfoWithStorage[127.0.0.1:43167,DS-c6818eb0-f78a-4255-b8f8-a6a5cc1a125d,DISK], DatanodeInfoWithStorage[127.0.0.1:39047,DS-94a867b1-d775-4228-a3b2-85053a64e1e5,DISK], DatanodeInfoWithStorage[127.0.0.1:39740,DS-4c9bef19-b19d-4a04-858d-aa30f4f4ad29,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-254450348-172.17.0.9-1597638986141:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43047,DS-1c60f3c4-86c5-4f0a-92ee-6f63c03a2f88,DISK], DatanodeInfoWithStorage[127.0.0.1:45709,DS-b96eb437-caf7-4052-a201-89b7609cfacc,DISK], DatanodeInfoWithStorage[127.0.0.1:44890,DS-cc104692-6422-4437-9a56-332f31fbf5ce,DISK], DatanodeInfoWithStorage[127.0.0.1:35823,DS-abecc84e-cd97-45e3-9554-3b8e05eeddf1,DISK], DatanodeInfoWithStorage[127.0.0.1:43999,DS-4fcbb3a1-89b9-4786-996d-cf1af95de8db,DISK], DatanodeInfoWithStorage[127.0.0.1:38592,DS-0a1a7fe4-f5d1-49a3-b453-9445aa7cb4e9,DISK], DatanodeInfoWithStorage[127.0.0.1:37503,DS-31bf0a47-e7b4-4cd0-80ee-658f1b0e02d6,DISK], DatanodeInfoWithStorage[127.0.0.1:37295,DS-9a82b194-5fe7-45ea-bd30-6b8338f48116,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-254450348-172.17.0.9-1597638986141:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43047,DS-1c60f3c4-86c5-4f0a-92ee-6f63c03a2f88,DISK], DatanodeInfoWithStorage[127.0.0.1:45709,DS-b96eb437-caf7-4052-a201-89b7609cfacc,DISK], DatanodeInfoWithStorage[127.0.0.1:44890,DS-cc104692-6422-4437-9a56-332f31fbf5ce,DISK], DatanodeInfoWithStorage[127.0.0.1:35823,DS-abecc84e-cd97-45e3-9554-3b8e05eeddf1,DISK], DatanodeInfoWithStorage[127.0.0.1:43999,DS-4fcbb3a1-89b9-4786-996d-cf1af95de8db,DISK], DatanodeInfoWithStorage[127.0.0.1:38592,DS-0a1a7fe4-f5d1-49a3-b453-9445aa7cb4e9,DISK], DatanodeInfoWithStorage[127.0.0.1:37503,DS-31bf0a47-e7b4-4cd0-80ee-658f1b0e02d6,DISK], DatanodeInfoWithStorage[127.0.0.1:37295,DS-9a82b194-5fe7-45ea-bd30-6b8338f48116,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1882142238-172.17.0.9-1597639027947:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41369,DS-f36ad44f-04fa-44a4-914b-965555ce0647,DISK], DatanodeInfoWithStorage[127.0.0.1:43601,DS-09b40b4e-a354-4c29-a0f0-deed83a603b8,DISK], DatanodeInfoWithStorage[127.0.0.1:42170,DS-a60ad5da-d59b-4458-9ea1-83277c2bfdef,DISK], DatanodeInfoWithStorage[127.0.0.1:35009,DS-a7d4a0bc-ed63-4367-9ebe-c950e13d3e02,DISK], DatanodeInfoWithStorage[127.0.0.1:44035,DS-868a3d0b-668f-412e-a7f0-4886b75f4194,DISK], DatanodeInfoWithStorage[127.0.0.1:37496,DS-cb8b97c0-056f-4821-b222-940eeddd27bc,DISK], DatanodeInfoWithStorage[127.0.0.1:45640,DS-609a941b-58ca-4bac-a40e-05648f4af06f,DISK], DatanodeInfoWithStorage[127.0.0.1:37420,DS-4d1fe9ee-f99c-4efd-be3a-272d28aa660b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1882142238-172.17.0.9-1597639027947:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41369,DS-f36ad44f-04fa-44a4-914b-965555ce0647,DISK], DatanodeInfoWithStorage[127.0.0.1:43601,DS-09b40b4e-a354-4c29-a0f0-deed83a603b8,DISK], DatanodeInfoWithStorage[127.0.0.1:42170,DS-a60ad5da-d59b-4458-9ea1-83277c2bfdef,DISK], DatanodeInfoWithStorage[127.0.0.1:35009,DS-a7d4a0bc-ed63-4367-9ebe-c950e13d3e02,DISK], DatanodeInfoWithStorage[127.0.0.1:44035,DS-868a3d0b-668f-412e-a7f0-4886b75f4194,DISK], DatanodeInfoWithStorage[127.0.0.1:37496,DS-cb8b97c0-056f-4821-b222-940eeddd27bc,DISK], DatanodeInfoWithStorage[127.0.0.1:45640,DS-609a941b-58ca-4bac-a40e-05648f4af06f,DISK], DatanodeInfoWithStorage[127.0.0.1:37420,DS-4d1fe9ee-f99c-4efd-be3a-272d28aa660b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-612442712-172.17.0.9-1597639151886:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43561,DS-f5fe5e25-0814-4bb9-81d8-b71a78cd2da4,DISK], DatanodeInfoWithStorage[127.0.0.1:41815,DS-d05af6af-7b0b-4a50-8846-5d5328cdac70,DISK], DatanodeInfoWithStorage[127.0.0.1:36161,DS-21310090-942c-446d-96b5-b8de3dcc9716,DISK], DatanodeInfoWithStorage[127.0.0.1:42049,DS-a0fd9e4d-363f-458d-9903-bcc3aa9de692,DISK], DatanodeInfoWithStorage[127.0.0.1:36388,DS-90826c0b-afd2-4c6a-8506-e31a547380f6,DISK], DatanodeInfoWithStorage[127.0.0.1:33627,DS-66b144a5-54d5-4a87-8fe7-4f357eefd417,DISK], DatanodeInfoWithStorage[127.0.0.1:40996,DS-cf0673cc-6539-4383-b682-69cca43fb53c,DISK], DatanodeInfoWithStorage[127.0.0.1:44976,DS-7e15cacf-704a-43dd-8ec4-ee366582a6b2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-612442712-172.17.0.9-1597639151886:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43561,DS-f5fe5e25-0814-4bb9-81d8-b71a78cd2da4,DISK], DatanodeInfoWithStorage[127.0.0.1:41815,DS-d05af6af-7b0b-4a50-8846-5d5328cdac70,DISK], DatanodeInfoWithStorage[127.0.0.1:36161,DS-21310090-942c-446d-96b5-b8de3dcc9716,DISK], DatanodeInfoWithStorage[127.0.0.1:42049,DS-a0fd9e4d-363f-458d-9903-bcc3aa9de692,DISK], DatanodeInfoWithStorage[127.0.0.1:36388,DS-90826c0b-afd2-4c6a-8506-e31a547380f6,DISK], DatanodeInfoWithStorage[127.0.0.1:33627,DS-66b144a5-54d5-4a87-8fe7-4f357eefd417,DISK], DatanodeInfoWithStorage[127.0.0.1:40996,DS-cf0673cc-6539-4383-b682-69cca43fb53c,DISK], DatanodeInfoWithStorage[127.0.0.1:44976,DS-7e15cacf-704a-43dd-8ec4-ee366582a6b2,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-534597692-172.17.0.9-1597639196826:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42128,DS-5b5c0895-1b2c-447c-a363-2efccfc23e74,DISK], DatanodeInfoWithStorage[127.0.0.1:36250,DS-fa6f4aca-de4f-4423-9a87-32f5b59a4079,DISK], DatanodeInfoWithStorage[127.0.0.1:37843,DS-1e3ab192-1205-4fa3-982f-58c7e9d3aef3,DISK], DatanodeInfoWithStorage[127.0.0.1:36162,DS-bd723791-60fc-4690-a984-3453697e5207,DISK], DatanodeInfoWithStorage[127.0.0.1:44246,DS-b776cc87-24b9-42fb-ac2f-c16297d417cd,DISK], DatanodeInfoWithStorage[127.0.0.1:43035,DS-e783a401-d687-43b3-8535-7752f744cf6f,DISK], DatanodeInfoWithStorage[127.0.0.1:46060,DS-66d2b035-f2fb-42a0-a800-c5332edc727c,DISK], DatanodeInfoWithStorage[127.0.0.1:45063,DS-c59cc5bb-4fe8-470e-9c83-f449a9c2770b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-534597692-172.17.0.9-1597639196826:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42128,DS-5b5c0895-1b2c-447c-a363-2efccfc23e74,DISK], DatanodeInfoWithStorage[127.0.0.1:36250,DS-fa6f4aca-de4f-4423-9a87-32f5b59a4079,DISK], DatanodeInfoWithStorage[127.0.0.1:37843,DS-1e3ab192-1205-4fa3-982f-58c7e9d3aef3,DISK], DatanodeInfoWithStorage[127.0.0.1:36162,DS-bd723791-60fc-4690-a984-3453697e5207,DISK], DatanodeInfoWithStorage[127.0.0.1:44246,DS-b776cc87-24b9-42fb-ac2f-c16297d417cd,DISK], DatanodeInfoWithStorage[127.0.0.1:43035,DS-e783a401-d687-43b3-8535-7752f744cf6f,DISK], DatanodeInfoWithStorage[127.0.0.1:46060,DS-66d2b035-f2fb-42a0-a800-c5332edc727c,DISK], DatanodeInfoWithStorage[127.0.0.1:45063,DS-c59cc5bb-4fe8-470e-9c83-f449a9c2770b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1999354351-172.17.0.9-1597639614948:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38384,DS-222a6b9e-69db-46a9-a516-05c9214994df,DISK], DatanodeInfoWithStorage[127.0.0.1:33891,DS-bf9d7aae-9d96-4414-9266-e59250e82896,DISK], DatanodeInfoWithStorage[127.0.0.1:38624,DS-e17b7656-3bd9-4f7d-bf1b-83072fc5be77,DISK], DatanodeInfoWithStorage[127.0.0.1:46293,DS-a0c955f3-0a05-4989-b825-d2834d375c8c,DISK], DatanodeInfoWithStorage[127.0.0.1:44095,DS-5ef53701-8045-4e3e-9bd4-a620ff09d24e,DISK], DatanodeInfoWithStorage[127.0.0.1:38280,DS-ba6e63dc-9940-4be7-83e0-4fb2e374e152,DISK], DatanodeInfoWithStorage[127.0.0.1:43986,DS-1dd01628-d7e6-4b02-82c8-a4047b567c27,DISK], DatanodeInfoWithStorage[127.0.0.1:46779,DS-81576090-afc0-4995-88c4-19f037bb1c68,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1999354351-172.17.0.9-1597639614948:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38384,DS-222a6b9e-69db-46a9-a516-05c9214994df,DISK], DatanodeInfoWithStorage[127.0.0.1:33891,DS-bf9d7aae-9d96-4414-9266-e59250e82896,DISK], DatanodeInfoWithStorage[127.0.0.1:38624,DS-e17b7656-3bd9-4f7d-bf1b-83072fc5be77,DISK], DatanodeInfoWithStorage[127.0.0.1:46293,DS-a0c955f3-0a05-4989-b825-d2834d375c8c,DISK], DatanodeInfoWithStorage[127.0.0.1:44095,DS-5ef53701-8045-4e3e-9bd4-a620ff09d24e,DISK], DatanodeInfoWithStorage[127.0.0.1:38280,DS-ba6e63dc-9940-4be7-83e0-4fb2e374e152,DISK], DatanodeInfoWithStorage[127.0.0.1:43986,DS-1dd01628-d7e6-4b02-82c8-a4047b567c27,DISK], DatanodeInfoWithStorage[127.0.0.1:46779,DS-81576090-afc0-4995-88c4-19f037bb1c68,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1761971829-172.17.0.9-1597639847256:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39726,DS-b721aa0e-807a-46d7-b055-0e48d545b08d,DISK], DatanodeInfoWithStorage[127.0.0.1:43918,DS-46cd788e-caa8-4722-ab22-0589f0497024,DISK], DatanodeInfoWithStorage[127.0.0.1:35006,DS-6f1f136e-5e63-490b-9437-602c81bac198,DISK], DatanodeInfoWithStorage[127.0.0.1:38033,DS-0d7b5f8c-ba5e-45c0-b9ae-7e908406daf7,DISK], DatanodeInfoWithStorage[127.0.0.1:33742,DS-3ea82a07-b190-421e-b4aa-5c483aab7f10,DISK], DatanodeInfoWithStorage[127.0.0.1:33571,DS-58eafa73-7579-4252-835b-ae50d09259b6,DISK], DatanodeInfoWithStorage[127.0.0.1:33435,DS-210a701e-6db3-46b1-8575-d764b488b148,DISK], DatanodeInfoWithStorage[127.0.0.1:36783,DS-4daf3ab3-c3e3-4798-aa54-1def0c543ee8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1761971829-172.17.0.9-1597639847256:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39726,DS-b721aa0e-807a-46d7-b055-0e48d545b08d,DISK], DatanodeInfoWithStorage[127.0.0.1:43918,DS-46cd788e-caa8-4722-ab22-0589f0497024,DISK], DatanodeInfoWithStorage[127.0.0.1:35006,DS-6f1f136e-5e63-490b-9437-602c81bac198,DISK], DatanodeInfoWithStorage[127.0.0.1:38033,DS-0d7b5f8c-ba5e-45c0-b9ae-7e908406daf7,DISK], DatanodeInfoWithStorage[127.0.0.1:33742,DS-3ea82a07-b190-421e-b4aa-5c483aab7f10,DISK], DatanodeInfoWithStorage[127.0.0.1:33571,DS-58eafa73-7579-4252-835b-ae50d09259b6,DISK], DatanodeInfoWithStorage[127.0.0.1:33435,DS-210a701e-6db3-46b1-8575-d764b488b148,DISK], DatanodeInfoWithStorage[127.0.0.1:36783,DS-4daf3ab3-c3e3-4798-aa54-1def0c543ee8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-885157766-172.17.0.9-1597639963674:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44505,DS-c10a1634-9a08-4c20-b056-a545b9246c05,DISK], DatanodeInfoWithStorage[127.0.0.1:36105,DS-bf896e3e-d2fa-45cb-92af-54ca867d94ce,DISK], DatanodeInfoWithStorage[127.0.0.1:45336,DS-f6dd76b8-7cd6-421a-8a06-5f494ee688f7,DISK], DatanodeInfoWithStorage[127.0.0.1:33536,DS-fdbff352-2418-49a9-8e07-6c1fb7219079,DISK], DatanodeInfoWithStorage[127.0.0.1:44102,DS-71ed527c-7dfb-426d-afc0-d72fd35fac15,DISK], DatanodeInfoWithStorage[127.0.0.1:40285,DS-24bc3331-b261-4f24-b41c-1e18d352886d,DISK], DatanodeInfoWithStorage[127.0.0.1:43073,DS-078609d3-1493-49f6-ab6b-dd532c05d27e,DISK], DatanodeInfoWithStorage[127.0.0.1:40149,DS-bab1a8dc-4530-4aa2-8c40-4cb44257d0df,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-885157766-172.17.0.9-1597639963674:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44505,DS-c10a1634-9a08-4c20-b056-a545b9246c05,DISK], DatanodeInfoWithStorage[127.0.0.1:36105,DS-bf896e3e-d2fa-45cb-92af-54ca867d94ce,DISK], DatanodeInfoWithStorage[127.0.0.1:45336,DS-f6dd76b8-7cd6-421a-8a06-5f494ee688f7,DISK], DatanodeInfoWithStorage[127.0.0.1:33536,DS-fdbff352-2418-49a9-8e07-6c1fb7219079,DISK], DatanodeInfoWithStorage[127.0.0.1:44102,DS-71ed527c-7dfb-426d-afc0-d72fd35fac15,DISK], DatanodeInfoWithStorage[127.0.0.1:40285,DS-24bc3331-b261-4f24-b41c-1e18d352886d,DISK], DatanodeInfoWithStorage[127.0.0.1:43073,DS-078609d3-1493-49f6-ab6b-dd532c05d27e,DISK], DatanodeInfoWithStorage[127.0.0.1:40149,DS-bab1a8dc-4530-4aa2-8c40-4cb44257d0df,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1453742252-172.17.0.9-1597640001298:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34516,DS-a589669e-5cad-4c12-b847-c7450d26ae0a,DISK], DatanodeInfoWithStorage[127.0.0.1:41010,DS-9a9221b4-9232-4d6e-b33b-29bf9cb23607,DISK], DatanodeInfoWithStorage[127.0.0.1:42720,DS-612223d7-9bd4-4205-bde8-e9358389320b,DISK], DatanodeInfoWithStorage[127.0.0.1:44513,DS-e4b6a1d2-cb70-4547-ae2b-c8607b583cff,DISK], DatanodeInfoWithStorage[127.0.0.1:43707,DS-ad5bcce7-8d40-4e37-a79f-efc3fedff118,DISK], DatanodeInfoWithStorage[127.0.0.1:38568,DS-61c435cc-f138-4595-926c-a9f6b63a034f,DISK], DatanodeInfoWithStorage[127.0.0.1:36701,DS-93e6b42a-0652-4d90-b6ab-b9a4a5185765,DISK], DatanodeInfoWithStorage[127.0.0.1:39320,DS-06d90a04-fa97-49a1-b3f0-4090128be538,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1453742252-172.17.0.9-1597640001298:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34516,DS-a589669e-5cad-4c12-b847-c7450d26ae0a,DISK], DatanodeInfoWithStorage[127.0.0.1:41010,DS-9a9221b4-9232-4d6e-b33b-29bf9cb23607,DISK], DatanodeInfoWithStorage[127.0.0.1:42720,DS-612223d7-9bd4-4205-bde8-e9358389320b,DISK], DatanodeInfoWithStorage[127.0.0.1:44513,DS-e4b6a1d2-cb70-4547-ae2b-c8607b583cff,DISK], DatanodeInfoWithStorage[127.0.0.1:43707,DS-ad5bcce7-8d40-4e37-a79f-efc3fedff118,DISK], DatanodeInfoWithStorage[127.0.0.1:38568,DS-61c435cc-f138-4595-926c-a9f6b63a034f,DISK], DatanodeInfoWithStorage[127.0.0.1:36701,DS-93e6b42a-0652-4d90-b6ab-b9a4a5185765,DISK], DatanodeInfoWithStorage[127.0.0.1:39320,DS-06d90a04-fa97-49a1-b3f0-4090128be538,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-72911975-172.17.0.9-1597640040828:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42347,DS-0b81807f-c09b-4ffb-894e-a312ee927aa6,DISK], DatanodeInfoWithStorage[127.0.0.1:38812,DS-8ea8feaf-c51f-4c2a-980c-b4dc034ba862,DISK], DatanodeInfoWithStorage[127.0.0.1:32917,DS-9beaf5cb-dc3b-4267-943e-1776fc124049,DISK], DatanodeInfoWithStorage[127.0.0.1:44294,DS-6ab319df-c9ee-41e8-8c8b-03a1fc15980a,DISK], DatanodeInfoWithStorage[127.0.0.1:33654,DS-efb98e15-8118-4c14-9bbc-9659704f1849,DISK], DatanodeInfoWithStorage[127.0.0.1:36600,DS-a1dc9bb9-e829-4633-9c27-ca17449a7db9,DISK], DatanodeInfoWithStorage[127.0.0.1:34255,DS-43eb0c48-e614-4305-8e85-ac50b1ce794b,DISK], DatanodeInfoWithStorage[127.0.0.1:43704,DS-b3559e04-46c5-4fd7-b4cf-13f9ac8906d9,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-72911975-172.17.0.9-1597640040828:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42347,DS-0b81807f-c09b-4ffb-894e-a312ee927aa6,DISK], DatanodeInfoWithStorage[127.0.0.1:38812,DS-8ea8feaf-c51f-4c2a-980c-b4dc034ba862,DISK], DatanodeInfoWithStorage[127.0.0.1:32917,DS-9beaf5cb-dc3b-4267-943e-1776fc124049,DISK], DatanodeInfoWithStorage[127.0.0.1:44294,DS-6ab319df-c9ee-41e8-8c8b-03a1fc15980a,DISK], DatanodeInfoWithStorage[127.0.0.1:33654,DS-efb98e15-8118-4c14-9bbc-9659704f1849,DISK], DatanodeInfoWithStorage[127.0.0.1:36600,DS-a1dc9bb9-e829-4633-9c27-ca17449a7db9,DISK], DatanodeInfoWithStorage[127.0.0.1:34255,DS-43eb0c48-e614-4305-8e85-ac50b1ce794b,DISK], DatanodeInfoWithStorage[127.0.0.1:43704,DS-b3559e04-46c5-4fd7-b4cf-13f9ac8906d9,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1401847949-172.17.0.9-1597640228217:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37124,DS-8e093f67-b78c-42fd-a5e5-f89d8d9dea9e,DISK], DatanodeInfoWithStorage[127.0.0.1:46088,DS-19c5658f-0501-4eb6-82fe-3880249fa1cb,DISK], DatanodeInfoWithStorage[127.0.0.1:44782,DS-e83de46e-058c-470f-bf4c-50c9e06eec4f,DISK], DatanodeInfoWithStorage[127.0.0.1:38584,DS-fe423b2c-0bd3-4995-a01f-bf6037c61fbb,DISK], DatanodeInfoWithStorage[127.0.0.1:43028,DS-4fe3cc2c-ad20-4185-952b-4f32e57da8ba,DISK], DatanodeInfoWithStorage[127.0.0.1:37564,DS-2e0c2c6b-b12d-4706-9852-9a585426db13,DISK], DatanodeInfoWithStorage[127.0.0.1:37654,DS-19ce6652-2c22-461f-b5b0-4e7de566798d,DISK], DatanodeInfoWithStorage[127.0.0.1:39862,DS-b837cc95-0944-46e6-bc82-cb49c8708705,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1401847949-172.17.0.9-1597640228217:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37124,DS-8e093f67-b78c-42fd-a5e5-f89d8d9dea9e,DISK], DatanodeInfoWithStorage[127.0.0.1:46088,DS-19c5658f-0501-4eb6-82fe-3880249fa1cb,DISK], DatanodeInfoWithStorage[127.0.0.1:44782,DS-e83de46e-058c-470f-bf4c-50c9e06eec4f,DISK], DatanodeInfoWithStorage[127.0.0.1:38584,DS-fe423b2c-0bd3-4995-a01f-bf6037c61fbb,DISK], DatanodeInfoWithStorage[127.0.0.1:43028,DS-4fe3cc2c-ad20-4185-952b-4f32e57da8ba,DISK], DatanodeInfoWithStorage[127.0.0.1:37564,DS-2e0c2c6b-b12d-4706-9852-9a585426db13,DISK], DatanodeInfoWithStorage[127.0.0.1:37654,DS-19ce6652-2c22-461f-b5b0-4e7de566798d,DISK], DatanodeInfoWithStorage[127.0.0.1:39862,DS-b837cc95-0944-46e6-bc82-cb49c8708705,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-588388429-172.17.0.9-1597640437065:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43047,DS-e6e5b812-d656-406f-9a7c-01edd5231a3e,DISK], DatanodeInfoWithStorage[127.0.0.1:43016,DS-04f3ac03-a9c5-43a5-83af-0bead04e57b5,DISK], DatanodeInfoWithStorage[127.0.0.1:42668,DS-1748df43-e9fb-45b0-8a89-c3f66c1e60d7,DISK], DatanodeInfoWithStorage[127.0.0.1:42546,DS-7d37f29d-70a1-4077-bb41-c6271e0132e1,DISK], DatanodeInfoWithStorage[127.0.0.1:33404,DS-612a6e31-256c-4865-af6d-7e441613e755,DISK], DatanodeInfoWithStorage[127.0.0.1:40034,DS-8ce549ed-52f4-4bb0-8cb4-44de5e423664,DISK], DatanodeInfoWithStorage[127.0.0.1:44453,DS-0afca29a-663f-4267-82bd-d3d9617005c0,DISK], DatanodeInfoWithStorage[127.0.0.1:41762,DS-dd59939b-4ebe-4293-8448-8d0394297080,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-588388429-172.17.0.9-1597640437065:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43047,DS-e6e5b812-d656-406f-9a7c-01edd5231a3e,DISK], DatanodeInfoWithStorage[127.0.0.1:43016,DS-04f3ac03-a9c5-43a5-83af-0bead04e57b5,DISK], DatanodeInfoWithStorage[127.0.0.1:42668,DS-1748df43-e9fb-45b0-8a89-c3f66c1e60d7,DISK], DatanodeInfoWithStorage[127.0.0.1:42546,DS-7d37f29d-70a1-4077-bb41-c6271e0132e1,DISK], DatanodeInfoWithStorage[127.0.0.1:33404,DS-612a6e31-256c-4865-af6d-7e441613e755,DISK], DatanodeInfoWithStorage[127.0.0.1:40034,DS-8ce549ed-52f4-4bb0-8cb4-44de5e423664,DISK], DatanodeInfoWithStorage[127.0.0.1:44453,DS-0afca29a-663f-4267-82bd-d3d9617005c0,DISK], DatanodeInfoWithStorage[127.0.0.1:41762,DS-dd59939b-4ebe-4293-8448-8d0394297080,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1533935187-172.17.0.9-1597640516032:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38616,DS-ec8c496c-a83a-4177-a0c6-d93dcf81a6e3,DISK], DatanodeInfoWithStorage[127.0.0.1:43525,DS-1737a24d-6c4d-480e-a3e8-6b99e83438c1,DISK], DatanodeInfoWithStorage[127.0.0.1:37465,DS-602d68a8-4a7f-432e-8b8c-6941aac89ae5,DISK], DatanodeInfoWithStorage[127.0.0.1:39846,DS-4ae89d0d-055e-4e27-bdc5-88db79ed827c,DISK], DatanodeInfoWithStorage[127.0.0.1:46466,DS-964bd5a3-63e9-4bdd-a6ef-97d7652576f4,DISK], DatanodeInfoWithStorage[127.0.0.1:34570,DS-e5a07a8a-58ec-4b9e-9130-0a924390162f,DISK], DatanodeInfoWithStorage[127.0.0.1:44964,DS-82115a49-1e94-47a7-96aa-67c8ea1bb14b,DISK], DatanodeInfoWithStorage[127.0.0.1:34636,DS-f85936fa-1150-4a6f-81dc-711665f6b416,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1533935187-172.17.0.9-1597640516032:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38616,DS-ec8c496c-a83a-4177-a0c6-d93dcf81a6e3,DISK], DatanodeInfoWithStorage[127.0.0.1:43525,DS-1737a24d-6c4d-480e-a3e8-6b99e83438c1,DISK], DatanodeInfoWithStorage[127.0.0.1:37465,DS-602d68a8-4a7f-432e-8b8c-6941aac89ae5,DISK], DatanodeInfoWithStorage[127.0.0.1:39846,DS-4ae89d0d-055e-4e27-bdc5-88db79ed827c,DISK], DatanodeInfoWithStorage[127.0.0.1:46466,DS-964bd5a3-63e9-4bdd-a6ef-97d7652576f4,DISK], DatanodeInfoWithStorage[127.0.0.1:34570,DS-e5a07a8a-58ec-4b9e-9130-0a924390162f,DISK], DatanodeInfoWithStorage[127.0.0.1:44964,DS-82115a49-1e94-47a7-96aa-67c8ea1bb14b,DISK], DatanodeInfoWithStorage[127.0.0.1:34636,DS-f85936fa-1150-4a6f-81dc-711665f6b416,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-440227844-172.17.0.9-1597640801248:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33478,DS-d225fb61-8f62-4ef2-adc4-d69c3c58d84f,DISK], DatanodeInfoWithStorage[127.0.0.1:40011,DS-2dea08be-2d50-4158-82ef-0c1d96c31ea8,DISK], DatanodeInfoWithStorage[127.0.0.1:45347,DS-9a28eac8-2eef-4c9d-bd52-e3e0585c1eec,DISK], DatanodeInfoWithStorage[127.0.0.1:46404,DS-782bac65-f9fd-4462-9ce9-ed383f4db06e,DISK], DatanodeInfoWithStorage[127.0.0.1:32822,DS-5f37e183-4394-4e6c-901d-bf775414960b,DISK], DatanodeInfoWithStorage[127.0.0.1:43976,DS-894800a7-7a11-4f6c-a76d-f2427048fd2d,DISK], DatanodeInfoWithStorage[127.0.0.1:42469,DS-91d079bf-e475-4df9-a32a-c0a11789b8d1,DISK], DatanodeInfoWithStorage[127.0.0.1:34796,DS-42f6b890-9407-49a4-b666-0bf8ecc11890,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-440227844-172.17.0.9-1597640801248:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33478,DS-d225fb61-8f62-4ef2-adc4-d69c3c58d84f,DISK], DatanodeInfoWithStorage[127.0.0.1:40011,DS-2dea08be-2d50-4158-82ef-0c1d96c31ea8,DISK], DatanodeInfoWithStorage[127.0.0.1:45347,DS-9a28eac8-2eef-4c9d-bd52-e3e0585c1eec,DISK], DatanodeInfoWithStorage[127.0.0.1:46404,DS-782bac65-f9fd-4462-9ce9-ed383f4db06e,DISK], DatanodeInfoWithStorage[127.0.0.1:32822,DS-5f37e183-4394-4e6c-901d-bf775414960b,DISK], DatanodeInfoWithStorage[127.0.0.1:43976,DS-894800a7-7a11-4f6c-a76d-f2427048fd2d,DISK], DatanodeInfoWithStorage[127.0.0.1:42469,DS-91d079bf-e475-4df9-a32a-c0a11789b8d1,DISK], DatanodeInfoWithStorage[127.0.0.1:34796,DS-42f6b890-9407-49a4-b666-0bf8ecc11890,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1364685058-172.17.0.9-1597641441773:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32894,DS-7ed3ced3-41e5-48ed-8a99-718e67cacd07,DISK], DatanodeInfoWithStorage[127.0.0.1:43554,DS-78dcae18-5b6e-49ec-9014-269d9c511ae2,DISK], DatanodeInfoWithStorage[127.0.0.1:38320,DS-603e6df8-2dae-41b9-81da-6972e87ec404,DISK], DatanodeInfoWithStorage[127.0.0.1:45162,DS-5408e37b-16b0-4761-a3f7-eae9612214f4,DISK], DatanodeInfoWithStorage[127.0.0.1:38758,DS-ebc2e830-37fd-46d2-ace7-dbc58508de46,DISK], DatanodeInfoWithStorage[127.0.0.1:38296,DS-7a8c813b-cf83-4d0a-aa6b-57de9faa980a,DISK], DatanodeInfoWithStorage[127.0.0.1:33331,DS-94d189fb-f3a6-488a-8bc1-4312537b4f41,DISK], DatanodeInfoWithStorage[127.0.0.1:40094,DS-936ef991-7c25-4beb-abdb-a28344bdccaf,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1364685058-172.17.0.9-1597641441773:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32894,DS-7ed3ced3-41e5-48ed-8a99-718e67cacd07,DISK], DatanodeInfoWithStorage[127.0.0.1:43554,DS-78dcae18-5b6e-49ec-9014-269d9c511ae2,DISK], DatanodeInfoWithStorage[127.0.0.1:38320,DS-603e6df8-2dae-41b9-81da-6972e87ec404,DISK], DatanodeInfoWithStorage[127.0.0.1:45162,DS-5408e37b-16b0-4761-a3f7-eae9612214f4,DISK], DatanodeInfoWithStorage[127.0.0.1:38758,DS-ebc2e830-37fd-46d2-ace7-dbc58508de46,DISK], DatanodeInfoWithStorage[127.0.0.1:38296,DS-7a8c813b-cf83-4d0a-aa6b-57de9faa980a,DISK], DatanodeInfoWithStorage[127.0.0.1:33331,DS-94d189fb-f3a6-488a-8bc1-4312537b4f41,DISK], DatanodeInfoWithStorage[127.0.0.1:40094,DS-936ef991-7c25-4beb-abdb-a28344bdccaf,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2042664008-172.17.0.9-1597641481038:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46598,DS-658f2017-496f-422f-998b-01e81f42cd5a,DISK], DatanodeInfoWithStorage[127.0.0.1:38297,DS-a421643c-1559-4fb4-925e-191b3e469335,DISK], DatanodeInfoWithStorage[127.0.0.1:45322,DS-7ec7c9d7-62b0-465e-8d64-47327a493098,DISK], DatanodeInfoWithStorage[127.0.0.1:36226,DS-60aae750-e3b3-483b-bd10-cf77109b5565,DISK], DatanodeInfoWithStorage[127.0.0.1:33507,DS-a9936d7c-f327-4410-8f77-2c31bbb98920,DISK], DatanodeInfoWithStorage[127.0.0.1:42461,DS-2727c6d3-e20a-49c0-af63-95a32a38625d,DISK], DatanodeInfoWithStorage[127.0.0.1:42374,DS-8f0d0537-89c2-4eca-bc51-a4de9df54e37,DISK], DatanodeInfoWithStorage[127.0.0.1:33252,DS-4a631f7c-cc77-4566-8dca-eef2b64ca190,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2042664008-172.17.0.9-1597641481038:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46598,DS-658f2017-496f-422f-998b-01e81f42cd5a,DISK], DatanodeInfoWithStorage[127.0.0.1:38297,DS-a421643c-1559-4fb4-925e-191b3e469335,DISK], DatanodeInfoWithStorage[127.0.0.1:45322,DS-7ec7c9d7-62b0-465e-8d64-47327a493098,DISK], DatanodeInfoWithStorage[127.0.0.1:36226,DS-60aae750-e3b3-483b-bd10-cf77109b5565,DISK], DatanodeInfoWithStorage[127.0.0.1:33507,DS-a9936d7c-f327-4410-8f77-2c31bbb98920,DISK], DatanodeInfoWithStorage[127.0.0.1:42461,DS-2727c6d3-e20a-49c0-af63-95a32a38625d,DISK], DatanodeInfoWithStorage[127.0.0.1:42374,DS-8f0d0537-89c2-4eca-bc51-a4de9df54e37,DISK], DatanodeInfoWithStorage[127.0.0.1:33252,DS-4a631f7c-cc77-4566-8dca-eef2b64ca190,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.write.exclude.nodes.cache.expiry.interval.millis
component: hdfs:NameNode
v1: 6000
v2: 600000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2135226931-172.17.0.9-1597641886044:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34360,DS-512f31ab-1218-4e77-8273-6f6b4ec39b74,DISK], DatanodeInfoWithStorage[127.0.0.1:38390,DS-1763cf44-88f4-4252-9a70-1d9601e24fc8,DISK], DatanodeInfoWithStorage[127.0.0.1:37940,DS-b752bc53-f3bc-42ce-a271-a3715b8c7cc4,DISK], DatanodeInfoWithStorage[127.0.0.1:39742,DS-c9a8021b-ce46-4d0c-b922-7b3cb08c6f2a,DISK], DatanodeInfoWithStorage[127.0.0.1:37503,DS-5ca4d6aa-37d1-4235-8972-3a2082e36a37,DISK], DatanodeInfoWithStorage[127.0.0.1:37914,DS-9b766b9c-7297-436f-b660-90cfdb252cd6,DISK], DatanodeInfoWithStorage[127.0.0.1:43548,DS-7e44c37c-36bb-4fed-bd12-2f997cac969e,DISK], DatanodeInfoWithStorage[127.0.0.1:39285,DS-6c5634e0-1004-457f-bb63-0c327cc2396c,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2135226931-172.17.0.9-1597641886044:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34360,DS-512f31ab-1218-4e77-8273-6f6b4ec39b74,DISK], DatanodeInfoWithStorage[127.0.0.1:38390,DS-1763cf44-88f4-4252-9a70-1d9601e24fc8,DISK], DatanodeInfoWithStorage[127.0.0.1:37940,DS-b752bc53-f3bc-42ce-a271-a3715b8c7cc4,DISK], DatanodeInfoWithStorage[127.0.0.1:39742,DS-c9a8021b-ce46-4d0c-b922-7b3cb08c6f2a,DISK], DatanodeInfoWithStorage[127.0.0.1:37503,DS-5ca4d6aa-37d1-4235-8972-3a2082e36a37,DISK], DatanodeInfoWithStorage[127.0.0.1:37914,DS-9b766b9c-7297-436f-b660-90cfdb252cd6,DISK], DatanodeInfoWithStorage[127.0.0.1:43548,DS-7e44c37c-36bb-4fed-bd12-2f997cac969e,DISK], DatanodeInfoWithStorage[127.0.0.1:39285,DS-6c5634e0-1004-457f-bb63-0c327cc2396c,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


v1v2 failed with probability 13 out of 50
v1v1v2v2 failed with probability 23 out of 50
result: false positive !!!
Total execution time in seconds : 5835

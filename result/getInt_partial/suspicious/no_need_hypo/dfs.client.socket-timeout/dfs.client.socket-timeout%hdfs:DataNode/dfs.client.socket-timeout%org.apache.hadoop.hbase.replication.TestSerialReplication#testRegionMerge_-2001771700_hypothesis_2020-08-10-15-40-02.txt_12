reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39327,DS-ff3b7b9f-6d4e-403c-ac31-d073ccb7a67c,DISK], DatanodeInfoWithStorage[127.0.0.1:37182,DS-b0f6d248-e881-4f00-9638-a1b300544201,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39327,DS-ff3b7b9f-6d4e-403c-ac31-d073ccb7a67c,DISK], DatanodeInfoWithStorage[127.0.0.1:37182,DS-b0f6d248-e881-4f00-9638-a1b300544201,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39327,DS-ff3b7b9f-6d4e-403c-ac31-d073ccb7a67c,DISK], DatanodeInfoWithStorage[127.0.0.1:37182,DS-b0f6d248-e881-4f00-9638-a1b300544201,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39327,DS-ff3b7b9f-6d4e-403c-ac31-d073ccb7a67c,DISK], DatanodeInfoWithStorage[127.0.0.1:37182,DS-b0f6d248-e881-4f00-9638-a1b300544201,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40677,DS-ff33fe97-8472-4e95-9812-7f39ef6f7872,DISK], DatanodeInfoWithStorage[127.0.0.1:37634,DS-ef538a74-f6d6-4961-8471-4d17f7a1ef3c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37634,DS-ef538a74-f6d6-4961-8471-4d17f7a1ef3c,DISK], DatanodeInfoWithStorage[127.0.0.1:40677,DS-ff33fe97-8472-4e95-9812-7f39ef6f7872,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40677,DS-ff33fe97-8472-4e95-9812-7f39ef6f7872,DISK], DatanodeInfoWithStorage[127.0.0.1:37634,DS-ef538a74-f6d6-4961-8471-4d17f7a1ef3c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37634,DS-ef538a74-f6d6-4961-8471-4d17f7a1ef3c,DISK], DatanodeInfoWithStorage[127.0.0.1:40677,DS-ff33fe97-8472-4e95-9812-7f39ef6f7872,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36428,DS-ac73480a-3d25-487e-aed5-01521291067d,DISK], DatanodeInfoWithStorage[127.0.0.1:36366,DS-3c7ea192-1121-4b4f-87d2-4fd1c2c77079,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36428,DS-ac73480a-3d25-487e-aed5-01521291067d,DISK], DatanodeInfoWithStorage[127.0.0.1:36366,DS-3c7ea192-1121-4b4f-87d2-4fd1c2c77079,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36428,DS-ac73480a-3d25-487e-aed5-01521291067d,DISK], DatanodeInfoWithStorage[127.0.0.1:36366,DS-3c7ea192-1121-4b4f-87d2-4fd1c2c77079,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36428,DS-ac73480a-3d25-487e-aed5-01521291067d,DISK], DatanodeInfoWithStorage[127.0.0.1:36366,DS-3c7ea192-1121-4b4f-87d2-4fd1c2c77079,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46234,DS-0786826e-cb8b-4e25-8c25-96dc6cf18cf1,DISK], DatanodeInfoWithStorage[127.0.0.1:40412,DS-945f2fd4-e521-4ace-b913-6a90dcbfe388,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46234,DS-0786826e-cb8b-4e25-8c25-96dc6cf18cf1,DISK], DatanodeInfoWithStorage[127.0.0.1:40412,DS-945f2fd4-e521-4ace-b913-6a90dcbfe388,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46234,DS-0786826e-cb8b-4e25-8c25-96dc6cf18cf1,DISK], DatanodeInfoWithStorage[127.0.0.1:40412,DS-945f2fd4-e521-4ace-b913-6a90dcbfe388,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46234,DS-0786826e-cb8b-4e25-8c25-96dc6cf18cf1,DISK], DatanodeInfoWithStorage[127.0.0.1:40412,DS-945f2fd4-e521-4ace-b913-6a90dcbfe388,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33374,DS-64bee468-ae53-44e9-a17c-b050fa3f809d,DISK], DatanodeInfoWithStorage[127.0.0.1:39210,DS-a58db4d4-9eca-493d-9973-158327ab0f4f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33374,DS-64bee468-ae53-44e9-a17c-b050fa3f809d,DISK], DatanodeInfoWithStorage[127.0.0.1:39210,DS-a58db4d4-9eca-493d-9973-158327ab0f4f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33374,DS-64bee468-ae53-44e9-a17c-b050fa3f809d,DISK], DatanodeInfoWithStorage[127.0.0.1:39210,DS-a58db4d4-9eca-493d-9973-158327ab0f4f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33374,DS-64bee468-ae53-44e9-a17c-b050fa3f809d,DISK], DatanodeInfoWithStorage[127.0.0.1:39210,DS-a58db4d4-9eca-493d-9973-158327ab0f4f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33034,DS-9aa0a2ee-2cf1-4fd8-9683-6e8124882ef2,DISK], DatanodeInfoWithStorage[127.0.0.1:45776,DS-5543ed46-23d9-4e0b-8775-04206544ee58,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45776,DS-5543ed46-23d9-4e0b-8775-04206544ee58,DISK], DatanodeInfoWithStorage[127.0.0.1:33034,DS-9aa0a2ee-2cf1-4fd8-9683-6e8124882ef2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33034,DS-9aa0a2ee-2cf1-4fd8-9683-6e8124882ef2,DISK], DatanodeInfoWithStorage[127.0.0.1:45776,DS-5543ed46-23d9-4e0b-8775-04206544ee58,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45776,DS-5543ed46-23d9-4e0b-8775-04206544ee58,DISK], DatanodeInfoWithStorage[127.0.0.1:33034,DS-9aa0a2ee-2cf1-4fd8-9683-6e8124882ef2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45772,DS-2a6e7e87-ed21-401f-bdc0-03271c25bd2f,DISK], DatanodeInfoWithStorage[127.0.0.1:33624,DS-d548cc3f-de00-4277-90d6-8ad17df93d89,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45772,DS-2a6e7e87-ed21-401f-bdc0-03271c25bd2f,DISK], DatanodeInfoWithStorage[127.0.0.1:33624,DS-d548cc3f-de00-4277-90d6-8ad17df93d89,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45772,DS-2a6e7e87-ed21-401f-bdc0-03271c25bd2f,DISK], DatanodeInfoWithStorage[127.0.0.1:33624,DS-d548cc3f-de00-4277-90d6-8ad17df93d89,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45772,DS-2a6e7e87-ed21-401f-bdc0-03271c25bd2f,DISK], DatanodeInfoWithStorage[127.0.0.1:33624,DS-d548cc3f-de00-4277-90d6-8ad17df93d89,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43033,DS-ff190f62-5801-4416-9a4a-f50d5952e050,DISK], DatanodeInfoWithStorage[127.0.0.1:41571,DS-9f019354-1ad3-437e-96c0-ead242deee9a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41571,DS-9f019354-1ad3-437e-96c0-ead242deee9a,DISK], DatanodeInfoWithStorage[127.0.0.1:43033,DS-ff190f62-5801-4416-9a4a-f50d5952e050,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43033,DS-ff190f62-5801-4416-9a4a-f50d5952e050,DISK], DatanodeInfoWithStorage[127.0.0.1:41571,DS-9f019354-1ad3-437e-96c0-ead242deee9a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41571,DS-9f019354-1ad3-437e-96c0-ead242deee9a,DISK], DatanodeInfoWithStorage[127.0.0.1:43033,DS-ff190f62-5801-4416-9a4a-f50d5952e050,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33271,DS-f3192bd5-19ad-4156-b19e-341a230458c6,DISK], DatanodeInfoWithStorage[127.0.0.1:46773,DS-29d44ae6-c3ca-4acc-a808-8c216a9a655c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46773,DS-29d44ae6-c3ca-4acc-a808-8c216a9a655c,DISK], DatanodeInfoWithStorage[127.0.0.1:33271,DS-f3192bd5-19ad-4156-b19e-341a230458c6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33271,DS-f3192bd5-19ad-4156-b19e-341a230458c6,DISK], DatanodeInfoWithStorage[127.0.0.1:46773,DS-29d44ae6-c3ca-4acc-a808-8c216a9a655c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46773,DS-29d44ae6-c3ca-4acc-a808-8c216a9a655c,DISK], DatanodeInfoWithStorage[127.0.0.1:33271,DS-f3192bd5-19ad-4156-b19e-341a230458c6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35565,DS-4ebc5dd3-4aa8-4d2a-a402-a0b7cca8266a,DISK], DatanodeInfoWithStorage[127.0.0.1:42244,DS-0fe07965-06c3-4259-bd44-aee38a5a5bd9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42244,DS-0fe07965-06c3-4259-bd44-aee38a5a5bd9,DISK], DatanodeInfoWithStorage[127.0.0.1:35565,DS-4ebc5dd3-4aa8-4d2a-a402-a0b7cca8266a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35565,DS-4ebc5dd3-4aa8-4d2a-a402-a0b7cca8266a,DISK], DatanodeInfoWithStorage[127.0.0.1:42244,DS-0fe07965-06c3-4259-bd44-aee38a5a5bd9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42244,DS-0fe07965-06c3-4259-bd44-aee38a5a5bd9,DISK], DatanodeInfoWithStorage[127.0.0.1:35565,DS-4ebc5dd3-4aa8-4d2a-a402-a0b7cca8266a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 2292

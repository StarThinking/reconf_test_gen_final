reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38580,DS-4d6185b1-20c8-4d7c-8309-bf843ddbbc8b,DISK], DatanodeInfoWithStorage[127.0.0.1:41194,DS-052ee425-d8b7-4816-9d13-fab8cd1e12a2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41194,DS-052ee425-d8b7-4816-9d13-fab8cd1e12a2,DISK], DatanodeInfoWithStorage[127.0.0.1:38580,DS-4d6185b1-20c8-4d7c-8309-bf843ddbbc8b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38580,DS-4d6185b1-20c8-4d7c-8309-bf843ddbbc8b,DISK], DatanodeInfoWithStorage[127.0.0.1:41194,DS-052ee425-d8b7-4816-9d13-fab8cd1e12a2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41194,DS-052ee425-d8b7-4816-9d13-fab8cd1e12a2,DISK], DatanodeInfoWithStorage[127.0.0.1:38580,DS-4d6185b1-20c8-4d7c-8309-bf843ddbbc8b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41676,DS-1f446188-4640-47dd-8801-5480a1368513,DISK], DatanodeInfoWithStorage[127.0.0.1:37600,DS-ff2e099f-d2a3-4653-b010-1fdef8daac76,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37600,DS-ff2e099f-d2a3-4653-b010-1fdef8daac76,DISK], DatanodeInfoWithStorage[127.0.0.1:41676,DS-1f446188-4640-47dd-8801-5480a1368513,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41676,DS-1f446188-4640-47dd-8801-5480a1368513,DISK], DatanodeInfoWithStorage[127.0.0.1:37600,DS-ff2e099f-d2a3-4653-b010-1fdef8daac76,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37600,DS-ff2e099f-d2a3-4653-b010-1fdef8daac76,DISK], DatanodeInfoWithStorage[127.0.0.1:41676,DS-1f446188-4640-47dd-8801-5480a1368513,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35051,DS-f5a27e6e-9c5d-4d17-9b43-eb180571087d,DISK], DatanodeInfoWithStorage[127.0.0.1:33219,DS-8eaddb8e-4238-400f-a5d6-836b2ec5bd81,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35051,DS-f5a27e6e-9c5d-4d17-9b43-eb180571087d,DISK], DatanodeInfoWithStorage[127.0.0.1:33219,DS-8eaddb8e-4238-400f-a5d6-836b2ec5bd81,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35051,DS-f5a27e6e-9c5d-4d17-9b43-eb180571087d,DISK], DatanodeInfoWithStorage[127.0.0.1:33219,DS-8eaddb8e-4238-400f-a5d6-836b2ec5bd81,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35051,DS-f5a27e6e-9c5d-4d17-9b43-eb180571087d,DISK], DatanodeInfoWithStorage[127.0.0.1:33219,DS-8eaddb8e-4238-400f-a5d6-836b2ec5bd81,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36984,DS-a6a89d76-9c03-4884-8878-7f641e4f8d24,DISK], DatanodeInfoWithStorage[127.0.0.1:32806,DS-0dd6cf67-cceb-4e63-a46c-257d70302e39,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36984,DS-a6a89d76-9c03-4884-8878-7f641e4f8d24,DISK], DatanodeInfoWithStorage[127.0.0.1:32806,DS-0dd6cf67-cceb-4e63-a46c-257d70302e39,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36984,DS-a6a89d76-9c03-4884-8878-7f641e4f8d24,DISK], DatanodeInfoWithStorage[127.0.0.1:32806,DS-0dd6cf67-cceb-4e63-a46c-257d70302e39,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36984,DS-a6a89d76-9c03-4884-8878-7f641e4f8d24,DISK], DatanodeInfoWithStorage[127.0.0.1:32806,DS-0dd6cf67-cceb-4e63-a46c-257d70302e39,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45482,DS-47e92487-d59c-411b-b3bb-334ec55ed95b,DISK], DatanodeInfoWithStorage[127.0.0.1:35457,DS-41a6892d-3d31-4de0-abc8-029560fb7fbb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45482,DS-47e92487-d59c-411b-b3bb-334ec55ed95b,DISK], DatanodeInfoWithStorage[127.0.0.1:35457,DS-41a6892d-3d31-4de0-abc8-029560fb7fbb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45482,DS-47e92487-d59c-411b-b3bb-334ec55ed95b,DISK], DatanodeInfoWithStorage[127.0.0.1:35457,DS-41a6892d-3d31-4de0-abc8-029560fb7fbb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45482,DS-47e92487-d59c-411b-b3bb-334ec55ed95b,DISK], DatanodeInfoWithStorage[127.0.0.1:35457,DS-41a6892d-3d31-4de0-abc8-029560fb7fbb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33382,DS-5eb3dfa5-9709-4645-82bd-4d5e3e981ec4,DISK], DatanodeInfoWithStorage[127.0.0.1:38689,DS-0a9a4b7b-4257-4ad4-9325-e705e385eb14,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33382,DS-5eb3dfa5-9709-4645-82bd-4d5e3e981ec4,DISK], DatanodeInfoWithStorage[127.0.0.1:38689,DS-0a9a4b7b-4257-4ad4-9325-e705e385eb14,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33382,DS-5eb3dfa5-9709-4645-82bd-4d5e3e981ec4,DISK], DatanodeInfoWithStorage[127.0.0.1:38689,DS-0a9a4b7b-4257-4ad4-9325-e705e385eb14,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33382,DS-5eb3dfa5-9709-4645-82bd-4d5e3e981ec4,DISK], DatanodeInfoWithStorage[127.0.0.1:38689,DS-0a9a4b7b-4257-4ad4-9325-e705e385eb14,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39791,DS-7096d713-0e3d-479c-9463-0075469c27a3,DISK], DatanodeInfoWithStorage[127.0.0.1:33050,DS-815db1a5-e7c4-4804-9f35-f0231894cbe9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39791,DS-7096d713-0e3d-479c-9463-0075469c27a3,DISK], DatanodeInfoWithStorage[127.0.0.1:33050,DS-815db1a5-e7c4-4804-9f35-f0231894cbe9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39791,DS-7096d713-0e3d-479c-9463-0075469c27a3,DISK], DatanodeInfoWithStorage[127.0.0.1:33050,DS-815db1a5-e7c4-4804-9f35-f0231894cbe9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39791,DS-7096d713-0e3d-479c-9463-0075469c27a3,DISK], DatanodeInfoWithStorage[127.0.0.1:33050,DS-815db1a5-e7c4-4804-9f35-f0231894cbe9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: test timed out after 780 seconds
stackTrace: org.junit.runners.model.TestTimedOutException: test timed out after 780 seconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
	at org.apache.hadoop.hbase.regionserver.wal.TestFSHLog.testUnflushedSeqIdTracking(TestFSHLog.java:214)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44670,DS-1a7d136c-1b2f-407e-abba-dc56947c39d2,DISK], DatanodeInfoWithStorage[127.0.0.1:38283,DS-ef99f53f-29f3-474c-935b-5a62be2607d1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44670,DS-1a7d136c-1b2f-407e-abba-dc56947c39d2,DISK], DatanodeInfoWithStorage[127.0.0.1:38283,DS-ef99f53f-29f3-474c-935b-5a62be2607d1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44670,DS-1a7d136c-1b2f-407e-abba-dc56947c39d2,DISK], DatanodeInfoWithStorage[127.0.0.1:38283,DS-ef99f53f-29f3-474c-935b-5a62be2607d1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44670,DS-1a7d136c-1b2f-407e-abba-dc56947c39d2,DISK], DatanodeInfoWithStorage[127.0.0.1:38283,DS-ef99f53f-29f3-474c-935b-5a62be2607d1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33371,DS-20d9fc55-724e-42de-9afe-319067ab7b15,DISK], DatanodeInfoWithStorage[127.0.0.1:45982,DS-bcc16a26-1c8d-4947-9b2c-9150698aea44,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33371,DS-20d9fc55-724e-42de-9afe-319067ab7b15,DISK], DatanodeInfoWithStorage[127.0.0.1:45982,DS-bcc16a26-1c8d-4947-9b2c-9150698aea44,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33371,DS-20d9fc55-724e-42de-9afe-319067ab7b15,DISK], DatanodeInfoWithStorage[127.0.0.1:45982,DS-bcc16a26-1c8d-4947-9b2c-9150698aea44,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33371,DS-20d9fc55-724e-42de-9afe-319067ab7b15,DISK], DatanodeInfoWithStorage[127.0.0.1:45982,DS-bcc16a26-1c8d-4947-9b2c-9150698aea44,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43615,DS-30c345b9-dca0-460f-98b2-42b4085c9bbb,DISK], DatanodeInfoWithStorage[127.0.0.1:34548,DS-f2ac267f-2708-4b8d-9c5a-66b134367b34,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34548,DS-f2ac267f-2708-4b8d-9c5a-66b134367b34,DISK], DatanodeInfoWithStorage[127.0.0.1:43615,DS-30c345b9-dca0-460f-98b2-42b4085c9bbb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43615,DS-30c345b9-dca0-460f-98b2-42b4085c9bbb,DISK], DatanodeInfoWithStorage[127.0.0.1:34548,DS-f2ac267f-2708-4b8d-9c5a-66b134367b34,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34548,DS-f2ac267f-2708-4b8d-9c5a-66b134367b34,DISK], DatanodeInfoWithStorage[127.0.0.1:43615,DS-30c345b9-dca0-460f-98b2-42b4085c9bbb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36759,DS-933a9e62-25bc-44ad-bde2-350be7e41c37,DISK], DatanodeInfoWithStorage[127.0.0.1:43375,DS-cab29986-11f5-48ee-ba20-2b1d5f079491,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43375,DS-cab29986-11f5-48ee-ba20-2b1d5f079491,DISK], DatanodeInfoWithStorage[127.0.0.1:36759,DS-933a9e62-25bc-44ad-bde2-350be7e41c37,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36759,DS-933a9e62-25bc-44ad-bde2-350be7e41c37,DISK], DatanodeInfoWithStorage[127.0.0.1:43375,DS-cab29986-11f5-48ee-ba20-2b1d5f079491,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43375,DS-cab29986-11f5-48ee-ba20-2b1d5f079491,DISK], DatanodeInfoWithStorage[127.0.0.1:36759,DS-933a9e62-25bc-44ad-bde2-350be7e41c37,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33922,DS-25795510-33ff-43fd-a963-21a65146b94c,DISK], DatanodeInfoWithStorage[127.0.0.1:45883,DS-4a997e9e-6b67-4cd0-bf41-e1b850219139,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33922,DS-25795510-33ff-43fd-a963-21a65146b94c,DISK], DatanodeInfoWithStorage[127.0.0.1:45883,DS-4a997e9e-6b67-4cd0-bf41-e1b850219139,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33922,DS-25795510-33ff-43fd-a963-21a65146b94c,DISK], DatanodeInfoWithStorage[127.0.0.1:45883,DS-4a997e9e-6b67-4cd0-bf41-e1b850219139,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33922,DS-25795510-33ff-43fd-a963-21a65146b94c,DISK], DatanodeInfoWithStorage[127.0.0.1:45883,DS-4a997e9e-6b67-4cd0-bf41-e1b850219139,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43225,DS-4264976c-903a-4af3-88ac-9278b2e0a587,DISK], DatanodeInfoWithStorage[127.0.0.1:35911,DS-c5abc079-8304-4cb7-b6f7-dc4612948782,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43225,DS-4264976c-903a-4af3-88ac-9278b2e0a587,DISK], DatanodeInfoWithStorage[127.0.0.1:35911,DS-c5abc079-8304-4cb7-b6f7-dc4612948782,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43225,DS-4264976c-903a-4af3-88ac-9278b2e0a587,DISK], DatanodeInfoWithStorage[127.0.0.1:35911,DS-c5abc079-8304-4cb7-b6f7-dc4612948782,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43225,DS-4264976c-903a-4af3-88ac-9278b2e0a587,DISK], DatanodeInfoWithStorage[127.0.0.1:35911,DS-c5abc079-8304-4cb7-b6f7-dc4612948782,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38399,DS-d587bf57-b949-41fd-ae94-8b4d79ff8cec,DISK], DatanodeInfoWithStorage[127.0.0.1:33190,DS-25040c27-bb4f-4b3d-8f83-a8c6fb07f2a1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38399,DS-d587bf57-b949-41fd-ae94-8b4d79ff8cec,DISK], DatanodeInfoWithStorage[127.0.0.1:33190,DS-25040c27-bb4f-4b3d-8f83-a8c6fb07f2a1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38399,DS-d587bf57-b949-41fd-ae94-8b4d79ff8cec,DISK], DatanodeInfoWithStorage[127.0.0.1:33190,DS-25040c27-bb4f-4b3d-8f83-a8c6fb07f2a1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38399,DS-d587bf57-b949-41fd-ae94-8b4d79ff8cec,DISK], DatanodeInfoWithStorage[127.0.0.1:33190,DS-25040c27-bb4f-4b3d-8f83-a8c6fb07f2a1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46717,DS-5549fb5b-ee6b-4d88-bb9b-20515820aed2,DISK], DatanodeInfoWithStorage[127.0.0.1:44972,DS-06dbdd51-63f6-4611-a22b-cecf8d7f012f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46717,DS-5549fb5b-ee6b-4d88-bb9b-20515820aed2,DISK], DatanodeInfoWithStorage[127.0.0.1:44972,DS-06dbdd51-63f6-4611-a22b-cecf8d7f012f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46717,DS-5549fb5b-ee6b-4d88-bb9b-20515820aed2,DISK], DatanodeInfoWithStorage[127.0.0.1:44972,DS-06dbdd51-63f6-4611-a22b-cecf8d7f012f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46717,DS-5549fb5b-ee6b-4d88-bb9b-20515820aed2,DISK], DatanodeInfoWithStorage[127.0.0.1:44972,DS-06dbdd51-63f6-4611-a22b-cecf8d7f012f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41912,DS-24a812c5-be28-4a6c-bfdc-2eaf388cf2e0,DISK], DatanodeInfoWithStorage[127.0.0.1:37377,DS-70c60fb8-4bed-49e4-8feb-ecaf4e00e5b7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37377,DS-70c60fb8-4bed-49e4-8feb-ecaf4e00e5b7,DISK], DatanodeInfoWithStorage[127.0.0.1:41912,DS-24a812c5-be28-4a6c-bfdc-2eaf388cf2e0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41912,DS-24a812c5-be28-4a6c-bfdc-2eaf388cf2e0,DISK], DatanodeInfoWithStorage[127.0.0.1:37377,DS-70c60fb8-4bed-49e4-8feb-ecaf4e00e5b7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37377,DS-70c60fb8-4bed-49e4-8feb-ecaf4e00e5b7,DISK], DatanodeInfoWithStorage[127.0.0.1:41912,DS-24a812c5-be28-4a6c-bfdc-2eaf388cf2e0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36275,DS-06d4fd72-654c-47c2-a6ee-4fc74d70f4a5,DISK], DatanodeInfoWithStorage[127.0.0.1:37095,DS-6eb4afe9-ec48-4b7c-97e5-ed2c52e996c0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36275,DS-06d4fd72-654c-47c2-a6ee-4fc74d70f4a5,DISK], DatanodeInfoWithStorage[127.0.0.1:37095,DS-6eb4afe9-ec48-4b7c-97e5-ed2c52e996c0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36275,DS-06d4fd72-654c-47c2-a6ee-4fc74d70f4a5,DISK], DatanodeInfoWithStorage[127.0.0.1:37095,DS-6eb4afe9-ec48-4b7c-97e5-ed2c52e996c0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36275,DS-06d4fd72-654c-47c2-a6ee-4fc74d70f4a5,DISK], DatanodeInfoWithStorage[127.0.0.1:37095,DS-6eb4afe9-ec48-4b7c-97e5-ed2c52e996c0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45596,DS-bacfc046-4e2b-4623-b1cf-683b53076cf8,DISK], DatanodeInfoWithStorage[127.0.0.1:36411,DS-764208a4-86cc-4e9c-95e7-8e58eb87604a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36411,DS-764208a4-86cc-4e9c-95e7-8e58eb87604a,DISK], DatanodeInfoWithStorage[127.0.0.1:45596,DS-bacfc046-4e2b-4623-b1cf-683b53076cf8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45596,DS-bacfc046-4e2b-4623-b1cf-683b53076cf8,DISK], DatanodeInfoWithStorage[127.0.0.1:36411,DS-764208a4-86cc-4e9c-95e7-8e58eb87604a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36411,DS-764208a4-86cc-4e9c-95e7-8e58eb87604a,DISK], DatanodeInfoWithStorage[127.0.0.1:45596,DS-bacfc046-4e2b-4623-b1cf-683b53076cf8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40414,DS-0ed7b796-6332-4ccf-a457-b4f77c6eec53,DISK], DatanodeInfoWithStorage[127.0.0.1:33741,DS-be3ec016-4779-4f81-a05f-716733ac68a2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40414,DS-0ed7b796-6332-4ccf-a457-b4f77c6eec53,DISK], DatanodeInfoWithStorage[127.0.0.1:33741,DS-be3ec016-4779-4f81-a05f-716733ac68a2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40414,DS-0ed7b796-6332-4ccf-a457-b4f77c6eec53,DISK], DatanodeInfoWithStorage[127.0.0.1:33741,DS-be3ec016-4779-4f81-a05f-716733ac68a2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40414,DS-0ed7b796-6332-4ccf-a457-b4f77c6eec53,DISK], DatanodeInfoWithStorage[127.0.0.1:33741,DS-be3ec016-4779-4f81-a05f-716733ac68a2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40084,DS-2cbbb624-ffcd-4743-a0ae-e4dcce1cacdd,DISK], DatanodeInfoWithStorage[127.0.0.1:39705,DS-93d4d7f6-119d-4eee-82d2-5d75597e9fdc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40084,DS-2cbbb624-ffcd-4743-a0ae-e4dcce1cacdd,DISK], DatanodeInfoWithStorage[127.0.0.1:39705,DS-93d4d7f6-119d-4eee-82d2-5d75597e9fdc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40084,DS-2cbbb624-ffcd-4743-a0ae-e4dcce1cacdd,DISK], DatanodeInfoWithStorage[127.0.0.1:39705,DS-93d4d7f6-119d-4eee-82d2-5d75597e9fdc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40084,DS-2cbbb624-ffcd-4743-a0ae-e4dcce1cacdd,DISK], DatanodeInfoWithStorage[127.0.0.1:39705,DS-93d4d7f6-119d-4eee-82d2-5d75597e9fdc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34991,DS-c9034d3a-6bec-4519-b795-ea2609545929,DISK], DatanodeInfoWithStorage[127.0.0.1:41935,DS-cee0e69f-90a2-46a0-bd91-b231a0b59968,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34991,DS-c9034d3a-6bec-4519-b795-ea2609545929,DISK], DatanodeInfoWithStorage[127.0.0.1:41935,DS-cee0e69f-90a2-46a0-bd91-b231a0b59968,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34991,DS-c9034d3a-6bec-4519-b795-ea2609545929,DISK], DatanodeInfoWithStorage[127.0.0.1:41935,DS-cee0e69f-90a2-46a0-bd91-b231a0b59968,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34991,DS-c9034d3a-6bec-4519-b795-ea2609545929,DISK], DatanodeInfoWithStorage[127.0.0.1:41935,DS-cee0e69f-90a2-46a0-bd91-b231a0b59968,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43092,DS-4c982b71-8326-49d4-a52d-db06f6904494,DISK], DatanodeInfoWithStorage[127.0.0.1:44565,DS-81467669-b1bd-4a33-9c2b-1c9ae6a8048b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43092,DS-4c982b71-8326-49d4-a52d-db06f6904494,DISK], DatanodeInfoWithStorage[127.0.0.1:44565,DS-81467669-b1bd-4a33-9c2b-1c9ae6a8048b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43092,DS-4c982b71-8326-49d4-a52d-db06f6904494,DISK], DatanodeInfoWithStorage[127.0.0.1:44565,DS-81467669-b1bd-4a33-9c2b-1c9ae6a8048b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43092,DS-4c982b71-8326-49d4-a52d-db06f6904494,DISK], DatanodeInfoWithStorage[127.0.0.1:44565,DS-81467669-b1bd-4a33-9c2b-1c9ae6a8048b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41331,DS-e529aebc-70e7-4e03-8e63-3da635393c02,DISK], DatanodeInfoWithStorage[127.0.0.1:41932,DS-f29886e1-77a5-45b4-acbe-ee691e398162,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41331,DS-e529aebc-70e7-4e03-8e63-3da635393c02,DISK], DatanodeInfoWithStorage[127.0.0.1:41932,DS-f29886e1-77a5-45b4-acbe-ee691e398162,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41331,DS-e529aebc-70e7-4e03-8e63-3da635393c02,DISK], DatanodeInfoWithStorage[127.0.0.1:41932,DS-f29886e1-77a5-45b4-acbe-ee691e398162,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41331,DS-e529aebc-70e7-4e03-8e63-3da635393c02,DISK], DatanodeInfoWithStorage[127.0.0.1:41932,DS-f29886e1-77a5-45b4-acbe-ee691e398162,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39913,DS-a9031fb8-0859-4c0e-b054-5c3457840c4f,DISK], DatanodeInfoWithStorage[127.0.0.1:35723,DS-437244c5-e2a9-4eb8-9383-48bf8342e3c2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35723,DS-437244c5-e2a9-4eb8-9383-48bf8342e3c2,DISK], DatanodeInfoWithStorage[127.0.0.1:39913,DS-a9031fb8-0859-4c0e-b054-5c3457840c4f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39913,DS-a9031fb8-0859-4c0e-b054-5c3457840c4f,DISK], DatanodeInfoWithStorage[127.0.0.1:35723,DS-437244c5-e2a9-4eb8-9383-48bf8342e3c2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35723,DS-437244c5-e2a9-4eb8-9383-48bf8342e3c2,DISK], DatanodeInfoWithStorage[127.0.0.1:39913,DS-a9031fb8-0859-4c0e-b054-5c3457840c4f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36367,DS-0b211214-5c22-43bf-89ed-afc412849854,DISK], DatanodeInfoWithStorage[127.0.0.1:46283,DS-50c3d444-026e-422d-afa7-e302d345436e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36367,DS-0b211214-5c22-43bf-89ed-afc412849854,DISK], DatanodeInfoWithStorage[127.0.0.1:46283,DS-50c3d444-026e-422d-afa7-e302d345436e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36367,DS-0b211214-5c22-43bf-89ed-afc412849854,DISK], DatanodeInfoWithStorage[127.0.0.1:46283,DS-50c3d444-026e-422d-afa7-e302d345436e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36367,DS-0b211214-5c22-43bf-89ed-afc412849854,DISK], DatanodeInfoWithStorage[127.0.0.1:46283,DS-50c3d444-026e-422d-afa7-e302d345436e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33135,DS-4c20fc12-318d-4e33-856f-b21b13c1ca80,DISK], DatanodeInfoWithStorage[127.0.0.1:42266,DS-af0fe1b7-a6de-465a-84b5-0eae9938e701,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42266,DS-af0fe1b7-a6de-465a-84b5-0eae9938e701,DISK], DatanodeInfoWithStorage[127.0.0.1:33135,DS-4c20fc12-318d-4e33-856f-b21b13c1ca80,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33135,DS-4c20fc12-318d-4e33-856f-b21b13c1ca80,DISK], DatanodeInfoWithStorage[127.0.0.1:42266,DS-af0fe1b7-a6de-465a-84b5-0eae9938e701,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42266,DS-af0fe1b7-a6de-465a-84b5-0eae9938e701,DISK], DatanodeInfoWithStorage[127.0.0.1:33135,DS-4c20fc12-318d-4e33-856f-b21b13c1ca80,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40438,DS-89c49f8f-09a4-467c-8b0d-0a3f3b42aaa9,DISK], DatanodeInfoWithStorage[127.0.0.1:34618,DS-e74436b2-3356-46d0-8bd2-356b2acb9074,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34618,DS-e74436b2-3356-46d0-8bd2-356b2acb9074,DISK], DatanodeInfoWithStorage[127.0.0.1:40438,DS-89c49f8f-09a4-467c-8b0d-0a3f3b42aaa9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40438,DS-89c49f8f-09a4-467c-8b0d-0a3f3b42aaa9,DISK], DatanodeInfoWithStorage[127.0.0.1:34618,DS-e74436b2-3356-46d0-8bd2-356b2acb9074,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34618,DS-e74436b2-3356-46d0-8bd2-356b2acb9074,DISK], DatanodeInfoWithStorage[127.0.0.1:40438,DS-89c49f8f-09a4-467c-8b0d-0a3f3b42aaa9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41164,DS-137d04bb-9eb5-4977-8f8b-82a974ae71de,DISK], DatanodeInfoWithStorage[127.0.0.1:38890,DS-120cd289-7605-478a-bd6d-6ac16291aacc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41164,DS-137d04bb-9eb5-4977-8f8b-82a974ae71de,DISK], DatanodeInfoWithStorage[127.0.0.1:38890,DS-120cd289-7605-478a-bd6d-6ac16291aacc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41164,DS-137d04bb-9eb5-4977-8f8b-82a974ae71de,DISK], DatanodeInfoWithStorage[127.0.0.1:38890,DS-120cd289-7605-478a-bd6d-6ac16291aacc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41164,DS-137d04bb-9eb5-4977-8f8b-82a974ae71de,DISK], DatanodeInfoWithStorage[127.0.0.1:38890,DS-120cd289-7605-478a-bd6d-6ac16291aacc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44456,DS-ae46ac92-f013-4e2d-ac89-eb70d94b7490,DISK], DatanodeInfoWithStorage[127.0.0.1:39389,DS-4f130673-73b0-4f18-bac2-6848aa57be1c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39389,DS-4f130673-73b0-4f18-bac2-6848aa57be1c,DISK], DatanodeInfoWithStorage[127.0.0.1:44456,DS-ae46ac92-f013-4e2d-ac89-eb70d94b7490,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44456,DS-ae46ac92-f013-4e2d-ac89-eb70d94b7490,DISK], DatanodeInfoWithStorage[127.0.0.1:39389,DS-4f130673-73b0-4f18-bac2-6848aa57be1c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39389,DS-4f130673-73b0-4f18-bac2-6848aa57be1c,DISK], DatanodeInfoWithStorage[127.0.0.1:44456,DS-ae46ac92-f013-4e2d-ac89-eb70d94b7490,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44617,DS-5a9158f8-8419-439d-b288-075bede26bf6,DISK], DatanodeInfoWithStorage[127.0.0.1:44865,DS-4a72a4fe-55a7-49d8-a903-b972953d58b0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44865,DS-4a72a4fe-55a7-49d8-a903-b972953d58b0,DISK], DatanodeInfoWithStorage[127.0.0.1:44617,DS-5a9158f8-8419-439d-b288-075bede26bf6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44617,DS-5a9158f8-8419-439d-b288-075bede26bf6,DISK], DatanodeInfoWithStorage[127.0.0.1:44865,DS-4a72a4fe-55a7-49d8-a903-b972953d58b0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44865,DS-4a72a4fe-55a7-49d8-a903-b972953d58b0,DISK], DatanodeInfoWithStorage[127.0.0.1:44617,DS-5a9158f8-8419-439d-b288-075bede26bf6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40744,DS-b208da29-25dc-42b6-946a-0eb1ec2007c6,DISK], DatanodeInfoWithStorage[127.0.0.1:40841,DS-ad897d72-7db7-4974-a3b1-cc3428d939c7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40744,DS-b208da29-25dc-42b6-946a-0eb1ec2007c6,DISK], DatanodeInfoWithStorage[127.0.0.1:40841,DS-ad897d72-7db7-4974-a3b1-cc3428d939c7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40744,DS-b208da29-25dc-42b6-946a-0eb1ec2007c6,DISK], DatanodeInfoWithStorage[127.0.0.1:40841,DS-ad897d72-7db7-4974-a3b1-cc3428d939c7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40744,DS-b208da29-25dc-42b6-946a-0eb1ec2007c6,DISK], DatanodeInfoWithStorage[127.0.0.1:40841,DS-ad897d72-7db7-4974-a3b1-cc3428d939c7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35243,DS-7e74e236-5612-484d-a47d-f3b5994312aa,DISK], DatanodeInfoWithStorage[127.0.0.1:45383,DS-48c1da21-a7f2-431b-830b-71a5261e5c0a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45383,DS-48c1da21-a7f2-431b-830b-71a5261e5c0a,DISK], DatanodeInfoWithStorage[127.0.0.1:35243,DS-7e74e236-5612-484d-a47d-f3b5994312aa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35243,DS-7e74e236-5612-484d-a47d-f3b5994312aa,DISK], DatanodeInfoWithStorage[127.0.0.1:45383,DS-48c1da21-a7f2-431b-830b-71a5261e5c0a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45383,DS-48c1da21-a7f2-431b-830b-71a5261e5c0a,DISK], DatanodeInfoWithStorage[127.0.0.1:35243,DS-7e74e236-5612-484d-a47d-f3b5994312aa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44909,DS-23cccc2d-fc44-433e-98d8-15fcb56b7db3,DISK], DatanodeInfoWithStorage[127.0.0.1:42791,DS-59dc60ab-c1cb-430e-bfa2-4fd2e235d389,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44909,DS-23cccc2d-fc44-433e-98d8-15fcb56b7db3,DISK], DatanodeInfoWithStorage[127.0.0.1:42791,DS-59dc60ab-c1cb-430e-bfa2-4fd2e235d389,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44909,DS-23cccc2d-fc44-433e-98d8-15fcb56b7db3,DISK], DatanodeInfoWithStorage[127.0.0.1:42791,DS-59dc60ab-c1cb-430e-bfa2-4fd2e235d389,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44909,DS-23cccc2d-fc44-433e-98d8-15fcb56b7db3,DISK], DatanodeInfoWithStorage[127.0.0.1:42791,DS-59dc60ab-c1cb-430e-bfa2-4fd2e235d389,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35934,DS-4a56ed1e-b79b-43aa-ab0d-7ee533c94129,DISK], DatanodeInfoWithStorage[127.0.0.1:42284,DS-5822e48a-0cf2-4428-b787-c3278e61ef09,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35934,DS-4a56ed1e-b79b-43aa-ab0d-7ee533c94129,DISK], DatanodeInfoWithStorage[127.0.0.1:42284,DS-5822e48a-0cf2-4428-b787-c3278e61ef09,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35934,DS-4a56ed1e-b79b-43aa-ab0d-7ee533c94129,DISK], DatanodeInfoWithStorage[127.0.0.1:42284,DS-5822e48a-0cf2-4428-b787-c3278e61ef09,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35934,DS-4a56ed1e-b79b-43aa-ab0d-7ee533c94129,DISK], DatanodeInfoWithStorage[127.0.0.1:42284,DS-5822e48a-0cf2-4428-b787-c3278e61ef09,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42510,DS-5b0ede74-2c56-4303-bce2-50c03ce191cc,DISK], DatanodeInfoWithStorage[127.0.0.1:42165,DS-e23dae43-038f-42df-9c84-6761c099299f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42510,DS-5b0ede74-2c56-4303-bce2-50c03ce191cc,DISK], DatanodeInfoWithStorage[127.0.0.1:42165,DS-e23dae43-038f-42df-9c84-6761c099299f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42510,DS-5b0ede74-2c56-4303-bce2-50c03ce191cc,DISK], DatanodeInfoWithStorage[127.0.0.1:42165,DS-e23dae43-038f-42df-9c84-6761c099299f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42510,DS-5b0ede74-2c56-4303-bce2-50c03ce191cc,DISK], DatanodeInfoWithStorage[127.0.0.1:42165,DS-e23dae43-038f-42df-9c84-6761c099299f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37564,DS-cfdfb45c-d6c8-4f2d-9a93-81a384ec56bd,DISK], DatanodeInfoWithStorage[127.0.0.1:38446,DS-dbef0692-97c7-4603-ac74-b3efc5658688,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37564,DS-cfdfb45c-d6c8-4f2d-9a93-81a384ec56bd,DISK], DatanodeInfoWithStorage[127.0.0.1:38446,DS-dbef0692-97c7-4603-ac74-b3efc5658688,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37564,DS-cfdfb45c-d6c8-4f2d-9a93-81a384ec56bd,DISK], DatanodeInfoWithStorage[127.0.0.1:38446,DS-dbef0692-97c7-4603-ac74-b3efc5658688,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37564,DS-cfdfb45c-d6c8-4f2d-9a93-81a384ec56bd,DISK], DatanodeInfoWithStorage[127.0.0.1:38446,DS-dbef0692-97c7-4603-ac74-b3efc5658688,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46565,DS-272661fa-e031-451c-9051-062e688c9bba,DISK], DatanodeInfoWithStorage[127.0.0.1:37466,DS-7f0ffee6-dca4-4715-b099-89c7089a5c06,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46565,DS-272661fa-e031-451c-9051-062e688c9bba,DISK], DatanodeInfoWithStorage[127.0.0.1:37466,DS-7f0ffee6-dca4-4715-b099-89c7089a5c06,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46565,DS-272661fa-e031-451c-9051-062e688c9bba,DISK], DatanodeInfoWithStorage[127.0.0.1:37466,DS-7f0ffee6-dca4-4715-b099-89c7089a5c06,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46565,DS-272661fa-e031-451c-9051-062e688c9bba,DISK], DatanodeInfoWithStorage[127.0.0.1:37466,DS-7f0ffee6-dca4-4715-b099-89c7089a5c06,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44472,DS-fa2f81b3-faf5-4b52-9a1d-f7b8986ccf10,DISK], DatanodeInfoWithStorage[127.0.0.1:43324,DS-e33e6e95-6783-4d57-989e-f670e868350e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44472,DS-fa2f81b3-faf5-4b52-9a1d-f7b8986ccf10,DISK], DatanodeInfoWithStorage[127.0.0.1:43324,DS-e33e6e95-6783-4d57-989e-f670e868350e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44472,DS-fa2f81b3-faf5-4b52-9a1d-f7b8986ccf10,DISK], DatanodeInfoWithStorage[127.0.0.1:43324,DS-e33e6e95-6783-4d57-989e-f670e868350e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44472,DS-fa2f81b3-faf5-4b52-9a1d-f7b8986ccf10,DISK], DatanodeInfoWithStorage[127.0.0.1:43324,DS-e33e6e95-6783-4d57-989e-f670e868350e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46550,DS-c9c30f7e-8949-4288-be94-d7f01831d508,DISK], DatanodeInfoWithStorage[127.0.0.1:40516,DS-570398e2-6220-498e-9eea-a81c75e5f0ba,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46550,DS-c9c30f7e-8949-4288-be94-d7f01831d508,DISK], DatanodeInfoWithStorage[127.0.0.1:40516,DS-570398e2-6220-498e-9eea-a81c75e5f0ba,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46550,DS-c9c30f7e-8949-4288-be94-d7f01831d508,DISK], DatanodeInfoWithStorage[127.0.0.1:40516,DS-570398e2-6220-498e-9eea-a81c75e5f0ba,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46550,DS-c9c30f7e-8949-4288-be94-d7f01831d508,DISK], DatanodeInfoWithStorage[127.0.0.1:40516,DS-570398e2-6220-498e-9eea-a81c75e5f0ba,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45379,DS-d6542a92-e0b3-4d4f-a2cf-dcb3f2539ec7,DISK], DatanodeInfoWithStorage[127.0.0.1:38783,DS-9e66e6fd-3149-4401-9ed2-3b072e80fb33,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45379,DS-d6542a92-e0b3-4d4f-a2cf-dcb3f2539ec7,DISK], DatanodeInfoWithStorage[127.0.0.1:38783,DS-9e66e6fd-3149-4401-9ed2-3b072e80fb33,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45379,DS-d6542a92-e0b3-4d4f-a2cf-dcb3f2539ec7,DISK], DatanodeInfoWithStorage[127.0.0.1:38783,DS-9e66e6fd-3149-4401-9ed2-3b072e80fb33,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45379,DS-d6542a92-e0b3-4d4f-a2cf-dcb3f2539ec7,DISK], DatanodeInfoWithStorage[127.0.0.1:38783,DS-9e66e6fd-3149-4401-9ed2-3b072e80fb33,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40590,DS-4559c4a3-6b8b-4046-9ebc-985e7f102bb5,DISK], DatanodeInfoWithStorage[127.0.0.1:36892,DS-50a2efb4-4b0b-482d-8cd0-bb185d1fb04c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40590,DS-4559c4a3-6b8b-4046-9ebc-985e7f102bb5,DISK], DatanodeInfoWithStorage[127.0.0.1:36892,DS-50a2efb4-4b0b-482d-8cd0-bb185d1fb04c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40590,DS-4559c4a3-6b8b-4046-9ebc-985e7f102bb5,DISK], DatanodeInfoWithStorage[127.0.0.1:36892,DS-50a2efb4-4b0b-482d-8cd0-bb185d1fb04c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40590,DS-4559c4a3-6b8b-4046-9ebc-985e7f102bb5,DISK], DatanodeInfoWithStorage[127.0.0.1:36892,DS-50a2efb4-4b0b-482d-8cd0-bb185d1fb04c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35270,DS-73b6b2bd-cae0-4e69-ac5e-ef5de05b1e3a,DISK], DatanodeInfoWithStorage[127.0.0.1:42556,DS-dcf47c7d-7294-4443-9c48-a4d8ac2cee83,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42556,DS-dcf47c7d-7294-4443-9c48-a4d8ac2cee83,DISK], DatanodeInfoWithStorage[127.0.0.1:35270,DS-73b6b2bd-cae0-4e69-ac5e-ef5de05b1e3a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35270,DS-73b6b2bd-cae0-4e69-ac5e-ef5de05b1e3a,DISK], DatanodeInfoWithStorage[127.0.0.1:42556,DS-dcf47c7d-7294-4443-9c48-a4d8ac2cee83,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42556,DS-dcf47c7d-7294-4443-9c48-a4d8ac2cee83,DISK], DatanodeInfoWithStorage[127.0.0.1:35270,DS-73b6b2bd-cae0-4e69-ac5e-ef5de05b1e3a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38926,DS-775e1c33-9f77-4be7-aa27-95b46e25efa1,DISK], DatanodeInfoWithStorage[127.0.0.1:37056,DS-9db0594c-a822-417d-9762-53c824289637,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37056,DS-9db0594c-a822-417d-9762-53c824289637,DISK], DatanodeInfoWithStorage[127.0.0.1:38926,DS-775e1c33-9f77-4be7-aa27-95b46e25efa1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38926,DS-775e1c33-9f77-4be7-aa27-95b46e25efa1,DISK], DatanodeInfoWithStorage[127.0.0.1:37056,DS-9db0594c-a822-417d-9762-53c824289637,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37056,DS-9db0594c-a822-417d-9762-53c824289637,DISK], DatanodeInfoWithStorage[127.0.0.1:38926,DS-775e1c33-9f77-4be7-aa27-95b46e25efa1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43719,DS-1f9793a2-e0f3-4a56-ae82-c38f09abe1b4,DISK], DatanodeInfoWithStorage[127.0.0.1:46347,DS-1944f760-159c-4646-85a6-10d42adcb83d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43719,DS-1f9793a2-e0f3-4a56-ae82-c38f09abe1b4,DISK], DatanodeInfoWithStorage[127.0.0.1:46347,DS-1944f760-159c-4646-85a6-10d42adcb83d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43719,DS-1f9793a2-e0f3-4a56-ae82-c38f09abe1b4,DISK], DatanodeInfoWithStorage[127.0.0.1:46347,DS-1944f760-159c-4646-85a6-10d42adcb83d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43719,DS-1f9793a2-e0f3-4a56-ae82-c38f09abe1b4,DISK], DatanodeInfoWithStorage[127.0.0.1:46347,DS-1944f760-159c-4646-85a6-10d42adcb83d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41645,DS-aec78830-f386-4e8b-bc8d-27a0772c5352,DISK], DatanodeInfoWithStorage[127.0.0.1:35367,DS-1fc03088-1023-4cf6-bdf7-eab7f364333e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41645,DS-aec78830-f386-4e8b-bc8d-27a0772c5352,DISK], DatanodeInfoWithStorage[127.0.0.1:35367,DS-1fc03088-1023-4cf6-bdf7-eab7f364333e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41645,DS-aec78830-f386-4e8b-bc8d-27a0772c5352,DISK], DatanodeInfoWithStorage[127.0.0.1:35367,DS-1fc03088-1023-4cf6-bdf7-eab7f364333e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41645,DS-aec78830-f386-4e8b-bc8d-27a0772c5352,DISK], DatanodeInfoWithStorage[127.0.0.1:35367,DS-1fc03088-1023-4cf6-bdf7-eab7f364333e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36178,DS-43b8322b-f14c-4aa6-9b01-b88dbce3e2df,DISK], DatanodeInfoWithStorage[127.0.0.1:45256,DS-3097f0c2-4164-4079-b6e4-b4f217026df5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36178,DS-43b8322b-f14c-4aa6-9b01-b88dbce3e2df,DISK], DatanodeInfoWithStorage[127.0.0.1:45256,DS-3097f0c2-4164-4079-b6e4-b4f217026df5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36178,DS-43b8322b-f14c-4aa6-9b01-b88dbce3e2df,DISK], DatanodeInfoWithStorage[127.0.0.1:45256,DS-3097f0c2-4164-4079-b6e4-b4f217026df5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36178,DS-43b8322b-f14c-4aa6-9b01-b88dbce3e2df,DISK], DatanodeInfoWithStorage[127.0.0.1:45256,DS-3097f0c2-4164-4079-b6e4-b4f217026df5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34313,DS-27e8e725-e828-4e09-940a-c8a0a17f7cbc,DISK], DatanodeInfoWithStorage[127.0.0.1:33976,DS-d50dcbf7-1888-4f6d-9e53-d79d5e868ea0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34313,DS-27e8e725-e828-4e09-940a-c8a0a17f7cbc,DISK], DatanodeInfoWithStorage[127.0.0.1:33976,DS-d50dcbf7-1888-4f6d-9e53-d79d5e868ea0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34313,DS-27e8e725-e828-4e09-940a-c8a0a17f7cbc,DISK], DatanodeInfoWithStorage[127.0.0.1:33976,DS-d50dcbf7-1888-4f6d-9e53-d79d5e868ea0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34313,DS-27e8e725-e828-4e09-940a-c8a0a17f7cbc,DISK], DatanodeInfoWithStorage[127.0.0.1:33976,DS-d50dcbf7-1888-4f6d-9e53-d79d5e868ea0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: test timed out after 780 seconds
stackTrace: org.junit.runners.model.TestTimedOutException: test timed out after 780 seconds
	at sun.misc.Unsafe.park(Native Method)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
	at org.apache.hadoop.hbase.regionserver.wal.TestFSHLog.testUnflushedSeqIdTracking(TestFSHLog.java:213)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46426,DS-9a57168d-7af2-470e-83de-3fad80cb5a95,DISK], DatanodeInfoWithStorage[127.0.0.1:44786,DS-3e231bb1-adbd-4956-8a2d-82bb67da6375,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46426,DS-9a57168d-7af2-470e-83de-3fad80cb5a95,DISK], DatanodeInfoWithStorage[127.0.0.1:44786,DS-3e231bb1-adbd-4956-8a2d-82bb67da6375,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46426,DS-9a57168d-7af2-470e-83de-3fad80cb5a95,DISK], DatanodeInfoWithStorage[127.0.0.1:44786,DS-3e231bb1-adbd-4956-8a2d-82bb67da6375,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46426,DS-9a57168d-7af2-470e-83de-3fad80cb5a95,DISK], DatanodeInfoWithStorage[127.0.0.1:44786,DS-3e231bb1-adbd-4956-8a2d-82bb67da6375,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39168,DS-c1c61b62-e7cc-4181-a30f-1c55ea9271fe,DISK], DatanodeInfoWithStorage[127.0.0.1:41278,DS-3fa6622e-853f-453c-8ecc-b909b2ecdbd2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39168,DS-c1c61b62-e7cc-4181-a30f-1c55ea9271fe,DISK], DatanodeInfoWithStorage[127.0.0.1:41278,DS-3fa6622e-853f-453c-8ecc-b909b2ecdbd2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39168,DS-c1c61b62-e7cc-4181-a30f-1c55ea9271fe,DISK], DatanodeInfoWithStorage[127.0.0.1:41278,DS-3fa6622e-853f-453c-8ecc-b909b2ecdbd2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39168,DS-c1c61b62-e7cc-4181-a30f-1c55ea9271fe,DISK], DatanodeInfoWithStorage[127.0.0.1:41278,DS-3fa6622e-853f-453c-8ecc-b909b2ecdbd2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44924,DS-e18eb507-6673-41b8-8673-90429a01e5f4,DISK], DatanodeInfoWithStorage[127.0.0.1:44887,DS-e891a4a7-cd03-44ef-aaa6-7e94e9329e99,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44924,DS-e18eb507-6673-41b8-8673-90429a01e5f4,DISK], DatanodeInfoWithStorage[127.0.0.1:44887,DS-e891a4a7-cd03-44ef-aaa6-7e94e9329e99,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44924,DS-e18eb507-6673-41b8-8673-90429a01e5f4,DISK], DatanodeInfoWithStorage[127.0.0.1:44887,DS-e891a4a7-cd03-44ef-aaa6-7e94e9329e99,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44924,DS-e18eb507-6673-41b8-8673-90429a01e5f4,DISK], DatanodeInfoWithStorage[127.0.0.1:44887,DS-e891a4a7-cd03-44ef-aaa6-7e94e9329e99,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46812,DS-a1b7a8f5-4eff-411b-a550-fc365c8f8ccb,DISK], DatanodeInfoWithStorage[127.0.0.1:38510,DS-97f43780-e41b-4546-98d0-2c78b7d2c8e1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46812,DS-a1b7a8f5-4eff-411b-a550-fc365c8f8ccb,DISK], DatanodeInfoWithStorage[127.0.0.1:38510,DS-97f43780-e41b-4546-98d0-2c78b7d2c8e1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46812,DS-a1b7a8f5-4eff-411b-a550-fc365c8f8ccb,DISK], DatanodeInfoWithStorage[127.0.0.1:38510,DS-97f43780-e41b-4546-98d0-2c78b7d2c8e1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46812,DS-a1b7a8f5-4eff-411b-a550-fc365c8f8ccb,DISK], DatanodeInfoWithStorage[127.0.0.1:38510,DS-97f43780-e41b-4546-98d0-2c78b7d2c8e1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36232,DS-e5667d46-32fc-498b-84b7-b2486cb3c9e4,DISK], DatanodeInfoWithStorage[127.0.0.1:39221,DS-59ae0128-43ab-4458-8b41-bdfc2fb2ab78,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39221,DS-59ae0128-43ab-4458-8b41-bdfc2fb2ab78,DISK], DatanodeInfoWithStorage[127.0.0.1:36232,DS-e5667d46-32fc-498b-84b7-b2486cb3c9e4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36232,DS-e5667d46-32fc-498b-84b7-b2486cb3c9e4,DISK], DatanodeInfoWithStorage[127.0.0.1:39221,DS-59ae0128-43ab-4458-8b41-bdfc2fb2ab78,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39221,DS-59ae0128-43ab-4458-8b41-bdfc2fb2ab78,DISK], DatanodeInfoWithStorage[127.0.0.1:36232,DS-e5667d46-32fc-498b-84b7-b2486cb3c9e4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35357,DS-c80386fa-8627-4a12-89b9-126c6cb461e8,DISK], DatanodeInfoWithStorage[127.0.0.1:39563,DS-180b8087-218c-417a-8697-b73443e7d971,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35357,DS-c80386fa-8627-4a12-89b9-126c6cb461e8,DISK], DatanodeInfoWithStorage[127.0.0.1:39563,DS-180b8087-218c-417a-8697-b73443e7d971,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35357,DS-c80386fa-8627-4a12-89b9-126c6cb461e8,DISK], DatanodeInfoWithStorage[127.0.0.1:39563,DS-180b8087-218c-417a-8697-b73443e7d971,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35357,DS-c80386fa-8627-4a12-89b9-126c6cb461e8,DISK], DatanodeInfoWithStorage[127.0.0.1:39563,DS-180b8087-218c-417a-8697-b73443e7d971,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45478,DS-44cd794c-3ca9-4389-9eb7-9f81c24b5f7a,DISK], DatanodeInfoWithStorage[127.0.0.1:34297,DS-12d35878-f6c2-46eb-a5cc-17b26bb978ce,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45478,DS-44cd794c-3ca9-4389-9eb7-9f81c24b5f7a,DISK], DatanodeInfoWithStorage[127.0.0.1:34297,DS-12d35878-f6c2-46eb-a5cc-17b26bb978ce,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45478,DS-44cd794c-3ca9-4389-9eb7-9f81c24b5f7a,DISK], DatanodeInfoWithStorage[127.0.0.1:34297,DS-12d35878-f6c2-46eb-a5cc-17b26bb978ce,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45478,DS-44cd794c-3ca9-4389-9eb7-9f81c24b5f7a,DISK], DatanodeInfoWithStorage[127.0.0.1:34297,DS-12d35878-f6c2-46eb-a5cc-17b26bb978ce,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34312,DS-d7f768f1-b17e-4393-bf56-3783b05c9439,DISK], DatanodeInfoWithStorage[127.0.0.1:43260,DS-cb8ad4cd-c868-4c4a-a2c2-53f709911c16,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34312,DS-d7f768f1-b17e-4393-bf56-3783b05c9439,DISK], DatanodeInfoWithStorage[127.0.0.1:43260,DS-cb8ad4cd-c868-4c4a-a2c2-53f709911c16,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34312,DS-d7f768f1-b17e-4393-bf56-3783b05c9439,DISK], DatanodeInfoWithStorage[127.0.0.1:43260,DS-cb8ad4cd-c868-4c4a-a2c2-53f709911c16,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34312,DS-d7f768f1-b17e-4393-bf56-3783b05c9439,DISK], DatanodeInfoWithStorage[127.0.0.1:43260,DS-cb8ad4cd-c868-4c4a-a2c2-53f709911c16,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41853,DS-bad71a8d-c97d-4271-977a-fe7590535bfb,DISK], DatanodeInfoWithStorage[127.0.0.1:41712,DS-cdef0441-9405-4f3f-be8e-37c51fa7551a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41712,DS-cdef0441-9405-4f3f-be8e-37c51fa7551a,DISK], DatanodeInfoWithStorage[127.0.0.1:41853,DS-bad71a8d-c97d-4271-977a-fe7590535bfb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41853,DS-bad71a8d-c97d-4271-977a-fe7590535bfb,DISK], DatanodeInfoWithStorage[127.0.0.1:41712,DS-cdef0441-9405-4f3f-be8e-37c51fa7551a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41712,DS-cdef0441-9405-4f3f-be8e-37c51fa7551a,DISK], DatanodeInfoWithStorage[127.0.0.1:41853,DS-bad71a8d-c97d-4271-977a-fe7590535bfb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTracking
reconfPoint: -3
result: -1
failureMessage: Append sequenceId=2, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=2, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34563,DS-cec9e09b-a332-4203-b160-1e39e9545719,DISK], DatanodeInfoWithStorage[127.0.0.1:40230,DS-87bcb1f4-28cd-426f-a954-a16b87e805fd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40230,DS-87bcb1f4-28cd-426f-a954-a16b87e805fd,DISK], DatanodeInfoWithStorage[127.0.0.1:34563,DS-cec9e09b-a332-4203-b160-1e39e9545719,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34563,DS-cec9e09b-a332-4203-b160-1e39e9545719,DISK], DatanodeInfoWithStorage[127.0.0.1:40230,DS-87bcb1f4-28cd-426f-a954-a16b87e805fd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40230,DS-87bcb1f4-28cd-426f-a954-a16b87e805fd,DISK], DatanodeInfoWithStorage[127.0.0.1:34563,DS-cec9e09b-a332-4203-b160-1e39e9545719,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 50 out of 50
v1v1v2v2 failed with probability 9 out of 50
result: might be true error
Total execution time in seconds : 9355

reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33561,DS-a96a70ea-0186-4f8e-84e6-9290067002c5,DISK], DatanodeInfoWithStorage[127.0.0.1:33093,DS-b0a1ab53-5346-45df-a925-c773604f9914,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33561,DS-a96a70ea-0186-4f8e-84e6-9290067002c5,DISK], DatanodeInfoWithStorage[127.0.0.1:33093,DS-b0a1ab53-5346-45df-a925-c773604f9914,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33561,DS-a96a70ea-0186-4f8e-84e6-9290067002c5,DISK], DatanodeInfoWithStorage[127.0.0.1:33093,DS-b0a1ab53-5346-45df-a925-c773604f9914,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33561,DS-a96a70ea-0186-4f8e-84e6-9290067002c5,DISK], DatanodeInfoWithStorage[127.0.0.1:33093,DS-b0a1ab53-5346-45df-a925-c773604f9914,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39772,DS-6ffab5d3-724f-4164-ada5-329b9a35f4a9,DISK], DatanodeInfoWithStorage[127.0.0.1:43154,DS-3b4bdbce-409f-400f-8d7f-3509e4f40bdd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39772,DS-6ffab5d3-724f-4164-ada5-329b9a35f4a9,DISK], DatanodeInfoWithStorage[127.0.0.1:43154,DS-3b4bdbce-409f-400f-8d7f-3509e4f40bdd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39772,DS-6ffab5d3-724f-4164-ada5-329b9a35f4a9,DISK], DatanodeInfoWithStorage[127.0.0.1:43154,DS-3b4bdbce-409f-400f-8d7f-3509e4f40bdd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39772,DS-6ffab5d3-724f-4164-ada5-329b9a35f4a9,DISK], DatanodeInfoWithStorage[127.0.0.1:43154,DS-3b4bdbce-409f-400f-8d7f-3509e4f40bdd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33442,DS-08c1e5cd-e291-41ea-80d3-2ccb9c651d6c,DISK], DatanodeInfoWithStorage[127.0.0.1:32773,DS-8c6f4ee1-b952-49c5-897e-33558884643e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32773,DS-8c6f4ee1-b952-49c5-897e-33558884643e,DISK], DatanodeInfoWithStorage[127.0.0.1:33442,DS-08c1e5cd-e291-41ea-80d3-2ccb9c651d6c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33442,DS-08c1e5cd-e291-41ea-80d3-2ccb9c651d6c,DISK], DatanodeInfoWithStorage[127.0.0.1:32773,DS-8c6f4ee1-b952-49c5-897e-33558884643e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32773,DS-8c6f4ee1-b952-49c5-897e-33558884643e,DISK], DatanodeInfoWithStorage[127.0.0.1:33442,DS-08c1e5cd-e291-41ea-80d3-2ccb9c651d6c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39976,DS-f36a2dbd-ee3f-4978-984a-b365ae105360,DISK], DatanodeInfoWithStorage[127.0.0.1:40258,DS-add51c3f-4059-4933-9d59-a9ec8151e483,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39976,DS-f36a2dbd-ee3f-4978-984a-b365ae105360,DISK], DatanodeInfoWithStorage[127.0.0.1:40258,DS-add51c3f-4059-4933-9d59-a9ec8151e483,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39976,DS-f36a2dbd-ee3f-4978-984a-b365ae105360,DISK], DatanodeInfoWithStorage[127.0.0.1:40258,DS-add51c3f-4059-4933-9d59-a9ec8151e483,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39976,DS-f36a2dbd-ee3f-4978-984a-b365ae105360,DISK], DatanodeInfoWithStorage[127.0.0.1:40258,DS-add51c3f-4059-4933-9d59-a9ec8151e483,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46281,DS-5381df06-54be-4e9a-8ff7-fa0b8407caf3,DISK], DatanodeInfoWithStorage[127.0.0.1:44914,DS-de54d7f2-68fb-4ca6-8017-3392efe63073,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46281,DS-5381df06-54be-4e9a-8ff7-fa0b8407caf3,DISK], DatanodeInfoWithStorage[127.0.0.1:44914,DS-de54d7f2-68fb-4ca6-8017-3392efe63073,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46281,DS-5381df06-54be-4e9a-8ff7-fa0b8407caf3,DISK], DatanodeInfoWithStorage[127.0.0.1:44914,DS-de54d7f2-68fb-4ca6-8017-3392efe63073,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46281,DS-5381df06-54be-4e9a-8ff7-fa0b8407caf3,DISK], DatanodeInfoWithStorage[127.0.0.1:44914,DS-de54d7f2-68fb-4ca6-8017-3392efe63073,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39955,DS-a88bad62-507c-415b-9d7a-c98b938ca9f0,DISK], DatanodeInfoWithStorage[127.0.0.1:36565,DS-a0bb2b66-08ee-4058-8492-b5b9c5163ed3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36565,DS-a0bb2b66-08ee-4058-8492-b5b9c5163ed3,DISK], DatanodeInfoWithStorage[127.0.0.1:39955,DS-a88bad62-507c-415b-9d7a-c98b938ca9f0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39955,DS-a88bad62-507c-415b-9d7a-c98b938ca9f0,DISK], DatanodeInfoWithStorage[127.0.0.1:36565,DS-a0bb2b66-08ee-4058-8492-b5b9c5163ed3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36565,DS-a0bb2b66-08ee-4058-8492-b5b9c5163ed3,DISK], DatanodeInfoWithStorage[127.0.0.1:39955,DS-a88bad62-507c-415b-9d7a-c98b938ca9f0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34342,DS-b49c707e-a0d4-40cf-ace3-1e3c410891ec,DISK], DatanodeInfoWithStorage[127.0.0.1:38987,DS-645e44ad-7959-4ff3-932c-7703d103a0f3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34342,DS-b49c707e-a0d4-40cf-ace3-1e3c410891ec,DISK], DatanodeInfoWithStorage[127.0.0.1:38987,DS-645e44ad-7959-4ff3-932c-7703d103a0f3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34342,DS-b49c707e-a0d4-40cf-ace3-1e3c410891ec,DISK], DatanodeInfoWithStorage[127.0.0.1:38987,DS-645e44ad-7959-4ff3-932c-7703d103a0f3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34342,DS-b49c707e-a0d4-40cf-ace3-1e3c410891ec,DISK], DatanodeInfoWithStorage[127.0.0.1:38987,DS-645e44ad-7959-4ff3-932c-7703d103a0f3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46553,DS-e7214459-7402-4a82-8ae3-c7302c50904b,DISK], DatanodeInfoWithStorage[127.0.0.1:39379,DS-3463765a-bfad-46c2-8eee-d5fb92ec8191,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39379,DS-3463765a-bfad-46c2-8eee-d5fb92ec8191,DISK], DatanodeInfoWithStorage[127.0.0.1:46553,DS-e7214459-7402-4a82-8ae3-c7302c50904b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46553,DS-e7214459-7402-4a82-8ae3-c7302c50904b,DISK], DatanodeInfoWithStorage[127.0.0.1:39379,DS-3463765a-bfad-46c2-8eee-d5fb92ec8191,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39379,DS-3463765a-bfad-46c2-8eee-d5fb92ec8191,DISK], DatanodeInfoWithStorage[127.0.0.1:46553,DS-e7214459-7402-4a82-8ae3-c7302c50904b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46591,DS-ce480223-0740-4430-b855-282f2ba4f010,DISK], DatanodeInfoWithStorage[127.0.0.1:42817,DS-b78fab04-afdb-49f4-9530-f4cdda26c1dd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42817,DS-b78fab04-afdb-49f4-9530-f4cdda26c1dd,DISK], DatanodeInfoWithStorage[127.0.0.1:46591,DS-ce480223-0740-4430-b855-282f2ba4f010,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46591,DS-ce480223-0740-4430-b855-282f2ba4f010,DISK], DatanodeInfoWithStorage[127.0.0.1:42817,DS-b78fab04-afdb-49f4-9530-f4cdda26c1dd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42817,DS-b78fab04-afdb-49f4-9530-f4cdda26c1dd,DISK], DatanodeInfoWithStorage[127.0.0.1:46591,DS-ce480223-0740-4430-b855-282f2ba4f010,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35884,DS-4ed9d73f-39f5-4cd3-8769-142f2f68d079,DISK], DatanodeInfoWithStorage[127.0.0.1:46811,DS-0a479b89-6c14-46c8-a63d-01850422fb14,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35884,DS-4ed9d73f-39f5-4cd3-8769-142f2f68d079,DISK], DatanodeInfoWithStorage[127.0.0.1:46811,DS-0a479b89-6c14-46c8-a63d-01850422fb14,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35884,DS-4ed9d73f-39f5-4cd3-8769-142f2f68d079,DISK], DatanodeInfoWithStorage[127.0.0.1:46811,DS-0a479b89-6c14-46c8-a63d-01850422fb14,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35884,DS-4ed9d73f-39f5-4cd3-8769-142f2f68d079,DISK], DatanodeInfoWithStorage[127.0.0.1:46811,DS-0a479b89-6c14-46c8-a63d-01850422fb14,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43689,DS-63318164-6e95-4a76-8695-4e1aea1ac8a5,DISK], DatanodeInfoWithStorage[127.0.0.1:42398,DS-d2cc09d9-1656-4d3a-881e-8936ff38548e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42398,DS-d2cc09d9-1656-4d3a-881e-8936ff38548e,DISK], DatanodeInfoWithStorage[127.0.0.1:43689,DS-63318164-6e95-4a76-8695-4e1aea1ac8a5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43689,DS-63318164-6e95-4a76-8695-4e1aea1ac8a5,DISK], DatanodeInfoWithStorage[127.0.0.1:42398,DS-d2cc09d9-1656-4d3a-881e-8936ff38548e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42398,DS-d2cc09d9-1656-4d3a-881e-8936ff38548e,DISK], DatanodeInfoWithStorage[127.0.0.1:43689,DS-63318164-6e95-4a76-8695-4e1aea1ac8a5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38375,DS-4993c5c1-6574-42e5-a6c7-6f5948b9c954,DISK], DatanodeInfoWithStorage[127.0.0.1:43535,DS-36dd2098-7708-4c71-9625-7fe26330447a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38375,DS-4993c5c1-6574-42e5-a6c7-6f5948b9c954,DISK], DatanodeInfoWithStorage[127.0.0.1:43535,DS-36dd2098-7708-4c71-9625-7fe26330447a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38375,DS-4993c5c1-6574-42e5-a6c7-6f5948b9c954,DISK], DatanodeInfoWithStorage[127.0.0.1:43535,DS-36dd2098-7708-4c71-9625-7fe26330447a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38375,DS-4993c5c1-6574-42e5-a6c7-6f5948b9c954,DISK], DatanodeInfoWithStorage[127.0.0.1:43535,DS-36dd2098-7708-4c71-9625-7fe26330447a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34272,DS-b46fbe24-5f1c-4b36-a394-79ea88027464,DISK], DatanodeInfoWithStorage[127.0.0.1:34262,DS-89b27b6b-b060-45a9-819b-9e419323765e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34262,DS-89b27b6b-b060-45a9-819b-9e419323765e,DISK], DatanodeInfoWithStorage[127.0.0.1:34272,DS-b46fbe24-5f1c-4b36-a394-79ea88027464,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34272,DS-b46fbe24-5f1c-4b36-a394-79ea88027464,DISK], DatanodeInfoWithStorage[127.0.0.1:34262,DS-89b27b6b-b060-45a9-819b-9e419323765e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34262,DS-89b27b6b-b060-45a9-819b-9e419323765e,DISK], DatanodeInfoWithStorage[127.0.0.1:34272,DS-b46fbe24-5f1c-4b36-a394-79ea88027464,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46529,DS-7ee01ed3-3922-4bdf-90b0-0ad106904e56,DISK], DatanodeInfoWithStorage[127.0.0.1:38703,DS-05110e45-266c-4693-b1a4-b1fa7dc2b42b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46529,DS-7ee01ed3-3922-4bdf-90b0-0ad106904e56,DISK], DatanodeInfoWithStorage[127.0.0.1:38703,DS-05110e45-266c-4693-b1a4-b1fa7dc2b42b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46529,DS-7ee01ed3-3922-4bdf-90b0-0ad106904e56,DISK], DatanodeInfoWithStorage[127.0.0.1:38703,DS-05110e45-266c-4693-b1a4-b1fa7dc2b42b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46529,DS-7ee01ed3-3922-4bdf-90b0-0ad106904e56,DISK], DatanodeInfoWithStorage[127.0.0.1:38703,DS-05110e45-266c-4693-b1a4-b1fa7dc2b42b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35517,DS-f7207301-2245-470f-bdcf-40ab6d4173fe,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35517,DS-f7207301-2245-470f-bdcf-40ab6d4173fe,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35517,DS-f7207301-2245-470f-bdcf-40ab6d4173fe,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35517,DS-f7207301-2245-470f-bdcf-40ab6d4173fe,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44426,DS-34155a9f-d00c-401a-b98f-135518dd7803,DISK], DatanodeInfoWithStorage[127.0.0.1:35051,DS-f04e1c3a-cb63-46f5-8db6-560e41122172,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44426,DS-34155a9f-d00c-401a-b98f-135518dd7803,DISK], DatanodeInfoWithStorage[127.0.0.1:35051,DS-f04e1c3a-cb63-46f5-8db6-560e41122172,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44426,DS-34155a9f-d00c-401a-b98f-135518dd7803,DISK], DatanodeInfoWithStorage[127.0.0.1:35051,DS-f04e1c3a-cb63-46f5-8db6-560e41122172,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44426,DS-34155a9f-d00c-401a-b98f-135518dd7803,DISK], DatanodeInfoWithStorage[127.0.0.1:35051,DS-f04e1c3a-cb63-46f5-8db6-560e41122172,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43052,DS-86bad3df-3db9-43ed-a976-5f0d4708dcb2,DISK], DatanodeInfoWithStorage[127.0.0.1:33383,DS-7496e14c-014a-4e2b-ab66-04d4ba5f2e18,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43052,DS-86bad3df-3db9-43ed-a976-5f0d4708dcb2,DISK], DatanodeInfoWithStorage[127.0.0.1:33383,DS-7496e14c-014a-4e2b-ab66-04d4ba5f2e18,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43052,DS-86bad3df-3db9-43ed-a976-5f0d4708dcb2,DISK], DatanodeInfoWithStorage[127.0.0.1:33383,DS-7496e14c-014a-4e2b-ab66-04d4ba5f2e18,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43052,DS-86bad3df-3db9-43ed-a976-5f0d4708dcb2,DISK], DatanodeInfoWithStorage[127.0.0.1:33383,DS-7496e14c-014a-4e2b-ab66-04d4ba5f2e18,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40529,DS-c208d140-d6e5-4cca-aeb8-64cc5ba5e763,DISK], DatanodeInfoWithStorage[127.0.0.1:43028,DS-56719569-7029-4f1a-ba5f-3bf7df899bda,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43028,DS-56719569-7029-4f1a-ba5f-3bf7df899bda,DISK], DatanodeInfoWithStorage[127.0.0.1:40529,DS-c208d140-d6e5-4cca-aeb8-64cc5ba5e763,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40529,DS-c208d140-d6e5-4cca-aeb8-64cc5ba5e763,DISK], DatanodeInfoWithStorage[127.0.0.1:43028,DS-56719569-7029-4f1a-ba5f-3bf7df899bda,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43028,DS-56719569-7029-4f1a-ba5f-3bf7df899bda,DISK], DatanodeInfoWithStorage[127.0.0.1:40529,DS-c208d140-d6e5-4cca-aeb8-64cc5ba5e763,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37166,DS-ce83a4cc-3896-4945-a636-934d4c8ea452,DISK], DatanodeInfoWithStorage[127.0.0.1:34903,DS-df9851e6-42d6-4892-af30-6e9828d7cdfe,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37166,DS-ce83a4cc-3896-4945-a636-934d4c8ea452,DISK], DatanodeInfoWithStorage[127.0.0.1:34903,DS-df9851e6-42d6-4892-af30-6e9828d7cdfe,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37166,DS-ce83a4cc-3896-4945-a636-934d4c8ea452,DISK], DatanodeInfoWithStorage[127.0.0.1:34903,DS-df9851e6-42d6-4892-af30-6e9828d7cdfe,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37166,DS-ce83a4cc-3896-4945-a636-934d4c8ea452,DISK], DatanodeInfoWithStorage[127.0.0.1:34903,DS-df9851e6-42d6-4892-af30-6e9828d7cdfe,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42008,DS-7ba94b8c-0561-45dc-940c-d7e5c9643bfd,DISK], DatanodeInfoWithStorage[127.0.0.1:37729,DS-8f7a8a95-5ff0-4c3b-b7a1-f44a7f1b4781,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42008,DS-7ba94b8c-0561-45dc-940c-d7e5c9643bfd,DISK], DatanodeInfoWithStorage[127.0.0.1:37729,DS-8f7a8a95-5ff0-4c3b-b7a1-f44a7f1b4781,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42008,DS-7ba94b8c-0561-45dc-940c-d7e5c9643bfd,DISK], DatanodeInfoWithStorage[127.0.0.1:37729,DS-8f7a8a95-5ff0-4c3b-b7a1-f44a7f1b4781,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42008,DS-7ba94b8c-0561-45dc-940c-d7e5c9643bfd,DISK], DatanodeInfoWithStorage[127.0.0.1:37729,DS-8f7a8a95-5ff0-4c3b-b7a1-f44a7f1b4781,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41373,DS-a886df84-d7db-4646-b534-0dff453dccee,DISK], DatanodeInfoWithStorage[127.0.0.1:33065,DS-ac5943bf-f654-4065-8ef9-dfa05458b5da,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33065,DS-ac5943bf-f654-4065-8ef9-dfa05458b5da,DISK], DatanodeInfoWithStorage[127.0.0.1:41373,DS-a886df84-d7db-4646-b534-0dff453dccee,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41373,DS-a886df84-d7db-4646-b534-0dff453dccee,DISK], DatanodeInfoWithStorage[127.0.0.1:33065,DS-ac5943bf-f654-4065-8ef9-dfa05458b5da,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33065,DS-ac5943bf-f654-4065-8ef9-dfa05458b5da,DISK], DatanodeInfoWithStorage[127.0.0.1:41373,DS-a886df84-d7db-4646-b534-0dff453dccee,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35583,DS-38d58d81-557c-414f-b80a-7d70dc4867be,DISK], DatanodeInfoWithStorage[127.0.0.1:44378,DS-a840f44b-2b4f-4f35-a7c4-d0ff3cb3728f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35583,DS-38d58d81-557c-414f-b80a-7d70dc4867be,DISK], DatanodeInfoWithStorage[127.0.0.1:44378,DS-a840f44b-2b4f-4f35-a7c4-d0ff3cb3728f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35583,DS-38d58d81-557c-414f-b80a-7d70dc4867be,DISK], DatanodeInfoWithStorage[127.0.0.1:44378,DS-a840f44b-2b4f-4f35-a7c4-d0ff3cb3728f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35583,DS-38d58d81-557c-414f-b80a-7d70dc4867be,DISK], DatanodeInfoWithStorage[127.0.0.1:44378,DS-a840f44b-2b4f-4f35-a7c4-d0ff3cb3728f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38045,DS-bbd6de8a-6b48-42bc-a354-1ac76c98fde6,DISK], DatanodeInfoWithStorage[127.0.0.1:33161,DS-f5db6592-ac42-4c2f-8c40-84f0b4b5ad64,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33161,DS-f5db6592-ac42-4c2f-8c40-84f0b4b5ad64,DISK], DatanodeInfoWithStorage[127.0.0.1:38045,DS-bbd6de8a-6b48-42bc-a354-1ac76c98fde6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38045,DS-bbd6de8a-6b48-42bc-a354-1ac76c98fde6,DISK], DatanodeInfoWithStorage[127.0.0.1:33161,DS-f5db6592-ac42-4c2f-8c40-84f0b4b5ad64,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33161,DS-f5db6592-ac42-4c2f-8c40-84f0b4b5ad64,DISK], DatanodeInfoWithStorage[127.0.0.1:38045,DS-bbd6de8a-6b48-42bc-a354-1ac76c98fde6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45897,DS-9040b973-dd52-4c56-a981-c754b4baef0d,DISK], DatanodeInfoWithStorage[127.0.0.1:34700,DS-4aa6a0e1-6db3-4b50-ba67-735852f2a2c5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45897,DS-9040b973-dd52-4c56-a981-c754b4baef0d,DISK], DatanodeInfoWithStorage[127.0.0.1:34700,DS-4aa6a0e1-6db3-4b50-ba67-735852f2a2c5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45897,DS-9040b973-dd52-4c56-a981-c754b4baef0d,DISK], DatanodeInfoWithStorage[127.0.0.1:34700,DS-4aa6a0e1-6db3-4b50-ba67-735852f2a2c5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45897,DS-9040b973-dd52-4c56-a981-c754b4baef0d,DISK], DatanodeInfoWithStorage[127.0.0.1:34700,DS-4aa6a0e1-6db3-4b50-ba67-735852f2a2c5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38831,DS-66ee20cb-86ba-4c09-a283-6c022a179e59,DISK], DatanodeInfoWithStorage[127.0.0.1:44678,DS-efc3e628-6b27-490b-9b4d-494100a26e4c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44678,DS-efc3e628-6b27-490b-9b4d-494100a26e4c,DISK], DatanodeInfoWithStorage[127.0.0.1:38831,DS-66ee20cb-86ba-4c09-a283-6c022a179e59,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38831,DS-66ee20cb-86ba-4c09-a283-6c022a179e59,DISK], DatanodeInfoWithStorage[127.0.0.1:44678,DS-efc3e628-6b27-490b-9b4d-494100a26e4c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44678,DS-efc3e628-6b27-490b-9b4d-494100a26e4c,DISK], DatanodeInfoWithStorage[127.0.0.1:38831,DS-66ee20cb-86ba-4c09-a283-6c022a179e59,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35756,DS-6dcb8625-5f06-4d75-b3a0-53066564e0a3,DISK], DatanodeInfoWithStorage[127.0.0.1:36145,DS-d5a3fba3-5b38-4a12-838f-2b357dcbd15d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36145,DS-d5a3fba3-5b38-4a12-838f-2b357dcbd15d,DISK], DatanodeInfoWithStorage[127.0.0.1:35756,DS-6dcb8625-5f06-4d75-b3a0-53066564e0a3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35756,DS-6dcb8625-5f06-4d75-b3a0-53066564e0a3,DISK], DatanodeInfoWithStorage[127.0.0.1:36145,DS-d5a3fba3-5b38-4a12-838f-2b357dcbd15d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36145,DS-d5a3fba3-5b38-4a12-838f-2b357dcbd15d,DISK], DatanodeInfoWithStorage[127.0.0.1:35756,DS-6dcb8625-5f06-4d75-b3a0-53066564e0a3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45931,DS-582feb2a-8039-4962-b405-1dbf606df809,DISK], DatanodeInfoWithStorage[127.0.0.1:37922,DS-4794ee29-f509-4e41-ad11-28cb10f15b02,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37922,DS-4794ee29-f509-4e41-ad11-28cb10f15b02,DISK], DatanodeInfoWithStorage[127.0.0.1:45931,DS-582feb2a-8039-4962-b405-1dbf606df809,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45931,DS-582feb2a-8039-4962-b405-1dbf606df809,DISK], DatanodeInfoWithStorage[127.0.0.1:37922,DS-4794ee29-f509-4e41-ad11-28cb10f15b02,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37922,DS-4794ee29-f509-4e41-ad11-28cb10f15b02,DISK], DatanodeInfoWithStorage[127.0.0.1:45931,DS-582feb2a-8039-4962-b405-1dbf606df809,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42497,DS-24c57405-e26a-44f1-9709-956d604903e0,DISK], DatanodeInfoWithStorage[127.0.0.1:37819,DS-6a103168-dfef-4000-90e5-5cf0e54a055d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42497,DS-24c57405-e26a-44f1-9709-956d604903e0,DISK], DatanodeInfoWithStorage[127.0.0.1:37819,DS-6a103168-dfef-4000-90e5-5cf0e54a055d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42497,DS-24c57405-e26a-44f1-9709-956d604903e0,DISK], DatanodeInfoWithStorage[127.0.0.1:37819,DS-6a103168-dfef-4000-90e5-5cf0e54a055d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42497,DS-24c57405-e26a-44f1-9709-956d604903e0,DISK], DatanodeInfoWithStorage[127.0.0.1:37819,DS-6a103168-dfef-4000-90e5-5cf0e54a055d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45330,DS-52061f77-a269-427c-9ffb-fe9cd3dc0fa2,DISK], DatanodeInfoWithStorage[127.0.0.1:41538,DS-843516e9-2b61-4657-8623-05e193d33c39,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41538,DS-843516e9-2b61-4657-8623-05e193d33c39,DISK], DatanodeInfoWithStorage[127.0.0.1:45330,DS-52061f77-a269-427c-9ffb-fe9cd3dc0fa2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45330,DS-52061f77-a269-427c-9ffb-fe9cd3dc0fa2,DISK], DatanodeInfoWithStorage[127.0.0.1:41538,DS-843516e9-2b61-4657-8623-05e193d33c39,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41538,DS-843516e9-2b61-4657-8623-05e193d33c39,DISK], DatanodeInfoWithStorage[127.0.0.1:45330,DS-52061f77-a269-427c-9ffb-fe9cd3dc0fa2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34019,DS-efbbc249-34b6-4d6a-8ee7-55d7bd6c905c,DISK], DatanodeInfoWithStorage[127.0.0.1:32935,DS-d40d068f-0095-42cf-80bf-49d9ecbbffa0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34019,DS-efbbc249-34b6-4d6a-8ee7-55d7bd6c905c,DISK], DatanodeInfoWithStorage[127.0.0.1:32935,DS-d40d068f-0095-42cf-80bf-49d9ecbbffa0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34019,DS-efbbc249-34b6-4d6a-8ee7-55d7bd6c905c,DISK], DatanodeInfoWithStorage[127.0.0.1:32935,DS-d40d068f-0095-42cf-80bf-49d9ecbbffa0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34019,DS-efbbc249-34b6-4d6a-8ee7-55d7bd6c905c,DISK], DatanodeInfoWithStorage[127.0.0.1:32935,DS-d40d068f-0095-42cf-80bf-49d9ecbbffa0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34708,DS-4cf5a887-aead-4ad3-840a-85df9ed67ca4,DISK], DatanodeInfoWithStorage[127.0.0.1:41323,DS-d9db7268-6410-409c-8c71-b3ac6c13ad7f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34708,DS-4cf5a887-aead-4ad3-840a-85df9ed67ca4,DISK], DatanodeInfoWithStorage[127.0.0.1:41323,DS-d9db7268-6410-409c-8c71-b3ac6c13ad7f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34708,DS-4cf5a887-aead-4ad3-840a-85df9ed67ca4,DISK], DatanodeInfoWithStorage[127.0.0.1:41323,DS-d9db7268-6410-409c-8c71-b3ac6c13ad7f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34708,DS-4cf5a887-aead-4ad3-840a-85df9ed67ca4,DISK], DatanodeInfoWithStorage[127.0.0.1:41323,DS-d9db7268-6410-409c-8c71-b3ac6c13ad7f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37973,DS-5c10b3cb-1bbb-48c5-9075-2a8244cb5cda,DISK], DatanodeInfoWithStorage[127.0.0.1:39431,DS-463376c6-bd21-478a-9588-d929f2b0df16,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39431,DS-463376c6-bd21-478a-9588-d929f2b0df16,DISK], DatanodeInfoWithStorage[127.0.0.1:37973,DS-5c10b3cb-1bbb-48c5-9075-2a8244cb5cda,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37973,DS-5c10b3cb-1bbb-48c5-9075-2a8244cb5cda,DISK], DatanodeInfoWithStorage[127.0.0.1:39431,DS-463376c6-bd21-478a-9588-d929f2b0df16,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39431,DS-463376c6-bd21-478a-9588-d929f2b0df16,DISK], DatanodeInfoWithStorage[127.0.0.1:37973,DS-5c10b3cb-1bbb-48c5-9075-2a8244cb5cda,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44216,DS-40d8ce13-0813-4174-aaed-211da5234864,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-f2f159ed-79af-445b-87f7-281340ecaa28,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44216,DS-40d8ce13-0813-4174-aaed-211da5234864,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-f2f159ed-79af-445b-87f7-281340ecaa28,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44216,DS-40d8ce13-0813-4174-aaed-211da5234864,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-f2f159ed-79af-445b-87f7-281340ecaa28,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44216,DS-40d8ce13-0813-4174-aaed-211da5234864,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-f2f159ed-79af-445b-87f7-281340ecaa28,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44961,DS-5606dbd8-1e23-45a2-bb20-a77680dc0a34,DISK], DatanodeInfoWithStorage[127.0.0.1:40815,DS-92ac6a16-c0b1-4aab-864e-97bf2bfdd239,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44961,DS-5606dbd8-1e23-45a2-bb20-a77680dc0a34,DISK], DatanodeInfoWithStorage[127.0.0.1:40815,DS-92ac6a16-c0b1-4aab-864e-97bf2bfdd239,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44961,DS-5606dbd8-1e23-45a2-bb20-a77680dc0a34,DISK], DatanodeInfoWithStorage[127.0.0.1:40815,DS-92ac6a16-c0b1-4aab-864e-97bf2bfdd239,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44961,DS-5606dbd8-1e23-45a2-bb20-a77680dc0a34,DISK], DatanodeInfoWithStorage[127.0.0.1:40815,DS-92ac6a16-c0b1-4aab-864e-97bf2bfdd239,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34246,DS-695651d9-dbd5-4c65-9335-e364bd3a6e6b,DISK], DatanodeInfoWithStorage[127.0.0.1:40820,DS-a45b0724-08e2-4fe2-894a-3a6ec37381a7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34246,DS-695651d9-dbd5-4c65-9335-e364bd3a6e6b,DISK], DatanodeInfoWithStorage[127.0.0.1:40820,DS-a45b0724-08e2-4fe2-894a-3a6ec37381a7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34246,DS-695651d9-dbd5-4c65-9335-e364bd3a6e6b,DISK], DatanodeInfoWithStorage[127.0.0.1:40820,DS-a45b0724-08e2-4fe2-894a-3a6ec37381a7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34246,DS-695651d9-dbd5-4c65-9335-e364bd3a6e6b,DISK], DatanodeInfoWithStorage[127.0.0.1:40820,DS-a45b0724-08e2-4fe2-894a-3a6ec37381a7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38542,DS-11b416cb-5788-4a9a-a7a1-a6e571fa0fec,DISK], DatanodeInfoWithStorage[127.0.0.1:35802,DS-42a55b18-5551-4473-973b-83a6def0f68d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38542,DS-11b416cb-5788-4a9a-a7a1-a6e571fa0fec,DISK], DatanodeInfoWithStorage[127.0.0.1:35802,DS-42a55b18-5551-4473-973b-83a6def0f68d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38542,DS-11b416cb-5788-4a9a-a7a1-a6e571fa0fec,DISK], DatanodeInfoWithStorage[127.0.0.1:35802,DS-42a55b18-5551-4473-973b-83a6def0f68d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38542,DS-11b416cb-5788-4a9a-a7a1-a6e571fa0fec,DISK], DatanodeInfoWithStorage[127.0.0.1:35802,DS-42a55b18-5551-4473-973b-83a6def0f68d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46595,DS-83ea2cd8-ff0b-49f1-87a4-c10c370c4653,DISK], DatanodeInfoWithStorage[127.0.0.1:44455,DS-e0062f91-c7f2-492e-9052-b27957d6e386,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44455,DS-e0062f91-c7f2-492e-9052-b27957d6e386,DISK], DatanodeInfoWithStorage[127.0.0.1:46595,DS-83ea2cd8-ff0b-49f1-87a4-c10c370c4653,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46595,DS-83ea2cd8-ff0b-49f1-87a4-c10c370c4653,DISK], DatanodeInfoWithStorage[127.0.0.1:44455,DS-e0062f91-c7f2-492e-9052-b27957d6e386,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44455,DS-e0062f91-c7f2-492e-9052-b27957d6e386,DISK], DatanodeInfoWithStorage[127.0.0.1:46595,DS-83ea2cd8-ff0b-49f1-87a4-c10c370c4653,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45286,DS-77bceec0-1b03-4a60-8f98-068cb109711e,DISK], DatanodeInfoWithStorage[127.0.0.1:34632,DS-24cb0db7-df9d-4d6a-a7c7-8af376f05883,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45286,DS-77bceec0-1b03-4a60-8f98-068cb109711e,DISK], DatanodeInfoWithStorage[127.0.0.1:34632,DS-24cb0db7-df9d-4d6a-a7c7-8af376f05883,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45286,DS-77bceec0-1b03-4a60-8f98-068cb109711e,DISK], DatanodeInfoWithStorage[127.0.0.1:34632,DS-24cb0db7-df9d-4d6a-a7c7-8af376f05883,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45286,DS-77bceec0-1b03-4a60-8f98-068cb109711e,DISK], DatanodeInfoWithStorage[127.0.0.1:34632,DS-24cb0db7-df9d-4d6a-a7c7-8af376f05883,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45060,DS-82d53beb-c14c-4840-ad53-197b29974fb5,DISK], DatanodeInfoWithStorage[127.0.0.1:41645,DS-85c2f858-7241-429f-98be-807f00bafb65,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45060,DS-82d53beb-c14c-4840-ad53-197b29974fb5,DISK], DatanodeInfoWithStorage[127.0.0.1:41645,DS-85c2f858-7241-429f-98be-807f00bafb65,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45060,DS-82d53beb-c14c-4840-ad53-197b29974fb5,DISK], DatanodeInfoWithStorage[127.0.0.1:41645,DS-85c2f858-7241-429f-98be-807f00bafb65,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45060,DS-82d53beb-c14c-4840-ad53-197b29974fb5,DISK], DatanodeInfoWithStorage[127.0.0.1:41645,DS-85c2f858-7241-429f-98be-807f00bafb65,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33556,DS-dbf50dc7-8e23-4e74-a831-a14d47d715a4,DISK], DatanodeInfoWithStorage[127.0.0.1:39985,DS-ae7b277f-f3b0-44df-a52f-ba6db5f222f7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33556,DS-dbf50dc7-8e23-4e74-a831-a14d47d715a4,DISK], DatanodeInfoWithStorage[127.0.0.1:39985,DS-ae7b277f-f3b0-44df-a52f-ba6db5f222f7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33556,DS-dbf50dc7-8e23-4e74-a831-a14d47d715a4,DISK], DatanodeInfoWithStorage[127.0.0.1:39985,DS-ae7b277f-f3b0-44df-a52f-ba6db5f222f7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33556,DS-dbf50dc7-8e23-4e74-a831-a14d47d715a4,DISK], DatanodeInfoWithStorage[127.0.0.1:39985,DS-ae7b277f-f3b0-44df-a52f-ba6db5f222f7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33267,DS-e9652c8f-a359-4a95-bde6-77fae4376516,DISK], DatanodeInfoWithStorage[127.0.0.1:36628,DS-ebad3df9-3e12-4ac0-890a-dced94c17669,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36628,DS-ebad3df9-3e12-4ac0-890a-dced94c17669,DISK], DatanodeInfoWithStorage[127.0.0.1:33267,DS-e9652c8f-a359-4a95-bde6-77fae4376516,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33267,DS-e9652c8f-a359-4a95-bde6-77fae4376516,DISK], DatanodeInfoWithStorage[127.0.0.1:36628,DS-ebad3df9-3e12-4ac0-890a-dced94c17669,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36628,DS-ebad3df9-3e12-4ac0-890a-dced94c17669,DISK], DatanodeInfoWithStorage[127.0.0.1:33267,DS-e9652c8f-a359-4a95-bde6-77fae4376516,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45976,DS-d32c123c-027e-4e1d-9ced-a9aa52d1c517,DISK], DatanodeInfoWithStorage[127.0.0.1:40996,DS-32d28336-c8b4-4c6d-975e-5a725a25186f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45976,DS-d32c123c-027e-4e1d-9ced-a9aa52d1c517,DISK], DatanodeInfoWithStorage[127.0.0.1:40996,DS-32d28336-c8b4-4c6d-975e-5a725a25186f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45976,DS-d32c123c-027e-4e1d-9ced-a9aa52d1c517,DISK], DatanodeInfoWithStorage[127.0.0.1:40996,DS-32d28336-c8b4-4c6d-975e-5a725a25186f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45976,DS-d32c123c-027e-4e1d-9ced-a9aa52d1c517,DISK], DatanodeInfoWithStorage[127.0.0.1:40996,DS-32d28336-c8b4-4c6d-975e-5a725a25186f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45641,DS-d1329a2c-9ad5-443b-b5f1-fbe53cab5acd,DISK], DatanodeInfoWithStorage[127.0.0.1:44170,DS-d1496726-9337-4c33-a6ad-320533fa7467,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45641,DS-d1329a2c-9ad5-443b-b5f1-fbe53cab5acd,DISK], DatanodeInfoWithStorage[127.0.0.1:44170,DS-d1496726-9337-4c33-a6ad-320533fa7467,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45641,DS-d1329a2c-9ad5-443b-b5f1-fbe53cab5acd,DISK], DatanodeInfoWithStorage[127.0.0.1:44170,DS-d1496726-9337-4c33-a6ad-320533fa7467,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45641,DS-d1329a2c-9ad5-443b-b5f1-fbe53cab5acd,DISK], DatanodeInfoWithStorage[127.0.0.1:44170,DS-d1496726-9337-4c33-a6ad-320533fa7467,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45948,DS-8fb49887-51c6-4dd2-bf59-d55395645361,DISK], DatanodeInfoWithStorage[127.0.0.1:40451,DS-daf624bb-93b8-4d28-98bd-05babddea7c8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40451,DS-daf624bb-93b8-4d28-98bd-05babddea7c8,DISK], DatanodeInfoWithStorage[127.0.0.1:45948,DS-8fb49887-51c6-4dd2-bf59-d55395645361,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45948,DS-8fb49887-51c6-4dd2-bf59-d55395645361,DISK], DatanodeInfoWithStorage[127.0.0.1:40451,DS-daf624bb-93b8-4d28-98bd-05babddea7c8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40451,DS-daf624bb-93b8-4d28-98bd-05babddea7c8,DISK], DatanodeInfoWithStorage[127.0.0.1:45948,DS-8fb49887-51c6-4dd2-bf59-d55395645361,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43355,DS-e4b60db5-ce29-4345-a6e6-149b1e344b52,DISK], DatanodeInfoWithStorage[127.0.0.1:35641,DS-86985299-f40e-4e51-ac9c-187af15d7830,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35641,DS-86985299-f40e-4e51-ac9c-187af15d7830,DISK], DatanodeInfoWithStorage[127.0.0.1:43355,DS-e4b60db5-ce29-4345-a6e6-149b1e344b52,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43355,DS-e4b60db5-ce29-4345-a6e6-149b1e344b52,DISK], DatanodeInfoWithStorage[127.0.0.1:35641,DS-86985299-f40e-4e51-ac9c-187af15d7830,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35641,DS-86985299-f40e-4e51-ac9c-187af15d7830,DISK], DatanodeInfoWithStorage[127.0.0.1:43355,DS-e4b60db5-ce29-4345-a6e6-149b1e344b52,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35776,DS-2402c952-9720-4cb6-a742-d5c2c72fadb6,DISK], DatanodeInfoWithStorage[127.0.0.1:43756,DS-54c2ca6c-bafd-4455-bbfc-24b784cdda35,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35776,DS-2402c952-9720-4cb6-a742-d5c2c72fadb6,DISK], DatanodeInfoWithStorage[127.0.0.1:43756,DS-54c2ca6c-bafd-4455-bbfc-24b784cdda35,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35776,DS-2402c952-9720-4cb6-a742-d5c2c72fadb6,DISK], DatanodeInfoWithStorage[127.0.0.1:43756,DS-54c2ca6c-bafd-4455-bbfc-24b784cdda35,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35776,DS-2402c952-9720-4cb6-a742-d5c2c72fadb6,DISK], DatanodeInfoWithStorage[127.0.0.1:43756,DS-54c2ca6c-bafd-4455-bbfc-24b784cdda35,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42416,DS-e21516d9-b345-48b0-8d9e-22740197fd0b,DISK], DatanodeInfoWithStorage[127.0.0.1:39070,DS-fad3a31b-8ca7-433e-9349-243a4d2be2e4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42416,DS-e21516d9-b345-48b0-8d9e-22740197fd0b,DISK], DatanodeInfoWithStorage[127.0.0.1:39070,DS-fad3a31b-8ca7-433e-9349-243a4d2be2e4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42416,DS-e21516d9-b345-48b0-8d9e-22740197fd0b,DISK], DatanodeInfoWithStorage[127.0.0.1:39070,DS-fad3a31b-8ca7-433e-9349-243a4d2be2e4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42416,DS-e21516d9-b345-48b0-8d9e-22740197fd0b,DISK], DatanodeInfoWithStorage[127.0.0.1:39070,DS-fad3a31b-8ca7-433e-9349-243a4d2be2e4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46127,DS-6f61e213-4d57-47b4-a7eb-2d505a2c8428,DISK], DatanodeInfoWithStorage[127.0.0.1:33412,DS-ed8c5f3d-790d-40fd-b5fa-05aae79cc668,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46127,DS-6f61e213-4d57-47b4-a7eb-2d505a2c8428,DISK], DatanodeInfoWithStorage[127.0.0.1:33412,DS-ed8c5f3d-790d-40fd-b5fa-05aae79cc668,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46127,DS-6f61e213-4d57-47b4-a7eb-2d505a2c8428,DISK], DatanodeInfoWithStorage[127.0.0.1:33412,DS-ed8c5f3d-790d-40fd-b5fa-05aae79cc668,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46127,DS-6f61e213-4d57-47b4-a7eb-2d505a2c8428,DISK], DatanodeInfoWithStorage[127.0.0.1:33412,DS-ed8c5f3d-790d-40fd-b5fa-05aae79cc668,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33858,DS-aed7efaf-7a5d-45f6-a580-93e2515952e0,DISK], DatanodeInfoWithStorage[127.0.0.1:33721,DS-15533169-3597-40d7-a6da-3f45624f7c4d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33858,DS-aed7efaf-7a5d-45f6-a580-93e2515952e0,DISK], DatanodeInfoWithStorage[127.0.0.1:33721,DS-15533169-3597-40d7-a6da-3f45624f7c4d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33858,DS-aed7efaf-7a5d-45f6-a580-93e2515952e0,DISK], DatanodeInfoWithStorage[127.0.0.1:33721,DS-15533169-3597-40d7-a6da-3f45624f7c4d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33858,DS-aed7efaf-7a5d-45f6-a580-93e2515952e0,DISK], DatanodeInfoWithStorage[127.0.0.1:33721,DS-15533169-3597-40d7-a6da-3f45624f7c4d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43104,DS-e0ea1f43-e566-4662-89f5-3cb7780eb6cb,DISK], DatanodeInfoWithStorage[127.0.0.1:40067,DS-ca4cb5de-7e62-4521-b465-c4b218acaa9a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43104,DS-e0ea1f43-e566-4662-89f5-3cb7780eb6cb,DISK], DatanodeInfoWithStorage[127.0.0.1:40067,DS-ca4cb5de-7e62-4521-b465-c4b218acaa9a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43104,DS-e0ea1f43-e566-4662-89f5-3cb7780eb6cb,DISK], DatanodeInfoWithStorage[127.0.0.1:40067,DS-ca4cb5de-7e62-4521-b465-c4b218acaa9a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43104,DS-e0ea1f43-e566-4662-89f5-3cb7780eb6cb,DISK], DatanodeInfoWithStorage[127.0.0.1:40067,DS-ca4cb5de-7e62-4521-b465-c4b218acaa9a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39992,DS-39810508-6a21-4a9a-b3c2-9e5e526b9b7e,DISK], DatanodeInfoWithStorage[127.0.0.1:35935,DS-6d9e3317-579c-41ac-b4ab-e9b00498ea91,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35935,DS-6d9e3317-579c-41ac-b4ab-e9b00498ea91,DISK], DatanodeInfoWithStorage[127.0.0.1:39992,DS-39810508-6a21-4a9a-b3c2-9e5e526b9b7e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39992,DS-39810508-6a21-4a9a-b3c2-9e5e526b9b7e,DISK], DatanodeInfoWithStorage[127.0.0.1:35935,DS-6d9e3317-579c-41ac-b4ab-e9b00498ea91,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35935,DS-6d9e3317-579c-41ac-b4ab-e9b00498ea91,DISK], DatanodeInfoWithStorage[127.0.0.1:39992,DS-39810508-6a21-4a9a-b3c2-9e5e526b9b7e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38311,DS-eb2b070a-6b47-4037-89e4-5c3330b54d53,DISK], DatanodeInfoWithStorage[127.0.0.1:37909,DS-8cfafb93-c76f-433c-9f01-f12688280cb7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38311,DS-eb2b070a-6b47-4037-89e4-5c3330b54d53,DISK], DatanodeInfoWithStorage[127.0.0.1:37909,DS-8cfafb93-c76f-433c-9f01-f12688280cb7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38311,DS-eb2b070a-6b47-4037-89e4-5c3330b54d53,DISK], DatanodeInfoWithStorage[127.0.0.1:37909,DS-8cfafb93-c76f-433c-9f01-f12688280cb7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38311,DS-eb2b070a-6b47-4037-89e4-5c3330b54d53,DISK], DatanodeInfoWithStorage[127.0.0.1:37909,DS-8cfafb93-c76f-433c-9f01-f12688280cb7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35555,DS-b806ef09-72e9-4e97-b975-c92c72ac0ad4,DISK], DatanodeInfoWithStorage[127.0.0.1:43138,DS-5862186c-eb89-49f9-a5f0-53dd1a1b1494,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43138,DS-5862186c-eb89-49f9-a5f0-53dd1a1b1494,DISK], DatanodeInfoWithStorage[127.0.0.1:35555,DS-b806ef09-72e9-4e97-b975-c92c72ac0ad4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35555,DS-b806ef09-72e9-4e97-b975-c92c72ac0ad4,DISK], DatanodeInfoWithStorage[127.0.0.1:43138,DS-5862186c-eb89-49f9-a5f0-53dd1a1b1494,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43138,DS-5862186c-eb89-49f9-a5f0-53dd1a1b1494,DISK], DatanodeInfoWithStorage[127.0.0.1:35555,DS-b806ef09-72e9-4e97-b975-c92c72ac0ad4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 60
v2: 0
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestPipelines#pipeline_01
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34621,DS-92319038-71f7-425d-836a-1d49181e9746,DISK], DatanodeInfoWithStorage[127.0.0.1:41920,DS-574b6827-08e6-4d23-821e-4c7a5875f14b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34621,DS-92319038-71f7-425d-836a-1d49181e9746,DISK], DatanodeInfoWithStorage[127.0.0.1:41920,DS-574b6827-08e6-4d23-821e-4c7a5875f14b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34621,DS-92319038-71f7-425d-836a-1d49181e9746,DISK], DatanodeInfoWithStorage[127.0.0.1:41920,DS-574b6827-08e6-4d23-821e-4c7a5875f14b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34621,DS-92319038-71f7-425d-836a-1d49181e9746,DISK], DatanodeInfoWithStorage[127.0.0.1:41920,DS-574b6827-08e6-4d23-821e-4c7a5875f14b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 49 out of 50
v1v1v2v2 failed with probability 5 out of 50
result: might be true error
Total execution time in seconds : 3762

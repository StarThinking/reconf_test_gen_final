reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 600
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTrackingWithAsyncWal
reconfPoint: 1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 600
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTrackingWithAsyncWal
reconfPoint: 1
result: -1
failureMessage: Append sequenceId=3, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=3, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39816,DS-0e6d3c26-f958-44da-a99b-d7a61667bf46,DISK], DatanodeInfoWithStorage[127.0.0.1:36168,DS-1ad74d38-b913-45c9-b649-7b59b291d98b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36168,DS-1ad74d38-b913-45c9-b649-7b59b291d98b,DISK], DatanodeInfoWithStorage[127.0.0.1:39816,DS-0e6d3c26-f958-44da-a99b-d7a61667bf46,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39816,DS-0e6d3c26-f958-44da-a99b-d7a61667bf46,DISK], DatanodeInfoWithStorage[127.0.0.1:36168,DS-1ad74d38-b913-45c9-b649-7b59b291d98b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36168,DS-1ad74d38-b913-45c9-b649-7b59b291d98b,DISK], DatanodeInfoWithStorage[127.0.0.1:39816,DS-0e6d3c26-f958-44da-a99b-d7a61667bf46,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 600
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTrackingWithAsyncWal
reconfPoint: 1
result: -1
failureMessage: Append sequenceId=3, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=3, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42997,DS-2b786337-df8c-4d33-b64b-53aefd980509,DISK], DatanodeInfoWithStorage[127.0.0.1:38775,DS-0519a2c9-a59d-4f45-ba70-edb81be5a85c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42997,DS-2b786337-df8c-4d33-b64b-53aefd980509,DISK], DatanodeInfoWithStorage[127.0.0.1:38775,DS-0519a2c9-a59d-4f45-ba70-edb81be5a85c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42997,DS-2b786337-df8c-4d33-b64b-53aefd980509,DISK], DatanodeInfoWithStorage[127.0.0.1:38775,DS-0519a2c9-a59d-4f45-ba70-edb81be5a85c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42997,DS-2b786337-df8c-4d33-b64b-53aefd980509,DISK], DatanodeInfoWithStorage[127.0.0.1:38775,DS-0519a2c9-a59d-4f45-ba70-edb81be5a85c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 600
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTrackingWithAsyncWal
reconfPoint: 1
result: -1
failureMessage: Append sequenceId=3, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=3, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39624,DS-9d5fc613-baa6-4c4a-871f-aa4bfeaad184,DISK], DatanodeInfoWithStorage[127.0.0.1:37369,DS-fec840c5-149c-42db-a47e-8de864cb6a60,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39624,DS-9d5fc613-baa6-4c4a-871f-aa4bfeaad184,DISK], DatanodeInfoWithStorage[127.0.0.1:37369,DS-fec840c5-149c-42db-a47e-8de864cb6a60,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39624,DS-9d5fc613-baa6-4c4a-871f-aa4bfeaad184,DISK], DatanodeInfoWithStorage[127.0.0.1:37369,DS-fec840c5-149c-42db-a47e-8de864cb6a60,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39624,DS-9d5fc613-baa6-4c4a-871f-aa4bfeaad184,DISK], DatanodeInfoWithStorage[127.0.0.1:37369,DS-fec840c5-149c-42db-a47e-8de864cb6a60,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 600
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTrackingWithAsyncWal
reconfPoint: 1
result: -1
failureMessage: Append sequenceId=3, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=3, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38489,DS-e6c864b4-47b2-48ef-8a34-c90634134be0,DISK], DatanodeInfoWithStorage[127.0.0.1:39443,DS-09cfbe60-03c8-418f-b7e0-2c26409e111b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38489,DS-e6c864b4-47b2-48ef-8a34-c90634134be0,DISK], DatanodeInfoWithStorage[127.0.0.1:39443,DS-09cfbe60-03c8-418f-b7e0-2c26409e111b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38489,DS-e6c864b4-47b2-48ef-8a34-c90634134be0,DISK], DatanodeInfoWithStorage[127.0.0.1:39443,DS-09cfbe60-03c8-418f-b7e0-2c26409e111b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38489,DS-e6c864b4-47b2-48ef-8a34-c90634134be0,DISK], DatanodeInfoWithStorage[127.0.0.1:39443,DS-09cfbe60-03c8-418f-b7e0-2c26409e111b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 600
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTrackingWithAsyncWal
reconfPoint: 1
result: -1
failureMessage: Append sequenceId=3, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=3, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45668,DS-ffbf3b34-dd6d-46ad-a068-44b1370d5f8d,DISK], DatanodeInfoWithStorage[127.0.0.1:37596,DS-4167d8be-69d2-455c-acd4-c300c563325f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45668,DS-ffbf3b34-dd6d-46ad-a068-44b1370d5f8d,DISK], DatanodeInfoWithStorage[127.0.0.1:37596,DS-4167d8be-69d2-455c-acd4-c300c563325f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45668,DS-ffbf3b34-dd6d-46ad-a068-44b1370d5f8d,DISK], DatanodeInfoWithStorage[127.0.0.1:37596,DS-4167d8be-69d2-455c-acd4-c300c563325f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45668,DS-ffbf3b34-dd6d-46ad-a068-44b1370d5f8d,DISK], DatanodeInfoWithStorage[127.0.0.1:37596,DS-4167d8be-69d2-455c-acd4-c300c563325f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 600
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTrackingWithAsyncWal
reconfPoint: 1
result: -1
failureMessage: Append sequenceId=3, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=3, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43144,DS-595b80e5-d12a-4502-97d6-707bce97b234,DISK], DatanodeInfoWithStorage[127.0.0.1:37465,DS-fdc08852-28f1-4484-8df1-ff49f409a0e9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43144,DS-595b80e5-d12a-4502-97d6-707bce97b234,DISK], DatanodeInfoWithStorage[127.0.0.1:37465,DS-fdc08852-28f1-4484-8df1-ff49f409a0e9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43144,DS-595b80e5-d12a-4502-97d6-707bce97b234,DISK], DatanodeInfoWithStorage[127.0.0.1:37465,DS-fdc08852-28f1-4484-8df1-ff49f409a0e9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43144,DS-595b80e5-d12a-4502-97d6-707bce97b234,DISK], DatanodeInfoWithStorage[127.0.0.1:37465,DS-fdc08852-28f1-4484-8df1-ff49f409a0e9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 600
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTrackingWithAsyncWal
reconfPoint: 1
result: -1
failureMessage: Append sequenceId=3, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=3, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39150,DS-c58b4c50-98fe-4755-ac75-5c5eaf2b26cb,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-4d4995b6-a5e5-4f2e-b9fc-2e82e3a53e03,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34075,DS-4d4995b6-a5e5-4f2e-b9fc-2e82e3a53e03,DISK], DatanodeInfoWithStorage[127.0.0.1:39150,DS-c58b4c50-98fe-4755-ac75-5c5eaf2b26cb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39150,DS-c58b4c50-98fe-4755-ac75-5c5eaf2b26cb,DISK], DatanodeInfoWithStorage[127.0.0.1:34075,DS-4d4995b6-a5e5-4f2e-b9fc-2e82e3a53e03,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34075,DS-4d4995b6-a5e5-4f2e-b9fc-2e82e3a53e03,DISK], DatanodeInfoWithStorage[127.0.0.1:39150,DS-c58b4c50-98fe-4755-ac75-5c5eaf2b26cb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 600
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTrackingWithAsyncWal
reconfPoint: 1
result: -1
failureMessage: Append sequenceId=3, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=3, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43118,DS-5bca74ea-19b6-4de2-9e90-d9faff6d7c3c,DISK], DatanodeInfoWithStorage[127.0.0.1:37409,DS-4cf35eef-27ba-4df7-b079-c330db780444,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37409,DS-4cf35eef-27ba-4df7-b079-c330db780444,DISK], DatanodeInfoWithStorage[127.0.0.1:43118,DS-5bca74ea-19b6-4de2-9e90-d9faff6d7c3c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43118,DS-5bca74ea-19b6-4de2-9e90-d9faff6d7c3c,DISK], DatanodeInfoWithStorage[127.0.0.1:37409,DS-4cf35eef-27ba-4df7-b079-c330db780444,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37409,DS-4cf35eef-27ba-4df7-b079-c330db780444,DISK], DatanodeInfoWithStorage[127.0.0.1:43118,DS-5bca74ea-19b6-4de2-9e90-d9faff6d7c3c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 600
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTrackingWithAsyncWal
reconfPoint: 1
result: -1
failureMessage: Append sequenceId=3, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=3, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44599,DS-2dde376b-b768-491f-842c-132c174f5210,DISK], DatanodeInfoWithStorage[127.0.0.1:41001,DS-1e0031e7-ef22-4cd6-9ac2-47c9a35ca4e8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41001,DS-1e0031e7-ef22-4cd6-9ac2-47c9a35ca4e8,DISK], DatanodeInfoWithStorage[127.0.0.1:44599,DS-2dde376b-b768-491f-842c-132c174f5210,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44599,DS-2dde376b-b768-491f-842c-132c174f5210,DISK], DatanodeInfoWithStorage[127.0.0.1:41001,DS-1e0031e7-ef22-4cd6-9ac2-47c9a35ca4e8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41001,DS-1e0031e7-ef22-4cd6-9ac2-47c9a35ca4e8,DISK], DatanodeInfoWithStorage[127.0.0.1:44599,DS-2dde376b-b768-491f-842c-132c174f5210,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:NameNode
v1: 600
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testUnflushedSeqIdTrackingWithAsyncWal
reconfPoint: 1
result: -1
failureMessage: Append sequenceId=3, requesting roll of WAL
stackTrace: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=3, requesting roll of WAL
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1081)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:964)
	at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:873)
	at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:129)
	at java.lang.Thread.run(Thread.java:748)
	Suppressed: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32936,DS-b5b1a943-bffb-43fb-ab40-378553347eed,DISK], DatanodeInfoWithStorage[127.0.0.1:46065,DS-bf06221d-7aaa-4e5c-b4df-66b014fec196,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32936,DS-b5b1a943-bffb-43fb-ab40-378553347eed,DISK], DatanodeInfoWithStorage[127.0.0.1:46065,DS-bf06221d-7aaa-4e5c-b4df-66b014fec196,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
		at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
		at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
		at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
		at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
		at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
		at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)
	[CIRCULAR REFERENCE:java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32936,DS-b5b1a943-bffb-43fb-ab40-378553347eed,DISK], DatanodeInfoWithStorage[127.0.0.1:46065,DS-bf06221d-7aaa-4e5c-b4df-66b014fec196,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32936,DS-b5b1a943-bffb-43fb-ab40-378553347eed,DISK], DatanodeInfoWithStorage[127.0.0.1:46065,DS-bf06221d-7aaa-4e5c-b4df-66b014fec196,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.]



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 1503

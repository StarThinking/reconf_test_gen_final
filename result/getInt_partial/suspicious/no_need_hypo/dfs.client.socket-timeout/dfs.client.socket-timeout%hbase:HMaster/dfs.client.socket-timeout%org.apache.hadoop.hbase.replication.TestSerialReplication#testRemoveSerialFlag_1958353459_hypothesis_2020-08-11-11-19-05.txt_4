reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: 1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46839,DS-45590503-94fd-493a-8b40-e6af90150d16,DISK], DatanodeInfoWithStorage[127.0.0.1:34798,DS-756191be-1a4e-4fea-a68b-a9efadd6ec97,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46839,DS-45590503-94fd-493a-8b40-e6af90150d16,DISK], DatanodeInfoWithStorage[127.0.0.1:34798,DS-756191be-1a4e-4fea-a68b-a9efadd6ec97,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46839,DS-45590503-94fd-493a-8b40-e6af90150d16,DISK], DatanodeInfoWithStorage[127.0.0.1:34798,DS-756191be-1a4e-4fea-a68b-a9efadd6ec97,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46839,DS-45590503-94fd-493a-8b40-e6af90150d16,DISK], DatanodeInfoWithStorage[127.0.0.1:34798,DS-756191be-1a4e-4fea-a68b-a9efadd6ec97,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: 1
result: -1
failureMessage: Waiting timed out after [30,000] msec Not enough entries replicated
stackTrace: java.lang.AssertionError: Waiting timed out after [30,000] msec Not enough entries replicated
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.hadoop.hbase.Waiter.waitFor(Waiter.java:203)
	at org.apache.hadoop.hbase.Waiter.waitFor(Waiter.java:137)
	at org.apache.hadoop.hbase.HBaseCommonTestingUtility.waitFor(HBaseCommonTestingUtility.java:251)
	at org.apache.hadoop.hbase.replication.SerialReplicationTestBase.waitUntilReplicationDone(SerialReplicationTestBase.java:190)
	at org.apache.hadoop.hbase.replication.SerialReplicationTestBase.enablePeerAndWaitUntilReplicationDone(SerialReplicationTestBase.java:214)
	at org.apache.hadoop.hbase.replication.TestSerialReplication.testRemoveSerialFlag(TestSerialReplication.java:240)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45623,DS-cf0a1844-2fb9-4523-a2b1-aada6b1e0f7d,DISK], DatanodeInfoWithStorage[127.0.0.1:38340,DS-350feaaf-ceb7-4bc7-b617-5065b9156885,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38340,DS-350feaaf-ceb7-4bc7-b617-5065b9156885,DISK], DatanodeInfoWithStorage[127.0.0.1:45623,DS-cf0a1844-2fb9-4523-a2b1-aada6b1e0f7d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45623,DS-cf0a1844-2fb9-4523-a2b1-aada6b1e0f7d,DISK], DatanodeInfoWithStorage[127.0.0.1:38340,DS-350feaaf-ceb7-4bc7-b617-5065b9156885,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38340,DS-350feaaf-ceb7-4bc7-b617-5065b9156885,DISK], DatanodeInfoWithStorage[127.0.0.1:45623,DS-cf0a1844-2fb9-4523-a2b1-aada6b1e0f7d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: 1
result: -1
failureMessage: expected:<100> but was:<101>
stackTrace: java.lang.AssertionError: expected:<100> but was:<101>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:631)
	at org.apache.hadoop.hbase.replication.SerialReplicationTestBase.checkOrder(SerialReplicationTestBase.java:241)
	at org.apache.hadoop.hbase.replication.TestSerialReplication.testRemoveSerialFlag(TestSerialReplication.java:241)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39305,DS-1635a399-1b3a-4d77-ac1f-95afbc83ba1f,DISK], DatanodeInfoWithStorage[127.0.0.1:46082,DS-3b1cc73a-df14-474a-b5e5-581d6b0c142b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46082,DS-3b1cc73a-df14-474a-b5e5-581d6b0c142b,DISK], DatanodeInfoWithStorage[127.0.0.1:39305,DS-1635a399-1b3a-4d77-ac1f-95afbc83ba1f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39305,DS-1635a399-1b3a-4d77-ac1f-95afbc83ba1f,DISK], DatanodeInfoWithStorage[127.0.0.1:46082,DS-3b1cc73a-df14-474a-b5e5-581d6b0c142b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46082,DS-3b1cc73a-df14-474a-b5e5-581d6b0c142b,DISK], DatanodeInfoWithStorage[127.0.0.1:39305,DS-1635a399-1b3a-4d77-ac1f-95afbc83ba1f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44028,DS-661cdcb5-f692-405f-a60f-f06d2a5c8434,DISK], DatanodeInfoWithStorage[127.0.0.1:34923,DS-0a7ae309-2aa2-4d07-bb10-a07bd9fd9c3d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44028,DS-661cdcb5-f692-405f-a60f-f06d2a5c8434,DISK], DatanodeInfoWithStorage[127.0.0.1:34923,DS-0a7ae309-2aa2-4d07-bb10-a07bd9fd9c3d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44028,DS-661cdcb5-f692-405f-a60f-f06d2a5c8434,DISK], DatanodeInfoWithStorage[127.0.0.1:34923,DS-0a7ae309-2aa2-4d07-bb10-a07bd9fd9c3d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44028,DS-661cdcb5-f692-405f-a60f-f06d2a5c8434,DISK], DatanodeInfoWithStorage[127.0.0.1:34923,DS-0a7ae309-2aa2-4d07-bb10-a07bd9fd9c3d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37146,DS-861cb364-60a5-4a16-94d1-37ca448166ba,DISK], DatanodeInfoWithStorage[127.0.0.1:34702,DS-bfd9494d-0db4-4353-9912-f594f0f3a90e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37146,DS-861cb364-60a5-4a16-94d1-37ca448166ba,DISK], DatanodeInfoWithStorage[127.0.0.1:34702,DS-bfd9494d-0db4-4353-9912-f594f0f3a90e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37146,DS-861cb364-60a5-4a16-94d1-37ca448166ba,DISK], DatanodeInfoWithStorage[127.0.0.1:34702,DS-bfd9494d-0db4-4353-9912-f594f0f3a90e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37146,DS-861cb364-60a5-4a16-94d1-37ca448166ba,DISK], DatanodeInfoWithStorage[127.0.0.1:34702,DS-bfd9494d-0db4-4353-9912-f594f0f3a90e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46741,DS-4a08ff0b-741c-4439-8f05-3bcc65b7d6ef,DISK], DatanodeInfoWithStorage[127.0.0.1:40006,DS-1e1439e5-b697-4bd5-81ff-3328a9db2040,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40006,DS-1e1439e5-b697-4bd5-81ff-3328a9db2040,DISK], DatanodeInfoWithStorage[127.0.0.1:46741,DS-4a08ff0b-741c-4439-8f05-3bcc65b7d6ef,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46741,DS-4a08ff0b-741c-4439-8f05-3bcc65b7d6ef,DISK], DatanodeInfoWithStorage[127.0.0.1:40006,DS-1e1439e5-b697-4bd5-81ff-3328a9db2040,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40006,DS-1e1439e5-b697-4bd5-81ff-3328a9db2040,DISK], DatanodeInfoWithStorage[127.0.0.1:46741,DS-4a08ff0b-741c-4439-8f05-3bcc65b7d6ef,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41541,DS-25bed12c-82c3-49fe-bd23-99d4c0dfbe25,DISK], DatanodeInfoWithStorage[127.0.0.1:44530,DS-31a8f5a4-1b87-4735-b281-526e703da66d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44530,DS-31a8f5a4-1b87-4735-b281-526e703da66d,DISK], DatanodeInfoWithStorage[127.0.0.1:41541,DS-25bed12c-82c3-49fe-bd23-99d4c0dfbe25,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41541,DS-25bed12c-82c3-49fe-bd23-99d4c0dfbe25,DISK], DatanodeInfoWithStorage[127.0.0.1:44530,DS-31a8f5a4-1b87-4735-b281-526e703da66d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44530,DS-31a8f5a4-1b87-4735-b281-526e703da66d,DISK], DatanodeInfoWithStorage[127.0.0.1:41541,DS-25bed12c-82c3-49fe-bd23-99d4c0dfbe25,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34612,DS-7529f6ad-e920-4cb9-a3b8-e5481db7e74c,DISK], DatanodeInfoWithStorage[127.0.0.1:43884,DS-0ae45685-c30e-46fb-868d-be28f5c05484,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34612,DS-7529f6ad-e920-4cb9-a3b8-e5481db7e74c,DISK], DatanodeInfoWithStorage[127.0.0.1:43884,DS-0ae45685-c30e-46fb-868d-be28f5c05484,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34612,DS-7529f6ad-e920-4cb9-a3b8-e5481db7e74c,DISK], DatanodeInfoWithStorage[127.0.0.1:43884,DS-0ae45685-c30e-46fb-868d-be28f5c05484,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34612,DS-7529f6ad-e920-4cb9-a3b8-e5481db7e74c,DISK], DatanodeInfoWithStorage[127.0.0.1:43884,DS-0ae45685-c30e-46fb-868d-be28f5c05484,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 3665

reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayBoundedLogWriterCreation#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayBoundedLogWriterCreation#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40870,DS-d3aea9aa-27d1-4e00-be5e-3bb4ed9552cf,DISK], DatanodeInfoWithStorage[127.0.0.1:44858,DS-307714f5-b73a-48fa-8c7d-10eb4177ccca,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40870,DS-d3aea9aa-27d1-4e00-be5e-3bb4ed9552cf,DISK], DatanodeInfoWithStorage[127.0.0.1:44858,DS-307714f5-b73a-48fa-8c7d-10eb4177ccca,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40870,DS-d3aea9aa-27d1-4e00-be5e-3bb4ed9552cf,DISK], DatanodeInfoWithStorage[127.0.0.1:44858,DS-307714f5-b73a-48fa-8c7d-10eb4177ccca,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40870,DS-d3aea9aa-27d1-4e00-be5e-3bb4ed9552cf,DISK], DatanodeInfoWithStorage[127.0.0.1:44858,DS-307714f5-b73a-48fa-8c7d-10eb4177ccca,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayBoundedLogWriterCreation#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45186,DS-2dc13e3c-4abd-487a-afac-e8216ed6b78b,DISK], DatanodeInfoWithStorage[127.0.0.1:37933,DS-cef33fc9-92e0-40b6-a455-5303d550e70b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37933,DS-cef33fc9-92e0-40b6-a455-5303d550e70b,DISK], DatanodeInfoWithStorage[127.0.0.1:45186,DS-2dc13e3c-4abd-487a-afac-e8216ed6b78b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45186,DS-2dc13e3c-4abd-487a-afac-e8216ed6b78b,DISK], DatanodeInfoWithStorage[127.0.0.1:37933,DS-cef33fc9-92e0-40b6-a455-5303d550e70b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37933,DS-cef33fc9-92e0-40b6-a455-5303d550e70b,DISK], DatanodeInfoWithStorage[127.0.0.1:45186,DS-2dc13e3c-4abd-487a-afac-e8216ed6b78b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayBoundedLogWriterCreation#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38583,DS-b66f8994-d84a-49af-8526-8482538ef69d,DISK], DatanodeInfoWithStorage[127.0.0.1:46709,DS-1fe4ff4d-7f03-4e7e-925a-825552b8d406,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38583,DS-b66f8994-d84a-49af-8526-8482538ef69d,DISK], DatanodeInfoWithStorage[127.0.0.1:46709,DS-1fe4ff4d-7f03-4e7e-925a-825552b8d406,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38583,DS-b66f8994-d84a-49af-8526-8482538ef69d,DISK], DatanodeInfoWithStorage[127.0.0.1:46709,DS-1fe4ff4d-7f03-4e7e-925a-825552b8d406,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38583,DS-b66f8994-d84a-49af-8526-8482538ef69d,DISK], DatanodeInfoWithStorage[127.0.0.1:46709,DS-1fe4ff4d-7f03-4e7e-925a-825552b8d406,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayBoundedLogWriterCreation#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44296,DS-f0b04892-06e4-4e33-adde-170b67cc5483,DISK], DatanodeInfoWithStorage[127.0.0.1:38960,DS-04b4718d-720a-4884-93c4-1e51e17226c6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44296,DS-f0b04892-06e4-4e33-adde-170b67cc5483,DISK], DatanodeInfoWithStorage[127.0.0.1:38960,DS-04b4718d-720a-4884-93c4-1e51e17226c6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44296,DS-f0b04892-06e4-4e33-adde-170b67cc5483,DISK], DatanodeInfoWithStorage[127.0.0.1:38960,DS-04b4718d-720a-4884-93c4-1e51e17226c6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44296,DS-f0b04892-06e4-4e33-adde-170b67cc5483,DISK], DatanodeInfoWithStorage[127.0.0.1:38960,DS-04b4718d-720a-4884-93c4-1e51e17226c6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayBoundedLogWriterCreation#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45593,DS-fb72d1d5-3b96-49a6-ae8d-c504734b2e75,DISK], DatanodeInfoWithStorage[127.0.0.1:43816,DS-7f912e31-7a30-4cc1-acd8-78eff9c627f1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45593,DS-fb72d1d5-3b96-49a6-ae8d-c504734b2e75,DISK], DatanodeInfoWithStorage[127.0.0.1:43816,DS-7f912e31-7a30-4cc1-acd8-78eff9c627f1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45593,DS-fb72d1d5-3b96-49a6-ae8d-c504734b2e75,DISK], DatanodeInfoWithStorage[127.0.0.1:43816,DS-7f912e31-7a30-4cc1-acd8-78eff9c627f1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45593,DS-fb72d1d5-3b96-49a6-ae8d-c504734b2e75,DISK], DatanodeInfoWithStorage[127.0.0.1:43816,DS-7f912e31-7a30-4cc1-acd8-78eff9c627f1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayBoundedLogWriterCreation#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43581,DS-b7f6dc5e-b1f3-4550-b9e6-0247d0ad60fb,DISK], DatanodeInfoWithStorage[127.0.0.1:46108,DS-376701ce-6e0f-4e2c-b56f-261dc91e840f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43581,DS-b7f6dc5e-b1f3-4550-b9e6-0247d0ad60fb,DISK], DatanodeInfoWithStorage[127.0.0.1:46108,DS-376701ce-6e0f-4e2c-b56f-261dc91e840f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43581,DS-b7f6dc5e-b1f3-4550-b9e6-0247d0ad60fb,DISK], DatanodeInfoWithStorage[127.0.0.1:46108,DS-376701ce-6e0f-4e2c-b56f-261dc91e840f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43581,DS-b7f6dc5e-b1f3-4550-b9e6-0247d0ad60fb,DISK], DatanodeInfoWithStorage[127.0.0.1:46108,DS-376701ce-6e0f-4e2c-b56f-261dc91e840f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayBoundedLogWriterCreation#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38222,DS-67ce335e-3f06-4b38-908c-8eb7a9eb7ee4,DISK], DatanodeInfoWithStorage[127.0.0.1:34742,DS-ed109a0c-2647-4629-b2bd-c53c7a0979b7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34742,DS-ed109a0c-2647-4629-b2bd-c53c7a0979b7,DISK], DatanodeInfoWithStorage[127.0.0.1:38222,DS-67ce335e-3f06-4b38-908c-8eb7a9eb7ee4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38222,DS-67ce335e-3f06-4b38-908c-8eb7a9eb7ee4,DISK], DatanodeInfoWithStorage[127.0.0.1:34742,DS-ed109a0c-2647-4629-b2bd-c53c7a0979b7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34742,DS-ed109a0c-2647-4629-b2bd-c53c7a0979b7,DISK], DatanodeInfoWithStorage[127.0.0.1:38222,DS-67ce335e-3f06-4b38-908c-8eb7a9eb7ee4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayBoundedLogWriterCreation#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37409,DS-6cf3a37a-a2b9-4d45-b16a-e41c5efe1279,DISK], DatanodeInfoWithStorage[127.0.0.1:34552,DS-cf656bfc-2001-480e-bfab-4b75da314c28,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34552,DS-cf656bfc-2001-480e-bfab-4b75da314c28,DISK], DatanodeInfoWithStorage[127.0.0.1:37409,DS-6cf3a37a-a2b9-4d45-b16a-e41c5efe1279,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37409,DS-6cf3a37a-a2b9-4d45-b16a-e41c5efe1279,DISK], DatanodeInfoWithStorage[127.0.0.1:34552,DS-cf656bfc-2001-480e-bfab-4b75da314c28,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34552,DS-cf656bfc-2001-480e-bfab-4b75da314c28,DISK], DatanodeInfoWithStorage[127.0.0.1:37409,DS-6cf3a37a-a2b9-4d45-b16a-e41c5efe1279,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayBoundedLogWriterCreation#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45286,DS-b8cb0748-73a8-4d2f-b28f-825d121d30d8,DISK], DatanodeInfoWithStorage[127.0.0.1:33922,DS-79aaec5c-2a04-462b-8cc3-3b687983f95f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33922,DS-79aaec5c-2a04-462b-8cc3-3b687983f95f,DISK], DatanodeInfoWithStorage[127.0.0.1:45286,DS-b8cb0748-73a8-4d2f-b28f-825d121d30d8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45286,DS-b8cb0748-73a8-4d2f-b28f-825d121d30d8,DISK], DatanodeInfoWithStorage[127.0.0.1:33922,DS-79aaec5c-2a04-462b-8cc3-3b687983f95f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33922,DS-79aaec5c-2a04-462b-8cc3-3b687983f95f,DISK], DatanodeInfoWithStorage[127.0.0.1:45286,DS-b8cb0748-73a8-4d2f-b28f-825d121d30d8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayBoundedLogWriterCreation#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39441,DS-256f5cb8-20d1-490c-83e0-81236b07e20b,DISK], DatanodeInfoWithStorage[127.0.0.1:35201,DS-7c71c0b0-696a-4301-9d82-30bf77030ad1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39441,DS-256f5cb8-20d1-490c-83e0-81236b07e20b,DISK], DatanodeInfoWithStorage[127.0.0.1:35201,DS-7c71c0b0-696a-4301-9d82-30bf77030ad1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39441,DS-256f5cb8-20d1-490c-83e0-81236b07e20b,DISK], DatanodeInfoWithStorage[127.0.0.1:35201,DS-7c71c0b0-696a-4301-9d82-30bf77030ad1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39441,DS-256f5cb8-20d1-490c-83e0-81236b07e20b,DISK], DatanodeInfoWithStorage[127.0.0.1:35201,DS-7c71c0b0-696a-4301-9d82-30bf77030ad1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 40
v1v1v2v2 failed with probability 0 out of 40
result: might be true error
Total execution time in seconds : 8066

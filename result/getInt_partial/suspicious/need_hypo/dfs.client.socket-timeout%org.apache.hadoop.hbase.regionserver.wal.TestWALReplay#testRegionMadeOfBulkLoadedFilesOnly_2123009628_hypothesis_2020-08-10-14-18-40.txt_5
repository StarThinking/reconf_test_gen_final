reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36563,DS-ad9d21f0-672c-4fe2-8f12-7128a5dd1dd2,DISK], DatanodeInfoWithStorage[127.0.0.1:34854,DS-d5aecb42-134c-4cdf-bdfe-560306168d49,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34854,DS-d5aecb42-134c-4cdf-bdfe-560306168d49,DISK], DatanodeInfoWithStorage[127.0.0.1:36563,DS-ad9d21f0-672c-4fe2-8f12-7128a5dd1dd2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36563,DS-ad9d21f0-672c-4fe2-8f12-7128a5dd1dd2,DISK], DatanodeInfoWithStorage[127.0.0.1:34854,DS-d5aecb42-134c-4cdf-bdfe-560306168d49,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34854,DS-d5aecb42-134c-4cdf-bdfe-560306168d49,DISK], DatanodeInfoWithStorage[127.0.0.1:36563,DS-ad9d21f0-672c-4fe2-8f12-7128a5dd1dd2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -2
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44317,DS-8f2790e8-9024-4b83-9646-25c79ab7f958,DISK], DatanodeInfoWithStorage[127.0.0.1:36134,DS-b5fb8f49-d613-4cd3-890c-abd3726a1f8e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44317,DS-8f2790e8-9024-4b83-9646-25c79ab7f958,DISK], DatanodeInfoWithStorage[127.0.0.1:36134,DS-b5fb8f49-d613-4cd3-890c-abd3726a1f8e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44317,DS-8f2790e8-9024-4b83-9646-25c79ab7f958,DISK], DatanodeInfoWithStorage[127.0.0.1:36134,DS-b5fb8f49-d613-4cd3-890c-abd3726a1f8e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44317,DS-8f2790e8-9024-4b83-9646-25c79ab7f958,DISK], DatanodeInfoWithStorage[127.0.0.1:36134,DS-b5fb8f49-d613-4cd3-890c-abd3726a1f8e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40341,DS-5760bd84-ef82-4f9a-b0a0-abbe2d67da37,DISK], DatanodeInfoWithStorage[127.0.0.1:34270,DS-1feadf53-c8c1-45e1-8a23-43310824e6ea,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34270,DS-1feadf53-c8c1-45e1-8a23-43310824e6ea,DISK], DatanodeInfoWithStorage[127.0.0.1:40341,DS-5760bd84-ef82-4f9a-b0a0-abbe2d67da37,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40341,DS-5760bd84-ef82-4f9a-b0a0-abbe2d67da37,DISK], DatanodeInfoWithStorage[127.0.0.1:34270,DS-1feadf53-c8c1-45e1-8a23-43310824e6ea,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34270,DS-1feadf53-c8c1-45e1-8a23-43310824e6ea,DISK], DatanodeInfoWithStorage[127.0.0.1:40341,DS-5760bd84-ef82-4f9a-b0a0-abbe2d67da37,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37869,DS-a10e0485-dd6f-4d1c-991b-f0550b28ac9c,DISK], DatanodeInfoWithStorage[127.0.0.1:38304,DS-af40f3f7-2981-42cf-ade2-1c4fa422bae3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37869,DS-a10e0485-dd6f-4d1c-991b-f0550b28ac9c,DISK], DatanodeInfoWithStorage[127.0.0.1:38304,DS-af40f3f7-2981-42cf-ade2-1c4fa422bae3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37869,DS-a10e0485-dd6f-4d1c-991b-f0550b28ac9c,DISK], DatanodeInfoWithStorage[127.0.0.1:38304,DS-af40f3f7-2981-42cf-ade2-1c4fa422bae3,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37869,DS-a10e0485-dd6f-4d1c-991b-f0550b28ac9c,DISK], DatanodeInfoWithStorage[127.0.0.1:38304,DS-af40f3f7-2981-42cf-ade2-1c4fa422bae3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35474,DS-08af9d15-3e1b-4000-84b3-79a3cb83d0c1,DISK], DatanodeInfoWithStorage[127.0.0.1:45163,DS-6a8d0cd4-8ebd-4857-b370-396ddf877430,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35474,DS-08af9d15-3e1b-4000-84b3-79a3cb83d0c1,DISK], DatanodeInfoWithStorage[127.0.0.1:45163,DS-6a8d0cd4-8ebd-4857-b370-396ddf877430,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35474,DS-08af9d15-3e1b-4000-84b3-79a3cb83d0c1,DISK], DatanodeInfoWithStorage[127.0.0.1:45163,DS-6a8d0cd4-8ebd-4857-b370-396ddf877430,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35474,DS-08af9d15-3e1b-4000-84b3-79a3cb83d0c1,DISK], DatanodeInfoWithStorage[127.0.0.1:45163,DS-6a8d0cd4-8ebd-4857-b370-396ddf877430,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38261,DS-ca5f1a37-e95d-4bdb-a239-ebd16c684c43,DISK], DatanodeInfoWithStorage[127.0.0.1:45720,DS-1f5707d2-69d2-4201-9f23-a1d7ca4d9821,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38261,DS-ca5f1a37-e95d-4bdb-a239-ebd16c684c43,DISK], DatanodeInfoWithStorage[127.0.0.1:45720,DS-1f5707d2-69d2-4201-9f23-a1d7ca4d9821,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38261,DS-ca5f1a37-e95d-4bdb-a239-ebd16c684c43,DISK], DatanodeInfoWithStorage[127.0.0.1:45720,DS-1f5707d2-69d2-4201-9f23-a1d7ca4d9821,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38261,DS-ca5f1a37-e95d-4bdb-a239-ebd16c684c43,DISK], DatanodeInfoWithStorage[127.0.0.1:45720,DS-1f5707d2-69d2-4201-9f23-a1d7ca4d9821,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -2
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36721,DS-0d6317df-8d6a-4c39-a675-f22408a2c4cb,DISK], DatanodeInfoWithStorage[127.0.0.1:39328,DS-b9f24a10-ea9b-43e0-90a8-b4ba082ed328,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39328,DS-b9f24a10-ea9b-43e0-90a8-b4ba082ed328,DISK], DatanodeInfoWithStorage[127.0.0.1:36721,DS-0d6317df-8d6a-4c39-a675-f22408a2c4cb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36721,DS-0d6317df-8d6a-4c39-a675-f22408a2c4cb,DISK], DatanodeInfoWithStorage[127.0.0.1:39328,DS-b9f24a10-ea9b-43e0-90a8-b4ba082ed328,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39328,DS-b9f24a10-ea9b-43e0-90a8-b4ba082ed328,DISK], DatanodeInfoWithStorage[127.0.0.1:36721,DS-0d6317df-8d6a-4c39-a675-f22408a2c4cb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testRegionMadeOfBulkLoadedFilesOnly
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37183,DS-f5e69964-5788-43f4-9a5b-e65013fc3601,DISK], DatanodeInfoWithStorage[127.0.0.1:44651,DS-76b9fbc3-12bb-4423-869c-691883221eb0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37183,DS-f5e69964-5788-43f4-9a5b-e65013fc3601,DISK], DatanodeInfoWithStorage[127.0.0.1:44651,DS-76b9fbc3-12bb-4423-869c-691883221eb0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37183,DS-f5e69964-5788-43f4-9a5b-e65013fc3601,DISK], DatanodeInfoWithStorage[127.0.0.1:44651,DS-76b9fbc3-12bb-4423-869c-691883221eb0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37183,DS-f5e69964-5788-43f4-9a5b-e65013fc3601,DISK], DatanodeInfoWithStorage[127.0.0.1:44651,DS-76b9fbc3-12bb-4423-869c-691883221eb0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 35
v1v1v2v2 failed with probability 0 out of 35
result: might be true error
Total execution time in seconds : 10140

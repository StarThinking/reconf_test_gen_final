reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1028251716-172.17.0.2-1598584566808:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33049,DS-2f860df6-7347-498e-a2f3-406a628370e2,DISK], DatanodeInfoWithStorage[127.0.0.1:39431,DS-84350e48-24c4-44f3-b845-afd1234d8430,DISK], DatanodeInfoWithStorage[127.0.0.1:44673,DS-133d3139-fcc6-437c-802e-1f7fe2d45146,DISK], DatanodeInfoWithStorage[127.0.0.1:39237,DS-5908a821-0cbe-46e5-bae7-c282e1655501,DISK], DatanodeInfoWithStorage[127.0.0.1:37074,DS-9146c55a-1b8f-48dd-a962-4e5330b6d0e9,DISK], DatanodeInfoWithStorage[127.0.0.1:33870,DS-6867df92-ac04-454d-8885-2f88b807e817,DISK], DatanodeInfoWithStorage[127.0.0.1:41336,DS-49f10237-25b5-4f75-9bbb-ed8d26471794,DISK], DatanodeInfoWithStorage[127.0.0.1:36195,DS-5e050d2d-3ba0-4f1f-b5c9-bee60246cefa,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1028251716-172.17.0.2-1598584566808:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33049,DS-2f860df6-7347-498e-a2f3-406a628370e2,DISK], DatanodeInfoWithStorage[127.0.0.1:39431,DS-84350e48-24c4-44f3-b845-afd1234d8430,DISK], DatanodeInfoWithStorage[127.0.0.1:44673,DS-133d3139-fcc6-437c-802e-1f7fe2d45146,DISK], DatanodeInfoWithStorage[127.0.0.1:39237,DS-5908a821-0cbe-46e5-bae7-c282e1655501,DISK], DatanodeInfoWithStorage[127.0.0.1:37074,DS-9146c55a-1b8f-48dd-a962-4e5330b6d0e9,DISK], DatanodeInfoWithStorage[127.0.0.1:33870,DS-6867df92-ac04-454d-8885-2f88b807e817,DISK], DatanodeInfoWithStorage[127.0.0.1:41336,DS-49f10237-25b5-4f75-9bbb-ed8d26471794,DISK], DatanodeInfoWithStorage[127.0.0.1:36195,DS-5e050d2d-3ba0-4f1f-b5c9-bee60246cefa,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1073148925-172.17.0.2-1598584637507:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35460,DS-cbf3d1a7-555a-4cf4-ba86-95d5965922ae,DISK], DatanodeInfoWithStorage[127.0.0.1:46446,DS-d61b3b04-e9d2-4371-b73a-9d43846c9371,DISK], DatanodeInfoWithStorage[127.0.0.1:35218,DS-d29b400a-a6de-4495-8192-bb4478229275,DISK], DatanodeInfoWithStorage[127.0.0.1:35504,DS-3d7ae969-e2aa-4309-b959-098c8d2b1e6a,DISK], DatanodeInfoWithStorage[127.0.0.1:41906,DS-6b6967d4-4ea9-496b-a6ab-9c010aa3c429,DISK], DatanodeInfoWithStorage[127.0.0.1:38313,DS-60ddc574-eee5-4395-886d-f0b573282398,DISK], DatanodeInfoWithStorage[127.0.0.1:39838,DS-b3da0544-6b6a-4c0c-8d48-d377042a4f61,DISK], DatanodeInfoWithStorage[127.0.0.1:39009,DS-ec03da51-ba71-4aa4-bbfb-52f6198e077b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1073148925-172.17.0.2-1598584637507:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35460,DS-cbf3d1a7-555a-4cf4-ba86-95d5965922ae,DISK], DatanodeInfoWithStorage[127.0.0.1:46446,DS-d61b3b04-e9d2-4371-b73a-9d43846c9371,DISK], DatanodeInfoWithStorage[127.0.0.1:35218,DS-d29b400a-a6de-4495-8192-bb4478229275,DISK], DatanodeInfoWithStorage[127.0.0.1:35504,DS-3d7ae969-e2aa-4309-b959-098c8d2b1e6a,DISK], DatanodeInfoWithStorage[127.0.0.1:41906,DS-6b6967d4-4ea9-496b-a6ab-9c010aa3c429,DISK], DatanodeInfoWithStorage[127.0.0.1:38313,DS-60ddc574-eee5-4395-886d-f0b573282398,DISK], DatanodeInfoWithStorage[127.0.0.1:39838,DS-b3da0544-6b6a-4c0c-8d48-d377042a4f61,DISK], DatanodeInfoWithStorage[127.0.0.1:39009,DS-ec03da51-ba71-4aa4-bbfb-52f6198e077b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-253073956-172.17.0.2-1598584667162:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44028,DS-9b831d7f-fb0a-419b-b465-078f6dca0f22,DISK], DatanodeInfoWithStorage[127.0.0.1:34125,DS-a3e17699-90db-459e-bd5f-98a3807316dc,DISK], DatanodeInfoWithStorage[127.0.0.1:38431,DS-3030e9bb-497d-4694-b972-0ef1df0e76f1,DISK], DatanodeInfoWithStorage[127.0.0.1:40974,DS-83bd1d95-9ca4-4a32-b607-1578a1e3d39a,DISK], DatanodeInfoWithStorage[127.0.0.1:44131,DS-b3aaf65a-3c00-47d3-a311-04f28a02dd10,DISK], DatanodeInfoWithStorage[127.0.0.1:34691,DS-87d93242-8af1-4417-886d-c85c366658e1,DISK], DatanodeInfoWithStorage[127.0.0.1:34936,DS-728ff2b1-e510-423a-abdf-99212630bf27,DISK], DatanodeInfoWithStorage[127.0.0.1:41477,DS-c9ef9b79-fac2-4bf9-b444-dc7e9d1301c0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-253073956-172.17.0.2-1598584667162:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44028,DS-9b831d7f-fb0a-419b-b465-078f6dca0f22,DISK], DatanodeInfoWithStorage[127.0.0.1:34125,DS-a3e17699-90db-459e-bd5f-98a3807316dc,DISK], DatanodeInfoWithStorage[127.0.0.1:38431,DS-3030e9bb-497d-4694-b972-0ef1df0e76f1,DISK], DatanodeInfoWithStorage[127.0.0.1:40974,DS-83bd1d95-9ca4-4a32-b607-1578a1e3d39a,DISK], DatanodeInfoWithStorage[127.0.0.1:44131,DS-b3aaf65a-3c00-47d3-a311-04f28a02dd10,DISK], DatanodeInfoWithStorage[127.0.0.1:34691,DS-87d93242-8af1-4417-886d-c85c366658e1,DISK], DatanodeInfoWithStorage[127.0.0.1:34936,DS-728ff2b1-e510-423a-abdf-99212630bf27,DISK], DatanodeInfoWithStorage[127.0.0.1:41477,DS-c9ef9b79-fac2-4bf9-b444-dc7e9d1301c0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-572921680-172.17.0.2-1598584796705:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45675,DS-3cececa2-8fcb-42c4-bf32-764e8a1d5327,DISK], DatanodeInfoWithStorage[127.0.0.1:35714,DS-671d1a15-dc12-471a-bb43-a178e109ec13,DISK], DatanodeInfoWithStorage[127.0.0.1:33579,DS-0768a467-881d-47ed-8051-68c03c0bd647,DISK], DatanodeInfoWithStorage[127.0.0.1:46771,DS-6930b3eb-9433-49bf-86e1-27a1e620325e,DISK], DatanodeInfoWithStorage[127.0.0.1:38645,DS-b4379768-bc74-4a26-9647-10e780d89d4e,DISK], DatanodeInfoWithStorage[127.0.0.1:40335,DS-6d6f2868-08db-4e0c-995e-601eb4c79475,DISK], DatanodeInfoWithStorage[127.0.0.1:34932,DS-d971ec67-d73c-45dd-9253-81d538e4755d,DISK], DatanodeInfoWithStorage[127.0.0.1:41599,DS-29740c33-9752-4141-b179-96c73d030bd6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-572921680-172.17.0.2-1598584796705:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45675,DS-3cececa2-8fcb-42c4-bf32-764e8a1d5327,DISK], DatanodeInfoWithStorage[127.0.0.1:35714,DS-671d1a15-dc12-471a-bb43-a178e109ec13,DISK], DatanodeInfoWithStorage[127.0.0.1:33579,DS-0768a467-881d-47ed-8051-68c03c0bd647,DISK], DatanodeInfoWithStorage[127.0.0.1:46771,DS-6930b3eb-9433-49bf-86e1-27a1e620325e,DISK], DatanodeInfoWithStorage[127.0.0.1:38645,DS-b4379768-bc74-4a26-9647-10e780d89d4e,DISK], DatanodeInfoWithStorage[127.0.0.1:40335,DS-6d6f2868-08db-4e0c-995e-601eb4c79475,DISK], DatanodeInfoWithStorage[127.0.0.1:34932,DS-d971ec67-d73c-45dd-9253-81d538e4755d,DISK], DatanodeInfoWithStorage[127.0.0.1:41599,DS-29740c33-9752-4141-b179-96c73d030bd6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-676216227-172.17.0.2-1598584954946:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46756,DS-74f0b091-e25e-4196-9a69-17ed3b86038f,DISK], DatanodeInfoWithStorage[127.0.0.1:39101,DS-7661834b-ea51-4b42-ba77-fffeef41f834,DISK], DatanodeInfoWithStorage[127.0.0.1:36677,DS-d4084b5f-7bfc-4be6-b665-36fffd9b39df,DISK], DatanodeInfoWithStorage[127.0.0.1:43716,DS-3677cfd0-e3ae-451e-8cbd-6e8b5cf9f543,DISK], DatanodeInfoWithStorage[127.0.0.1:34523,DS-1eb0c31c-2400-47df-b118-fed99f24e8d9,DISK], DatanodeInfoWithStorage[127.0.0.1:42110,DS-935f384b-d14c-44c7-96f5-c67073399312,DISK], DatanodeInfoWithStorage[127.0.0.1:35708,DS-36d59dd0-be96-4df5-ba57-4afd2574c5e6,DISK], DatanodeInfoWithStorage[127.0.0.1:43137,DS-162609b6-a406-41f1-af69-d19a50d3899c,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-676216227-172.17.0.2-1598584954946:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46756,DS-74f0b091-e25e-4196-9a69-17ed3b86038f,DISK], DatanodeInfoWithStorage[127.0.0.1:39101,DS-7661834b-ea51-4b42-ba77-fffeef41f834,DISK], DatanodeInfoWithStorage[127.0.0.1:36677,DS-d4084b5f-7bfc-4be6-b665-36fffd9b39df,DISK], DatanodeInfoWithStorage[127.0.0.1:43716,DS-3677cfd0-e3ae-451e-8cbd-6e8b5cf9f543,DISK], DatanodeInfoWithStorage[127.0.0.1:34523,DS-1eb0c31c-2400-47df-b118-fed99f24e8d9,DISK], DatanodeInfoWithStorage[127.0.0.1:42110,DS-935f384b-d14c-44c7-96f5-c67073399312,DISK], DatanodeInfoWithStorage[127.0.0.1:35708,DS-36d59dd0-be96-4df5-ba57-4afd2574c5e6,DISK], DatanodeInfoWithStorage[127.0.0.1:43137,DS-162609b6-a406-41f1-af69-d19a50d3899c,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-479614843-172.17.0.2-1598584990288:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33059,DS-0dffde7f-acee-4c36-ba0e-2bbfe035e30b,DISK], DatanodeInfoWithStorage[127.0.0.1:43984,DS-9e5468dc-d10e-4da9-9c63-8c7a9fc19ce8,DISK], DatanodeInfoWithStorage[127.0.0.1:33775,DS-74da45b2-7cff-4970-98fa-e35bea290d7d,DISK], DatanodeInfoWithStorage[127.0.0.1:38841,DS-d3fe5252-6d15-40e9-88b8-99921db6e0a8,DISK], DatanodeInfoWithStorage[127.0.0.1:34347,DS-b14da2c4-52a4-4a5d-91b6-05af6bcd97ba,DISK], DatanodeInfoWithStorage[127.0.0.1:45906,DS-e97a8769-7d7a-4204-b2fd-98907fec6798,DISK], DatanodeInfoWithStorage[127.0.0.1:40660,DS-4c598063-f038-455f-94db-4ba11f1c0e01,DISK], DatanodeInfoWithStorage[127.0.0.1:33510,DS-01101c1f-1522-47d9-a4e9-c4fd40118fe4,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-479614843-172.17.0.2-1598584990288:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33059,DS-0dffde7f-acee-4c36-ba0e-2bbfe035e30b,DISK], DatanodeInfoWithStorage[127.0.0.1:43984,DS-9e5468dc-d10e-4da9-9c63-8c7a9fc19ce8,DISK], DatanodeInfoWithStorage[127.0.0.1:33775,DS-74da45b2-7cff-4970-98fa-e35bea290d7d,DISK], DatanodeInfoWithStorage[127.0.0.1:38841,DS-d3fe5252-6d15-40e9-88b8-99921db6e0a8,DISK], DatanodeInfoWithStorage[127.0.0.1:34347,DS-b14da2c4-52a4-4a5d-91b6-05af6bcd97ba,DISK], DatanodeInfoWithStorage[127.0.0.1:45906,DS-e97a8769-7d7a-4204-b2fd-98907fec6798,DISK], DatanodeInfoWithStorage[127.0.0.1:40660,DS-4c598063-f038-455f-94db-4ba11f1c0e01,DISK], DatanodeInfoWithStorage[127.0.0.1:33510,DS-01101c1f-1522-47d9-a4e9-c4fd40118fe4,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-368148049-172.17.0.2-1598585055700:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34092,DS-99f9199c-21fd-4326-87fd-34d2e240a608,DISK], DatanodeInfoWithStorage[127.0.0.1:43042,DS-d9ab4997-6941-45cc-9646-ea51d58bebfe,DISK], DatanodeInfoWithStorage[127.0.0.1:35526,DS-791e936b-0873-4cbc-8bf6-d08e5b39cd82,DISK], DatanodeInfoWithStorage[127.0.0.1:43510,DS-14619fc8-f685-4f52-aa31-2c882f322d9e,DISK], DatanodeInfoWithStorage[127.0.0.1:43016,DS-3fffd4f9-f964-4a14-93c9-f2807b8b5012,DISK], DatanodeInfoWithStorage[127.0.0.1:38358,DS-b70eac31-d9a7-469b-83a3-2b6efc023c43,DISK], DatanodeInfoWithStorage[127.0.0.1:45732,DS-debdfe88-eb75-4ae0-b3d6-e3b26021bd66,DISK], DatanodeInfoWithStorage[127.0.0.1:35444,DS-769830a2-8310-4743-89e6-518c89174184,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-368148049-172.17.0.2-1598585055700:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34092,DS-99f9199c-21fd-4326-87fd-34d2e240a608,DISK], DatanodeInfoWithStorage[127.0.0.1:43042,DS-d9ab4997-6941-45cc-9646-ea51d58bebfe,DISK], DatanodeInfoWithStorage[127.0.0.1:35526,DS-791e936b-0873-4cbc-8bf6-d08e5b39cd82,DISK], DatanodeInfoWithStorage[127.0.0.1:43510,DS-14619fc8-f685-4f52-aa31-2c882f322d9e,DISK], DatanodeInfoWithStorage[127.0.0.1:43016,DS-3fffd4f9-f964-4a14-93c9-f2807b8b5012,DISK], DatanodeInfoWithStorage[127.0.0.1:38358,DS-b70eac31-d9a7-469b-83a3-2b6efc023c43,DISK], DatanodeInfoWithStorage[127.0.0.1:45732,DS-debdfe88-eb75-4ae0-b3d6-e3b26021bd66,DISK], DatanodeInfoWithStorage[127.0.0.1:35444,DS-769830a2-8310-4743-89e6-518c89174184,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-91412075-172.17.0.2-1598585331793:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46633,DS-6b88fabb-6365-4fa0-88e5-d9f8399ca739,DISK], DatanodeInfoWithStorage[127.0.0.1:38493,DS-c8929d68-7e80-44e3-872e-524b757ddc87,DISK], DatanodeInfoWithStorage[127.0.0.1:45779,DS-3ac3c552-4d0d-4312-8e5c-d4ad9e1a1a5e,DISK], DatanodeInfoWithStorage[127.0.0.1:32949,DS-d960899a-296e-4264-837b-df88453cdf34,DISK], DatanodeInfoWithStorage[127.0.0.1:38129,DS-7bca5bf2-1efd-4788-ad90-c3fd769c88f9,DISK], DatanodeInfoWithStorage[127.0.0.1:36179,DS-53b607b0-623f-41ef-b711-12f1ec763f7a,DISK], DatanodeInfoWithStorage[127.0.0.1:46862,DS-6af3b06c-7c18-49ef-9ac3-7cf938390967,DISK], DatanodeInfoWithStorage[127.0.0.1:42938,DS-33550f97-61a3-4a53-92e0-f488d0b33571,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-91412075-172.17.0.2-1598585331793:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46633,DS-6b88fabb-6365-4fa0-88e5-d9f8399ca739,DISK], DatanodeInfoWithStorage[127.0.0.1:38493,DS-c8929d68-7e80-44e3-872e-524b757ddc87,DISK], DatanodeInfoWithStorage[127.0.0.1:45779,DS-3ac3c552-4d0d-4312-8e5c-d4ad9e1a1a5e,DISK], DatanodeInfoWithStorage[127.0.0.1:32949,DS-d960899a-296e-4264-837b-df88453cdf34,DISK], DatanodeInfoWithStorage[127.0.0.1:38129,DS-7bca5bf2-1efd-4788-ad90-c3fd769c88f9,DISK], DatanodeInfoWithStorage[127.0.0.1:36179,DS-53b607b0-623f-41ef-b711-12f1ec763f7a,DISK], DatanodeInfoWithStorage[127.0.0.1:46862,DS-6af3b06c-7c18-49ef-9ac3-7cf938390967,DISK], DatanodeInfoWithStorage[127.0.0.1:42938,DS-33550f97-61a3-4a53-92e0-f488d0b33571,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-141161892-172.17.0.2-1598585366347:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44501,DS-f39ec21e-6fb6-4185-97f8-2d43862e4b28,DISK], DatanodeInfoWithStorage[127.0.0.1:41846,DS-551dae5a-f9b5-47c8-b3a2-9c904a72c052,DISK], DatanodeInfoWithStorage[127.0.0.1:40983,DS-b7686954-141f-4f86-8cb4-9dae1209aeb8,DISK], DatanodeInfoWithStorage[127.0.0.1:33949,DS-a4a9840e-3749-4c27-a61e-2fa53e0fe56d,DISK], DatanodeInfoWithStorage[127.0.0.1:33084,DS-c427e1c8-6a8f-43f2-8b78-b825ddfd9df8,DISK], DatanodeInfoWithStorage[127.0.0.1:38953,DS-130baa9d-4869-4a90-8a16-ff685f925fe7,DISK], DatanodeInfoWithStorage[127.0.0.1:38644,DS-af1a8ea0-9349-4a4f-a4c7-d04beec21c1b,DISK], DatanodeInfoWithStorage[127.0.0.1:33879,DS-c1effbdc-9431-422e-9701-9c2a78e6125b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-141161892-172.17.0.2-1598585366347:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44501,DS-f39ec21e-6fb6-4185-97f8-2d43862e4b28,DISK], DatanodeInfoWithStorage[127.0.0.1:41846,DS-551dae5a-f9b5-47c8-b3a2-9c904a72c052,DISK], DatanodeInfoWithStorage[127.0.0.1:40983,DS-b7686954-141f-4f86-8cb4-9dae1209aeb8,DISK], DatanodeInfoWithStorage[127.0.0.1:33949,DS-a4a9840e-3749-4c27-a61e-2fa53e0fe56d,DISK], DatanodeInfoWithStorage[127.0.0.1:33084,DS-c427e1c8-6a8f-43f2-8b78-b825ddfd9df8,DISK], DatanodeInfoWithStorage[127.0.0.1:38953,DS-130baa9d-4869-4a90-8a16-ff685f925fe7,DISK], DatanodeInfoWithStorage[127.0.0.1:38644,DS-af1a8ea0-9349-4a4f-a4c7-d04beec21c1b,DISK], DatanodeInfoWithStorage[127.0.0.1:33879,DS-c1effbdc-9431-422e-9701-9c2a78e6125b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-698552220-172.17.0.2-1598585737837:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39043,DS-769d7af4-7a63-42c8-9b1a-8aa1a28c1a42,DISK], DatanodeInfoWithStorage[127.0.0.1:46630,DS-f5b85f7a-04e4-41ce-b46e-e93c6d5d81f7,DISK], DatanodeInfoWithStorage[127.0.0.1:38368,DS-91139b95-191d-45ce-8d72-864a72ed9342,DISK], DatanodeInfoWithStorage[127.0.0.1:45286,DS-8e72a5d3-e75a-41c8-a711-653649e3eddd,DISK], DatanodeInfoWithStorage[127.0.0.1:33531,DS-3f7fed14-0ff4-42d1-a01e-aba138b646fc,DISK], DatanodeInfoWithStorage[127.0.0.1:41884,DS-c6c85100-0e3b-41aa-8005-2a63a9517b7b,DISK], DatanodeInfoWithStorage[127.0.0.1:43300,DS-f0683f47-f254-401b-ab4d-18de4d3efccf,DISK], DatanodeInfoWithStorage[127.0.0.1:34434,DS-fcf7ccd9-0ac4-4cb2-8aa0-c6ed97ed8f87,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-698552220-172.17.0.2-1598585737837:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39043,DS-769d7af4-7a63-42c8-9b1a-8aa1a28c1a42,DISK], DatanodeInfoWithStorage[127.0.0.1:46630,DS-f5b85f7a-04e4-41ce-b46e-e93c6d5d81f7,DISK], DatanodeInfoWithStorage[127.0.0.1:38368,DS-91139b95-191d-45ce-8d72-864a72ed9342,DISK], DatanodeInfoWithStorage[127.0.0.1:45286,DS-8e72a5d3-e75a-41c8-a711-653649e3eddd,DISK], DatanodeInfoWithStorage[127.0.0.1:33531,DS-3f7fed14-0ff4-42d1-a01e-aba138b646fc,DISK], DatanodeInfoWithStorage[127.0.0.1:41884,DS-c6c85100-0e3b-41aa-8005-2a63a9517b7b,DISK], DatanodeInfoWithStorage[127.0.0.1:43300,DS-f0683f47-f254-401b-ab4d-18de4d3efccf,DISK], DatanodeInfoWithStorage[127.0.0.1:34434,DS-fcf7ccd9-0ac4-4cb2-8aa0-c6ed97ed8f87,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-784016274-172.17.0.2-1598585901315:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35810,DS-41d3a9d5-bcd4-4cf8-ba89-a1c3b86feac0,DISK], DatanodeInfoWithStorage[127.0.0.1:35982,DS-ddf2b86f-a95a-4f38-b994-4e2a46ddf7ea,DISK], DatanodeInfoWithStorage[127.0.0.1:41810,DS-99a340d3-2e59-465c-b59d-05a9170af2b2,DISK], DatanodeInfoWithStorage[127.0.0.1:33965,DS-d6c49f45-7a17-4da5-96b9-c587f99a4610,DISK], DatanodeInfoWithStorage[127.0.0.1:39262,DS-7a2ecc85-9e62-41e4-ad93-f4a42a679eb3,DISK], DatanodeInfoWithStorage[127.0.0.1:44432,DS-bf9306d5-4c8d-41c4-9fe8-007c4a6c14a4,DISK], DatanodeInfoWithStorage[127.0.0.1:38199,DS-140ebe2f-8c35-488f-86b6-83496498ab09,DISK], DatanodeInfoWithStorage[127.0.0.1:42298,DS-2e29e372-7f0f-4704-8da1-8096d5714fa4,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-784016274-172.17.0.2-1598585901315:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35810,DS-41d3a9d5-bcd4-4cf8-ba89-a1c3b86feac0,DISK], DatanodeInfoWithStorage[127.0.0.1:35982,DS-ddf2b86f-a95a-4f38-b994-4e2a46ddf7ea,DISK], DatanodeInfoWithStorage[127.0.0.1:41810,DS-99a340d3-2e59-465c-b59d-05a9170af2b2,DISK], DatanodeInfoWithStorage[127.0.0.1:33965,DS-d6c49f45-7a17-4da5-96b9-c587f99a4610,DISK], DatanodeInfoWithStorage[127.0.0.1:39262,DS-7a2ecc85-9e62-41e4-ad93-f4a42a679eb3,DISK], DatanodeInfoWithStorage[127.0.0.1:44432,DS-bf9306d5-4c8d-41c4-9fe8-007c4a6c14a4,DISK], DatanodeInfoWithStorage[127.0.0.1:38199,DS-140ebe2f-8c35-488f-86b6-83496498ab09,DISK], DatanodeInfoWithStorage[127.0.0.1:42298,DS-2e29e372-7f0f-4704-8da1-8096d5714fa4,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-226237383-172.17.0.2-1598586039796:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37895,DS-23c77a2d-a0e9-4413-99a8-a987c52dd9cb,DISK], DatanodeInfoWithStorage[127.0.0.1:36434,DS-7bff9679-436b-4eaa-82c3-ced31d8cd24b,DISK], DatanodeInfoWithStorage[127.0.0.1:36029,DS-e9662f76-3458-4c1f-831a-b632caf38e45,DISK], DatanodeInfoWithStorage[127.0.0.1:44606,DS-2a9f0226-2682-4f41-88c8-4716bd3a6d86,DISK], DatanodeInfoWithStorage[127.0.0.1:40009,DS-1c479695-b8eb-42c1-9374-c92da94bb540,DISK], DatanodeInfoWithStorage[127.0.0.1:36319,DS-9b29caee-df8a-4cba-a36e-dd898d7ec60b,DISK], DatanodeInfoWithStorage[127.0.0.1:45829,DS-f26716e4-5e13-48d1-b886-ee3c6868e476,DISK], DatanodeInfoWithStorage[127.0.0.1:33843,DS-d80b6b6d-ba84-4fb5-962b-6b61d367fdef,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-226237383-172.17.0.2-1598586039796:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37895,DS-23c77a2d-a0e9-4413-99a8-a987c52dd9cb,DISK], DatanodeInfoWithStorage[127.0.0.1:36434,DS-7bff9679-436b-4eaa-82c3-ced31d8cd24b,DISK], DatanodeInfoWithStorage[127.0.0.1:36029,DS-e9662f76-3458-4c1f-831a-b632caf38e45,DISK], DatanodeInfoWithStorage[127.0.0.1:44606,DS-2a9f0226-2682-4f41-88c8-4716bd3a6d86,DISK], DatanodeInfoWithStorage[127.0.0.1:40009,DS-1c479695-b8eb-42c1-9374-c92da94bb540,DISK], DatanodeInfoWithStorage[127.0.0.1:36319,DS-9b29caee-df8a-4cba-a36e-dd898d7ec60b,DISK], DatanodeInfoWithStorage[127.0.0.1:45829,DS-f26716e4-5e13-48d1-b886-ee3c6868e476,DISK], DatanodeInfoWithStorage[127.0.0.1:33843,DS-d80b6b6d-ba84-4fb5-962b-6b61d367fdef,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-473296830-172.17.0.2-1598586141484:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41132,DS-5ff2ee55-7324-4258-a6fb-699ab2794870,DISK], DatanodeInfoWithStorage[127.0.0.1:36554,DS-efddcbd6-740d-4654-88d2-31e956dd13b4,DISK], DatanodeInfoWithStorage[127.0.0.1:39576,DS-a0bf7f69-855b-4d8a-b8fb-bb824cf23dda,DISK], DatanodeInfoWithStorage[127.0.0.1:37374,DS-e6579ce8-513b-4f3a-b322-377e1358313a,DISK], DatanodeInfoWithStorage[127.0.0.1:42842,DS-1ff4d2e3-e53f-4561-ad3f-a5066410bc58,DISK], DatanodeInfoWithStorage[127.0.0.1:33816,DS-227426f7-ebf3-4151-98c6-6bb393f07d5b,DISK], DatanodeInfoWithStorage[127.0.0.1:40340,DS-8ebfe7c7-e16c-4e72-8bb4-82a769bc5cc1,DISK], DatanodeInfoWithStorage[127.0.0.1:36801,DS-0fa7638e-4789-47ea-b50e-e0231de7d6a5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-473296830-172.17.0.2-1598586141484:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41132,DS-5ff2ee55-7324-4258-a6fb-699ab2794870,DISK], DatanodeInfoWithStorage[127.0.0.1:36554,DS-efddcbd6-740d-4654-88d2-31e956dd13b4,DISK], DatanodeInfoWithStorage[127.0.0.1:39576,DS-a0bf7f69-855b-4d8a-b8fb-bb824cf23dda,DISK], DatanodeInfoWithStorage[127.0.0.1:37374,DS-e6579ce8-513b-4f3a-b322-377e1358313a,DISK], DatanodeInfoWithStorage[127.0.0.1:42842,DS-1ff4d2e3-e53f-4561-ad3f-a5066410bc58,DISK], DatanodeInfoWithStorage[127.0.0.1:33816,DS-227426f7-ebf3-4151-98c6-6bb393f07d5b,DISK], DatanodeInfoWithStorage[127.0.0.1:40340,DS-8ebfe7c7-e16c-4e72-8bb4-82a769bc5cc1,DISK], DatanodeInfoWithStorage[127.0.0.1:36801,DS-0fa7638e-4789-47ea-b50e-e0231de7d6a5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-445442087-172.17.0.2-1598586280530:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45143,DS-ce572de3-68eb-45d5-8a74-dfee69b39416,DISK], DatanodeInfoWithStorage[127.0.0.1:34781,DS-5267a0ec-4998-4054-a4c6-34f3897959ad,DISK], DatanodeInfoWithStorage[127.0.0.1:37911,DS-b3ba9b3a-96be-40bd-8280-e94d1c7ddfb6,DISK], DatanodeInfoWithStorage[127.0.0.1:44184,DS-ca1ac07e-72d3-43a3-8a2d-58db99b3bb3a,DISK], DatanodeInfoWithStorage[127.0.0.1:33027,DS-f2fd43c1-d85f-4227-b09c-1d9cdacd20bc,DISK], DatanodeInfoWithStorage[127.0.0.1:35253,DS-2312fc71-31fa-46e5-85cf-498fdf2ab234,DISK], DatanodeInfoWithStorage[127.0.0.1:34912,DS-f01eb6cd-ff26-4df0-9890-f563eb327233,DISK], DatanodeInfoWithStorage[127.0.0.1:40299,DS-e3ffc713-c32b-4e56-a9e7-24c0f780270e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-445442087-172.17.0.2-1598586280530:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45143,DS-ce572de3-68eb-45d5-8a74-dfee69b39416,DISK], DatanodeInfoWithStorage[127.0.0.1:34781,DS-5267a0ec-4998-4054-a4c6-34f3897959ad,DISK], DatanodeInfoWithStorage[127.0.0.1:37911,DS-b3ba9b3a-96be-40bd-8280-e94d1c7ddfb6,DISK], DatanodeInfoWithStorage[127.0.0.1:44184,DS-ca1ac07e-72d3-43a3-8a2d-58db99b3bb3a,DISK], DatanodeInfoWithStorage[127.0.0.1:33027,DS-f2fd43c1-d85f-4227-b09c-1d9cdacd20bc,DISK], DatanodeInfoWithStorage[127.0.0.1:35253,DS-2312fc71-31fa-46e5-85cf-498fdf2ab234,DISK], DatanodeInfoWithStorage[127.0.0.1:34912,DS-f01eb6cd-ff26-4df0-9890-f563eb327233,DISK], DatanodeInfoWithStorage[127.0.0.1:40299,DS-e3ffc713-c32b-4e56-a9e7-24c0f780270e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1255572653-172.17.0.2-1598586542716:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46139,DS-de745807-5284-4b05-8259-c71692605dff,DISK], DatanodeInfoWithStorage[127.0.0.1:38714,DS-4a8a7c05-7557-45bd-b7f6-9e6d18eef53d,DISK], DatanodeInfoWithStorage[127.0.0.1:40966,DS-cceb3f87-c216-4df3-83ba-2a4e185e12dd,DISK], DatanodeInfoWithStorage[127.0.0.1:39214,DS-30217a3d-2b88-471a-9f3d-eda75afc1ff4,DISK], DatanodeInfoWithStorage[127.0.0.1:41246,DS-66d23dd0-f390-487c-83da-e96ab310062c,DISK], DatanodeInfoWithStorage[127.0.0.1:43861,DS-a306e491-ee8c-4f5a-bc57-fc89f101f804,DISK], DatanodeInfoWithStorage[127.0.0.1:33505,DS-6e38cafa-1f32-46c3-94e2-ec04c6dad223,DISK], DatanodeInfoWithStorage[127.0.0.1:39665,DS-0ad9ebb6-fef2-40a2-9fca-72ad82450c3e,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1255572653-172.17.0.2-1598586542716:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46139,DS-de745807-5284-4b05-8259-c71692605dff,DISK], DatanodeInfoWithStorage[127.0.0.1:38714,DS-4a8a7c05-7557-45bd-b7f6-9e6d18eef53d,DISK], DatanodeInfoWithStorage[127.0.0.1:40966,DS-cceb3f87-c216-4df3-83ba-2a4e185e12dd,DISK], DatanodeInfoWithStorage[127.0.0.1:39214,DS-30217a3d-2b88-471a-9f3d-eda75afc1ff4,DISK], DatanodeInfoWithStorage[127.0.0.1:41246,DS-66d23dd0-f390-487c-83da-e96ab310062c,DISK], DatanodeInfoWithStorage[127.0.0.1:43861,DS-a306e491-ee8c-4f5a-bc57-fc89f101f804,DISK], DatanodeInfoWithStorage[127.0.0.1:33505,DS-6e38cafa-1f32-46c3-94e2-ec04c6dad223,DISK], DatanodeInfoWithStorage[127.0.0.1:39665,DS-0ad9ebb6-fef2-40a2-9fca-72ad82450c3e,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1502604563-172.17.0.2-1598586570916:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44048,DS-28cd8094-faf2-4b24-9da7-3d8913f7a54e,DISK], DatanodeInfoWithStorage[127.0.0.1:38404,DS-b540a131-bdf9-42ee-9d2c-4937e166eeab,DISK], DatanodeInfoWithStorage[127.0.0.1:34351,DS-bd0fcdff-e165-48b9-a4e1-afa5cd38244d,DISK], DatanodeInfoWithStorage[127.0.0.1:41199,DS-bd8660e2-c856-427a-a089-fb9e2ea85f6d,DISK], DatanodeInfoWithStorage[127.0.0.1:43994,DS-3b8222cf-ac87-402b-9420-92e079e70ca8,DISK], DatanodeInfoWithStorage[127.0.0.1:41189,DS-9dac3279-25ca-44d2-8e5a-07ef12f8fcae,DISK], DatanodeInfoWithStorage[127.0.0.1:42401,DS-bb2535ca-c3bf-4e80-9fba-9a8d6bb6d119,DISK], DatanodeInfoWithStorage[127.0.0.1:37072,DS-df7e63d3-39b1-4292-a52e-c8b9c11ac9a8,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1502604563-172.17.0.2-1598586570916:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44048,DS-28cd8094-faf2-4b24-9da7-3d8913f7a54e,DISK], DatanodeInfoWithStorage[127.0.0.1:38404,DS-b540a131-bdf9-42ee-9d2c-4937e166eeab,DISK], DatanodeInfoWithStorage[127.0.0.1:34351,DS-bd0fcdff-e165-48b9-a4e1-afa5cd38244d,DISK], DatanodeInfoWithStorage[127.0.0.1:41199,DS-bd8660e2-c856-427a-a089-fb9e2ea85f6d,DISK], DatanodeInfoWithStorage[127.0.0.1:43994,DS-3b8222cf-ac87-402b-9420-92e079e70ca8,DISK], DatanodeInfoWithStorage[127.0.0.1:41189,DS-9dac3279-25ca-44d2-8e5a-07ef12f8fcae,DISK], DatanodeInfoWithStorage[127.0.0.1:42401,DS-bb2535ca-c3bf-4e80-9fba-9a8d6bb6d119,DISK], DatanodeInfoWithStorage[127.0.0.1:37072,DS-df7e63d3-39b1-4292-a52e-c8b9c11ac9a8,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-58912591-172.17.0.2-1598586608248:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40542,DS-d93bf43f-8b2e-476a-bacd-44d9ebc92330,DISK], DatanodeInfoWithStorage[127.0.0.1:46585,DS-14c7b777-368c-4d5d-8509-6c8447502fdd,DISK], DatanodeInfoWithStorage[127.0.0.1:44210,DS-20cdd4a0-ea12-4538-814a-e885971f9212,DISK], DatanodeInfoWithStorage[127.0.0.1:38736,DS-ba50017f-10a3-4f74-b750-a4072d14bbe2,DISK], DatanodeInfoWithStorage[127.0.0.1:42915,DS-deeaf3e7-3f15-4b2d-824e-752e980fb819,DISK], DatanodeInfoWithStorage[127.0.0.1:36151,DS-9092ef42-6dae-4ea7-bce3-c59ec9efb6f6,DISK], DatanodeInfoWithStorage[127.0.0.1:38639,DS-50aa9f22-cf42-4527-89b6-ccd274c4bed7,DISK], DatanodeInfoWithStorage[127.0.0.1:43017,DS-3540ce7a-ce29-4e1a-84ef-1ec716b7c8ba,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-58912591-172.17.0.2-1598586608248:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40542,DS-d93bf43f-8b2e-476a-bacd-44d9ebc92330,DISK], DatanodeInfoWithStorage[127.0.0.1:46585,DS-14c7b777-368c-4d5d-8509-6c8447502fdd,DISK], DatanodeInfoWithStorage[127.0.0.1:44210,DS-20cdd4a0-ea12-4538-814a-e885971f9212,DISK], DatanodeInfoWithStorage[127.0.0.1:38736,DS-ba50017f-10a3-4f74-b750-a4072d14bbe2,DISK], DatanodeInfoWithStorage[127.0.0.1:42915,DS-deeaf3e7-3f15-4b2d-824e-752e980fb819,DISK], DatanodeInfoWithStorage[127.0.0.1:36151,DS-9092ef42-6dae-4ea7-bce3-c59ec9efb6f6,DISK], DatanodeInfoWithStorage[127.0.0.1:38639,DS-50aa9f22-cf42-4527-89b6-ccd274c4bed7,DISK], DatanodeInfoWithStorage[127.0.0.1:43017,DS-3540ce7a-ce29-4e1a-84ef-1ec716b7c8ba,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1471434664-172.17.0.2-1598586861214:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42702,DS-aae66da2-b1a6-486c-b41c-ea9c698b8428,DISK], DatanodeInfoWithStorage[127.0.0.1:39170,DS-260a9ef4-a541-4c58-add1-70da90702490,DISK], DatanodeInfoWithStorage[127.0.0.1:43774,DS-46eca299-d7e4-4a2a-a963-d5fd935eb26d,DISK], DatanodeInfoWithStorage[127.0.0.1:45572,DS-725bdc96-f06f-4554-b906-79d03f8d8b4d,DISK], DatanodeInfoWithStorage[127.0.0.1:46759,DS-9080fa4c-8967-4d8a-b92d-6e21516324a5,DISK], DatanodeInfoWithStorage[127.0.0.1:37884,DS-aa823c25-2bf1-475f-ad7a-2805341b5acf,DISK], DatanodeInfoWithStorage[127.0.0.1:40277,DS-bb5fa7b4-aac1-4081-b1f6-d400da14f6bf,DISK], DatanodeInfoWithStorage[127.0.0.1:38754,DS-12c1ac03-ccda-4a46-9a7e-8bef2c9ef1bb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1471434664-172.17.0.2-1598586861214:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42702,DS-aae66da2-b1a6-486c-b41c-ea9c698b8428,DISK], DatanodeInfoWithStorage[127.0.0.1:39170,DS-260a9ef4-a541-4c58-add1-70da90702490,DISK], DatanodeInfoWithStorage[127.0.0.1:43774,DS-46eca299-d7e4-4a2a-a963-d5fd935eb26d,DISK], DatanodeInfoWithStorage[127.0.0.1:45572,DS-725bdc96-f06f-4554-b906-79d03f8d8b4d,DISK], DatanodeInfoWithStorage[127.0.0.1:46759,DS-9080fa4c-8967-4d8a-b92d-6e21516324a5,DISK], DatanodeInfoWithStorage[127.0.0.1:37884,DS-aa823c25-2bf1-475f-ad7a-2805341b5acf,DISK], DatanodeInfoWithStorage[127.0.0.1:40277,DS-bb5fa7b4-aac1-4081-b1f6-d400da14f6bf,DISK], DatanodeInfoWithStorage[127.0.0.1:38754,DS-12c1ac03-ccda-4a46-9a7e-8bef2c9ef1bb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2057001574-172.17.0.2-1598586923848:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39128,DS-c9ba3ec0-c2ef-44a9-a705-af756f373a71,DISK], DatanodeInfoWithStorage[127.0.0.1:39916,DS-52b7806c-5575-4d96-a3b3-b0e1e77c9522,DISK], DatanodeInfoWithStorage[127.0.0.1:46505,DS-e41cb2f4-ce23-41eb-9bc4-dc81fc051ec3,DISK], DatanodeInfoWithStorage[127.0.0.1:39080,DS-a60a2381-c2c2-40a8-9211-c3b76ce0741c,DISK], DatanodeInfoWithStorage[127.0.0.1:39858,DS-15a5ba36-6e99-42a9-ba25-ecd1020ae396,DISK], DatanodeInfoWithStorage[127.0.0.1:39041,DS-54417ea3-d6b2-4510-b3fa-2332e912f7cc,DISK], DatanodeInfoWithStorage[127.0.0.1:43623,DS-09aff8f1-40bd-4b2e-99ec-28ba9ca6947f,DISK], DatanodeInfoWithStorage[127.0.0.1:33049,DS-7fd601ec-0250-4127-b61b-c38a0a61dfef,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2057001574-172.17.0.2-1598586923848:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39128,DS-c9ba3ec0-c2ef-44a9-a705-af756f373a71,DISK], DatanodeInfoWithStorage[127.0.0.1:39916,DS-52b7806c-5575-4d96-a3b3-b0e1e77c9522,DISK], DatanodeInfoWithStorage[127.0.0.1:46505,DS-e41cb2f4-ce23-41eb-9bc4-dc81fc051ec3,DISK], DatanodeInfoWithStorage[127.0.0.1:39080,DS-a60a2381-c2c2-40a8-9211-c3b76ce0741c,DISK], DatanodeInfoWithStorage[127.0.0.1:39858,DS-15a5ba36-6e99-42a9-ba25-ecd1020ae396,DISK], DatanodeInfoWithStorage[127.0.0.1:39041,DS-54417ea3-d6b2-4510-b3fa-2332e912f7cc,DISK], DatanodeInfoWithStorage[127.0.0.1:43623,DS-09aff8f1-40bd-4b2e-99ec-28ba9ca6947f,DISK], DatanodeInfoWithStorage[127.0.0.1:33049,DS-7fd601ec-0250-4127-b61b-c38a0a61dfef,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1479722781-172.17.0.2-1598586958441:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41828,DS-68af6c55-68a1-499f-bc67-288ed419f420,DISK], DatanodeInfoWithStorage[127.0.0.1:38509,DS-eb4e826b-aaac-4416-b5d9-8096c675be1a,DISK], DatanodeInfoWithStorage[127.0.0.1:36491,DS-300bddc4-b390-4b9d-96be-7103caad85e5,DISK], DatanodeInfoWithStorage[127.0.0.1:39517,DS-57607cc8-27b5-45a3-a3ff-5f60f2c8139d,DISK], DatanodeInfoWithStorage[127.0.0.1:45991,DS-f26337ef-2b6a-414a-b566-2a605a297562,DISK], DatanodeInfoWithStorage[127.0.0.1:34635,DS-a2b17399-3980-43f0-88d8-828faab5b655,DISK], DatanodeInfoWithStorage[127.0.0.1:46359,DS-7b8b942a-331c-4f49-8f50-aa54e451448e,DISK], DatanodeInfoWithStorage[127.0.0.1:41144,DS-7df99c6f-d521-42c6-8f59-f55a1cb9625b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1479722781-172.17.0.2-1598586958441:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41828,DS-68af6c55-68a1-499f-bc67-288ed419f420,DISK], DatanodeInfoWithStorage[127.0.0.1:38509,DS-eb4e826b-aaac-4416-b5d9-8096c675be1a,DISK], DatanodeInfoWithStorage[127.0.0.1:36491,DS-300bddc4-b390-4b9d-96be-7103caad85e5,DISK], DatanodeInfoWithStorage[127.0.0.1:39517,DS-57607cc8-27b5-45a3-a3ff-5f60f2c8139d,DISK], DatanodeInfoWithStorage[127.0.0.1:45991,DS-f26337ef-2b6a-414a-b566-2a605a297562,DISK], DatanodeInfoWithStorage[127.0.0.1:34635,DS-a2b17399-3980-43f0-88d8-828faab5b655,DISK], DatanodeInfoWithStorage[127.0.0.1:46359,DS-7b8b942a-331c-4f49-8f50-aa54e451448e,DISK], DatanodeInfoWithStorage[127.0.0.1:41144,DS-7df99c6f-d521-42c6-8f59-f55a1cb9625b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2036867985-172.17.0.2-1598586998311:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45778,DS-0098d538-8156-49af-be2e-f65192c6a02e,DISK], DatanodeInfoWithStorage[127.0.0.1:44960,DS-2775f91c-975c-42b5-934f-145f7bdfbee0,DISK], DatanodeInfoWithStorage[127.0.0.1:45684,DS-255cde87-92e2-4561-9ef6-022741ab9a29,DISK], DatanodeInfoWithStorage[127.0.0.1:39428,DS-585c1610-0c29-4ac7-86b5-97e437c140ee,DISK], DatanodeInfoWithStorage[127.0.0.1:36638,DS-39bf6365-1de8-472c-ad6f-629dd7bd574c,DISK], DatanodeInfoWithStorage[127.0.0.1:36002,DS-b16627b5-3850-484c-aa0c-68ae9759519a,DISK], DatanodeInfoWithStorage[127.0.0.1:38831,DS-00b487cc-df53-4155-b677-6868f385efa2,DISK], DatanodeInfoWithStorage[127.0.0.1:37447,DS-5ab4e1e1-a689-41a0-a60b-21a9b12a14dc,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2036867985-172.17.0.2-1598586998311:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45778,DS-0098d538-8156-49af-be2e-f65192c6a02e,DISK], DatanodeInfoWithStorage[127.0.0.1:44960,DS-2775f91c-975c-42b5-934f-145f7bdfbee0,DISK], DatanodeInfoWithStorage[127.0.0.1:45684,DS-255cde87-92e2-4561-9ef6-022741ab9a29,DISK], DatanodeInfoWithStorage[127.0.0.1:39428,DS-585c1610-0c29-4ac7-86b5-97e437c140ee,DISK], DatanodeInfoWithStorage[127.0.0.1:36638,DS-39bf6365-1de8-472c-ad6f-629dd7bd574c,DISK], DatanodeInfoWithStorage[127.0.0.1:36002,DS-b16627b5-3850-484c-aa0c-68ae9759519a,DISK], DatanodeInfoWithStorage[127.0.0.1:38831,DS-00b487cc-df53-4155-b677-6868f385efa2,DISK], DatanodeInfoWithStorage[127.0.0.1:37447,DS-5ab4e1e1-a689-41a0-a60b-21a9b12a14dc,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1322066939-172.17.0.2-1598587195916:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36378,DS-c1f6ea58-a2a3-46c9-834b-af39c79b9f8a,DISK], DatanodeInfoWithStorage[127.0.0.1:43950,DS-6156a45e-be97-44ef-9bab-bcf66025e965,DISK], DatanodeInfoWithStorage[127.0.0.1:33388,DS-e002444a-8d67-493d-942b-ab310e76fa7a,DISK], DatanodeInfoWithStorage[127.0.0.1:42424,DS-b5753473-568a-4da3-9ae8-1355f6728bf4,DISK], DatanodeInfoWithStorage[127.0.0.1:40206,DS-4be486ca-8ee2-4787-9c15-ca3afefe6453,DISK], DatanodeInfoWithStorage[127.0.0.1:42132,DS-ca8ee8b9-d9b8-4275-abc7-7cbea0132dfb,DISK], DatanodeInfoWithStorage[127.0.0.1:33843,DS-f23c888c-495d-4adf-87d8-8efdcf6cf1ca,DISK], DatanodeInfoWithStorage[127.0.0.1:34769,DS-3904645b-7764-4d56-a0d0-af078a3e57c8,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1322066939-172.17.0.2-1598587195916:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36378,DS-c1f6ea58-a2a3-46c9-834b-af39c79b9f8a,DISK], DatanodeInfoWithStorage[127.0.0.1:43950,DS-6156a45e-be97-44ef-9bab-bcf66025e965,DISK], DatanodeInfoWithStorage[127.0.0.1:33388,DS-e002444a-8d67-493d-942b-ab310e76fa7a,DISK], DatanodeInfoWithStorage[127.0.0.1:42424,DS-b5753473-568a-4da3-9ae8-1355f6728bf4,DISK], DatanodeInfoWithStorage[127.0.0.1:40206,DS-4be486ca-8ee2-4787-9c15-ca3afefe6453,DISK], DatanodeInfoWithStorage[127.0.0.1:42132,DS-ca8ee8b9-d9b8-4275-abc7-7cbea0132dfb,DISK], DatanodeInfoWithStorage[127.0.0.1:33843,DS-f23c888c-495d-4adf-87d8-8efdcf6cf1ca,DISK], DatanodeInfoWithStorage[127.0.0.1:34769,DS-3904645b-7764-4d56-a0d0-af078a3e57c8,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1493421352-172.17.0.2-1598587507581:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37124,DS-c82189c9-f79e-4d30-94e0-6278ccb8e609,DISK], DatanodeInfoWithStorage[127.0.0.1:35731,DS-3f812495-0efb-4335-adf1-2952eea13693,DISK], DatanodeInfoWithStorage[127.0.0.1:45385,DS-120d1687-83d4-4f94-91a3-4dd4e5f71691,DISK], DatanodeInfoWithStorage[127.0.0.1:35954,DS-1ca699e5-b3cc-499e-8471-d8ef72cc4b7e,DISK], DatanodeInfoWithStorage[127.0.0.1:38876,DS-1f8554e6-de31-4be3-9b0e-e5253a7bfb09,DISK], DatanodeInfoWithStorage[127.0.0.1:33298,DS-a4a8f173-bf71-458e-869e-dd958de691a9,DISK], DatanodeInfoWithStorage[127.0.0.1:32949,DS-b5831c6a-5c88-4284-ba01-2958de489a78,DISK], DatanodeInfoWithStorage[127.0.0.1:38576,DS-c9a44fbb-dccd-4fab-bbc6-8291edddf196,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1493421352-172.17.0.2-1598587507581:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37124,DS-c82189c9-f79e-4d30-94e0-6278ccb8e609,DISK], DatanodeInfoWithStorage[127.0.0.1:35731,DS-3f812495-0efb-4335-adf1-2952eea13693,DISK], DatanodeInfoWithStorage[127.0.0.1:45385,DS-120d1687-83d4-4f94-91a3-4dd4e5f71691,DISK], DatanodeInfoWithStorage[127.0.0.1:35954,DS-1ca699e5-b3cc-499e-8471-d8ef72cc4b7e,DISK], DatanodeInfoWithStorage[127.0.0.1:38876,DS-1f8554e6-de31-4be3-9b0e-e5253a7bfb09,DISK], DatanodeInfoWithStorage[127.0.0.1:33298,DS-a4a8f173-bf71-458e-869e-dd958de691a9,DISK], DatanodeInfoWithStorage[127.0.0.1:32949,DS-b5831c6a-5c88-4284-ba01-2958de489a78,DISK], DatanodeInfoWithStorage[127.0.0.1:38576,DS-c9a44fbb-dccd-4fab-bbc6-8291edddf196,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-938634778-172.17.0.2-1598587535284:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44652,DS-46dc5870-1c5a-4b9e-99e5-5b1aca82d199,DISK], DatanodeInfoWithStorage[127.0.0.1:42537,DS-b270d94f-afa2-4c1b-8295-7d88fb0787ac,DISK], DatanodeInfoWithStorage[127.0.0.1:46148,DS-fce1482f-b953-4d95-9300-d79bdda9ffa7,DISK], DatanodeInfoWithStorage[127.0.0.1:46342,DS-3be9bd85-0311-4fc7-968e-4d32ccdcf0e4,DISK], DatanodeInfoWithStorage[127.0.0.1:34757,DS-2bb281c6-87e8-4f87-80af-8e510f1eee0c,DISK], DatanodeInfoWithStorage[127.0.0.1:35431,DS-02607a62-f58c-42f7-99f0-1242de295ed6,DISK], DatanodeInfoWithStorage[127.0.0.1:45081,DS-b1e34979-32b8-47e4-9d1f-df3fb131fea5,DISK], DatanodeInfoWithStorage[127.0.0.1:40161,DS-18a1d229-462e-43c1-9eb5-afcefcfc397f,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-938634778-172.17.0.2-1598587535284:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44652,DS-46dc5870-1c5a-4b9e-99e5-5b1aca82d199,DISK], DatanodeInfoWithStorage[127.0.0.1:42537,DS-b270d94f-afa2-4c1b-8295-7d88fb0787ac,DISK], DatanodeInfoWithStorage[127.0.0.1:46148,DS-fce1482f-b953-4d95-9300-d79bdda9ffa7,DISK], DatanodeInfoWithStorage[127.0.0.1:46342,DS-3be9bd85-0311-4fc7-968e-4d32ccdcf0e4,DISK], DatanodeInfoWithStorage[127.0.0.1:34757,DS-2bb281c6-87e8-4f87-80af-8e510f1eee0c,DISK], DatanodeInfoWithStorage[127.0.0.1:35431,DS-02607a62-f58c-42f7-99f0-1242de295ed6,DISK], DatanodeInfoWithStorage[127.0.0.1:45081,DS-b1e34979-32b8-47e4-9d1f-df3fb131fea5,DISK], DatanodeInfoWithStorage[127.0.0.1:40161,DS-18a1d229-462e-43c1-9eb5-afcefcfc397f,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1983587736-172.17.0.2-1598587602088:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33207,DS-bcd2f591-b54e-408b-87cf-97b795a737e9,DISK], DatanodeInfoWithStorage[127.0.0.1:37771,DS-05732b61-b3b9-45c5-a4ca-291339e9d707,DISK], DatanodeInfoWithStorage[127.0.0.1:44315,DS-2682ee26-a3ee-42ad-ab71-4c24ea289f8d,DISK], DatanodeInfoWithStorage[127.0.0.1:36922,DS-2662642e-9bb9-4f63-b35e-96da4f820b4c,DISK], DatanodeInfoWithStorage[127.0.0.1:41139,DS-c07c87c3-5c39-4633-8129-76faa7601f44,DISK], DatanodeInfoWithStorage[127.0.0.1:39079,DS-4ca9b306-4b10-4077-9731-8581715cebf9,DISK], DatanodeInfoWithStorage[127.0.0.1:32980,DS-f3063357-65a5-467c-b185-e7ec9ab3e63b,DISK], DatanodeInfoWithStorage[127.0.0.1:43944,DS-51b67b9e-3a9f-44a9-8a57-789c61246c2b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1983587736-172.17.0.2-1598587602088:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33207,DS-bcd2f591-b54e-408b-87cf-97b795a737e9,DISK], DatanodeInfoWithStorage[127.0.0.1:37771,DS-05732b61-b3b9-45c5-a4ca-291339e9d707,DISK], DatanodeInfoWithStorage[127.0.0.1:44315,DS-2682ee26-a3ee-42ad-ab71-4c24ea289f8d,DISK], DatanodeInfoWithStorage[127.0.0.1:36922,DS-2662642e-9bb9-4f63-b35e-96da4f820b4c,DISK], DatanodeInfoWithStorage[127.0.0.1:41139,DS-c07c87c3-5c39-4633-8129-76faa7601f44,DISK], DatanodeInfoWithStorage[127.0.0.1:39079,DS-4ca9b306-4b10-4077-9731-8581715cebf9,DISK], DatanodeInfoWithStorage[127.0.0.1:32980,DS-f3063357-65a5-467c-b185-e7ec9ab3e63b,DISK], DatanodeInfoWithStorage[127.0.0.1:43944,DS-51b67b9e-3a9f-44a9-8a57-789c61246c2b,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-159781945-172.17.0.2-1598587701639:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46627,DS-b32cb6b0-c63f-446c-a362-9f5c2e3f936a,DISK], DatanodeInfoWithStorage[127.0.0.1:37032,DS-ec21041a-0c6c-4f62-a9b7-e32bc3c733fc,DISK], DatanodeInfoWithStorage[127.0.0.1:42994,DS-487a000d-ae69-4f42-9c98-d16bcef77e03,DISK], DatanodeInfoWithStorage[127.0.0.1:36305,DS-ca241238-ae55-444e-98c3-b604d8489d0a,DISK], DatanodeInfoWithStorage[127.0.0.1:45765,DS-1e8ee470-40ae-4741-86b9-0b9803b08bc8,DISK], DatanodeInfoWithStorage[127.0.0.1:38876,DS-5f770aad-0464-48b4-93e6-5e7219ecc109,DISK], DatanodeInfoWithStorage[127.0.0.1:46451,DS-43d7a90c-4658-49c8-ab43-a6df3ec18456,DISK], DatanodeInfoWithStorage[127.0.0.1:46357,DS-b576150f-e4bf-4eff-846d-e81793c6ab09,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-159781945-172.17.0.2-1598587701639:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46627,DS-b32cb6b0-c63f-446c-a362-9f5c2e3f936a,DISK], DatanodeInfoWithStorage[127.0.0.1:37032,DS-ec21041a-0c6c-4f62-a9b7-e32bc3c733fc,DISK], DatanodeInfoWithStorage[127.0.0.1:42994,DS-487a000d-ae69-4f42-9c98-d16bcef77e03,DISK], DatanodeInfoWithStorage[127.0.0.1:36305,DS-ca241238-ae55-444e-98c3-b604d8489d0a,DISK], DatanodeInfoWithStorage[127.0.0.1:45765,DS-1e8ee470-40ae-4741-86b9-0b9803b08bc8,DISK], DatanodeInfoWithStorage[127.0.0.1:38876,DS-5f770aad-0464-48b4-93e6-5e7219ecc109,DISK], DatanodeInfoWithStorage[127.0.0.1:46451,DS-43d7a90c-4658-49c8-ab43-a6df3ec18456,DISK], DatanodeInfoWithStorage[127.0.0.1:46357,DS-b576150f-e4bf-4eff-846d-e81793c6ab09,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1855498576-172.17.0.2-1598587764579:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45677,DS-ca749b40-54ec-4755-9818-3897c8896210,DISK], DatanodeInfoWithStorage[127.0.0.1:38445,DS-4bdbe58d-98a4-4e60-a52d-8080e02bd103,DISK], DatanodeInfoWithStorage[127.0.0.1:33575,DS-a7cc69ef-946b-4cb9-bcb6-16353a9fe31f,DISK], DatanodeInfoWithStorage[127.0.0.1:39652,DS-e1ecdcea-f511-43ba-923e-c3d96aa6db0c,DISK], DatanodeInfoWithStorage[127.0.0.1:32929,DS-8d04b231-7cec-4f48-b64b-b69bac8b54f4,DISK], DatanodeInfoWithStorage[127.0.0.1:34118,DS-edf87c44-9063-4ce4-9798-ee80c939d3ca,DISK], DatanodeInfoWithStorage[127.0.0.1:33296,DS-d3b228d5-ae8e-4d3b-ab0c-1a15cacf8ee8,DISK], DatanodeInfoWithStorage[127.0.0.1:42185,DS-fe37afdd-1289-41c1-a495-504c1f51d69d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1855498576-172.17.0.2-1598587764579:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45677,DS-ca749b40-54ec-4755-9818-3897c8896210,DISK], DatanodeInfoWithStorage[127.0.0.1:38445,DS-4bdbe58d-98a4-4e60-a52d-8080e02bd103,DISK], DatanodeInfoWithStorage[127.0.0.1:33575,DS-a7cc69ef-946b-4cb9-bcb6-16353a9fe31f,DISK], DatanodeInfoWithStorage[127.0.0.1:39652,DS-e1ecdcea-f511-43ba-923e-c3d96aa6db0c,DISK], DatanodeInfoWithStorage[127.0.0.1:32929,DS-8d04b231-7cec-4f48-b64b-b69bac8b54f4,DISK], DatanodeInfoWithStorage[127.0.0.1:34118,DS-edf87c44-9063-4ce4-9798-ee80c939d3ca,DISK], DatanodeInfoWithStorage[127.0.0.1:33296,DS-d3b228d5-ae8e-4d3b-ab0c-1a15cacf8ee8,DISK], DatanodeInfoWithStorage[127.0.0.1:42185,DS-fe37afdd-1289-41c1-a495-504c1f51d69d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1591878263-172.17.0.2-1598587932680:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40339,DS-f0eb2312-1cd6-43af-9a12-4316c783d2ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40122,DS-570e95b1-95ec-424a-aabe-d5a0fe594f03,DISK], DatanodeInfoWithStorage[127.0.0.1:42559,DS-abfbf217-1c52-477e-a5be-a4aab4582f27,DISK], DatanodeInfoWithStorage[127.0.0.1:33807,DS-86f27ff0-635d-4f3d-be0b-fce6c29624d8,DISK], DatanodeInfoWithStorage[127.0.0.1:37787,DS-1108bbcb-56d9-4279-9230-fbc328b8b16b,DISK], DatanodeInfoWithStorage[127.0.0.1:34886,DS-1ccff911-19d1-4c45-8e5b-b8d55b035684,DISK], DatanodeInfoWithStorage[127.0.0.1:46072,DS-cd7505ff-6c6e-475a-b8a9-8fdca3661b9b,DISK], DatanodeInfoWithStorage[127.0.0.1:40783,DS-83e0eeb3-8fcf-4c54-a734-a1b3f5283b79,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1591878263-172.17.0.2-1598587932680:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40339,DS-f0eb2312-1cd6-43af-9a12-4316c783d2ce,DISK], DatanodeInfoWithStorage[127.0.0.1:40122,DS-570e95b1-95ec-424a-aabe-d5a0fe594f03,DISK], DatanodeInfoWithStorage[127.0.0.1:42559,DS-abfbf217-1c52-477e-a5be-a4aab4582f27,DISK], DatanodeInfoWithStorage[127.0.0.1:33807,DS-86f27ff0-635d-4f3d-be0b-fce6c29624d8,DISK], DatanodeInfoWithStorage[127.0.0.1:37787,DS-1108bbcb-56d9-4279-9230-fbc328b8b16b,DISK], DatanodeInfoWithStorage[127.0.0.1:34886,DS-1ccff911-19d1-4c45-8e5b-b8d55b035684,DISK], DatanodeInfoWithStorage[127.0.0.1:46072,DS-cd7505ff-6c6e-475a-b8a9-8fdca3661b9b,DISK], DatanodeInfoWithStorage[127.0.0.1:40783,DS-83e0eeb3-8fcf-4c54-a734-a1b3f5283b79,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1741319117-172.17.0.2-1598588202022:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41389,DS-07bdef83-770e-4872-a7cd-51121ee7efd6,DISK], DatanodeInfoWithStorage[127.0.0.1:34671,DS-ed8fa953-0545-4655-b020-ae4bc613edab,DISK], DatanodeInfoWithStorage[127.0.0.1:42668,DS-33768dae-dd8d-4a00-923a-cddbdeb8b259,DISK], DatanodeInfoWithStorage[127.0.0.1:38850,DS-e01d0fd1-214d-4cec-a7b5-fd157aca1e50,DISK], DatanodeInfoWithStorage[127.0.0.1:41771,DS-aa3b1b80-c4d1-4812-a96a-e5afffa6c712,DISK], DatanodeInfoWithStorage[127.0.0.1:32870,DS-9015451e-2122-4619-936f-9f92ea8aa8b9,DISK], DatanodeInfoWithStorage[127.0.0.1:35859,DS-9c2048d3-138a-4da8-837d-0936d4ef6521,DISK], DatanodeInfoWithStorage[127.0.0.1:32983,DS-6b787437-cd6b-436c-ac01-7b4495c3beab,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1741319117-172.17.0.2-1598588202022:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41389,DS-07bdef83-770e-4872-a7cd-51121ee7efd6,DISK], DatanodeInfoWithStorage[127.0.0.1:34671,DS-ed8fa953-0545-4655-b020-ae4bc613edab,DISK], DatanodeInfoWithStorage[127.0.0.1:42668,DS-33768dae-dd8d-4a00-923a-cddbdeb8b259,DISK], DatanodeInfoWithStorage[127.0.0.1:38850,DS-e01d0fd1-214d-4cec-a7b5-fd157aca1e50,DISK], DatanodeInfoWithStorage[127.0.0.1:41771,DS-aa3b1b80-c4d1-4812-a96a-e5afffa6c712,DISK], DatanodeInfoWithStorage[127.0.0.1:32870,DS-9015451e-2122-4619-936f-9f92ea8aa8b9,DISK], DatanodeInfoWithStorage[127.0.0.1:35859,DS-9c2048d3-138a-4da8-837d-0936d4ef6521,DISK], DatanodeInfoWithStorage[127.0.0.1:32983,DS-6b787437-cd6b-436c-ac01-7b4495c3beab,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-52542169-172.17.0.2-1598588374156:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46529,DS-0037ee7c-701b-446b-8fc5-8d7d1298f57a,DISK], DatanodeInfoWithStorage[127.0.0.1:45871,DS-9bea200b-0502-4e63-a8cb-a547c49d241b,DISK], DatanodeInfoWithStorage[127.0.0.1:33463,DS-f35c6e14-7c87-4c09-ba5f-c50f6086524a,DISK], DatanodeInfoWithStorage[127.0.0.1:36232,DS-cd20b79e-0d20-446c-a331-678fdcfbf1e3,DISK], DatanodeInfoWithStorage[127.0.0.1:37305,DS-f7f5c471-61fb-4a80-b330-6fbe2f245eb3,DISK], DatanodeInfoWithStorage[127.0.0.1:43358,DS-366ea390-05b9-4903-a355-cdb2373a298e,DISK], DatanodeInfoWithStorage[127.0.0.1:34835,DS-1be3c2db-69b8-40bb-8e9c-c99223766848,DISK], DatanodeInfoWithStorage[127.0.0.1:35202,DS-8f35cf22-81ed-48c4-971e-15d54a439410,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-52542169-172.17.0.2-1598588374156:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46529,DS-0037ee7c-701b-446b-8fc5-8d7d1298f57a,DISK], DatanodeInfoWithStorage[127.0.0.1:45871,DS-9bea200b-0502-4e63-a8cb-a547c49d241b,DISK], DatanodeInfoWithStorage[127.0.0.1:33463,DS-f35c6e14-7c87-4c09-ba5f-c50f6086524a,DISK], DatanodeInfoWithStorage[127.0.0.1:36232,DS-cd20b79e-0d20-446c-a331-678fdcfbf1e3,DISK], DatanodeInfoWithStorage[127.0.0.1:37305,DS-f7f5c471-61fb-4a80-b330-6fbe2f245eb3,DISK], DatanodeInfoWithStorage[127.0.0.1:43358,DS-366ea390-05b9-4903-a355-cdb2373a298e,DISK], DatanodeInfoWithStorage[127.0.0.1:34835,DS-1be3c2db-69b8-40bb-8e9c-c99223766848,DISK], DatanodeInfoWithStorage[127.0.0.1:35202,DS-8f35cf22-81ed-48c4-971e-15d54a439410,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1192287170-172.17.0.2-1598588856656:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39130,DS-c0ee5a0f-4660-4419-9cf3-8d020bcd911f,DISK], DatanodeInfoWithStorage[127.0.0.1:39441,DS-c113ede7-41a5-40b7-90c0-bb97036a12fb,DISK], DatanodeInfoWithStorage[127.0.0.1:38775,DS-0cb35998-a8ef-466c-b5d9-2e8beddca7f6,DISK], DatanodeInfoWithStorage[127.0.0.1:34659,DS-7e56372a-622e-4544-8f8e-4ea98687ad89,DISK], DatanodeInfoWithStorage[127.0.0.1:44633,DS-641830ac-a604-4a79-aaa0-39f01b28a156,DISK], DatanodeInfoWithStorage[127.0.0.1:42414,DS-4b1235a7-5310-4ca0-b31b-c55bb3c2700b,DISK], DatanodeInfoWithStorage[127.0.0.1:45920,DS-d0e96520-359f-4b64-b7cf-5911f0aa7d92,DISK], DatanodeInfoWithStorage[127.0.0.1:37698,DS-2e0f3f8c-a320-4829-8884-2c03f99bb1ca,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1192287170-172.17.0.2-1598588856656:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39130,DS-c0ee5a0f-4660-4419-9cf3-8d020bcd911f,DISK], DatanodeInfoWithStorage[127.0.0.1:39441,DS-c113ede7-41a5-40b7-90c0-bb97036a12fb,DISK], DatanodeInfoWithStorage[127.0.0.1:38775,DS-0cb35998-a8ef-466c-b5d9-2e8beddca7f6,DISK], DatanodeInfoWithStorage[127.0.0.1:34659,DS-7e56372a-622e-4544-8f8e-4ea98687ad89,DISK], DatanodeInfoWithStorage[127.0.0.1:44633,DS-641830ac-a604-4a79-aaa0-39f01b28a156,DISK], DatanodeInfoWithStorage[127.0.0.1:42414,DS-4b1235a7-5310-4ca0-b31b-c55bb3c2700b,DISK], DatanodeInfoWithStorage[127.0.0.1:45920,DS-d0e96520-359f-4b64-b7cf-5911f0aa7d92,DISK], DatanodeInfoWithStorage[127.0.0.1:37698,DS-2e0f3f8c-a320-4829-8884-2c03f99bb1ca,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1007813799-172.17.0.2-1598589317943:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44121,DS-9c840874-0728-4749-acd9-797a8d6e2cf8,DISK], DatanodeInfoWithStorage[127.0.0.1:35081,DS-901ed8fd-610c-48c8-a904-721bf6726453,DISK], DatanodeInfoWithStorage[127.0.0.1:46843,DS-94141b3a-e050-4715-9e4e-0e3af12261c8,DISK], DatanodeInfoWithStorage[127.0.0.1:38173,DS-ac189662-4c6c-4903-83b8-596ed76ea98a,DISK], DatanodeInfoWithStorage[127.0.0.1:38092,DS-0faf00b7-b258-4c07-82cc-e4cd675ce0c4,DISK], DatanodeInfoWithStorage[127.0.0.1:40233,DS-c58ccce9-ad90-4a88-995c-bdcc9c1e7eb3,DISK], DatanodeInfoWithStorage[127.0.0.1:43692,DS-5d9a246d-7917-468d-9916-46eb8e0ced8a,DISK], DatanodeInfoWithStorage[127.0.0.1:40994,DS-f4851114-b55c-4afa-8bb8-6ede16b9a457,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1007813799-172.17.0.2-1598589317943:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44121,DS-9c840874-0728-4749-acd9-797a8d6e2cf8,DISK], DatanodeInfoWithStorage[127.0.0.1:35081,DS-901ed8fd-610c-48c8-a904-721bf6726453,DISK], DatanodeInfoWithStorage[127.0.0.1:46843,DS-94141b3a-e050-4715-9e4e-0e3af12261c8,DISK], DatanodeInfoWithStorage[127.0.0.1:38173,DS-ac189662-4c6c-4903-83b8-596ed76ea98a,DISK], DatanodeInfoWithStorage[127.0.0.1:38092,DS-0faf00b7-b258-4c07-82cc-e4cd675ce0c4,DISK], DatanodeInfoWithStorage[127.0.0.1:40233,DS-c58ccce9-ad90-4a88-995c-bdcc9c1e7eb3,DISK], DatanodeInfoWithStorage[127.0.0.1:43692,DS-5d9a246d-7917-468d-9916-46eb8e0ced8a,DISK], DatanodeInfoWithStorage[127.0.0.1:40994,DS-f4851114-b55c-4afa-8bb8-6ede16b9a457,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2130288896-172.17.0.2-1598589382249:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36417,DS-bcaf10ee-748a-4761-8c82-732ea073e296,DISK], DatanodeInfoWithStorage[127.0.0.1:43689,DS-8f06c7e2-251c-4c9c-b9b8-a935158c2eab,DISK], DatanodeInfoWithStorage[127.0.0.1:44940,DS-31697f68-c45a-4f43-9422-86c1c378def5,DISK], DatanodeInfoWithStorage[127.0.0.1:44609,DS-876febff-d270-4fdf-868b-0d6732bf3813,DISK], DatanodeInfoWithStorage[127.0.0.1:38478,DS-47fc2d26-2760-4ba3-937d-698ea11a485f,DISK], DatanodeInfoWithStorage[127.0.0.1:39667,DS-897e6c7d-a8bc-42a6-8ed0-8a59e3a55462,DISK], DatanodeInfoWithStorage[127.0.0.1:36895,DS-cbc5cce9-8028-4224-b971-4bfb6575ac5b,DISK], DatanodeInfoWithStorage[127.0.0.1:34594,DS-6a1d71d3-6e2f-4b62-b033-5f4e9be08f62,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2130288896-172.17.0.2-1598589382249:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36417,DS-bcaf10ee-748a-4761-8c82-732ea073e296,DISK], DatanodeInfoWithStorage[127.0.0.1:43689,DS-8f06c7e2-251c-4c9c-b9b8-a935158c2eab,DISK], DatanodeInfoWithStorage[127.0.0.1:44940,DS-31697f68-c45a-4f43-9422-86c1c378def5,DISK], DatanodeInfoWithStorage[127.0.0.1:44609,DS-876febff-d270-4fdf-868b-0d6732bf3813,DISK], DatanodeInfoWithStorage[127.0.0.1:38478,DS-47fc2d26-2760-4ba3-937d-698ea11a485f,DISK], DatanodeInfoWithStorage[127.0.0.1:39667,DS-897e6c7d-a8bc-42a6-8ed0-8a59e3a55462,DISK], DatanodeInfoWithStorage[127.0.0.1:36895,DS-cbc5cce9-8028-4224-b971-4bfb6575ac5b,DISK], DatanodeInfoWithStorage[127.0.0.1:34594,DS-6a1d71d3-6e2f-4b62-b033-5f4e9be08f62,DISK]]; indices=[0, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.http.max.request.header.size
component: hdfs:DataNode
v1: 64
v2: 65536
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksum#testStripedFileChecksumWithMissedDataBlocksRangeQuery7
reconfPoint: -1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2053848151-172.17.0.2-1598589475558:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36578,DS-bd44b1a1-a405-44d9-8a8b-ad3705973b6e,DISK], DatanodeInfoWithStorage[127.0.0.1:46157,DS-a7904a7d-8e60-4e19-b78a-2fa676127d92,DISK], DatanodeInfoWithStorage[127.0.0.1:38639,DS-29478619-3b34-4810-ab96-d01c616f9622,DISK], DatanodeInfoWithStorage[127.0.0.1:39636,DS-2b0280ea-8a60-4f06-a712-ed28ca2b1793,DISK], DatanodeInfoWithStorage[127.0.0.1:37928,DS-5ff0117e-4c0d-4455-a8b7-ac9199fb33a3,DISK], DatanodeInfoWithStorage[127.0.0.1:39176,DS-1d7d64af-0da3-4752-a7be-8a46b22150b6,DISK], DatanodeInfoWithStorage[127.0.0.1:36188,DS-6d3b8c22-e11f-4d93-b7b7-3ca6b88b48ca,DISK], DatanodeInfoWithStorage[127.0.0.1:33927,DS-9e0ed896-e5e2-4270-a8a8-6ea6c2c0eb1c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2053848151-172.17.0.2-1598589475558:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36578,DS-bd44b1a1-a405-44d9-8a8b-ad3705973b6e,DISK], DatanodeInfoWithStorage[127.0.0.1:46157,DS-a7904a7d-8e60-4e19-b78a-2fa676127d92,DISK], DatanodeInfoWithStorage[127.0.0.1:38639,DS-29478619-3b34-4810-ab96-d01c616f9622,DISK], DatanodeInfoWithStorage[127.0.0.1:39636,DS-2b0280ea-8a60-4f06-a712-ed28ca2b1793,DISK], DatanodeInfoWithStorage[127.0.0.1:37928,DS-5ff0117e-4c0d-4455-a8b7-ac9199fb33a3,DISK], DatanodeInfoWithStorage[127.0.0.1:39176,DS-1d7d64af-0da3-4752-a7be-8a46b22150b6,DISK], DatanodeInfoWithStorage[127.0.0.1:36188,DS-6d3b8c22-e11f-4d93-b7b7-3ca6b88b48ca,DISK], DatanodeInfoWithStorage[127.0.0.1:33927,DS-9e0ed896-e5e2-4270-a8a8-6ea6c2c0eb1c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery7(TestFileChecksum.java:377)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


v1v2 failed with probability 13 out of 50
v1v1v2v2 failed with probability 19 out of 50
result: false positive !!!
Total execution time in seconds : 4965

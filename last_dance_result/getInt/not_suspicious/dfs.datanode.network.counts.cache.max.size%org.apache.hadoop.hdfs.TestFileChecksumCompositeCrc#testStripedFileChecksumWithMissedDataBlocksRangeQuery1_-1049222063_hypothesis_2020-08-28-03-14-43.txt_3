reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1958347167-172.17.0.13-1598584743629:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43457,DS-8b431263-ee29-472b-86f7-1f48f8339d6e,DISK], DatanodeInfoWithStorage[127.0.0.1:35116,DS-e8f05562-ed66-469b-9011-22294f80b056,DISK], DatanodeInfoWithStorage[127.0.0.1:33071,DS-ea0ea149-94e8-4608-859f-997d6a56c120,DISK], DatanodeInfoWithStorage[127.0.0.1:35291,DS-0f0ecfce-a1f1-4476-9db7-8ea72e4b29fc,DISK], DatanodeInfoWithStorage[127.0.0.1:39706,DS-460d42b4-4fbb-415a-a7ab-fb82ef04cf3d,DISK], DatanodeInfoWithStorage[127.0.0.1:32960,DS-85672869-6c32-43e5-80ba-c0d0201ee791,DISK], DatanodeInfoWithStorage[127.0.0.1:33769,DS-ef79ee48-b8e5-4c28-88d8-c678065dbc65,DISK], DatanodeInfoWithStorage[127.0.0.1:33925,DS-f837d9fd-e79b-439b-8b01-72f68defd85b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1958347167-172.17.0.13-1598584743629:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43457,DS-8b431263-ee29-472b-86f7-1f48f8339d6e,DISK], DatanodeInfoWithStorage[127.0.0.1:35116,DS-e8f05562-ed66-469b-9011-22294f80b056,DISK], DatanodeInfoWithStorage[127.0.0.1:33071,DS-ea0ea149-94e8-4608-859f-997d6a56c120,DISK], DatanodeInfoWithStorage[127.0.0.1:35291,DS-0f0ecfce-a1f1-4476-9db7-8ea72e4b29fc,DISK], DatanodeInfoWithStorage[127.0.0.1:39706,DS-460d42b4-4fbb-415a-a7ab-fb82ef04cf3d,DISK], DatanodeInfoWithStorage[127.0.0.1:32960,DS-85672869-6c32-43e5-80ba-c0d0201ee791,DISK], DatanodeInfoWithStorage[127.0.0.1:33769,DS-ef79ee48-b8e5-4c28-88d8-c678065dbc65,DISK], DatanodeInfoWithStorage[127.0.0.1:33925,DS-f837d9fd-e79b-439b-8b01-72f68defd85b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1114904145-172.17.0.13-1598585390104:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39342,DS-7c1b600d-4721-4620-8d0f-1e5a0c3aadef,DISK], DatanodeInfoWithStorage[127.0.0.1:42549,DS-b1a9093a-a2cd-4eca-b73d-00d6b9c4df77,DISK], DatanodeInfoWithStorage[127.0.0.1:46062,DS-f98af24c-7a70-4bed-9306-a6def44d69ec,DISK], DatanodeInfoWithStorage[127.0.0.1:37662,DS-965bd914-faca-4b9d-a3ae-619369354f93,DISK], DatanodeInfoWithStorage[127.0.0.1:42890,DS-012447b8-9676-42f2-8cbc-09984ac8c263,DISK], DatanodeInfoWithStorage[127.0.0.1:45555,DS-ad349fa9-8801-4096-9c00-0a2fb5018d50,DISK], DatanodeInfoWithStorage[127.0.0.1:39281,DS-c9439abf-ebfb-45fa-a674-e3ec52e69a48,DISK], DatanodeInfoWithStorage[127.0.0.1:33247,DS-a78f787a-ce5c-4b0b-8503-895735092c58,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1114904145-172.17.0.13-1598585390104:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39342,DS-7c1b600d-4721-4620-8d0f-1e5a0c3aadef,DISK], DatanodeInfoWithStorage[127.0.0.1:42549,DS-b1a9093a-a2cd-4eca-b73d-00d6b9c4df77,DISK], DatanodeInfoWithStorage[127.0.0.1:46062,DS-f98af24c-7a70-4bed-9306-a6def44d69ec,DISK], DatanodeInfoWithStorage[127.0.0.1:37662,DS-965bd914-faca-4b9d-a3ae-619369354f93,DISK], DatanodeInfoWithStorage[127.0.0.1:42890,DS-012447b8-9676-42f2-8cbc-09984ac8c263,DISK], DatanodeInfoWithStorage[127.0.0.1:45555,DS-ad349fa9-8801-4096-9c00-0a2fb5018d50,DISK], DatanodeInfoWithStorage[127.0.0.1:39281,DS-c9439abf-ebfb-45fa-a674-e3ec52e69a48,DISK], DatanodeInfoWithStorage[127.0.0.1:33247,DS-a78f787a-ce5c-4b0b-8503-895735092c58,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2123801584-172.17.0.13-1598585426274:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33422,DS-2c1fe590-b257-4f68-bcd0-5672b20b35cf,DISK], DatanodeInfoWithStorage[127.0.0.1:33521,DS-f7c0ffd6-175a-4032-b92d-f231afcc6148,DISK], DatanodeInfoWithStorage[127.0.0.1:44843,DS-4046e6d6-a6cd-417f-b715-12f78b5fdec4,DISK], DatanodeInfoWithStorage[127.0.0.1:41880,DS-939deb03-a33f-470e-a3d9-650b746da596,DISK], DatanodeInfoWithStorage[127.0.0.1:43570,DS-ea0e41ca-ef79-418e-9697-f569a49ba489,DISK], DatanodeInfoWithStorage[127.0.0.1:46777,DS-a1766d67-6a1a-4b41-bebb-44c87452a187,DISK], DatanodeInfoWithStorage[127.0.0.1:46723,DS-01e29fa2-98a0-42b8-afae-6e0fce11263f,DISK], DatanodeInfoWithStorage[127.0.0.1:38933,DS-fa44c212-bfe6-466a-8349-56aac6268531,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-2123801584-172.17.0.13-1598585426274:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33422,DS-2c1fe590-b257-4f68-bcd0-5672b20b35cf,DISK], DatanodeInfoWithStorage[127.0.0.1:33521,DS-f7c0ffd6-175a-4032-b92d-f231afcc6148,DISK], DatanodeInfoWithStorage[127.0.0.1:44843,DS-4046e6d6-a6cd-417f-b715-12f78b5fdec4,DISK], DatanodeInfoWithStorage[127.0.0.1:41880,DS-939deb03-a33f-470e-a3d9-650b746da596,DISK], DatanodeInfoWithStorage[127.0.0.1:43570,DS-ea0e41ca-ef79-418e-9697-f569a49ba489,DISK], DatanodeInfoWithStorage[127.0.0.1:46777,DS-a1766d67-6a1a-4b41-bebb-44c87452a187,DISK], DatanodeInfoWithStorage[127.0.0.1:46723,DS-01e29fa2-98a0-42b8-afae-6e0fce11263f,DISK], DatanodeInfoWithStorage[127.0.0.1:38933,DS-fa44c212-bfe6-466a-8349-56aac6268531,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1886301684-172.17.0.13-1598585736935:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37455,DS-360a5d03-e6b1-46df-ace9-923c9d743cd7,DISK], DatanodeInfoWithStorage[127.0.0.1:45047,DS-aad0f3b0-bd11-42cc-8bac-91d85eaaeff2,DISK], DatanodeInfoWithStorage[127.0.0.1:37971,DS-cb48cff2-b311-42ef-be7c-b1939981d184,DISK], DatanodeInfoWithStorage[127.0.0.1:33827,DS-1270b97a-7953-4087-bc7f-15b95d739d79,DISK], DatanodeInfoWithStorage[127.0.0.1:36033,DS-25b63067-abc2-41f3-b5ac-af3767ef97e4,DISK], DatanodeInfoWithStorage[127.0.0.1:43136,DS-32ecf8a1-93a6-4f00-87b0-95f80025b43d,DISK], DatanodeInfoWithStorage[127.0.0.1:41229,DS-4de832d6-7dfb-4ee9-834b-be09497e0928,DISK], DatanodeInfoWithStorage[127.0.0.1:40234,DS-0528a99d-ae9f-451e-8148-8d266e689ddb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1886301684-172.17.0.13-1598585736935:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:37455,DS-360a5d03-e6b1-46df-ace9-923c9d743cd7,DISK], DatanodeInfoWithStorage[127.0.0.1:45047,DS-aad0f3b0-bd11-42cc-8bac-91d85eaaeff2,DISK], DatanodeInfoWithStorage[127.0.0.1:37971,DS-cb48cff2-b311-42ef-be7c-b1939981d184,DISK], DatanodeInfoWithStorage[127.0.0.1:33827,DS-1270b97a-7953-4087-bc7f-15b95d739d79,DISK], DatanodeInfoWithStorage[127.0.0.1:36033,DS-25b63067-abc2-41f3-b5ac-af3767ef97e4,DISK], DatanodeInfoWithStorage[127.0.0.1:43136,DS-32ecf8a1-93a6-4f00-87b0-95f80025b43d,DISK], DatanodeInfoWithStorage[127.0.0.1:41229,DS-4de832d6-7dfb-4ee9-834b-be09497e0928,DISK], DatanodeInfoWithStorage[127.0.0.1:40234,DS-0528a99d-ae9f-451e-8148-8d266e689ddb,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1448652918-172.17.0.13-1598585814319:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35923,DS-7f1d85ac-e178-4ef6-9b65-adc56548fe33,DISK], DatanodeInfoWithStorage[127.0.0.1:38271,DS-869dac91-970d-4333-941d-4b75bb87d9a2,DISK], DatanodeInfoWithStorage[127.0.0.1:41042,DS-de5cf43d-9207-485c-bee0-cf3a1faa5c5d,DISK], DatanodeInfoWithStorage[127.0.0.1:34919,DS-be621e22-41f2-4e77-8b53-53503cd95e71,DISK], DatanodeInfoWithStorage[127.0.0.1:40503,DS-f75834e1-731a-4554-9bcf-d31a9d624b3d,DISK], DatanodeInfoWithStorage[127.0.0.1:46463,DS-06706587-b133-4c0a-8c52-b88fdce27f19,DISK], DatanodeInfoWithStorage[127.0.0.1:45938,DS-73d3e290-d357-439b-9483-6ce9ed556fb2,DISK], DatanodeInfoWithStorage[127.0.0.1:37468,DS-62dce598-d0b3-4962-b1c0-3f961f7c709e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1448652918-172.17.0.13-1598585814319:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35923,DS-7f1d85ac-e178-4ef6-9b65-adc56548fe33,DISK], DatanodeInfoWithStorage[127.0.0.1:38271,DS-869dac91-970d-4333-941d-4b75bb87d9a2,DISK], DatanodeInfoWithStorage[127.0.0.1:41042,DS-de5cf43d-9207-485c-bee0-cf3a1faa5c5d,DISK], DatanodeInfoWithStorage[127.0.0.1:34919,DS-be621e22-41f2-4e77-8b53-53503cd95e71,DISK], DatanodeInfoWithStorage[127.0.0.1:40503,DS-f75834e1-731a-4554-9bcf-d31a9d624b3d,DISK], DatanodeInfoWithStorage[127.0.0.1:46463,DS-06706587-b133-4c0a-8c52-b88fdce27f19,DISK], DatanodeInfoWithStorage[127.0.0.1:45938,DS-73d3e290-d357-439b-9483-6ce9ed556fb2,DISK], DatanodeInfoWithStorage[127.0.0.1:37468,DS-62dce598-d0b3-4962-b1c0-3f961f7c709e,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-698421828-172.17.0.13-1598586085187:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38530,DS-bafb599c-bc78-4ead-8656-dd7825b5ee6c,DISK], DatanodeInfoWithStorage[127.0.0.1:41291,DS-aece5ab3-2542-4b12-930c-9d140de9b673,DISK], DatanodeInfoWithStorage[127.0.0.1:43625,DS-6af7beba-eddf-43d8-89e4-fcee930b3b56,DISK], DatanodeInfoWithStorage[127.0.0.1:44731,DS-58e37a5f-1905-4b8b-8154-3873ab5361fe,DISK], DatanodeInfoWithStorage[127.0.0.1:42204,DS-b59fcebd-e33e-4ae0-8c27-a4fa4a963dfb,DISK], DatanodeInfoWithStorage[127.0.0.1:38477,DS-bf4df92b-cf3f-4e43-8051-3084dbea92dd,DISK], DatanodeInfoWithStorage[127.0.0.1:33356,DS-737acf50-6c65-4681-bf51-d2c2187b5abf,DISK], DatanodeInfoWithStorage[127.0.0.1:37856,DS-6c44e359-8ac2-442b-a29c-fdc30f846c33,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-698421828-172.17.0.13-1598586085187:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38530,DS-bafb599c-bc78-4ead-8656-dd7825b5ee6c,DISK], DatanodeInfoWithStorage[127.0.0.1:41291,DS-aece5ab3-2542-4b12-930c-9d140de9b673,DISK], DatanodeInfoWithStorage[127.0.0.1:43625,DS-6af7beba-eddf-43d8-89e4-fcee930b3b56,DISK], DatanodeInfoWithStorage[127.0.0.1:44731,DS-58e37a5f-1905-4b8b-8154-3873ab5361fe,DISK], DatanodeInfoWithStorage[127.0.0.1:42204,DS-b59fcebd-e33e-4ae0-8c27-a4fa4a963dfb,DISK], DatanodeInfoWithStorage[127.0.0.1:38477,DS-bf4df92b-cf3f-4e43-8051-3084dbea92dd,DISK], DatanodeInfoWithStorage[127.0.0.1:33356,DS-737acf50-6c65-4681-bf51-d2c2187b5abf,DISK], DatanodeInfoWithStorage[127.0.0.1:37856,DS-6c44e359-8ac2-442b-a29c-fdc30f846c33,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-213331338-172.17.0.13-1598586165357:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46144,DS-5dba3a40-a41a-4d2e-80a4-14a3664cedc7,DISK], DatanodeInfoWithStorage[127.0.0.1:45464,DS-e3f77666-cbf3-4c02-b599-70f5b581969a,DISK], DatanodeInfoWithStorage[127.0.0.1:33901,DS-8f624902-1181-4634-9b6e-dfe77a4267e9,DISK], DatanodeInfoWithStorage[127.0.0.1:33284,DS-7cddf389-e613-43a2-8a9d-ae539ea103eb,DISK], DatanodeInfoWithStorage[127.0.0.1:44754,DS-7f9c1d9c-b23b-4d14-982a-9b5898281234,DISK], DatanodeInfoWithStorage[127.0.0.1:45704,DS-562f48c0-c9fb-4c4c-8e20-bdf2ba44c63c,DISK], DatanodeInfoWithStorage[127.0.0.1:37991,DS-2d16dae5-33b1-4cdb-b0ff-19ddee28c1b8,DISK], DatanodeInfoWithStorage[127.0.0.1:34772,DS-8cfab688-4ce0-4893-8ec1-737ef64d49f5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-213331338-172.17.0.13-1598586165357:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46144,DS-5dba3a40-a41a-4d2e-80a4-14a3664cedc7,DISK], DatanodeInfoWithStorage[127.0.0.1:45464,DS-e3f77666-cbf3-4c02-b599-70f5b581969a,DISK], DatanodeInfoWithStorage[127.0.0.1:33901,DS-8f624902-1181-4634-9b6e-dfe77a4267e9,DISK], DatanodeInfoWithStorage[127.0.0.1:33284,DS-7cddf389-e613-43a2-8a9d-ae539ea103eb,DISK], DatanodeInfoWithStorage[127.0.0.1:44754,DS-7f9c1d9c-b23b-4d14-982a-9b5898281234,DISK], DatanodeInfoWithStorage[127.0.0.1:45704,DS-562f48c0-c9fb-4c4c-8e20-bdf2ba44c63c,DISK], DatanodeInfoWithStorage[127.0.0.1:37991,DS-2d16dae5-33b1-4cdb-b0ff-19ddee28c1b8,DISK], DatanodeInfoWithStorage[127.0.0.1:34772,DS-8cfab688-4ce0-4893-8ec1-737ef64d49f5,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1513629566-172.17.0.13-1598586331048:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34189,DS-5650752c-e35b-47a9-872f-1a994a2e49f3,DISK], DatanodeInfoWithStorage[127.0.0.1:39357,DS-26b1ca47-3baa-456c-8572-1fe5b7974ea4,DISK], DatanodeInfoWithStorage[127.0.0.1:40328,DS-ec467aa7-2224-4d0b-a29d-1c1626eb6207,DISK], DatanodeInfoWithStorage[127.0.0.1:43663,DS-82c0431c-4422-4fe5-b85d-3e8513e75f4f,DISK], DatanodeInfoWithStorage[127.0.0.1:39519,DS-b38d76cb-9e8f-4b9d-9f66-3933cef66b84,DISK], DatanodeInfoWithStorage[127.0.0.1:36525,DS-55a43eec-dc4c-4a11-8b96-5fe805042906,DISK], DatanodeInfoWithStorage[127.0.0.1:34817,DS-dfd70355-5e33-438c-ac20-e75a677bf1f3,DISK], DatanodeInfoWithStorage[127.0.0.1:40467,DS-530f789e-8dc7-41e4-80aa-e962f86c971d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1513629566-172.17.0.13-1598586331048:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:34189,DS-5650752c-e35b-47a9-872f-1a994a2e49f3,DISK], DatanodeInfoWithStorage[127.0.0.1:39357,DS-26b1ca47-3baa-456c-8572-1fe5b7974ea4,DISK], DatanodeInfoWithStorage[127.0.0.1:40328,DS-ec467aa7-2224-4d0b-a29d-1c1626eb6207,DISK], DatanodeInfoWithStorage[127.0.0.1:43663,DS-82c0431c-4422-4fe5-b85d-3e8513e75f4f,DISK], DatanodeInfoWithStorage[127.0.0.1:39519,DS-b38d76cb-9e8f-4b9d-9f66-3933cef66b84,DISK], DatanodeInfoWithStorage[127.0.0.1:36525,DS-55a43eec-dc4c-4a11-8b96-5fe805042906,DISK], DatanodeInfoWithStorage[127.0.0.1:34817,DS-dfd70355-5e33-438c-ac20-e75a677bf1f3,DISK], DatanodeInfoWithStorage[127.0.0.1:40467,DS-530f789e-8dc7-41e4-80aa-e962f86c971d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1673343711-172.17.0.13-1598586907640:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35579,DS-a788bc8e-8dde-43f6-b32e-efd2606a3ec6,DISK], DatanodeInfoWithStorage[127.0.0.1:41656,DS-0dc7832c-b54a-4d2b-822a-6a99e4316b38,DISK], DatanodeInfoWithStorage[127.0.0.1:44162,DS-fe73f0fa-ca44-4862-a5e1-646ffe894bbb,DISK], DatanodeInfoWithStorage[127.0.0.1:35259,DS-4db6b93c-bd6b-4426-80ae-7a7a726ec145,DISK], DatanodeInfoWithStorage[127.0.0.1:45434,DS-c13ae1e0-7a18-4a7d-af11-7473a04d4400,DISK], DatanodeInfoWithStorage[127.0.0.1:39476,DS-be9eab9b-a851-4883-8dda-cd470af9a870,DISK], DatanodeInfoWithStorage[127.0.0.1:37319,DS-0d97786e-c26e-454c-b5a9-4b92577030b7,DISK], DatanodeInfoWithStorage[127.0.0.1:42336,DS-ff6d2c01-ff59-415b-9507-6d4f2b7d7a65,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1673343711-172.17.0.13-1598586907640:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35579,DS-a788bc8e-8dde-43f6-b32e-efd2606a3ec6,DISK], DatanodeInfoWithStorage[127.0.0.1:41656,DS-0dc7832c-b54a-4d2b-822a-6a99e4316b38,DISK], DatanodeInfoWithStorage[127.0.0.1:44162,DS-fe73f0fa-ca44-4862-a5e1-646ffe894bbb,DISK], DatanodeInfoWithStorage[127.0.0.1:35259,DS-4db6b93c-bd6b-4426-80ae-7a7a726ec145,DISK], DatanodeInfoWithStorage[127.0.0.1:45434,DS-c13ae1e0-7a18-4a7d-af11-7473a04d4400,DISK], DatanodeInfoWithStorage[127.0.0.1:39476,DS-be9eab9b-a851-4883-8dda-cd470af9a870,DISK], DatanodeInfoWithStorage[127.0.0.1:37319,DS-0d97786e-c26e-454c-b5a9-4b92577030b7,DISK], DatanodeInfoWithStorage[127.0.0.1:42336,DS-ff6d2c01-ff59-415b-9507-6d4f2b7d7a65,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-848160217-172.17.0.13-1598586997830:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41251,DS-a7dcb98e-979f-45f8-bc42-892f82fd68b7,DISK], DatanodeInfoWithStorage[127.0.0.1:44700,DS-fe27437f-d81e-44ca-a8ae-c0bce9918cb6,DISK], DatanodeInfoWithStorage[127.0.0.1:39590,DS-432504f1-6f37-46ae-bf22-d8e4fd4d5bfb,DISK], DatanodeInfoWithStorage[127.0.0.1:43641,DS-04cd011a-a0c7-45d9-8988-08430c7d896e,DISK], DatanodeInfoWithStorage[127.0.0.1:43234,DS-2e1b266d-248f-447c-a4dd-afc72175486d,DISK], DatanodeInfoWithStorage[127.0.0.1:43540,DS-baf10992-993b-4073-b619-87e480d6645d,DISK], DatanodeInfoWithStorage[127.0.0.1:36513,DS-a6732227-fe14-45e2-bd53-eed6bae12f4e,DISK], DatanodeInfoWithStorage[127.0.0.1:34410,DS-b3936c43-341b-444f-9c3d-225538662e72,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-848160217-172.17.0.13-1598586997830:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41251,DS-a7dcb98e-979f-45f8-bc42-892f82fd68b7,DISK], DatanodeInfoWithStorage[127.0.0.1:44700,DS-fe27437f-d81e-44ca-a8ae-c0bce9918cb6,DISK], DatanodeInfoWithStorage[127.0.0.1:39590,DS-432504f1-6f37-46ae-bf22-d8e4fd4d5bfb,DISK], DatanodeInfoWithStorage[127.0.0.1:43641,DS-04cd011a-a0c7-45d9-8988-08430c7d896e,DISK], DatanodeInfoWithStorage[127.0.0.1:43234,DS-2e1b266d-248f-447c-a4dd-afc72175486d,DISK], DatanodeInfoWithStorage[127.0.0.1:43540,DS-baf10992-993b-4073-b619-87e480d6645d,DISK], DatanodeInfoWithStorage[127.0.0.1:36513,DS-a6732227-fe14-45e2-bd53-eed6bae12f4e,DISK], DatanodeInfoWithStorage[127.0.0.1:34410,DS-b3936c43-341b-444f-9c3d-225538662e72,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1243066109-172.17.0.13-1598587396820:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42439,DS-d5112bad-8dbc-4e46-880b-c00f727b7f75,DISK], DatanodeInfoWithStorage[127.0.0.1:36660,DS-d25cc5dd-1a6d-4361-b2ee-7b2c5a5413e4,DISK], DatanodeInfoWithStorage[127.0.0.1:45981,DS-91c6988f-382f-4905-a14b-534da46e9d24,DISK], DatanodeInfoWithStorage[127.0.0.1:38271,DS-731a2689-c01d-4b6b-b946-c6056923e34e,DISK], DatanodeInfoWithStorage[127.0.0.1:35392,DS-da0a8d7e-d78a-4e28-b317-59e9194509d1,DISK], DatanodeInfoWithStorage[127.0.0.1:38669,DS-4173fe73-d11b-4572-ad01-2b70f63381f5,DISK], DatanodeInfoWithStorage[127.0.0.1:36700,DS-fe15cf48-4f36-4168-82af-1224b6d1fe1a,DISK], DatanodeInfoWithStorage[127.0.0.1:39612,DS-0969c469-7f3d-4731-825c-81adb5b46b8d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1243066109-172.17.0.13-1598587396820:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42439,DS-d5112bad-8dbc-4e46-880b-c00f727b7f75,DISK], DatanodeInfoWithStorage[127.0.0.1:36660,DS-d25cc5dd-1a6d-4361-b2ee-7b2c5a5413e4,DISK], DatanodeInfoWithStorage[127.0.0.1:45981,DS-91c6988f-382f-4905-a14b-534da46e9d24,DISK], DatanodeInfoWithStorage[127.0.0.1:38271,DS-731a2689-c01d-4b6b-b946-c6056923e34e,DISK], DatanodeInfoWithStorage[127.0.0.1:35392,DS-da0a8d7e-d78a-4e28-b317-59e9194509d1,DISK], DatanodeInfoWithStorage[127.0.0.1:38669,DS-4173fe73-d11b-4572-ad01-2b70f63381f5,DISK], DatanodeInfoWithStorage[127.0.0.1:36700,DS-fe15cf48-4f36-4168-82af-1224b6d1fe1a,DISK], DatanodeInfoWithStorage[127.0.0.1:39612,DS-0969c469-7f3d-4731-825c-81adb5b46b8d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1392485750-172.17.0.13-1598587807903:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42916,DS-b9254e08-6b80-40b4-b9f1-e75f9699c363,DISK], DatanodeInfoWithStorage[127.0.0.1:33293,DS-3eec84ec-4bd8-4cd4-86b9-d88ccc0175b0,DISK], DatanodeInfoWithStorage[127.0.0.1:41796,DS-ea9b10fb-0f96-4ce0-bc76-7804dba43e80,DISK], DatanodeInfoWithStorage[127.0.0.1:34458,DS-0edcffc8-082c-445a-922b-67139e0db726,DISK], DatanodeInfoWithStorage[127.0.0.1:42168,DS-61af6a5c-9d1d-41d6-8a6e-8e889152a2ab,DISK], DatanodeInfoWithStorage[127.0.0.1:41397,DS-a7afac3a-7096-4bc1-803f-d1d9bd799b3e,DISK], DatanodeInfoWithStorage[127.0.0.1:38264,DS-7f35eb89-2681-4653-8530-bfff6c2a5211,DISK], DatanodeInfoWithStorage[127.0.0.1:46189,DS-d2804c47-0aa7-41e6-80f7-114877c533b3,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1392485750-172.17.0.13-1598587807903:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42916,DS-b9254e08-6b80-40b4-b9f1-e75f9699c363,DISK], DatanodeInfoWithStorage[127.0.0.1:33293,DS-3eec84ec-4bd8-4cd4-86b9-d88ccc0175b0,DISK], DatanodeInfoWithStorage[127.0.0.1:41796,DS-ea9b10fb-0f96-4ce0-bc76-7804dba43e80,DISK], DatanodeInfoWithStorage[127.0.0.1:34458,DS-0edcffc8-082c-445a-922b-67139e0db726,DISK], DatanodeInfoWithStorage[127.0.0.1:42168,DS-61af6a5c-9d1d-41d6-8a6e-8e889152a2ab,DISK], DatanodeInfoWithStorage[127.0.0.1:41397,DS-a7afac3a-7096-4bc1-803f-d1d9bd799b3e,DISK], DatanodeInfoWithStorage[127.0.0.1:38264,DS-7f35eb89-2681-4653-8530-bfff6c2a5211,DISK], DatanodeInfoWithStorage[127.0.0.1:46189,DS-d2804c47-0aa7-41e6-80f7-114877c533b3,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1639763973-172.17.0.13-1598587838675:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38168,DS-32db54aa-c485-44f8-b853-81ecfe4fd047,DISK], DatanodeInfoWithStorage[127.0.0.1:39966,DS-3f77c6a4-4f1e-4d5f-accc-5e23984cf86d,DISK], DatanodeInfoWithStorage[127.0.0.1:37899,DS-232b7bf8-96b9-4747-9cf8-f2df6051d56b,DISK], DatanodeInfoWithStorage[127.0.0.1:45594,DS-27bed72e-4d3a-429f-90f0-5c70043e4c98,DISK], DatanodeInfoWithStorage[127.0.0.1:39429,DS-ca5670b5-0f0e-4670-84ff-752a0561b7e9,DISK], DatanodeInfoWithStorage[127.0.0.1:33840,DS-ec60559e-9d6d-4a41-968e-00f6595c77ca,DISK], DatanodeInfoWithStorage[127.0.0.1:35795,DS-c04649bb-c14e-4baf-904a-f8e717418fab,DISK], DatanodeInfoWithStorage[127.0.0.1:45229,DS-b73def2a-2506-4d80-97b2-008a4cf0976b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1639763973-172.17.0.13-1598587838675:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38168,DS-32db54aa-c485-44f8-b853-81ecfe4fd047,DISK], DatanodeInfoWithStorage[127.0.0.1:39966,DS-3f77c6a4-4f1e-4d5f-accc-5e23984cf86d,DISK], DatanodeInfoWithStorage[127.0.0.1:37899,DS-232b7bf8-96b9-4747-9cf8-f2df6051d56b,DISK], DatanodeInfoWithStorage[127.0.0.1:45594,DS-27bed72e-4d3a-429f-90f0-5c70043e4c98,DISK], DatanodeInfoWithStorage[127.0.0.1:39429,DS-ca5670b5-0f0e-4670-84ff-752a0561b7e9,DISK], DatanodeInfoWithStorage[127.0.0.1:33840,DS-ec60559e-9d6d-4a41-968e-00f6595c77ca,DISK], DatanodeInfoWithStorage[127.0.0.1:35795,DS-c04649bb-c14e-4baf-904a-f8e717418fab,DISK], DatanodeInfoWithStorage[127.0.0.1:45229,DS-b73def2a-2506-4d80-97b2-008a4cf0976b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1450541472-172.17.0.13-1598587866595:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35344,DS-c85ae100-a8e1-4c24-acc4-83a6b83419b8,DISK], DatanodeInfoWithStorage[127.0.0.1:35544,DS-2c028494-d16f-4da8-81ac-13b727afb513,DISK], DatanodeInfoWithStorage[127.0.0.1:38820,DS-6a37e964-2d65-43eb-8857-54e8fb0426f1,DISK], DatanodeInfoWithStorage[127.0.0.1:39792,DS-c20418fe-df11-4874-b186-1535d9c8a94b,DISK], DatanodeInfoWithStorage[127.0.0.1:40695,DS-7f7856eb-9393-4add-b232-78a57c7ad5bc,DISK], DatanodeInfoWithStorage[127.0.0.1:38641,DS-000c4a63-f7e2-444e-b943-3aa6f8161270,DISK], DatanodeInfoWithStorage[127.0.0.1:40610,DS-0fb4efe0-08f3-4677-b34d-8a84d6ce48d9,DISK], DatanodeInfoWithStorage[127.0.0.1:33503,DS-134b248a-d4b5-456f-bada-e732cc1097a0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1450541472-172.17.0.13-1598587866595:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:35344,DS-c85ae100-a8e1-4c24-acc4-83a6b83419b8,DISK], DatanodeInfoWithStorage[127.0.0.1:35544,DS-2c028494-d16f-4da8-81ac-13b727afb513,DISK], DatanodeInfoWithStorage[127.0.0.1:38820,DS-6a37e964-2d65-43eb-8857-54e8fb0426f1,DISK], DatanodeInfoWithStorage[127.0.0.1:39792,DS-c20418fe-df11-4874-b186-1535d9c8a94b,DISK], DatanodeInfoWithStorage[127.0.0.1:40695,DS-7f7856eb-9393-4add-b232-78a57c7ad5bc,DISK], DatanodeInfoWithStorage[127.0.0.1:38641,DS-000c4a63-f7e2-444e-b943-3aa6f8161270,DISK], DatanodeInfoWithStorage[127.0.0.1:40610,DS-0fb4efe0-08f3-4677-b34d-8a84d6ce48d9,DISK], DatanodeInfoWithStorage[127.0.0.1:33503,DS-134b248a-d4b5-456f-bada-e732cc1097a0,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1171713067-172.17.0.13-1598588074967:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36138,DS-764644c8-8c7d-4d62-aeef-f3520619ae90,DISK], DatanodeInfoWithStorage[127.0.0.1:33408,DS-cf280d44-3fed-4f4d-a451-cd17e0119ad5,DISK], DatanodeInfoWithStorage[127.0.0.1:43323,DS-e81e6f2a-6b9a-47d4-9923-f6b711cc3f97,DISK], DatanodeInfoWithStorage[127.0.0.1:34177,DS-d1efbf84-aae0-456c-8f30-c6d7d7dc0383,DISK], DatanodeInfoWithStorage[127.0.0.1:44679,DS-3e0b7658-deee-4d41-bcdb-0138eb1131b9,DISK], DatanodeInfoWithStorage[127.0.0.1:39757,DS-a241f48f-362d-4701-a45a-2ec9b231dac8,DISK], DatanodeInfoWithStorage[127.0.0.1:41176,DS-74744a91-8477-42c2-a5be-d088fa23bd7b,DISK], DatanodeInfoWithStorage[127.0.0.1:41567,DS-1d1a4f9a-19c4-4c79-a22c-1e74b17338e6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1171713067-172.17.0.13-1598588074967:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36138,DS-764644c8-8c7d-4d62-aeef-f3520619ae90,DISK], DatanodeInfoWithStorage[127.0.0.1:33408,DS-cf280d44-3fed-4f4d-a451-cd17e0119ad5,DISK], DatanodeInfoWithStorage[127.0.0.1:43323,DS-e81e6f2a-6b9a-47d4-9923-f6b711cc3f97,DISK], DatanodeInfoWithStorage[127.0.0.1:34177,DS-d1efbf84-aae0-456c-8f30-c6d7d7dc0383,DISK], DatanodeInfoWithStorage[127.0.0.1:44679,DS-3e0b7658-deee-4d41-bcdb-0138eb1131b9,DISK], DatanodeInfoWithStorage[127.0.0.1:39757,DS-a241f48f-362d-4701-a45a-2ec9b231dac8,DISK], DatanodeInfoWithStorage[127.0.0.1:41176,DS-74744a91-8477-42c2-a5be-d088fa23bd7b,DISK], DatanodeInfoWithStorage[127.0.0.1:41567,DS-1d1a4f9a-19c4-4c79-a22c-1e74b17338e6,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-556567184-172.17.0.13-1598588290012:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44034,DS-2e380240-b2da-4d00-a514-94ea10425e28,DISK], DatanodeInfoWithStorage[127.0.0.1:43305,DS-51a9832f-a8bb-44eb-acfa-4befc58adf68,DISK], DatanodeInfoWithStorage[127.0.0.1:43442,DS-a630c0b3-0a5e-4ac6-a62f-807897dcbc82,DISK], DatanodeInfoWithStorage[127.0.0.1:36757,DS-3160a84f-fcd2-40d0-b213-410eb9535fda,DISK], DatanodeInfoWithStorage[127.0.0.1:43601,DS-cb7dc06a-22e2-452d-b59d-e17aca80ff7d,DISK], DatanodeInfoWithStorage[127.0.0.1:42023,DS-f6fb5866-a027-48f4-8a3a-476fe480c23a,DISK], DatanodeInfoWithStorage[127.0.0.1:34466,DS-8daf5e63-a503-4558-b6c0-85b861d2f2b9,DISK], DatanodeInfoWithStorage[127.0.0.1:45244,DS-5f4a6658-5611-4549-826d-f1a6f25f2e4b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-556567184-172.17.0.13-1598588290012:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44034,DS-2e380240-b2da-4d00-a514-94ea10425e28,DISK], DatanodeInfoWithStorage[127.0.0.1:43305,DS-51a9832f-a8bb-44eb-acfa-4befc58adf68,DISK], DatanodeInfoWithStorage[127.0.0.1:43442,DS-a630c0b3-0a5e-4ac6-a62f-807897dcbc82,DISK], DatanodeInfoWithStorage[127.0.0.1:36757,DS-3160a84f-fcd2-40d0-b213-410eb9535fda,DISK], DatanodeInfoWithStorage[127.0.0.1:43601,DS-cb7dc06a-22e2-452d-b59d-e17aca80ff7d,DISK], DatanodeInfoWithStorage[127.0.0.1:42023,DS-f6fb5866-a027-48f4-8a3a-476fe480c23a,DISK], DatanodeInfoWithStorage[127.0.0.1:34466,DS-8daf5e63-a503-4558-b6c0-85b861d2f2b9,DISK], DatanodeInfoWithStorage[127.0.0.1:45244,DS-5f4a6658-5611-4549-826d-f1a6f25f2e4b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-873195696-172.17.0.13-1598588913832:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41366,DS-205142ae-e6e5-4f82-a584-80a97f586917,DISK], DatanodeInfoWithStorage[127.0.0.1:39547,DS-9004cb18-41aa-45a8-8efc-a00e20e3228c,DISK], DatanodeInfoWithStorage[127.0.0.1:35772,DS-2a316ef3-06b7-4a70-a7f3-9f185f93f9d8,DISK], DatanodeInfoWithStorage[127.0.0.1:33226,DS-232bc3ee-db51-4ada-8b62-e6aaa7ccbe3f,DISK], DatanodeInfoWithStorage[127.0.0.1:34215,DS-74e842d7-aa0a-426c-b141-cad076d0cfdc,DISK], DatanodeInfoWithStorage[127.0.0.1:43695,DS-61e83c45-a157-4c8d-afbc-2bd4430a7482,DISK], DatanodeInfoWithStorage[127.0.0.1:45830,DS-ffdd4686-a784-4a64-a3d0-f59d615a473e,DISK], DatanodeInfoWithStorage[127.0.0.1:35455,DS-a6187a58-1a96-4f53-a901-1900f20ab370,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-873195696-172.17.0.13-1598588913832:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41366,DS-205142ae-e6e5-4f82-a584-80a97f586917,DISK], DatanodeInfoWithStorage[127.0.0.1:39547,DS-9004cb18-41aa-45a8-8efc-a00e20e3228c,DISK], DatanodeInfoWithStorage[127.0.0.1:35772,DS-2a316ef3-06b7-4a70-a7f3-9f185f93f9d8,DISK], DatanodeInfoWithStorage[127.0.0.1:33226,DS-232bc3ee-db51-4ada-8b62-e6aaa7ccbe3f,DISK], DatanodeInfoWithStorage[127.0.0.1:34215,DS-74e842d7-aa0a-426c-b141-cad076d0cfdc,DISK], DatanodeInfoWithStorage[127.0.0.1:43695,DS-61e83c45-a157-4c8d-afbc-2bd4430a7482,DISK], DatanodeInfoWithStorage[127.0.0.1:45830,DS-ffdd4686-a784-4a64-a3d0-f59d615a473e,DISK], DatanodeInfoWithStorage[127.0.0.1:35455,DS-a6187a58-1a96-4f53-a901-1900f20ab370,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-367022259-172.17.0.13-1598589060838:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33411,DS-3dbfaa04-4862-4435-a304-a8569539fa12,DISK], DatanodeInfoWithStorage[127.0.0.1:41750,DS-9f2fd24e-e235-4fd3-956c-a5fd86d47a3d,DISK], DatanodeInfoWithStorage[127.0.0.1:41216,DS-fbe4a2aa-ee4c-4d82-b3b6-2a56b62d8fcc,DISK], DatanodeInfoWithStorage[127.0.0.1:41837,DS-67144f2e-e95d-4c15-a863-300b0129f6b2,DISK], DatanodeInfoWithStorage[127.0.0.1:43331,DS-d09104b7-b43a-4ea1-b48a-20c073fdb6e6,DISK], DatanodeInfoWithStorage[127.0.0.1:43356,DS-30f17bbc-ca15-4bd5-8129-3494ba756f01,DISK], DatanodeInfoWithStorage[127.0.0.1:45659,DS-83e7c102-4958-40d0-aa94-2857b937d9f0,DISK], DatanodeInfoWithStorage[127.0.0.1:37361,DS-e5f05551-86ea-4606-8f0c-3edbcf17c086,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-367022259-172.17.0.13-1598589060838:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:33411,DS-3dbfaa04-4862-4435-a304-a8569539fa12,DISK], DatanodeInfoWithStorage[127.0.0.1:41750,DS-9f2fd24e-e235-4fd3-956c-a5fd86d47a3d,DISK], DatanodeInfoWithStorage[127.0.0.1:41216,DS-fbe4a2aa-ee4c-4d82-b3b6-2a56b62d8fcc,DISK], DatanodeInfoWithStorage[127.0.0.1:41837,DS-67144f2e-e95d-4c15-a863-300b0129f6b2,DISK], DatanodeInfoWithStorage[127.0.0.1:43331,DS-d09104b7-b43a-4ea1-b48a-20c073fdb6e6,DISK], DatanodeInfoWithStorage[127.0.0.1:43356,DS-30f17bbc-ca15-4bd5-8129-3494ba756f01,DISK], DatanodeInfoWithStorage[127.0.0.1:45659,DS-83e7c102-4958-40d0-aa94-2857b937d9f0,DISK], DatanodeInfoWithStorage[127.0.0.1:37361,DS-e5f05551-86ea-4606-8f0c-3edbcf17c086,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1992915122-172.17.0.13-1598589256516:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38122,DS-04a5aea6-5136-4b92-af72-b75aec9f019e,DISK], DatanodeInfoWithStorage[127.0.0.1:37153,DS-f6504580-998f-49b8-bcc2-66087ca6c01f,DISK], DatanodeInfoWithStorage[127.0.0.1:38794,DS-e629b8df-8b1f-4f70-bfff-3da17bb3526d,DISK], DatanodeInfoWithStorage[127.0.0.1:38502,DS-e415baf8-28e7-438d-85b0-383abd8d8742,DISK], DatanodeInfoWithStorage[127.0.0.1:38297,DS-99a8acaf-a03d-4f79-a694-c581363a8778,DISK], DatanodeInfoWithStorage[127.0.0.1:32878,DS-ca6b3122-b36e-4e1c-83b9-50afc22d27ef,DISK], DatanodeInfoWithStorage[127.0.0.1:36935,DS-dad163d2-c1f1-4f2a-8215-046737c08e63,DISK], DatanodeInfoWithStorage[127.0.0.1:44985,DS-1d9d469e-0fc5-4d81-b458-9b8f69be16a8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1992915122-172.17.0.13-1598589256516:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:38122,DS-04a5aea6-5136-4b92-af72-b75aec9f019e,DISK], DatanodeInfoWithStorage[127.0.0.1:37153,DS-f6504580-998f-49b8-bcc2-66087ca6c01f,DISK], DatanodeInfoWithStorage[127.0.0.1:38794,DS-e629b8df-8b1f-4f70-bfff-3da17bb3526d,DISK], DatanodeInfoWithStorage[127.0.0.1:38502,DS-e415baf8-28e7-438d-85b0-383abd8d8742,DISK], DatanodeInfoWithStorage[127.0.0.1:38297,DS-99a8acaf-a03d-4f79-a694-c581363a8778,DISK], DatanodeInfoWithStorage[127.0.0.1:32878,DS-ca6b3122-b36e-4e1c-83b9-50afc22d27ef,DISK], DatanodeInfoWithStorage[127.0.0.1:36935,DS-dad163d2-c1f1-4f2a-8215-046737c08e63,DISK], DatanodeInfoWithStorage[127.0.0.1:44985,DS-1d9d469e-0fc5-4d81-b458-9b8f69be16a8,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.datanode.network.counts.cache.max.size
component: hdfs:DataNode
v1: 2147483647
v2: 2047
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery1
reconfPoint: -3
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-657535721-172.17.0.13-1598589416254:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41671,DS-f94cb22e-734c-4f36-8fdc-c995ae3f56cd,DISK], DatanodeInfoWithStorage[127.0.0.1:44854,DS-c6dc3405-8347-4f48-85c3-ffa8cfddf5c2,DISK], DatanodeInfoWithStorage[127.0.0.1:38519,DS-d3c2d1b8-86f0-4bd6-93fe-87c9662f5089,DISK], DatanodeInfoWithStorage[127.0.0.1:36396,DS-e5f73407-df28-4ab4-9ac3-060e0751181a,DISK], DatanodeInfoWithStorage[127.0.0.1:34112,DS-d9c78328-2d11-4444-ac52-5e9e87f9863d,DISK], DatanodeInfoWithStorage[127.0.0.1:40626,DS-c6c56c1b-2048-4dd4-bc8c-07fdc304a73c,DISK], DatanodeInfoWithStorage[127.0.0.1:32792,DS-7a9ac44b-f492-495b-9ba7-7a8c45e7dc10,DISK], DatanodeInfoWithStorage[127.0.0.1:36073,DS-f0bf99e5-68a5-4e52-b576-abe3cf3e30fe,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-657535721-172.17.0.13-1598589416254:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:41671,DS-f94cb22e-734c-4f36-8fdc-c995ae3f56cd,DISK], DatanodeInfoWithStorage[127.0.0.1:44854,DS-c6dc3405-8347-4f48-85c3-ffa8cfddf5c2,DISK], DatanodeInfoWithStorage[127.0.0.1:38519,DS-d3c2d1b8-86f0-4bd6-93fe-87c9662f5089,DISK], DatanodeInfoWithStorage[127.0.0.1:36396,DS-e5f73407-df28-4ab4-9ac3-060e0751181a,DISK], DatanodeInfoWithStorage[127.0.0.1:34112,DS-d9c78328-2d11-4444-ac52-5e9e87f9863d,DISK], DatanodeInfoWithStorage[127.0.0.1:40626,DS-c6c56c1b-2048-4dd4-bc8c-07fdc304a73c,DISK], DatanodeInfoWithStorage[127.0.0.1:32792,DS-7a9ac44b-f492-495b-9ba7-7a8c45e7dc10,DISK], DatanodeInfoWithStorage[127.0.0.1:36073,DS-f0bf99e5-68a5-4e52-b576-abe3cf3e30fe,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery1(TestFileChecksum.java:312)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)


v1v2 failed with probability 8 out of 50
v1v1v2v2 failed with probability 12 out of 50
result: false positive !!!
Total execution time in seconds : 4991

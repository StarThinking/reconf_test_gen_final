reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1589810013-172.17.0.15-1598691304538:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40481,DS-736f1456-94c8-4100-b40d-be6f90611933,DISK], DatanodeInfoWithStorage[127.0.0.1:43113,DS-fc7ba9d8-b454-43e0-82e2-2b19c0e24eda,DISK], DatanodeInfoWithStorage[127.0.0.1:44772,DS-ab626b30-f736-4948-a1ba-3518ecf06b69,DISK], DatanodeInfoWithStorage[127.0.0.1:41055,DS-d215cf13-8df7-4f6c-87c9-9e0dab436ca9,DISK], DatanodeInfoWithStorage[127.0.0.1:38737,DS-27a5a171-a63f-4b25-b7f7-7624f3ee3a22,DISK], DatanodeInfoWithStorage[127.0.0.1:33708,DS-721af273-5ed7-4ead-ac82-7c7fcbb02088,DISK], DatanodeInfoWithStorage[127.0.0.1:39631,DS-5ce970d2-7a8c-4bb5-b9aa-23f2f47dae90,DISK], DatanodeInfoWithStorage[127.0.0.1:46446,DS-afce3421-6bc3-4648-8733-5550b9481b15,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1589810013-172.17.0.15-1598691304538:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40481,DS-736f1456-94c8-4100-b40d-be6f90611933,DISK], DatanodeInfoWithStorage[127.0.0.1:43113,DS-fc7ba9d8-b454-43e0-82e2-2b19c0e24eda,DISK], DatanodeInfoWithStorage[127.0.0.1:44772,DS-ab626b30-f736-4948-a1ba-3518ecf06b69,DISK], DatanodeInfoWithStorage[127.0.0.1:41055,DS-d215cf13-8df7-4f6c-87c9-9e0dab436ca9,DISK], DatanodeInfoWithStorage[127.0.0.1:38737,DS-27a5a171-a63f-4b25-b7f7-7624f3ee3a22,DISK], DatanodeInfoWithStorage[127.0.0.1:33708,DS-721af273-5ed7-4ead-ac82-7c7fcbb02088,DISK], DatanodeInfoWithStorage[127.0.0.1:39631,DS-5ce970d2-7a8c-4bb5-b9aa-23f2f47dae90,DISK], DatanodeInfoWithStorage[127.0.0.1:46446,DS-afce3421-6bc3-4648-8733-5550b9481b15,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-849976389-172.17.0.15-1598691479401:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32800,DS-ad7b1d47-fa3d-41f8-823f-bc6ab99c196b,DISK], DatanodeInfoWithStorage[127.0.0.1:38215,DS-325534bc-46ec-428b-b172-bd1b964058dd,DISK], DatanodeInfoWithStorage[127.0.0.1:41605,DS-c36d495d-5ca3-49bf-ba8c-e2ca1a26c71d,DISK], DatanodeInfoWithStorage[127.0.0.1:36567,DS-7d586dd4-ae08-4850-be5b-d8dc23da6e36,DISK], DatanodeInfoWithStorage[127.0.0.1:38163,DS-773baa30-efa3-4afc-bb70-126d06a7ece8,DISK], DatanodeInfoWithStorage[127.0.0.1:38944,DS-55fa1fb1-7e0d-48a2-98ae-68d2b2e1f054,DISK], DatanodeInfoWithStorage[127.0.0.1:45627,DS-646ffb2e-7528-4c45-83ed-3fa446fa0fa4,DISK], DatanodeInfoWithStorage[127.0.0.1:40945,DS-3754f2eb-f7e3-4d9a-ba5b-636c24771180,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-849976389-172.17.0.15-1598691479401:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32800,DS-ad7b1d47-fa3d-41f8-823f-bc6ab99c196b,DISK], DatanodeInfoWithStorage[127.0.0.1:38215,DS-325534bc-46ec-428b-b172-bd1b964058dd,DISK], DatanodeInfoWithStorage[127.0.0.1:41605,DS-c36d495d-5ca3-49bf-ba8c-e2ca1a26c71d,DISK], DatanodeInfoWithStorage[127.0.0.1:36567,DS-7d586dd4-ae08-4850-be5b-d8dc23da6e36,DISK], DatanodeInfoWithStorage[127.0.0.1:38163,DS-773baa30-efa3-4afc-bb70-126d06a7ece8,DISK], DatanodeInfoWithStorage[127.0.0.1:38944,DS-55fa1fb1-7e0d-48a2-98ae-68d2b2e1f054,DISK], DatanodeInfoWithStorage[127.0.0.1:45627,DS-646ffb2e-7528-4c45-83ed-3fa446fa0fa4,DISK], DatanodeInfoWithStorage[127.0.0.1:40945,DS-3754f2eb-f7e3-4d9a-ba5b-636c24771180,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is -1

Test vvMode=v2v2
tr.result is 1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1593343160-172.17.0.15-1598691511764:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40668,DS-e82d3da2-90ff-4ba6-ad89-0ecaa11b5e00,DISK], DatanodeInfoWithStorage[127.0.0.1:34359,DS-41bd60d0-8d2a-40d7-9c71-7c5c9cb7ac1c,DISK], DatanodeInfoWithStorage[127.0.0.1:34488,DS-35d998be-495e-4b8a-8d94-a92c4210a44f,DISK], DatanodeInfoWithStorage[127.0.0.1:34521,DS-9c606bef-1561-4a9a-93f4-e01ca0a00671,DISK], DatanodeInfoWithStorage[127.0.0.1:39987,DS-f26f4e06-ce49-4fba-9b9e-362e9f64f823,DISK], DatanodeInfoWithStorage[127.0.0.1:33693,DS-0ee04f5c-d6cb-435c-b30a-9420d57d5cd5,DISK], DatanodeInfoWithStorage[127.0.0.1:38836,DS-ebee6731-e9cf-477c-8d60-892a4e30f848,DISK], DatanodeInfoWithStorage[127.0.0.1:34962,DS-42b9e3ed-8f38-4d3b-ae8f-155f8636d614,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1593343160-172.17.0.15-1598691511764:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:40668,DS-e82d3da2-90ff-4ba6-ad89-0ecaa11b5e00,DISK], DatanodeInfoWithStorage[127.0.0.1:34359,DS-41bd60d0-8d2a-40d7-9c71-7c5c9cb7ac1c,DISK], DatanodeInfoWithStorage[127.0.0.1:34488,DS-35d998be-495e-4b8a-8d94-a92c4210a44f,DISK], DatanodeInfoWithStorage[127.0.0.1:34521,DS-9c606bef-1561-4a9a-93f4-e01ca0a00671,DISK], DatanodeInfoWithStorage[127.0.0.1:39987,DS-f26f4e06-ce49-4fba-9b9e-362e9f64f823,DISK], DatanodeInfoWithStorage[127.0.0.1:33693,DS-0ee04f5c-d6cb-435c-b30a-9420d57d5cd5,DISK], DatanodeInfoWithStorage[127.0.0.1:38836,DS-ebee6731-e9cf-477c-8d60-892a4e30f848,DISK], DatanodeInfoWithStorage[127.0.0.1:34962,DS-42b9e3ed-8f38-4d3b-ae8f-155f8636d614,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-37676667-172.17.0.15-1598691705824:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45635,DS-9965fee1-3e4b-4bfc-a6b5-49e66047e43a,DISK], DatanodeInfoWithStorage[127.0.0.1:33137,DS-81c7b2ec-36da-47b6-bc6a-92ae8de97eba,DISK], DatanodeInfoWithStorage[127.0.0.1:40902,DS-3822b26e-f631-4ebd-bc2c-7d5e00717f92,DISK], DatanodeInfoWithStorage[127.0.0.1:39411,DS-2b3539e6-3ac1-4a12-aaba-41037541f868,DISK], DatanodeInfoWithStorage[127.0.0.1:34670,DS-c7ea7361-c1f9-4cd7-984f-8a9d3a662454,DISK], DatanodeInfoWithStorage[127.0.0.1:34971,DS-ad75bec7-7e19-4bda-8731-51c0ab529e90,DISK], DatanodeInfoWithStorage[127.0.0.1:42218,DS-afd9e164-3d33-4fdc-87c9-3243b77eaf1a,DISK], DatanodeInfoWithStorage[127.0.0.1:44352,DS-1faacf68-30fa-46af-9e4f-810f37f204ea,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-37676667-172.17.0.15-1598691705824:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:45635,DS-9965fee1-3e4b-4bfc-a6b5-49e66047e43a,DISK], DatanodeInfoWithStorage[127.0.0.1:33137,DS-81c7b2ec-36da-47b6-bc6a-92ae8de97eba,DISK], DatanodeInfoWithStorage[127.0.0.1:40902,DS-3822b26e-f631-4ebd-bc2c-7d5e00717f92,DISK], DatanodeInfoWithStorage[127.0.0.1:39411,DS-2b3539e6-3ac1-4a12-aaba-41037541f868,DISK], DatanodeInfoWithStorage[127.0.0.1:34670,DS-c7ea7361-c1f9-4cd7-984f-8a9d3a662454,DISK], DatanodeInfoWithStorage[127.0.0.1:34971,DS-ad75bec7-7e19-4bda-8731-51c0ab529e90,DISK], DatanodeInfoWithStorage[127.0.0.1:42218,DS-afd9e164-3d33-4fdc-87c9-3243b77eaf1a,DISK], DatanodeInfoWithStorage[127.0.0.1:44352,DS-1faacf68-30fa-46af-9e4f-810f37f204ea,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1875517111-172.17.0.15-1598692013958:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42956,DS-f4d69d05-140d-463d-bc97-448eb5f20530,DISK], DatanodeInfoWithStorage[127.0.0.1:35608,DS-da8fd29b-e707-4972-a48f-c50268c46dcc,DISK], DatanodeInfoWithStorage[127.0.0.1:35583,DS-2b14f6d3-e2a0-440e-95a1-75d6989273ff,DISK], DatanodeInfoWithStorage[127.0.0.1:43742,DS-e66fca18-06ff-430f-8848-96e56dbe9ece,DISK], DatanodeInfoWithStorage[127.0.0.1:35381,DS-a2484d33-455a-4e45-bb96-9eb3f84c32f2,DISK], DatanodeInfoWithStorage[127.0.0.1:39080,DS-0a54ea38-1d98-453d-9110-71d13304803b,DISK], DatanodeInfoWithStorage[127.0.0.1:33472,DS-d2f677ee-68d5-4dba-a0c4-6e9faf8bf8dd,DISK], DatanodeInfoWithStorage[127.0.0.1:34465,DS-88221e87-0e2e-4722-a602-42a8bd4ff8fe,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1875517111-172.17.0.15-1598692013958:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:42956,DS-f4d69d05-140d-463d-bc97-448eb5f20530,DISK], DatanodeInfoWithStorage[127.0.0.1:35608,DS-da8fd29b-e707-4972-a48f-c50268c46dcc,DISK], DatanodeInfoWithStorage[127.0.0.1:35583,DS-2b14f6d3-e2a0-440e-95a1-75d6989273ff,DISK], DatanodeInfoWithStorage[127.0.0.1:43742,DS-e66fca18-06ff-430f-8848-96e56dbe9ece,DISK], DatanodeInfoWithStorage[127.0.0.1:35381,DS-a2484d33-455a-4e45-bb96-9eb3f84c32f2,DISK], DatanodeInfoWithStorage[127.0.0.1:39080,DS-0a54ea38-1d98-453d-9110-71d13304803b,DISK], DatanodeInfoWithStorage[127.0.0.1:33472,DS-d2f677ee-68d5-4dba-a0c4-6e9faf8bf8dd,DISK], DatanodeInfoWithStorage[127.0.0.1:34465,DS-88221e87-0e2e-4722-a602-42a8bd4ff8fe,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1830851114-172.17.0.15-1598692609586:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32814,DS-db96272e-2733-4c64-9695-a7fb7598a66d,DISK], DatanodeInfoWithStorage[127.0.0.1:45380,DS-7463127b-7db5-4af7-a6fd-b1fefebccde9,DISK], DatanodeInfoWithStorage[127.0.0.1:33985,DS-f9b6503b-9e5e-4d95-8a2e-ac8770868d8c,DISK], DatanodeInfoWithStorage[127.0.0.1:46564,DS-40925813-44de-4a23-9997-1f9077e0b641,DISK], DatanodeInfoWithStorage[127.0.0.1:33426,DS-d4c90ec6-a9cb-4ab0-89a2-a938affc9ffb,DISK], DatanodeInfoWithStorage[127.0.0.1:34634,DS-0aefba1b-321d-415a-ab19-ea2f22f09241,DISK], DatanodeInfoWithStorage[127.0.0.1:40275,DS-330d350a-7f4c-45be-af88-88250d60ac24,DISK], DatanodeInfoWithStorage[127.0.0.1:35296,DS-a1d921f4-c3fb-4aa6-b0ae-90e9744dd90d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1830851114-172.17.0.15-1598692609586:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:32814,DS-db96272e-2733-4c64-9695-a7fb7598a66d,DISK], DatanodeInfoWithStorage[127.0.0.1:45380,DS-7463127b-7db5-4af7-a6fd-b1fefebccde9,DISK], DatanodeInfoWithStorage[127.0.0.1:33985,DS-f9b6503b-9e5e-4d95-8a2e-ac8770868d8c,DISK], DatanodeInfoWithStorage[127.0.0.1:46564,DS-40925813-44de-4a23-9997-1f9077e0b641,DISK], DatanodeInfoWithStorage[127.0.0.1:33426,DS-d4c90ec6-a9cb-4ab0-89a2-a938affc9ffb,DISK], DatanodeInfoWithStorage[127.0.0.1:34634,DS-0aefba1b-321d-415a-ab19-ea2f22f09241,DISK], DatanodeInfoWithStorage[127.0.0.1:40275,DS-330d350a-7f4c-45be-af88-88250d60ac24,DISK], DatanodeInfoWithStorage[127.0.0.1:35296,DS-a1d921f4-c3fb-4aa6-b0ae-90e9744dd90d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-297130153-172.17.0.15-1598693223498:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36940,DS-8a1841a3-edef-48ed-86cd-675e2ab7fd08,DISK], DatanodeInfoWithStorage[127.0.0.1:35524,DS-cfe359eb-641f-4582-aeaf-7454c296ba0f,DISK], DatanodeInfoWithStorage[127.0.0.1:41008,DS-9d4a4be1-2a46-4244-a910-a82c4ef9c836,DISK], DatanodeInfoWithStorage[127.0.0.1:45734,DS-a803614b-41dc-4c27-97c1-0e87c960bef3,DISK], DatanodeInfoWithStorage[127.0.0.1:42483,DS-dfac5a74-fdfe-4730-9b1d-9ddaa90b8819,DISK], DatanodeInfoWithStorage[127.0.0.1:45614,DS-2d8278f2-ec52-45c8-a8d1-de9c35f601d9,DISK], DatanodeInfoWithStorage[127.0.0.1:41788,DS-588f27b4-085c-4f9c-adbe-00401cad8a8c,DISK], DatanodeInfoWithStorage[127.0.0.1:42018,DS-62cb8237-d9e0-47ed-b97a-a2010676b163,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-297130153-172.17.0.15-1598693223498:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36940,DS-8a1841a3-edef-48ed-86cd-675e2ab7fd08,DISK], DatanodeInfoWithStorage[127.0.0.1:35524,DS-cfe359eb-641f-4582-aeaf-7454c296ba0f,DISK], DatanodeInfoWithStorage[127.0.0.1:41008,DS-9d4a4be1-2a46-4244-a910-a82c4ef9c836,DISK], DatanodeInfoWithStorage[127.0.0.1:45734,DS-a803614b-41dc-4c27-97c1-0e87c960bef3,DISK], DatanodeInfoWithStorage[127.0.0.1:42483,DS-dfac5a74-fdfe-4730-9b1d-9ddaa90b8819,DISK], DatanodeInfoWithStorage[127.0.0.1:45614,DS-2d8278f2-ec52-45c8-a8d1-de9c35f601d9,DISK], DatanodeInfoWithStorage[127.0.0.1:41788,DS-588f27b4-085c-4f9c-adbe-00401cad8a8c,DISK], DatanodeInfoWithStorage[127.0.0.1:42018,DS-62cb8237-d9e0-47ed-b97a-a2010676b163,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1041467810-172.17.0.15-1598694926892:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44981,DS-6d6b5e7d-aa13-4885-a588-2a474876d08c,DISK], DatanodeInfoWithStorage[127.0.0.1:42538,DS-fe557625-fd2d-4a51-a541-71f392ee4da8,DISK], DatanodeInfoWithStorage[127.0.0.1:40881,DS-47fd0dfb-1b14-4672-af71-a0c5b8d6d919,DISK], DatanodeInfoWithStorage[127.0.0.1:45537,DS-d43195a8-1355-49a6-9dc2-98930d048ec4,DISK], DatanodeInfoWithStorage[127.0.0.1:38585,DS-0db2486d-d737-4c78-9e5d-9cc0ee712771,DISK], DatanodeInfoWithStorage[127.0.0.1:43355,DS-c63e96b4-72aa-430d-8632-2521fd1f2581,DISK], DatanodeInfoWithStorage[127.0.0.1:33481,DS-05dba576-8115-4aed-9a27-470948eef0a2,DISK], DatanodeInfoWithStorage[127.0.0.1:46092,DS-274a285f-f659-44fb-8a67-68fef534784b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-1041467810-172.17.0.15-1598694926892:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:44981,DS-6d6b5e7d-aa13-4885-a588-2a474876d08c,DISK], DatanodeInfoWithStorage[127.0.0.1:42538,DS-fe557625-fd2d-4a51-a541-71f392ee4da8,DISK], DatanodeInfoWithStorage[127.0.0.1:40881,DS-47fd0dfb-1b14-4672-af71-a0c5b8d6d919,DISK], DatanodeInfoWithStorage[127.0.0.1:45537,DS-d43195a8-1355-49a6-9dc2-98930d048ec4,DISK], DatanodeInfoWithStorage[127.0.0.1:38585,DS-0db2486d-d737-4c78-9e5d-9cc0ee712771,DISK], DatanodeInfoWithStorage[127.0.0.1:43355,DS-c63e96b4-72aa-430d-8632-2521fd1f2581,DISK], DatanodeInfoWithStorage[127.0.0.1:33481,DS-05dba576-8115-4aed-9a27-470948eef0a2,DISK], DatanodeInfoWithStorage[127.0.0.1:46092,DS-274a285f-f659-44fb-8a67-68fef534784b,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-659808453-172.17.0.15-1598695078828:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39766,DS-7455ea43-01f5-426d-b2d3-57b8b7abf63d,DISK], DatanodeInfoWithStorage[127.0.0.1:32964,DS-42b4a12c-0f8b-4456-b672-d11078fa0284,DISK], DatanodeInfoWithStorage[127.0.0.1:37517,DS-ff868331-707f-49e9-9e73-177afda49089,DISK], DatanodeInfoWithStorage[127.0.0.1:34472,DS-5ec57bb4-740a-447b-abfa-c82121457878,DISK], DatanodeInfoWithStorage[127.0.0.1:33912,DS-01c9f04c-d973-4562-a1cc-0f5f7b1a44d8,DISK], DatanodeInfoWithStorage[127.0.0.1:41599,DS-506e3b39-892c-430f-8742-b8a76b239b30,DISK], DatanodeInfoWithStorage[127.0.0.1:36237,DS-51b9ad5b-b068-4f77-bc06-3de1f8fb24a8,DISK], DatanodeInfoWithStorage[127.0.0.1:37715,DS-7c0ea0b9-b5c9-42e7-86c3-4b9e7f578d55,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-659808453-172.17.0.15-1598695078828:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:39766,DS-7455ea43-01f5-426d-b2d3-57b8b7abf63d,DISK], DatanodeInfoWithStorage[127.0.0.1:32964,DS-42b4a12c-0f8b-4456-b672-d11078fa0284,DISK], DatanodeInfoWithStorage[127.0.0.1:37517,DS-ff868331-707f-49e9-9e73-177afda49089,DISK], DatanodeInfoWithStorage[127.0.0.1:34472,DS-5ec57bb4-740a-447b-abfa-c82121457878,DISK], DatanodeInfoWithStorage[127.0.0.1:33912,DS-01c9f04c-d973-4562-a1cc-0f5f7b1a44d8,DISK], DatanodeInfoWithStorage[127.0.0.1:41599,DS-506e3b39-892c-430f-8742-b8a76b239b30,DISK], DatanodeInfoWithStorage[127.0.0.1:36237,DS-51b9ad5b-b068-4f77-bc06-3de1f8fb24a8,DISK], DatanodeInfoWithStorage[127.0.0.1:37715,DS-7c0ea0b9-b5c9-42e7-86c3-4b9e7f578d55,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-254996764-172.17.0.15-1598695661373:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43065,DS-d6b1279c-0d0b-4ac3-af4c-00e8020ea9bd,DISK], DatanodeInfoWithStorage[127.0.0.1:41840,DS-a97ff02c-711c-4d9a-893f-1e5490cedf47,DISK], DatanodeInfoWithStorage[127.0.0.1:34969,DS-d11f1ece-6110-4ca9-9297-d2873bbeac14,DISK], DatanodeInfoWithStorage[127.0.0.1:33636,DS-132a5c87-2c5f-477e-a6d7-1578791842ac,DISK], DatanodeInfoWithStorage[127.0.0.1:35237,DS-7423523f-1506-4489-ab38-89e4e90701e4,DISK], DatanodeInfoWithStorage[127.0.0.1:40725,DS-8b5fafa9-3a51-42cb-9097-29fdc809f7c6,DISK], DatanodeInfoWithStorage[127.0.0.1:37398,DS-2f72237c-820d-4388-8c4d-7adc2ef0f3ac,DISK], DatanodeInfoWithStorage[127.0.0.1:45060,DS-fb14d1ed-90d0-4715-9f82-2596ea7af65a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-254996764-172.17.0.15-1598695661373:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:43065,DS-d6b1279c-0d0b-4ac3-af4c-00e8020ea9bd,DISK], DatanodeInfoWithStorage[127.0.0.1:41840,DS-a97ff02c-711c-4d9a-893f-1e5490cedf47,DISK], DatanodeInfoWithStorage[127.0.0.1:34969,DS-d11f1ece-6110-4ca9-9297-d2873bbeac14,DISK], DatanodeInfoWithStorage[127.0.0.1:33636,DS-132a5c87-2c5f-477e-a6d7-1578791842ac,DISK], DatanodeInfoWithStorage[127.0.0.1:35237,DS-7423523f-1506-4489-ab38-89e4e90701e4,DISK], DatanodeInfoWithStorage[127.0.0.1:40725,DS-8b5fafa9-3a51-42cb-9097-29fdc809f7c6,DISK], DatanodeInfoWithStorage[127.0.0.1:37398,DS-2f72237c-820d-4388-8c4d-7adc2ef0f3ac,DISK], DatanodeInfoWithStorage[127.0.0.1:45060,DS-fb14d1ed-90d0-4715-9f82-2596ea7af65a,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-342443594-172.17.0.15-1598696261700:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46441,DS-d76c60e0-2267-46a4-a000-e612522b3d0d,DISK], DatanodeInfoWithStorage[127.0.0.1:44989,DS-45458317-cac1-437b-8ec5-5075fca09ee2,DISK], DatanodeInfoWithStorage[127.0.0.1:40746,DS-0cecf312-0e55-4a2e-a93e-8e98dde8b743,DISK], DatanodeInfoWithStorage[127.0.0.1:39596,DS-eefc43aa-2af8-4201-a78e-ce80956baef0,DISK], DatanodeInfoWithStorage[127.0.0.1:37530,DS-fff86142-e61f-42c9-9c06-acaee3146efc,DISK], DatanodeInfoWithStorage[127.0.0.1:39672,DS-e23de8bb-bc08-4fc6-a40c-60f36f5553a9,DISK], DatanodeInfoWithStorage[127.0.0.1:38680,DS-c40fe878-7c22-4d13-b5ea-76705463954e,DISK], DatanodeInfoWithStorage[127.0.0.1:45589,DS-b510ca9b-f5b4-4211-8f5b-24eff030a21d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-342443594-172.17.0.15-1598696261700:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:46441,DS-d76c60e0-2267-46a4-a000-e612522b3d0d,DISK], DatanodeInfoWithStorage[127.0.0.1:44989,DS-45458317-cac1-437b-8ec5-5075fca09ee2,DISK], DatanodeInfoWithStorage[127.0.0.1:40746,DS-0cecf312-0e55-4a2e-a93e-8e98dde8b743,DISK], DatanodeInfoWithStorage[127.0.0.1:39596,DS-eefc43aa-2af8-4201-a78e-ce80956baef0,DISK], DatanodeInfoWithStorage[127.0.0.1:37530,DS-fff86142-e61f-42c9-9c06-acaee3146efc,DISK], DatanodeInfoWithStorage[127.0.0.1:39672,DS-e23de8bb-bc08-4fc6-a40c-60f36f5553a9,DISK], DatanodeInfoWithStorage[127.0.0.1:38680,DS-c40fe878-7c22-4d13-b5ea-76705463954e,DISK], DatanodeInfoWithStorage[127.0.0.1:45589,DS-b510ca9b-f5b4-4211-8f5b-24eff030a21d,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: hadoop.caller.context.signature.max.size
component: hdfs:NameNode
v1: 40
v2: 1
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.TestFileChecksumCompositeCrc#testStripedFileChecksumWithMissedDataBlocksRangeQuery3
reconfPoint: 1
result: -1
failureMessage: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-162900647-172.17.0.15-1598696335297:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36752,DS-47b42b41-a9c4-404c-a219-1f23bfa94a45,DISK], DatanodeInfoWithStorage[127.0.0.1:44329,DS-3f70a5ce-4480-4651-a9a9-71fbc9f66c87,DISK], DatanodeInfoWithStorage[127.0.0.1:35064,DS-809ff246-2dae-4032-8518-657680b776a0,DISK], DatanodeInfoWithStorage[127.0.0.1:43615,DS-64942bd7-ec01-4de0-993b-4bb158a7b2ef,DISK], DatanodeInfoWithStorage[127.0.0.1:40194,DS-73e76561-3d2b-43c1-b596-2a9a4cd35387,DISK], DatanodeInfoWithStorage[127.0.0.1:42882,DS-00541091-351f-4d21-aafb-8de4ba0a029b,DISK], DatanodeInfoWithStorage[127.0.0.1:41885,DS-fc3b6413-005d-42a8-8a56-0829dd046e9a,DISK], DatanodeInfoWithStorage[127.0.0.1:45093,DS-18b1603f-19a4-4392-8591-14f671a41a3c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
stackTrace: org.apache.hadoop.fs.PathIOException: `/striped/stripedFileChecksum1': Fail to get block checksum for LocatedStripedBlock{BP-162900647-172.17.0.15-1598696335297:blk_-9223372036854775792_1001; getBlockSize()=37748736; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[127.0.0.1:36752,DS-47b42b41-a9c4-404c-a219-1f23bfa94a45,DISK], DatanodeInfoWithStorage[127.0.0.1:44329,DS-3f70a5ce-4480-4651-a9a9-71fbc9f66c87,DISK], DatanodeInfoWithStorage[127.0.0.1:35064,DS-809ff246-2dae-4032-8518-657680b776a0,DISK], DatanodeInfoWithStorage[127.0.0.1:43615,DS-64942bd7-ec01-4de0-993b-4bb158a7b2ef,DISK], DatanodeInfoWithStorage[127.0.0.1:40194,DS-73e76561-3d2b-43c1-b596-2a9a4cd35387,DISK], DatanodeInfoWithStorage[127.0.0.1:42882,DS-00541091-351f-4d21-aafb-8de4ba0a029b,DISK], DatanodeInfoWithStorage[127.0.0.1:41885,DS-fc3b6413-005d-42a8-8a56-0829dd046e9a,DISK], DatanodeInfoWithStorage[127.0.0.1:45093,DS-18b1603f-19a4-4392-8591-14f671a41a3c,DISK]]; indices=[1, 2, 3, 4, 5, 6, 7, 8]}
	at org.apache.hadoop.hdfs.FileChecksumHelper$StripedFileNonStripedChecksumComputer.checksumBlocks(FileChecksumHelper.java:640)
	at org.apache.hadoop.hdfs.FileChecksumHelper$FileChecksumComputer.compute(FileChecksumHelper.java:252)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumInternal(DFSClient.java:1790)
	at org.apache.hadoop.hdfs.DFSClient.getFileChecksumWithCombineMode(DFSClient.java:1810)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1712)
	at org.apache.hadoop.hdfs.DistributedFileSystem$34.doCall(DistributedFileSystem.java:1709)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileChecksum(DistributedFileSystem.java:1726)
	at org.apache.hadoop.hdfs.TestFileChecksum.getFileChecksum(TestFileChecksum.java:584)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery(TestFileChecksum.java:295)
	at org.apache.hadoop.hdfs.TestFileChecksum.testStripedFileChecksumWithMissedDataBlocksRangeQuery3(TestFileChecksum.java:333)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)



Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
v1v2 failed with probability 5 out of 50
v1v1v2v2 failed with probability 7 out of 50
result: false positive !!!
Total execution time in seconds : 5525

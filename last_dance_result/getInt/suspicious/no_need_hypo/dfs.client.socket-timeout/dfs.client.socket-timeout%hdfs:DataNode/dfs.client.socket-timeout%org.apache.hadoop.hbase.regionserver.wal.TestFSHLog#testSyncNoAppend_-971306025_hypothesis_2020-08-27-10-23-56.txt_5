reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncNoAppend
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncNoAppend
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34783,DS-ba24fd76-6b8f-4f79-befd-d450ed08a807,DISK], DatanodeInfoWithStorage[127.0.0.1:38894,DS-cdeda3db-a9fa-4e6b-a6e5-4a0882fbe9fa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34783,DS-ba24fd76-6b8f-4f79-befd-d450ed08a807,DISK], DatanodeInfoWithStorage[127.0.0.1:38894,DS-cdeda3db-a9fa-4e6b-a6e5-4a0882fbe9fa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34783,DS-ba24fd76-6b8f-4f79-befd-d450ed08a807,DISK], DatanodeInfoWithStorage[127.0.0.1:38894,DS-cdeda3db-a9fa-4e6b-a6e5-4a0882fbe9fa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34783,DS-ba24fd76-6b8f-4f79-befd-d450ed08a807,DISK], DatanodeInfoWithStorage[127.0.0.1:38894,DS-cdeda3db-a9fa-4e6b-a6e5-4a0882fbe9fa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncNoAppend
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41633,DS-8588d4fe-aab4-4c0f-a05d-878dbd481bc7,DISK], DatanodeInfoWithStorage[127.0.0.1:36360,DS-7e1adaa6-3a0c-41b1-bf45-0a7bfa151845,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41633,DS-8588d4fe-aab4-4c0f-a05d-878dbd481bc7,DISK], DatanodeInfoWithStorage[127.0.0.1:36360,DS-7e1adaa6-3a0c-41b1-bf45-0a7bfa151845,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41633,DS-8588d4fe-aab4-4c0f-a05d-878dbd481bc7,DISK], DatanodeInfoWithStorage[127.0.0.1:36360,DS-7e1adaa6-3a0c-41b1-bf45-0a7bfa151845,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41633,DS-8588d4fe-aab4-4c0f-a05d-878dbd481bc7,DISK], DatanodeInfoWithStorage[127.0.0.1:36360,DS-7e1adaa6-3a0c-41b1-bf45-0a7bfa151845,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncNoAppend
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38286,DS-b2ed259e-43d6-4995-aedb-8b64b8cb4adc,DISK], DatanodeInfoWithStorage[127.0.0.1:45696,DS-440c8bf0-3658-4cc0-affe-3a78d0746123,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45696,DS-440c8bf0-3658-4cc0-affe-3a78d0746123,DISK], DatanodeInfoWithStorage[127.0.0.1:38286,DS-b2ed259e-43d6-4995-aedb-8b64b8cb4adc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38286,DS-b2ed259e-43d6-4995-aedb-8b64b8cb4adc,DISK], DatanodeInfoWithStorage[127.0.0.1:45696,DS-440c8bf0-3658-4cc0-affe-3a78d0746123,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45696,DS-440c8bf0-3658-4cc0-affe-3a78d0746123,DISK], DatanodeInfoWithStorage[127.0.0.1:38286,DS-b2ed259e-43d6-4995-aedb-8b64b8cb4adc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncNoAppend
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33876,DS-4dc3c0f0-27fb-4f44-b455-24349effc6dc,DISK], DatanodeInfoWithStorage[127.0.0.1:43455,DS-fd8134cc-103c-4ff1-9b59-02eaaad80d70,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33876,DS-4dc3c0f0-27fb-4f44-b455-24349effc6dc,DISK], DatanodeInfoWithStorage[127.0.0.1:43455,DS-fd8134cc-103c-4ff1-9b59-02eaaad80d70,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33876,DS-4dc3c0f0-27fb-4f44-b455-24349effc6dc,DISK], DatanodeInfoWithStorage[127.0.0.1:43455,DS-fd8134cc-103c-4ff1-9b59-02eaaad80d70,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33876,DS-4dc3c0f0-27fb-4f44-b455-24349effc6dc,DISK], DatanodeInfoWithStorage[127.0.0.1:43455,DS-fd8134cc-103c-4ff1-9b59-02eaaad80d70,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncNoAppend
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44960,DS-2b5eb900-91d1-4951-92a2-67014b00b236,DISK], DatanodeInfoWithStorage[127.0.0.1:43286,DS-6570d7c1-425a-406f-a552-f26b6adcf6a2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44960,DS-2b5eb900-91d1-4951-92a2-67014b00b236,DISK], DatanodeInfoWithStorage[127.0.0.1:43286,DS-6570d7c1-425a-406f-a552-f26b6adcf6a2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44960,DS-2b5eb900-91d1-4951-92a2-67014b00b236,DISK], DatanodeInfoWithStorage[127.0.0.1:43286,DS-6570d7c1-425a-406f-a552-f26b6adcf6a2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44960,DS-2b5eb900-91d1-4951-92a2-67014b00b236,DISK], DatanodeInfoWithStorage[127.0.0.1:43286,DS-6570d7c1-425a-406f-a552-f26b6adcf6a2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncNoAppend
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44416,DS-62d90868-c1ab-4d37-bd8d-eb49d09bb9c3,DISK], DatanodeInfoWithStorage[127.0.0.1:36086,DS-d86e58bd-b549-44ca-adc1-f2d5991b744d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36086,DS-d86e58bd-b549-44ca-adc1-f2d5991b744d,DISK], DatanodeInfoWithStorage[127.0.0.1:44416,DS-62d90868-c1ab-4d37-bd8d-eb49d09bb9c3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44416,DS-62d90868-c1ab-4d37-bd8d-eb49d09bb9c3,DISK], DatanodeInfoWithStorage[127.0.0.1:36086,DS-d86e58bd-b549-44ca-adc1-f2d5991b744d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36086,DS-d86e58bd-b549-44ca-adc1-f2d5991b744d,DISK], DatanodeInfoWithStorage[127.0.0.1:44416,DS-62d90868-c1ab-4d37-bd8d-eb49d09bb9c3,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncNoAppend
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38717,DS-a4daf9e7-9206-4a87-83ec-1e1d314e4ad1,DISK], DatanodeInfoWithStorage[127.0.0.1:40399,DS-a65a36ec-2a54-4aae-97f6-1acb2f6b2c17,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40399,DS-a65a36ec-2a54-4aae-97f6-1acb2f6b2c17,DISK], DatanodeInfoWithStorage[127.0.0.1:38717,DS-a4daf9e7-9206-4a87-83ec-1e1d314e4ad1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38717,DS-a4daf9e7-9206-4a87-83ec-1e1d314e4ad1,DISK], DatanodeInfoWithStorage[127.0.0.1:40399,DS-a65a36ec-2a54-4aae-97f6-1acb2f6b2c17,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40399,DS-a65a36ec-2a54-4aae-97f6-1acb2f6b2c17,DISK], DatanodeInfoWithStorage[127.0.0.1:38717,DS-a4daf9e7-9206-4a87-83ec-1e1d314e4ad1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncNoAppend
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44594,DS-ef0cfcfe-90f5-46a1-9c15-8c35a618730f,DISK], DatanodeInfoWithStorage[127.0.0.1:42217,DS-6c18cfb1-363f-4a84-9b36-10f5ce3c1d53,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44594,DS-ef0cfcfe-90f5-46a1-9c15-8c35a618730f,DISK], DatanodeInfoWithStorage[127.0.0.1:42217,DS-6c18cfb1-363f-4a84-9b36-10f5ce3c1d53,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44594,DS-ef0cfcfe-90f5-46a1-9c15-8c35a618730f,DISK], DatanodeInfoWithStorage[127.0.0.1:42217,DS-6c18cfb1-363f-4a84-9b36-10f5ce3c1d53,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44594,DS-ef0cfcfe-90f5-46a1-9c15-8c35a618730f,DISK], DatanodeInfoWithStorage[127.0.0.1:42217,DS-6c18cfb1-363f-4a84-9b36-10f5ce3c1d53,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncNoAppend
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33356,DS-536cc614-8012-4ddc-90f7-7a7d0d177369,DISK], DatanodeInfoWithStorage[127.0.0.1:44547,DS-321f9c9e-a997-4b09-9338-cd6b534268fb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44547,DS-321f9c9e-a997-4b09-9338-cd6b534268fb,DISK], DatanodeInfoWithStorage[127.0.0.1:33356,DS-536cc614-8012-4ddc-90f7-7a7d0d177369,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33356,DS-536cc614-8012-4ddc-90f7-7a7d0d177369,DISK], DatanodeInfoWithStorage[127.0.0.1:44547,DS-321f9c9e-a997-4b09-9338-cd6b534268fb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44547,DS-321f9c9e-a997-4b09-9338-cd6b534268fb,DISK], DatanodeInfoWithStorage[127.0.0.1:33356,DS-536cc614-8012-4ddc-90f7-7a7d0d177369,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 5000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestFSHLog#testSyncNoAppend
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40699,DS-7613cb11-7886-40d2-94c0-5cd6e70e3827,DISK], DatanodeInfoWithStorage[127.0.0.1:40360,DS-470e066d-be0c-44a8-a0fb-e91dae8710fe,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40699,DS-7613cb11-7886-40d2-94c0-5cd6e70e3827,DISK], DatanodeInfoWithStorage[127.0.0.1:40360,DS-470e066d-be0c-44a8-a0fb-e91dae8710fe,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40699,DS-7613cb11-7886-40d2-94c0-5cd6e70e3827,DISK], DatanodeInfoWithStorage[127.0.0.1:40360,DS-470e066d-be0c-44a8-a0fb-e91dae8710fe,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40699,DS-7613cb11-7886-40d2-94c0-5cd6e70e3827,DISK], DatanodeInfoWithStorage[127.0.0.1:40360,DS-470e066d-be0c-44a8-a0fb-e91dae8710fe,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 1441

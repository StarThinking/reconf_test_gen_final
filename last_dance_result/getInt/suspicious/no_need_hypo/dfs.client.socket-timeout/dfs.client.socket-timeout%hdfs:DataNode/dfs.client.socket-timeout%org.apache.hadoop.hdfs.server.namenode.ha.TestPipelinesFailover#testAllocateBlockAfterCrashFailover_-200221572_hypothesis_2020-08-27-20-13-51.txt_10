reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testAllocateBlockAfterCrashFailover
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testAllocateBlockAfterCrashFailover
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42436,DS-2e6d6fe4-1f6e-4f00-a5c8-81de9749afea,DISK], DatanodeInfoWithStorage[127.0.0.1:44368,DS-102988dd-1490-42e0-9fb9-2d441c6c6af2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42436,DS-2e6d6fe4-1f6e-4f00-a5c8-81de9749afea,DISK], DatanodeInfoWithStorage[127.0.0.1:44368,DS-102988dd-1490-42e0-9fb9-2d441c6c6af2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42436,DS-2e6d6fe4-1f6e-4f00-a5c8-81de9749afea,DISK], DatanodeInfoWithStorage[127.0.0.1:44368,DS-102988dd-1490-42e0-9fb9-2d441c6c6af2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42436,DS-2e6d6fe4-1f6e-4f00-a5c8-81de9749afea,DISK], DatanodeInfoWithStorage[127.0.0.1:44368,DS-102988dd-1490-42e0-9fb9-2d441c6c6af2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testAllocateBlockAfterCrashFailover
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43436,DS-7aa33260-16e8-4ab4-a4c8-8cb8d38d62ae,DISK], DatanodeInfoWithStorage[127.0.0.1:35596,DS-c02ff8a7-cf84-42ee-b8bb-d90db1a74e42,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35596,DS-c02ff8a7-cf84-42ee-b8bb-d90db1a74e42,DISK], DatanodeInfoWithStorage[127.0.0.1:43436,DS-7aa33260-16e8-4ab4-a4c8-8cb8d38d62ae,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43436,DS-7aa33260-16e8-4ab4-a4c8-8cb8d38d62ae,DISK], DatanodeInfoWithStorage[127.0.0.1:35596,DS-c02ff8a7-cf84-42ee-b8bb-d90db1a74e42,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35596,DS-c02ff8a7-cf84-42ee-b8bb-d90db1a74e42,DISK], DatanodeInfoWithStorage[127.0.0.1:43436,DS-7aa33260-16e8-4ab4-a4c8-8cb8d38d62ae,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testAllocateBlockAfterCrashFailover
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43934,DS-659b3eb2-0b2a-44f6-9034-4f93074b1ebe,DISK], DatanodeInfoWithStorage[127.0.0.1:43259,DS-02eb45c0-7569-409c-bb65-c3d5db8740a9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43259,DS-02eb45c0-7569-409c-bb65-c3d5db8740a9,DISK], DatanodeInfoWithStorage[127.0.0.1:43934,DS-659b3eb2-0b2a-44f6-9034-4f93074b1ebe,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43934,DS-659b3eb2-0b2a-44f6-9034-4f93074b1ebe,DISK], DatanodeInfoWithStorage[127.0.0.1:43259,DS-02eb45c0-7569-409c-bb65-c3d5db8740a9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43259,DS-02eb45c0-7569-409c-bb65-c3d5db8740a9,DISK], DatanodeInfoWithStorage[127.0.0.1:43934,DS-659b3eb2-0b2a-44f6-9034-4f93074b1ebe,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testAllocateBlockAfterCrashFailover
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46031,DS-ed41eac7-1918-4b17-892f-cb38cc2f6b6e,DISK], DatanodeInfoWithStorage[127.0.0.1:37415,DS-9028a018-a9e6-4f27-bf4a-bf403f5107b2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46031,DS-ed41eac7-1918-4b17-892f-cb38cc2f6b6e,DISK], DatanodeInfoWithStorage[127.0.0.1:37415,DS-9028a018-a9e6-4f27-bf4a-bf403f5107b2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46031,DS-ed41eac7-1918-4b17-892f-cb38cc2f6b6e,DISK], DatanodeInfoWithStorage[127.0.0.1:37415,DS-9028a018-a9e6-4f27-bf4a-bf403f5107b2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46031,DS-ed41eac7-1918-4b17-892f-cb38cc2f6b6e,DISK], DatanodeInfoWithStorage[127.0.0.1:37415,DS-9028a018-a9e6-4f27-bf4a-bf403f5107b2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testAllocateBlockAfterCrashFailover
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34698,DS-517ca053-03ad-4b88-92fe-21d11c840ce0,DISK], DatanodeInfoWithStorage[127.0.0.1:33812,DS-7908f43b-8a61-48b7-bb50-e9c29fb1729e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34698,DS-517ca053-03ad-4b88-92fe-21d11c840ce0,DISK], DatanodeInfoWithStorage[127.0.0.1:33812,DS-7908f43b-8a61-48b7-bb50-e9c29fb1729e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34698,DS-517ca053-03ad-4b88-92fe-21d11c840ce0,DISK], DatanodeInfoWithStorage[127.0.0.1:33812,DS-7908f43b-8a61-48b7-bb50-e9c29fb1729e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34698,DS-517ca053-03ad-4b88-92fe-21d11c840ce0,DISK], DatanodeInfoWithStorage[127.0.0.1:33812,DS-7908f43b-8a61-48b7-bb50-e9c29fb1729e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testAllocateBlockAfterCrashFailover
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39350,DS-245871ac-9df0-4140-a523-038d9bab55f9,DISK], DatanodeInfoWithStorage[127.0.0.1:46461,DS-c2896f2d-a827-47e5-b138-6a250a9b9e3b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46461,DS-c2896f2d-a827-47e5-b138-6a250a9b9e3b,DISK], DatanodeInfoWithStorage[127.0.0.1:39350,DS-245871ac-9df0-4140-a523-038d9bab55f9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39350,DS-245871ac-9df0-4140-a523-038d9bab55f9,DISK], DatanodeInfoWithStorage[127.0.0.1:46461,DS-c2896f2d-a827-47e5-b138-6a250a9b9e3b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46461,DS-c2896f2d-a827-47e5-b138-6a250a9b9e3b,DISK], DatanodeInfoWithStorage[127.0.0.1:39350,DS-245871ac-9df0-4140-a523-038d9bab55f9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testAllocateBlockAfterCrashFailover
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39161,DS-d659e861-1b95-4ff9-888c-cb6168f4e0e8,DISK], DatanodeInfoWithStorage[127.0.0.1:39203,DS-9686925f-f424-4906-8185-0c8b60794759,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39161,DS-d659e861-1b95-4ff9-888c-cb6168f4e0e8,DISK], DatanodeInfoWithStorage[127.0.0.1:39203,DS-9686925f-f424-4906-8185-0c8b60794759,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39161,DS-d659e861-1b95-4ff9-888c-cb6168f4e0e8,DISK], DatanodeInfoWithStorage[127.0.0.1:39203,DS-9686925f-f424-4906-8185-0c8b60794759,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39161,DS-d659e861-1b95-4ff9-888c-cb6168f4e0e8,DISK], DatanodeInfoWithStorage[127.0.0.1:39203,DS-9686925f-f424-4906-8185-0c8b60794759,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testAllocateBlockAfterCrashFailover
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37526,DS-472856ea-9a18-4b7d-9e61-674ca865d7a0,DISK], DatanodeInfoWithStorage[127.0.0.1:35475,DS-f4f253e4-7198-4d8e-9c9e-c0d2bd41b67f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37526,DS-472856ea-9a18-4b7d-9e61-674ca865d7a0,DISK], DatanodeInfoWithStorage[127.0.0.1:35475,DS-f4f253e4-7198-4d8e-9c9e-c0d2bd41b67f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37526,DS-472856ea-9a18-4b7d-9e61-674ca865d7a0,DISK], DatanodeInfoWithStorage[127.0.0.1:35475,DS-f4f253e4-7198-4d8e-9c9e-c0d2bd41b67f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37526,DS-472856ea-9a18-4b7d-9e61-674ca865d7a0,DISK], DatanodeInfoWithStorage[127.0.0.1:35475,DS-f4f253e4-7198-4d8e-9c9e-c0d2bd41b67f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testAllocateBlockAfterCrashFailover
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44642,DS-a7003aaa-0d48-4483-a521-8890a91dda0a,DISK], DatanodeInfoWithStorage[127.0.0.1:37874,DS-4bd1018e-b972-42ff-9447-202be4243d3b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44642,DS-a7003aaa-0d48-4483-a521-8890a91dda0a,DISK], DatanodeInfoWithStorage[127.0.0.1:37874,DS-4bd1018e-b972-42ff-9447-202be4243d3b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44642,DS-a7003aaa-0d48-4483-a521-8890a91dda0a,DISK], DatanodeInfoWithStorage[127.0.0.1:37874,DS-4bd1018e-b972-42ff-9447-202be4243d3b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44642,DS-a7003aaa-0d48-4483-a521-8890a91dda0a,DISK], DatanodeInfoWithStorage[127.0.0.1:37874,DS-4bd1018e-b972-42ff-9447-202be4243d3b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.ha.TestPipelinesFailover#testAllocateBlockAfterCrashFailover
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37616,DS-c55e8e56-3998-453f-9918-a69f136d64d6,DISK], DatanodeInfoWithStorage[127.0.0.1:36428,DS-e1d57f63-909c-41e8-a89e-4cc190177ecc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37616,DS-c55e8e56-3998-453f-9918-a69f136d64d6,DISK], DatanodeInfoWithStorage[127.0.0.1:36428,DS-e1d57f63-909c-41e8-a89e-4cc190177ecc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37616,DS-c55e8e56-3998-453f-9918-a69f136d64d6,DISK], DatanodeInfoWithStorage[127.0.0.1:36428,DS-e1d57f63-909c-41e8-a89e-4cc190177ecc,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37616,DS-c55e8e56-3998-453f-9918-a69f136d64d6,DISK], DatanodeInfoWithStorage[127.0.0.1:36428,DS-e1d57f63-909c-41e8-a89e-4cc190177ecc,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 836

reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39584,DS-37bfc128-d98d-44b4-bd9c-ee2238c1c92d,DISK], DatanodeInfoWithStorage[127.0.0.1:36480,DS-d1b0baec-a634-4556-96a8-531f4630ada4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39584,DS-37bfc128-d98d-44b4-bd9c-ee2238c1c92d,DISK], DatanodeInfoWithStorage[127.0.0.1:36480,DS-d1b0baec-a634-4556-96a8-531f4630ada4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39584,DS-37bfc128-d98d-44b4-bd9c-ee2238c1c92d,DISK], DatanodeInfoWithStorage[127.0.0.1:36480,DS-d1b0baec-a634-4556-96a8-531f4630ada4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39584,DS-37bfc128-d98d-44b4-bd9c-ee2238c1c92d,DISK], DatanodeInfoWithStorage[127.0.0.1:36480,DS-d1b0baec-a634-4556-96a8-531f4630ada4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34406,DS-7e9b3aec-40f6-478d-b379-1a9bddf088eb,DISK], DatanodeInfoWithStorage[127.0.0.1:37366,DS-414ebbd8-0ef1-4895-a7e3-b9f5b0a5ac92,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37366,DS-414ebbd8-0ef1-4895-a7e3-b9f5b0a5ac92,DISK], DatanodeInfoWithStorage[127.0.0.1:34406,DS-7e9b3aec-40f6-478d-b379-1a9bddf088eb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34406,DS-7e9b3aec-40f6-478d-b379-1a9bddf088eb,DISK], DatanodeInfoWithStorage[127.0.0.1:37366,DS-414ebbd8-0ef1-4895-a7e3-b9f5b0a5ac92,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37366,DS-414ebbd8-0ef1-4895-a7e3-b9f5b0a5ac92,DISK], DatanodeInfoWithStorage[127.0.0.1:34406,DS-7e9b3aec-40f6-478d-b379-1a9bddf088eb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44084,DS-8cd2aae6-cf3c-4c29-91c8-fa9304dd54b4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44084,DS-8cd2aae6-cf3c-4c29-91c8-fa9304dd54b4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44084,DS-8cd2aae6-cf3c-4c29-91c8-fa9304dd54b4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44084,DS-8cd2aae6-cf3c-4c29-91c8-fa9304dd54b4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37023,DS-1d35b468-4ab3-49c9-a9a0-41f35e2bbc7c,DISK], DatanodeInfoWithStorage[127.0.0.1:33493,DS-153bf361-4e5c-482a-b0f3-94f35825edf7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33493,DS-153bf361-4e5c-482a-b0f3-94f35825edf7,DISK], DatanodeInfoWithStorage[127.0.0.1:37023,DS-1d35b468-4ab3-49c9-a9a0-41f35e2bbc7c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37023,DS-1d35b468-4ab3-49c9-a9a0-41f35e2bbc7c,DISK], DatanodeInfoWithStorage[127.0.0.1:33493,DS-153bf361-4e5c-482a-b0f3-94f35825edf7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33493,DS-153bf361-4e5c-482a-b0f3-94f35825edf7,DISK], DatanodeInfoWithStorage[127.0.0.1:37023,DS-1d35b468-4ab3-49c9-a9a0-41f35e2bbc7c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38002,DS-40b95bb1-6823-4a51-94d2-631a1d02034f,DISK], DatanodeInfoWithStorage[127.0.0.1:42563,DS-54aa0758-0421-4a31-a836-923f57aa12b4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38002,DS-40b95bb1-6823-4a51-94d2-631a1d02034f,DISK], DatanodeInfoWithStorage[127.0.0.1:42563,DS-54aa0758-0421-4a31-a836-923f57aa12b4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38002,DS-40b95bb1-6823-4a51-94d2-631a1d02034f,DISK], DatanodeInfoWithStorage[127.0.0.1:42563,DS-54aa0758-0421-4a31-a836-923f57aa12b4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38002,DS-40b95bb1-6823-4a51-94d2-631a1d02034f,DISK], DatanodeInfoWithStorage[127.0.0.1:42563,DS-54aa0758-0421-4a31-a836-923f57aa12b4,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43543,DS-05a1cbea-0f4c-4e9d-8eef-67a470628b3e,DISK], DatanodeInfoWithStorage[127.0.0.1:35874,DS-9417d6c6-2edc-40ca-af51-85cfd9944ede,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43543,DS-05a1cbea-0f4c-4e9d-8eef-67a470628b3e,DISK], DatanodeInfoWithStorage[127.0.0.1:35874,DS-9417d6c6-2edc-40ca-af51-85cfd9944ede,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43543,DS-05a1cbea-0f4c-4e9d-8eef-67a470628b3e,DISK], DatanodeInfoWithStorage[127.0.0.1:35874,DS-9417d6c6-2edc-40ca-af51-85cfd9944ede,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43543,DS-05a1cbea-0f4c-4e9d-8eef-67a470628b3e,DISK], DatanodeInfoWithStorage[127.0.0.1:35874,DS-9417d6c6-2edc-40ca-af51-85cfd9944ede,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46451,DS-b4f88226-d409-4c6c-869f-fd1d0ca264c1,DISK], DatanodeInfoWithStorage[127.0.0.1:40174,DS-62ec93d4-7f4f-4949-8887-289cb54e06c6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46451,DS-b4f88226-d409-4c6c-869f-fd1d0ca264c1,DISK], DatanodeInfoWithStorage[127.0.0.1:40174,DS-62ec93d4-7f4f-4949-8887-289cb54e06c6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46451,DS-b4f88226-d409-4c6c-869f-fd1d0ca264c1,DISK], DatanodeInfoWithStorage[127.0.0.1:40174,DS-62ec93d4-7f4f-4949-8887-289cb54e06c6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46451,DS-b4f88226-d409-4c6c-869f-fd1d0ca264c1,DISK], DatanodeInfoWithStorage[127.0.0.1:40174,DS-62ec93d4-7f4f-4949-8887-289cb54e06c6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39619,DS-c737b3e8-d9cf-4f43-8e63-86df8a0ca4aa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39619,DS-c737b3e8-d9cf-4f43-8e63-86df8a0ca4aa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39619,DS-c737b3e8-d9cf-4f43-8e63-86df8a0ca4aa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39619,DS-c737b3e8-d9cf-4f43-8e63-86df8a0ca4aa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37190,DS-1f588ec2-a412-4550-a842-4613a9cf61e1,DISK], DatanodeInfoWithStorage[127.0.0.1:36590,DS-be5c1927-9796-4440-b05c-babcfaae47e0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37190,DS-1f588ec2-a412-4550-a842-4613a9cf61e1,DISK], DatanodeInfoWithStorage[127.0.0.1:36590,DS-be5c1927-9796-4440-b05c-babcfaae47e0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37190,DS-1f588ec2-a412-4550-a842-4613a9cf61e1,DISK], DatanodeInfoWithStorage[127.0.0.1:36590,DS-be5c1927-9796-4440-b05c-babcfaae47e0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37190,DS-1f588ec2-a412-4550-a842-4613a9cf61e1,DISK], DatanodeInfoWithStorage[127.0.0.1:36590,DS-be5c1927-9796-4440-b05c-babcfaae47e0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40988,DS-571d6fbf-ac31-4476-8254-84f4bb2ccdc8,DISK], DatanodeInfoWithStorage[127.0.0.1:35453,DS-49ad4017-ddc1-45a6-9d04-2a50a0d0740d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35453,DS-49ad4017-ddc1-45a6-9d04-2a50a0d0740d,DISK], DatanodeInfoWithStorage[127.0.0.1:40988,DS-571d6fbf-ac31-4476-8254-84f4bb2ccdc8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40988,DS-571d6fbf-ac31-4476-8254-84f4bb2ccdc8,DISK], DatanodeInfoWithStorage[127.0.0.1:35453,DS-49ad4017-ddc1-45a6-9d04-2a50a0d0740d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35453,DS-49ad4017-ddc1-45a6-9d04-2a50a0d0740d,DISK], DatanodeInfoWithStorage[127.0.0.1:40988,DS-571d6fbf-ac31-4476-8254-84f4bb2ccdc8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37652,DS-8e619764-8bb4-48b3-bafd-b3ae0e2bd0ca,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37652,DS-8e619764-8bb4-48b3-bafd-b3ae0e2bd0ca,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37652,DS-8e619764-8bb4-48b3-bafd-b3ae0e2bd0ca,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37652,DS-8e619764-8bb4-48b3-bafd-b3ae0e2bd0ca,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44175,DS-76ff9888-5bb4-4669-ac4f-b8f135f307e5,DISK], DatanodeInfoWithStorage[127.0.0.1:46033,DS-474262b9-0524-4bad-a149-b037a30327a1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44175,DS-76ff9888-5bb4-4669-ac4f-b8f135f307e5,DISK], DatanodeInfoWithStorage[127.0.0.1:46033,DS-474262b9-0524-4bad-a149-b037a30327a1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44175,DS-76ff9888-5bb4-4669-ac4f-b8f135f307e5,DISK], DatanodeInfoWithStorage[127.0.0.1:46033,DS-474262b9-0524-4bad-a149-b037a30327a1,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44175,DS-76ff9888-5bb4-4669-ac4f-b8f135f307e5,DISK], DatanodeInfoWithStorage[127.0.0.1:46033,DS-474262b9-0524-4bad-a149-b037a30327a1,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41445,DS-0a35f7f2-5ce0-491b-bb46-33876f69db82,DISK], DatanodeInfoWithStorage[127.0.0.1:41628,DS-d7c839e3-fd1f-4682-bb6b-66afc9b39dfd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41628,DS-d7c839e3-fd1f-4682-bb6b-66afc9b39dfd,DISK], DatanodeInfoWithStorage[127.0.0.1:41445,DS-0a35f7f2-5ce0-491b-bb46-33876f69db82,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41445,DS-0a35f7f2-5ce0-491b-bb46-33876f69db82,DISK], DatanodeInfoWithStorage[127.0.0.1:41628,DS-d7c839e3-fd1f-4682-bb6b-66afc9b39dfd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41628,DS-d7c839e3-fd1f-4682-bb6b-66afc9b39dfd,DISK], DatanodeInfoWithStorage[127.0.0.1:41445,DS-0a35f7f2-5ce0-491b-bb46-33876f69db82,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43722,DS-1221f593-09c9-4f3f-98dc-8ba4993753ce,DISK], DatanodeInfoWithStorage[127.0.0.1:35831,DS-5e7bd3e5-613f-45d5-9aae-257404e6bf37,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43722,DS-1221f593-09c9-4f3f-98dc-8ba4993753ce,DISK], DatanodeInfoWithStorage[127.0.0.1:35831,DS-5e7bd3e5-613f-45d5-9aae-257404e6bf37,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43722,DS-1221f593-09c9-4f3f-98dc-8ba4993753ce,DISK], DatanodeInfoWithStorage[127.0.0.1:35831,DS-5e7bd3e5-613f-45d5-9aae-257404e6bf37,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43722,DS-1221f593-09c9-4f3f-98dc-8ba4993753ce,DISK], DatanodeInfoWithStorage[127.0.0.1:35831,DS-5e7bd3e5-613f-45d5-9aae-257404e6bf37,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41245,DS-3e7ce97e-9cb5-49c7-8a8f-3752f440b1d9,DISK], DatanodeInfoWithStorage[127.0.0.1:40379,DS-3d4c9e6e-9048-4722-92b2-7d49f8157b95,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41245,DS-3e7ce97e-9cb5-49c7-8a8f-3752f440b1d9,DISK], DatanodeInfoWithStorage[127.0.0.1:40379,DS-3d4c9e6e-9048-4722-92b2-7d49f8157b95,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41245,DS-3e7ce97e-9cb5-49c7-8a8f-3752f440b1d9,DISK], DatanodeInfoWithStorage[127.0.0.1:40379,DS-3d4c9e6e-9048-4722-92b2-7d49f8157b95,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41245,DS-3e7ce97e-9cb5-49c7-8a8f-3752f440b1d9,DISK], DatanodeInfoWithStorage[127.0.0.1:40379,DS-3d4c9e6e-9048-4722-92b2-7d49f8157b95,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35916,DS-963fe7dc-423c-4853-a706-b164b08bacd3,DISK], DatanodeInfoWithStorage[127.0.0.1:35326,DS-fb39a81b-654b-423d-a213-2493feb911a5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35916,DS-963fe7dc-423c-4853-a706-b164b08bacd3,DISK], DatanodeInfoWithStorage[127.0.0.1:35326,DS-fb39a81b-654b-423d-a213-2493feb911a5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35916,DS-963fe7dc-423c-4853-a706-b164b08bacd3,DISK], DatanodeInfoWithStorage[127.0.0.1:35326,DS-fb39a81b-654b-423d-a213-2493feb911a5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35916,DS-963fe7dc-423c-4853-a706-b164b08bacd3,DISK], DatanodeInfoWithStorage[127.0.0.1:35326,DS-fb39a81b-654b-423d-a213-2493feb911a5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1 has not been updated !
tr.result is -1
v1v1 or v2v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 60
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#testNameConflictWhenSplit1
reconfPoint: -1
result: -1
failureMessage: 
stackTrace: 

v1v2 failed with probability 50 out of 50
v1v1v2v2 failed with probability 19 out of 50
result: might be true error
Total execution time in seconds : 19530

reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplicationFailover#testKillRS
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplicationFailover#testKillRS
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38518,DS-4eb01cfb-f824-4a3b-96b5-c5cfdb3c535e,DISK], DatanodeInfoWithStorage[127.0.0.1:36761,DS-de622778-c32f-4241-9a7f-bfac30f52698,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36761,DS-de622778-c32f-4241-9a7f-bfac30f52698,DISK], DatanodeInfoWithStorage[127.0.0.1:38518,DS-4eb01cfb-f824-4a3b-96b5-c5cfdb3c535e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38518,DS-4eb01cfb-f824-4a3b-96b5-c5cfdb3c535e,DISK], DatanodeInfoWithStorage[127.0.0.1:36761,DS-de622778-c32f-4241-9a7f-bfac30f52698,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36761,DS-de622778-c32f-4241-9a7f-bfac30f52698,DISK], DatanodeInfoWithStorage[127.0.0.1:38518,DS-4eb01cfb-f824-4a3b-96b5-c5cfdb3c535e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplicationFailover#testKillRS
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38369,DS-6c447766-6fcc-4c83-a4ff-91b643e54dee,DISK], DatanodeInfoWithStorage[127.0.0.1:39920,DS-2af6ca55-839c-454c-a4a3-1e554ec63a6f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38369,DS-6c447766-6fcc-4c83-a4ff-91b643e54dee,DISK], DatanodeInfoWithStorage[127.0.0.1:39920,DS-2af6ca55-839c-454c-a4a3-1e554ec63a6f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38369,DS-6c447766-6fcc-4c83-a4ff-91b643e54dee,DISK], DatanodeInfoWithStorage[127.0.0.1:39920,DS-2af6ca55-839c-454c-a4a3-1e554ec63a6f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38369,DS-6c447766-6fcc-4c83-a4ff-91b643e54dee,DISK], DatanodeInfoWithStorage[127.0.0.1:39920,DS-2af6ca55-839c-454c-a4a3-1e554ec63a6f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplicationFailover#testKillRS
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41508,DS-cd31dc55-1e21-4d46-9223-e8be372374b5,DISK], DatanodeInfoWithStorage[127.0.0.1:33692,DS-16af71d2-abca-4f93-ab05-79a088c3b226,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41508,DS-cd31dc55-1e21-4d46-9223-e8be372374b5,DISK], DatanodeInfoWithStorage[127.0.0.1:33692,DS-16af71d2-abca-4f93-ab05-79a088c3b226,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41508,DS-cd31dc55-1e21-4d46-9223-e8be372374b5,DISK], DatanodeInfoWithStorage[127.0.0.1:33692,DS-16af71d2-abca-4f93-ab05-79a088c3b226,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41508,DS-cd31dc55-1e21-4d46-9223-e8be372374b5,DISK], DatanodeInfoWithStorage[127.0.0.1:33692,DS-16af71d2-abca-4f93-ab05-79a088c3b226,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplicationFailover#testKillRS
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41746,DS-5ab17468-6e5e-4457-9770-82bd83bea48b,DISK], DatanodeInfoWithStorage[127.0.0.1:44476,DS-52611320-835d-431a-bd62-62ae7cfd42b5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44476,DS-52611320-835d-431a-bd62-62ae7cfd42b5,DISK], DatanodeInfoWithStorage[127.0.0.1:41746,DS-5ab17468-6e5e-4457-9770-82bd83bea48b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41746,DS-5ab17468-6e5e-4457-9770-82bd83bea48b,DISK], DatanodeInfoWithStorage[127.0.0.1:44476,DS-52611320-835d-431a-bd62-62ae7cfd42b5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44476,DS-52611320-835d-431a-bd62-62ae7cfd42b5,DISK], DatanodeInfoWithStorage[127.0.0.1:41746,DS-5ab17468-6e5e-4457-9770-82bd83bea48b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplicationFailover#testKillRS
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39537,DS-a1d24bda-0567-427c-b31d-1f9c114d7ba5,DISK], DatanodeInfoWithStorage[127.0.0.1:37557,DS-c2e36433-1882-45a1-b8f8-a06b21bf3e85,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37557,DS-c2e36433-1882-45a1-b8f8-a06b21bf3e85,DISK], DatanodeInfoWithStorage[127.0.0.1:39537,DS-a1d24bda-0567-427c-b31d-1f9c114d7ba5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39537,DS-a1d24bda-0567-427c-b31d-1f9c114d7ba5,DISK], DatanodeInfoWithStorage[127.0.0.1:37557,DS-c2e36433-1882-45a1-b8f8-a06b21bf3e85,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37557,DS-c2e36433-1882-45a1-b8f8-a06b21bf3e85,DISK], DatanodeInfoWithStorage[127.0.0.1:39537,DS-a1d24bda-0567-427c-b31d-1f9c114d7ba5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplicationFailover#testKillRS
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36322,DS-69a80740-fa3a-41b4-bbda-2c1b83479d0d,DISK], DatanodeInfoWithStorage[127.0.0.1:43424,DS-1a9e4e2c-e78b-4b6b-9eb0-e425efd721a6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43424,DS-1a9e4e2c-e78b-4b6b-9eb0-e425efd721a6,DISK], DatanodeInfoWithStorage[127.0.0.1:36322,DS-69a80740-fa3a-41b4-bbda-2c1b83479d0d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36322,DS-69a80740-fa3a-41b4-bbda-2c1b83479d0d,DISK], DatanodeInfoWithStorage[127.0.0.1:43424,DS-1a9e4e2c-e78b-4b6b-9eb0-e425efd721a6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43424,DS-1a9e4e2c-e78b-4b6b-9eb0-e425efd721a6,DISK], DatanodeInfoWithStorage[127.0.0.1:36322,DS-69a80740-fa3a-41b4-bbda-2c1b83479d0d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplicationFailover#testKillRS
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45188,DS-260c2cf5-d428-4f43-a719-349121f24022,DISK], DatanodeInfoWithStorage[127.0.0.1:35080,DS-b5f663a5-e863-4d9e-a113-87693900800a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45188,DS-260c2cf5-d428-4f43-a719-349121f24022,DISK], DatanodeInfoWithStorage[127.0.0.1:35080,DS-b5f663a5-e863-4d9e-a113-87693900800a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45188,DS-260c2cf5-d428-4f43-a719-349121f24022,DISK], DatanodeInfoWithStorage[127.0.0.1:35080,DS-b5f663a5-e863-4d9e-a113-87693900800a,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45188,DS-260c2cf5-d428-4f43-a719-349121f24022,DISK], DatanodeInfoWithStorage[127.0.0.1:35080,DS-b5f663a5-e863-4d9e-a113-87693900800a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplicationFailover#testKillRS
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42735,DS-f7a6d4eb-836d-4627-8113-06c600a78099,DISK], DatanodeInfoWithStorage[127.0.0.1:37012,DS-017bfb4d-f6c7-4ac4-bd78-672ce264e5f5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42735,DS-f7a6d4eb-836d-4627-8113-06c600a78099,DISK], DatanodeInfoWithStorage[127.0.0.1:37012,DS-017bfb4d-f6c7-4ac4-bd78-672ce264e5f5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42735,DS-f7a6d4eb-836d-4627-8113-06c600a78099,DISK], DatanodeInfoWithStorage[127.0.0.1:37012,DS-017bfb4d-f6c7-4ac4-bd78-672ce264e5f5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42735,DS-f7a6d4eb-836d-4627-8113-06c600a78099,DISK], DatanodeInfoWithStorage[127.0.0.1:37012,DS-017bfb4d-f6c7-4ac4-bd78-672ce264e5f5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplicationFailover#testKillRS
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46480,DS-3655e309-e165-4711-a7da-75acfcf6061c,DISK], DatanodeInfoWithStorage[127.0.0.1:39715,DS-d39223d0-38db-47a7-9b4d-fee730587b59,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39715,DS-d39223d0-38db-47a7-9b4d-fee730587b59,DISK], DatanodeInfoWithStorage[127.0.0.1:46480,DS-3655e309-e165-4711-a7da-75acfcf6061c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46480,DS-3655e309-e165-4711-a7da-75acfcf6061c,DISK], DatanodeInfoWithStorage[127.0.0.1:39715,DS-d39223d0-38db-47a7-9b4d-fee730587b59,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39715,DS-d39223d0-38db-47a7-9b4d-fee730587b59,DISK], DatanodeInfoWithStorage[127.0.0.1:46480,DS-3655e309-e165-4711-a7da-75acfcf6061c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplicationFailover#testKillRS
reconfPoint: -1
result: -1
failureMessage: Waiting timed out after [30,000] msec Not enough entries replicated
stackTrace: java.lang.AssertionError: Waiting timed out after [30,000] msec Not enough entries replicated
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.hadoop.hbase.Waiter.waitFor(Waiter.java:203)
	at org.apache.hadoop.hbase.Waiter.waitFor(Waiter.java:137)
	at org.apache.hadoop.hbase.HBaseCommonTestingUtility.waitFor(HBaseCommonTestingUtility.java:251)
	at org.apache.hadoop.hbase.replication.SerialReplicationTestBase.waitUntilReplicationDone(SerialReplicationTestBase.java:190)
	at org.apache.hadoop.hbase.replication.SerialReplicationTestBase.enablePeerAndWaitUntilReplicationDone(SerialReplicationTestBase.java:214)
	at org.apache.hadoop.hbase.replication.TestSerialReplicationFailover.testKillRS(TestSerialReplicationFailover.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 2393

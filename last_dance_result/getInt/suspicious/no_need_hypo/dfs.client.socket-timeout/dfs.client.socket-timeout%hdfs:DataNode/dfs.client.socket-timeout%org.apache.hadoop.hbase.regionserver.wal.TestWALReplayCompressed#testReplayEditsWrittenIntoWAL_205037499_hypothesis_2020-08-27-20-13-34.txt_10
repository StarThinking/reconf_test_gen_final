reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL
reconfPoint: -2
result: -1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL
reconfPoint: -2
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL
reconfPoint: -2
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45040,DS-be87492a-5706-42c8-b561-2ac88a741c55,DISK], DatanodeInfoWithStorage[127.0.0.1:44474,DS-c1271d91-b832-4186-a84f-e3fa00093cdf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44474,DS-c1271d91-b832-4186-a84f-e3fa00093cdf,DISK], DatanodeInfoWithStorage[127.0.0.1:45040,DS-be87492a-5706-42c8-b561-2ac88a741c55,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45040,DS-be87492a-5706-42c8-b561-2ac88a741c55,DISK], DatanodeInfoWithStorage[127.0.0.1:44474,DS-c1271d91-b832-4186-a84f-e3fa00093cdf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44474,DS-c1271d91-b832-4186-a84f-e3fa00093cdf,DISK], DatanodeInfoWithStorage[127.0.0.1:45040,DS-be87492a-5706-42c8-b561-2ac88a741c55,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39084,DS-c04e0d71-61c1-4677-8f8d-08dca06bff64,DISK], DatanodeInfoWithStorage[127.0.0.1:34914,DS-741f3c74-2b23-4351-a962-8fd94e6ab37d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39084,DS-c04e0d71-61c1-4677-8f8d-08dca06bff64,DISK], DatanodeInfoWithStorage[127.0.0.1:34914,DS-741f3c74-2b23-4351-a962-8fd94e6ab37d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39084,DS-c04e0d71-61c1-4677-8f8d-08dca06bff64,DISK], DatanodeInfoWithStorage[127.0.0.1:34914,DS-741f3c74-2b23-4351-a962-8fd94e6ab37d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39084,DS-c04e0d71-61c1-4677-8f8d-08dca06bff64,DISK], DatanodeInfoWithStorage[127.0.0.1:34914,DS-741f3c74-2b23-4351-a962-8fd94e6ab37d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33887,DS-7141c3dc-ca5c-4001-8826-f45b909ebced,DISK], DatanodeInfoWithStorage[127.0.0.1:34630,DS-1383f53f-d910-4509-b46a-ff2fbadf4130,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33887,DS-7141c3dc-ca5c-4001-8826-f45b909ebced,DISK], DatanodeInfoWithStorage[127.0.0.1:34630,DS-1383f53f-d910-4509-b46a-ff2fbadf4130,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33887,DS-7141c3dc-ca5c-4001-8826-f45b909ebced,DISK], DatanodeInfoWithStorage[127.0.0.1:34630,DS-1383f53f-d910-4509-b46a-ff2fbadf4130,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33887,DS-7141c3dc-ca5c-4001-8826-f45b909ebced,DISK], DatanodeInfoWithStorage[127.0.0.1:34630,DS-1383f53f-d910-4509-b46a-ff2fbadf4130,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL
reconfPoint: -2
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40000,DS-c98857a8-697a-4b2e-b39d-981cf35a28f7,DISK], DatanodeInfoWithStorage[127.0.0.1:46769,DS-14135049-662e-4cfb-896c-85117992af45,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40000,DS-c98857a8-697a-4b2e-b39d-981cf35a28f7,DISK], DatanodeInfoWithStorage[127.0.0.1:46769,DS-14135049-662e-4cfb-896c-85117992af45,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40000,DS-c98857a8-697a-4b2e-b39d-981cf35a28f7,DISK], DatanodeInfoWithStorage[127.0.0.1:46769,DS-14135049-662e-4cfb-896c-85117992af45,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40000,DS-c98857a8-697a-4b2e-b39d-981cf35a28f7,DISK], DatanodeInfoWithStorage[127.0.0.1:46769,DS-14135049-662e-4cfb-896c-85117992af45,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL
reconfPoint: -2
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL
reconfPoint: -2
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed#testReplayEditsWrittenIntoWAL
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42445,DS-6308fa93-72fa-441d-a19a-4550633ebce9,DISK], DatanodeInfoWithStorage[127.0.0.1:46718,DS-f49d7777-4d13-450f-8929-0d71e7ddedd9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46718,DS-f49d7777-4d13-450f-8929-0d71e7ddedd9,DISK], DatanodeInfoWithStorage[127.0.0.1:42445,DS-6308fa93-72fa-441d-a19a-4550633ebce9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42445,DS-6308fa93-72fa-441d-a19a-4550633ebce9,DISK], DatanodeInfoWithStorage[127.0.0.1:46718,DS-f49d7777-4d13-450f-8929-0d71e7ddedd9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46718,DS-f49d7777-4d13-450f-8929-0d71e7ddedd9,DISK], DatanodeInfoWithStorage[127.0.0.1:42445,DS-6308fa93-72fa-441d-a19a-4550633ebce9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 2779

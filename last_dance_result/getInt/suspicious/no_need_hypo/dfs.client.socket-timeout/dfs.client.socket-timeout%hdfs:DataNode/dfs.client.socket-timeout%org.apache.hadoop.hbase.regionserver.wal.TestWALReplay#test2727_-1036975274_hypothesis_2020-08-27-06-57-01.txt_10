reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727
reconfPoint: -3
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36533,DS-8ed8877c-6482-4c66-958f-e4821a151538,DISK], DatanodeInfoWithStorage[127.0.0.1:40495,DS-8c12f0e4-de4f-42af-82a2-486343b363a9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36533,DS-8ed8877c-6482-4c66-958f-e4821a151538,DISK], DatanodeInfoWithStorage[127.0.0.1:40495,DS-8c12f0e4-de4f-42af-82a2-486343b363a9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36533,DS-8ed8877c-6482-4c66-958f-e4821a151538,DISK], DatanodeInfoWithStorage[127.0.0.1:40495,DS-8c12f0e4-de4f-42af-82a2-486343b363a9,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36533,DS-8ed8877c-6482-4c66-958f-e4821a151538,DISK], DatanodeInfoWithStorage[127.0.0.1:40495,DS-8c12f0e4-de4f-42af-82a2-486343b363a9,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39861,DS-d0c411e1-b0aa-4c68-936e-a11b20a390d6,DISK], DatanodeInfoWithStorage[127.0.0.1:42304,DS-08253e1e-9410-4148-9bb0-21d100cb8db8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42304,DS-08253e1e-9410-4148-9bb0-21d100cb8db8,DISK], DatanodeInfoWithStorage[127.0.0.1:39861,DS-d0c411e1-b0aa-4c68-936e-a11b20a390d6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39861,DS-d0c411e1-b0aa-4c68-936e-a11b20a390d6,DISK], DatanodeInfoWithStorage[127.0.0.1:42304,DS-08253e1e-9410-4148-9bb0-21d100cb8db8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42304,DS-08253e1e-9410-4148-9bb0-21d100cb8db8,DISK], DatanodeInfoWithStorage[127.0.0.1:39861,DS-d0c411e1-b0aa-4c68-936e-a11b20a390d6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727
reconfPoint: -3
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44430,DS-fcda8180-3f88-4aab-adf1-78663c3b824e,DISK], DatanodeInfoWithStorage[127.0.0.1:42593,DS-85d2e499-7c8c-40b1-8cd4-c6584f24113d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44430,DS-fcda8180-3f88-4aab-adf1-78663c3b824e,DISK], DatanodeInfoWithStorage[127.0.0.1:42593,DS-85d2e499-7c8c-40b1-8cd4-c6584f24113d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44430,DS-fcda8180-3f88-4aab-adf1-78663c3b824e,DISK], DatanodeInfoWithStorage[127.0.0.1:42593,DS-85d2e499-7c8c-40b1-8cd4-c6584f24113d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44430,DS-fcda8180-3f88-4aab-adf1-78663c3b824e,DISK], DatanodeInfoWithStorage[127.0.0.1:42593,DS-85d2e499-7c8c-40b1-8cd4-c6584f24113d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727
reconfPoint: -3
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39774,DS-be1bc002-193f-45b7-8423-81281f679c70,DISK], DatanodeInfoWithStorage[127.0.0.1:41914,DS-4771f061-ab79-44b3-bd25-afcc3edf5b8b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41914,DS-4771f061-ab79-44b3-bd25-afcc3edf5b8b,DISK], DatanodeInfoWithStorage[127.0.0.1:39774,DS-be1bc002-193f-45b7-8423-81281f679c70,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39774,DS-be1bc002-193f-45b7-8423-81281f679c70,DISK], DatanodeInfoWithStorage[127.0.0.1:41914,DS-4771f061-ab79-44b3-bd25-afcc3edf5b8b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41914,DS-4771f061-ab79-44b3-bd25-afcc3edf5b8b,DISK], DatanodeInfoWithStorage[127.0.0.1:39774,DS-be1bc002-193f-45b7-8423-81281f679c70,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46311,DS-7dc30e71-68ba-4b58-9510-21f9b66ea158,DISK], DatanodeInfoWithStorage[127.0.0.1:39705,DS-2f2c2597-039f-4fe5-8ac2-47d1a3239ad5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46311,DS-7dc30e71-68ba-4b58-9510-21f9b66ea158,DISK], DatanodeInfoWithStorage[127.0.0.1:39705,DS-2f2c2597-039f-4fe5-8ac2-47d1a3239ad5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46311,DS-7dc30e71-68ba-4b58-9510-21f9b66ea158,DISK], DatanodeInfoWithStorage[127.0.0.1:39705,DS-2f2c2597-039f-4fe5-8ac2-47d1a3239ad5,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46311,DS-7dc30e71-68ba-4b58-9510-21f9b66ea158,DISK], DatanodeInfoWithStorage[127.0.0.1:39705,DS-2f2c2597-039f-4fe5-8ac2-47d1a3239ad5,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727
reconfPoint: -3
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727
reconfPoint: -3
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
Warn: test org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727 has not been updated !
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.regionserver.wal.TestWALReplay#test2727
reconfPoint: -3
result: -1
failureMessage: 
stackTrace: 


Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 2798

reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38328,DS-d77c4b90-bb1d-47a9-90fd-992152918f3b,DISK], DatanodeInfoWithStorage[127.0.0.1:40182,DS-2bcfa3e6-6d0c-4aad-a324-7f7cf1ad21ec,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38328,DS-d77c4b90-bb1d-47a9-90fd-992152918f3b,DISK], DatanodeInfoWithStorage[127.0.0.1:40182,DS-2bcfa3e6-6d0c-4aad-a324-7f7cf1ad21ec,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38328,DS-d77c4b90-bb1d-47a9-90fd-992152918f3b,DISK], DatanodeInfoWithStorage[127.0.0.1:40182,DS-2bcfa3e6-6d0c-4aad-a324-7f7cf1ad21ec,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38328,DS-d77c4b90-bb1d-47a9-90fd-992152918f3b,DISK], DatanodeInfoWithStorage[127.0.0.1:40182,DS-2bcfa3e6-6d0c-4aad-a324-7f7cf1ad21ec,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40821,DS-5e7cb066-4c8e-44cf-ae7a-c3a18a93b874,DISK], DatanodeInfoWithStorage[127.0.0.1:36630,DS-8f92f4fc-072e-4c9f-bf22-5f52c2330401,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40821,DS-5e7cb066-4c8e-44cf-ae7a-c3a18a93b874,DISK], DatanodeInfoWithStorage[127.0.0.1:36630,DS-8f92f4fc-072e-4c9f-bf22-5f52c2330401,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40821,DS-5e7cb066-4c8e-44cf-ae7a-c3a18a93b874,DISK], DatanodeInfoWithStorage[127.0.0.1:36630,DS-8f92f4fc-072e-4c9f-bf22-5f52c2330401,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40821,DS-5e7cb066-4c8e-44cf-ae7a-c3a18a93b874,DISK], DatanodeInfoWithStorage[127.0.0.1:36630,DS-8f92f4fc-072e-4c9f-bf22-5f52c2330401,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33619,DS-530b9076-d428-46ce-939a-f39ca047f441,DISK], DatanodeInfoWithStorage[127.0.0.1:34916,DS-022516c4-829c-465a-9aa9-03a873401589,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34916,DS-022516c4-829c-465a-9aa9-03a873401589,DISK], DatanodeInfoWithStorage[127.0.0.1:33619,DS-530b9076-d428-46ce-939a-f39ca047f441,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33619,DS-530b9076-d428-46ce-939a-f39ca047f441,DISK], DatanodeInfoWithStorage[127.0.0.1:34916,DS-022516c4-829c-465a-9aa9-03a873401589,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34916,DS-022516c4-829c-465a-9aa9-03a873401589,DISK], DatanodeInfoWithStorage[127.0.0.1:33619,DS-530b9076-d428-46ce-939a-f39ca047f441,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46001,DS-72ee963a-3f8f-41c1-ac90-960bd9522da8,DISK], DatanodeInfoWithStorage[127.0.0.1:40391,DS-00497db6-09ce-47d4-80d4-4a61a6375c9c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40391,DS-00497db6-09ce-47d4-80d4-4a61a6375c9c,DISK], DatanodeInfoWithStorage[127.0.0.1:46001,DS-72ee963a-3f8f-41c1-ac90-960bd9522da8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46001,DS-72ee963a-3f8f-41c1-ac90-960bd9522da8,DISK], DatanodeInfoWithStorage[127.0.0.1:40391,DS-00497db6-09ce-47d4-80d4-4a61a6375c9c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40391,DS-00497db6-09ce-47d4-80d4-4a61a6375c9c,DISK], DatanodeInfoWithStorage[127.0.0.1:46001,DS-72ee963a-3f8f-41c1-ac90-960bd9522da8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37656,DS-179dbb32-c5ae-45b3-9b48-708c34616a93,DISK], DatanodeInfoWithStorage[127.0.0.1:40584,DS-fd838d80-0cfc-41fd-933a-bfebee201ba2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37656,DS-179dbb32-c5ae-45b3-9b48-708c34616a93,DISK], DatanodeInfoWithStorage[127.0.0.1:40584,DS-fd838d80-0cfc-41fd-933a-bfebee201ba2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37656,DS-179dbb32-c5ae-45b3-9b48-708c34616a93,DISK], DatanodeInfoWithStorage[127.0.0.1:40584,DS-fd838d80-0cfc-41fd-933a-bfebee201ba2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37656,DS-179dbb32-c5ae-45b3-9b48-708c34616a93,DISK], DatanodeInfoWithStorage[127.0.0.1:40584,DS-fd838d80-0cfc-41fd-933a-bfebee201ba2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45953,DS-fc5f3b12-56a7-4b38-b402-1136a33c3a65,DISK], DatanodeInfoWithStorage[127.0.0.1:40572,DS-1538abd7-cd4c-4e62-938a-228657a44ea6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45953,DS-fc5f3b12-56a7-4b38-b402-1136a33c3a65,DISK], DatanodeInfoWithStorage[127.0.0.1:40572,DS-1538abd7-cd4c-4e62-938a-228657a44ea6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45953,DS-fc5f3b12-56a7-4b38-b402-1136a33c3a65,DISK], DatanodeInfoWithStorage[127.0.0.1:40572,DS-1538abd7-cd4c-4e62-938a-228657a44ea6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45953,DS-fc5f3b12-56a7-4b38-b402-1136a33c3a65,DISK], DatanodeInfoWithStorage[127.0.0.1:40572,DS-1538abd7-cd4c-4e62-938a-228657a44ea6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38340,DS-fb2d57b7-741b-4bc2-b220-4af0f7bf3c4c,DISK], DatanodeInfoWithStorage[127.0.0.1:39857,DS-4bec4b38-667b-4266-a403-1974857e0c46,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38340,DS-fb2d57b7-741b-4bc2-b220-4af0f7bf3c4c,DISK], DatanodeInfoWithStorage[127.0.0.1:39857,DS-4bec4b38-667b-4266-a403-1974857e0c46,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38340,DS-fb2d57b7-741b-4bc2-b220-4af0f7bf3c4c,DISK], DatanodeInfoWithStorage[127.0.0.1:39857,DS-4bec4b38-667b-4266-a403-1974857e0c46,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38340,DS-fb2d57b7-741b-4bc2-b220-4af0f7bf3c4c,DISK], DatanodeInfoWithStorage[127.0.0.1:39857,DS-4bec4b38-667b-4266-a403-1974857e0c46,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36734,DS-3956cb44-946a-49e0-8639-d4a8e19cd05d,DISK], DatanodeInfoWithStorage[127.0.0.1:46073,DS-19775383-3b76-4ba7-8973-f7ba57e603cd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36734,DS-3956cb44-946a-49e0-8639-d4a8e19cd05d,DISK], DatanodeInfoWithStorage[127.0.0.1:46073,DS-19775383-3b76-4ba7-8973-f7ba57e603cd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36734,DS-3956cb44-946a-49e0-8639-d4a8e19cd05d,DISK], DatanodeInfoWithStorage[127.0.0.1:46073,DS-19775383-3b76-4ba7-8973-f7ba57e603cd,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36734,DS-3956cb44-946a-49e0-8639-d4a8e19cd05d,DISK], DatanodeInfoWithStorage[127.0.0.1:46073,DS-19775383-3b76-4ba7-8973-f7ba57e603cd,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38004,DS-f90b0302-1fb0-4cca-bbf7-451ac962161d,DISK], DatanodeInfoWithStorage[127.0.0.1:46575,DS-3cdf7353-0292-4fc2-a20f-3bd84b7bc740,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46575,DS-3cdf7353-0292-4fc2-a20f-3bd84b7bc740,DISK], DatanodeInfoWithStorage[127.0.0.1:38004,DS-f90b0302-1fb0-4cca-bbf7-451ac962161d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38004,DS-f90b0302-1fb0-4cca-bbf7-451ac962161d,DISK], DatanodeInfoWithStorage[127.0.0.1:46575,DS-3cdf7353-0292-4fc2-a20f-3bd84b7bc740,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46575,DS-3cdf7353-0292-4fc2-a20f-3bd84b7bc740,DISK], DatanodeInfoWithStorage[127.0.0.1:38004,DS-f90b0302-1fb0-4cca-bbf7-451ac962161d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRemoveSerialFlag
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45200,DS-49237ff7-bfbe-44b2-9084-936e6333bf63,DISK], DatanodeInfoWithStorage[127.0.0.1:38746,DS-b613aaa1-03cf-48c1-ac5d-92cfe67ca076,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45200,DS-49237ff7-bfbe-44b2-9084-936e6333bf63,DISK], DatanodeInfoWithStorage[127.0.0.1:38746,DS-b613aaa1-03cf-48c1-ac5d-92cfe67ca076,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45200,DS-49237ff7-bfbe-44b2-9084-936e6333bf63,DISK], DatanodeInfoWithStorage[127.0.0.1:38746,DS-b613aaa1-03cf-48c1-ac5d-92cfe67ca076,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45200,DS-49237ff7-bfbe-44b2-9084-936e6333bf63,DISK], DatanodeInfoWithStorage[127.0.0.1:38746,DS-b613aaa1-03cf-48c1-ac5d-92cfe67ca076,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 2198

reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.wal.TestFSHLogProvider#testLogCleaning
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.wal.TestFSHLogProvider#testLogCleaning
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39330,DS-1566a19f-a1b5-46c7-9589-382a1f23f773,DISK], DatanodeInfoWithStorage[127.0.0.1:34734,DS-8094b2ea-78a1-4c75-9a4b-4c279f67948c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34734,DS-8094b2ea-78a1-4c75-9a4b-4c279f67948c,DISK], DatanodeInfoWithStorage[127.0.0.1:39330,DS-1566a19f-a1b5-46c7-9589-382a1f23f773,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39330,DS-1566a19f-a1b5-46c7-9589-382a1f23f773,DISK], DatanodeInfoWithStorage[127.0.0.1:34734,DS-8094b2ea-78a1-4c75-9a4b-4c279f67948c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34734,DS-8094b2ea-78a1-4c75-9a4b-4c279f67948c,DISK], DatanodeInfoWithStorage[127.0.0.1:39330,DS-1566a19f-a1b5-46c7-9589-382a1f23f773,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.wal.TestFSHLogProvider#testLogCleaning
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41273,DS-e9bd00fb-6e7f-4e0a-a949-d84a0728541b,DISK], DatanodeInfoWithStorage[127.0.0.1:36095,DS-2bb1a2a6-c9f0-40f4-8a6d-9b14765202e6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41273,DS-e9bd00fb-6e7f-4e0a-a949-d84a0728541b,DISK], DatanodeInfoWithStorage[127.0.0.1:36095,DS-2bb1a2a6-c9f0-40f4-8a6d-9b14765202e6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41273,DS-e9bd00fb-6e7f-4e0a-a949-d84a0728541b,DISK], DatanodeInfoWithStorage[127.0.0.1:36095,DS-2bb1a2a6-c9f0-40f4-8a6d-9b14765202e6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41273,DS-e9bd00fb-6e7f-4e0a-a949-d84a0728541b,DISK], DatanodeInfoWithStorage[127.0.0.1:36095,DS-2bb1a2a6-c9f0-40f4-8a6d-9b14765202e6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.wal.TestFSHLogProvider#testLogCleaning
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35650,DS-51a924a8-8df3-4347-92a9-dd419fbb9fb4,DISK], DatanodeInfoWithStorage[127.0.0.1:33012,DS-fb0f8e34-869c-4a32-b849-c48975007edb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35650,DS-51a924a8-8df3-4347-92a9-dd419fbb9fb4,DISK], DatanodeInfoWithStorage[127.0.0.1:33012,DS-fb0f8e34-869c-4a32-b849-c48975007edb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35650,DS-51a924a8-8df3-4347-92a9-dd419fbb9fb4,DISK], DatanodeInfoWithStorage[127.0.0.1:33012,DS-fb0f8e34-869c-4a32-b849-c48975007edb,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35650,DS-51a924a8-8df3-4347-92a9-dd419fbb9fb4,DISK], DatanodeInfoWithStorage[127.0.0.1:33012,DS-fb0f8e34-869c-4a32-b849-c48975007edb,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.wal.TestFSHLogProvider#testLogCleaning
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34191,DS-3988d7f8-c6a6-46fc-b202-a2a6a365783c,DISK], DatanodeInfoWithStorage[127.0.0.1:33539,DS-cd55fd09-70ab-46ee-bd73-544ee260cbbe,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33539,DS-cd55fd09-70ab-46ee-bd73-544ee260cbbe,DISK], DatanodeInfoWithStorage[127.0.0.1:34191,DS-3988d7f8-c6a6-46fc-b202-a2a6a365783c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34191,DS-3988d7f8-c6a6-46fc-b202-a2a6a365783c,DISK], DatanodeInfoWithStorage[127.0.0.1:33539,DS-cd55fd09-70ab-46ee-bd73-544ee260cbbe,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33539,DS-cd55fd09-70ab-46ee-bd73-544ee260cbbe,DISK], DatanodeInfoWithStorage[127.0.0.1:34191,DS-3988d7f8-c6a6-46fc-b202-a2a6a365783c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.wal.TestFSHLogProvider#testLogCleaning
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46438,DS-655d718d-0a05-435d-a1b9-7ecfde429cee,DISK], DatanodeInfoWithStorage[127.0.0.1:37489,DS-09588b39-10d2-4f58-96de-3f28b0c8b80d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46438,DS-655d718d-0a05-435d-a1b9-7ecfde429cee,DISK], DatanodeInfoWithStorage[127.0.0.1:37489,DS-09588b39-10d2-4f58-96de-3f28b0c8b80d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46438,DS-655d718d-0a05-435d-a1b9-7ecfde429cee,DISK], DatanodeInfoWithStorage[127.0.0.1:37489,DS-09588b39-10d2-4f58-96de-3f28b0c8b80d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46438,DS-655d718d-0a05-435d-a1b9-7ecfde429cee,DISK], DatanodeInfoWithStorage[127.0.0.1:37489,DS-09588b39-10d2-4f58-96de-3f28b0c8b80d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.wal.TestFSHLogProvider#testLogCleaning
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43389,DS-f8ad42b5-c218-4166-a78c-4d53f115b512,DISK], DatanodeInfoWithStorage[127.0.0.1:35685,DS-fcacebb8-0d63-4998-9b28-57644895056c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43389,DS-f8ad42b5-c218-4166-a78c-4d53f115b512,DISK], DatanodeInfoWithStorage[127.0.0.1:35685,DS-fcacebb8-0d63-4998-9b28-57644895056c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43389,DS-f8ad42b5-c218-4166-a78c-4d53f115b512,DISK], DatanodeInfoWithStorage[127.0.0.1:35685,DS-fcacebb8-0d63-4998-9b28-57644895056c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43389,DS-f8ad42b5-c218-4166-a78c-4d53f115b512,DISK], DatanodeInfoWithStorage[127.0.0.1:35685,DS-fcacebb8-0d63-4998-9b28-57644895056c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.wal.TestFSHLogProvider#testLogCleaning
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39440,DS-b90bed84-06d2-4e6e-b75a-82b6ede03816,DISK], DatanodeInfoWithStorage[127.0.0.1:46161,DS-c41fb629-00fa-4a15-8acc-ff03270e0c6b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39440,DS-b90bed84-06d2-4e6e-b75a-82b6ede03816,DISK], DatanodeInfoWithStorage[127.0.0.1:46161,DS-c41fb629-00fa-4a15-8acc-ff03270e0c6b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39440,DS-b90bed84-06d2-4e6e-b75a-82b6ede03816,DISK], DatanodeInfoWithStorage[127.0.0.1:46161,DS-c41fb629-00fa-4a15-8acc-ff03270e0c6b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39440,DS-b90bed84-06d2-4e6e-b75a-82b6ede03816,DISK], DatanodeInfoWithStorage[127.0.0.1:46161,DS-c41fb629-00fa-4a15-8acc-ff03270e0c6b,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.wal.TestFSHLogProvider#testLogCleaning
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44971,DS-c1614a6b-9d7e-4f66-a608-9b0868fa0c53,DISK], DatanodeInfoWithStorage[127.0.0.1:41365,DS-f842830f-346a-4814-a89b-54ebc472e1d2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44971,DS-c1614a6b-9d7e-4f66-a608-9b0868fa0c53,DISK], DatanodeInfoWithStorage[127.0.0.1:41365,DS-f842830f-346a-4814-a89b-54ebc472e1d2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44971,DS-c1614a6b-9d7e-4f66-a608-9b0868fa0c53,DISK], DatanodeInfoWithStorage[127.0.0.1:41365,DS-f842830f-346a-4814-a89b-54ebc472e1d2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44971,DS-c1614a6b-9d7e-4f66-a608-9b0868fa0c53,DISK], DatanodeInfoWithStorage[127.0.0.1:41365,DS-f842830f-346a-4814-a89b-54ebc472e1d2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.wal.TestFSHLogProvider#testLogCleaning
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42162,DS-f479c2be-3461-495c-852b-c560c715811c,DISK], DatanodeInfoWithStorage[127.0.0.1:33635,DS-a117bf5f-499f-4312-85bd-8f063da03c5b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33635,DS-a117bf5f-499f-4312-85bd-8f063da03c5b,DISK], DatanodeInfoWithStorage[127.0.0.1:42162,DS-f479c2be-3461-495c-852b-c560c715811c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42162,DS-f479c2be-3461-495c-852b-c560c715811c,DISK], DatanodeInfoWithStorage[127.0.0.1:33635,DS-a117bf5f-499f-4312-85bd-8f063da03c5b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33635,DS-a117bf5f-499f-4312-85bd-8f063da03c5b,DISK], DatanodeInfoWithStorage[127.0.0.1:42162,DS-f479c2be-3461-495c-852b-c560c715811c,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 5000
testProject: hbase
unitTest: org.apache.hadoop.hbase.wal.TestFSHLogProvider#testLogCleaning
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41839,DS-a80aed76-6ba7-4963-ad2e-9656b79cfc49,DISK], DatanodeInfoWithStorage[127.0.0.1:39727,DS-def0d733-da71-44ac-8d55-bc01024e8f1b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39727,DS-def0d733-da71-44ac-8d55-bc01024e8f1b,DISK], DatanodeInfoWithStorage[127.0.0.1:41839,DS-a80aed76-6ba7-4963-ad2e-9656b79cfc49,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41839,DS-a80aed76-6ba7-4963-ad2e-9656b79cfc49,DISK], DatanodeInfoWithStorage[127.0.0.1:39727,DS-def0d733-da71-44ac-8d55-bc01024e8f1b,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39727,DS-def0d733-da71-44ac-8d55-bc01024e8f1b,DISK], DatanodeInfoWithStorage[127.0.0.1:41839,DS-a80aed76-6ba7-4963-ad2e-9656b79cfc49,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 1449

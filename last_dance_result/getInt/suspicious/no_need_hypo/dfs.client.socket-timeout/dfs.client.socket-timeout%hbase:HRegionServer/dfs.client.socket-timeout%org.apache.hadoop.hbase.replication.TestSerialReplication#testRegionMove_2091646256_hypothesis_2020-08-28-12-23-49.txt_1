reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMove
reconfPoint: -1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMove
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43810,DS-fed7d40d-af42-4c6c-a158-d319c706a075,DISK], DatanodeInfoWithStorage[127.0.0.1:39501,DS-107cab3b-addd-46d0-8588-cc365d896267,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39501,DS-107cab3b-addd-46d0-8588-cc365d896267,DISK], DatanodeInfoWithStorage[127.0.0.1:43810,DS-fed7d40d-af42-4c6c-a158-d319c706a075,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43810,DS-fed7d40d-af42-4c6c-a158-d319c706a075,DISK], DatanodeInfoWithStorage[127.0.0.1:39501,DS-107cab3b-addd-46d0-8588-cc365d896267,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39501,DS-107cab3b-addd-46d0-8588-cc365d896267,DISK], DatanodeInfoWithStorage[127.0.0.1:43810,DS-fed7d40d-af42-4c6c-a158-d319c706a075,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMove
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37898,DS-9dd45bbf-5322-4fad-baf5-233f89d7baf2,DISK], DatanodeInfoWithStorage[127.0.0.1:41080,DS-5b465754-7c89-47f8-80d1-4d20f0ec211c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41080,DS-5b465754-7c89-47f8-80d1-4d20f0ec211c,DISK], DatanodeInfoWithStorage[127.0.0.1:37898,DS-9dd45bbf-5322-4fad-baf5-233f89d7baf2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37898,DS-9dd45bbf-5322-4fad-baf5-233f89d7baf2,DISK], DatanodeInfoWithStorage[127.0.0.1:41080,DS-5b465754-7c89-47f8-80d1-4d20f0ec211c,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41080,DS-5b465754-7c89-47f8-80d1-4d20f0ec211c,DISK], DatanodeInfoWithStorage[127.0.0.1:37898,DS-9dd45bbf-5322-4fad-baf5-233f89d7baf2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMove
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36258,DS-b3673609-776b-4ec9-b61e-88a786392ae1,DISK], DatanodeInfoWithStorage[127.0.0.1:38241,DS-08fcc000-7df0-4dd1-a691-f8822aab21b2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36258,DS-b3673609-776b-4ec9-b61e-88a786392ae1,DISK], DatanodeInfoWithStorage[127.0.0.1:38241,DS-08fcc000-7df0-4dd1-a691-f8822aab21b2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36258,DS-b3673609-776b-4ec9-b61e-88a786392ae1,DISK], DatanodeInfoWithStorage[127.0.0.1:38241,DS-08fcc000-7df0-4dd1-a691-f8822aab21b2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36258,DS-b3673609-776b-4ec9-b61e-88a786392ae1,DISK], DatanodeInfoWithStorage[127.0.0.1:38241,DS-08fcc000-7df0-4dd1-a691-f8822aab21b2,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMove
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42404,DS-93a3fe38-955b-41a1-b353-f618049e8463,DISK], DatanodeInfoWithStorage[127.0.0.1:42760,DS-d8345d9a-9686-4094-8f4a-07cffcdaa5f4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42760,DS-d8345d9a-9686-4094-8f4a-07cffcdaa5f4,DISK], DatanodeInfoWithStorage[127.0.0.1:42404,DS-93a3fe38-955b-41a1-b353-f618049e8463,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42404,DS-93a3fe38-955b-41a1-b353-f618049e8463,DISK], DatanodeInfoWithStorage[127.0.0.1:42760,DS-d8345d9a-9686-4094-8f4a-07cffcdaa5f4,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42760,DS-d8345d9a-9686-4094-8f4a-07cffcdaa5f4,DISK], DatanodeInfoWithStorage[127.0.0.1:42404,DS-93a3fe38-955b-41a1-b353-f618049e8463,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMove
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39861,DS-ee54164b-7c86-49e0-abeb-b2135c69804a,DISK], DatanodeInfoWithStorage[127.0.0.1:40062,DS-6f2644fc-a169-45ca-ae35-0d016f509bc0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40062,DS-6f2644fc-a169-45ca-ae35-0d016f509bc0,DISK], DatanodeInfoWithStorage[127.0.0.1:39861,DS-ee54164b-7c86-49e0-abeb-b2135c69804a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39861,DS-ee54164b-7c86-49e0-abeb-b2135c69804a,DISK], DatanodeInfoWithStorage[127.0.0.1:40062,DS-6f2644fc-a169-45ca-ae35-0d016f509bc0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40062,DS-6f2644fc-a169-45ca-ae35-0d016f509bc0,DISK], DatanodeInfoWithStorage[127.0.0.1:39861,DS-ee54164b-7c86-49e0-abeb-b2135c69804a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMove
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43503,DS-71f9bff3-cdb7-4ff6-a30c-7e85b65f97d1,DISK], DatanodeInfoWithStorage[127.0.0.1:35145,DS-bbac4812-a366-4bb4-8e89-505f094deefa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43503,DS-71f9bff3-cdb7-4ff6-a30c-7e85b65f97d1,DISK], DatanodeInfoWithStorage[127.0.0.1:35145,DS-bbac4812-a366-4bb4-8e89-505f094deefa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43503,DS-71f9bff3-cdb7-4ff6-a30c-7e85b65f97d1,DISK], DatanodeInfoWithStorage[127.0.0.1:35145,DS-bbac4812-a366-4bb4-8e89-505f094deefa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43503,DS-71f9bff3-cdb7-4ff6-a30c-7e85b65f97d1,DISK], DatanodeInfoWithStorage[127.0.0.1:35145,DS-bbac4812-a366-4bb4-8e89-505f094deefa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMove
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38292,DS-9313f579-a74c-4cb3-b49d-32994941e51d,DISK], DatanodeInfoWithStorage[127.0.0.1:46213,DS-64048bd3-f269-4ef2-9b62-4a62c9fd4227,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38292,DS-9313f579-a74c-4cb3-b49d-32994941e51d,DISK], DatanodeInfoWithStorage[127.0.0.1:46213,DS-64048bd3-f269-4ef2-9b62-4a62c9fd4227,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38292,DS-9313f579-a74c-4cb3-b49d-32994941e51d,DISK], DatanodeInfoWithStorage[127.0.0.1:46213,DS-64048bd3-f269-4ef2-9b62-4a62c9fd4227,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38292,DS-9313f579-a74c-4cb3-b49d-32994941e51d,DISK], DatanodeInfoWithStorage[127.0.0.1:46213,DS-64048bd3-f269-4ef2-9b62-4a62c9fd4227,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMove
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38929,DS-410c2ea9-c8fa-4742-8b7d-e22d1a85cd7d,DISK], DatanodeInfoWithStorage[127.0.0.1:39456,DS-3d4ce52c-6d03-4c49-83ca-05f02afbde5f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38929,DS-410c2ea9-c8fa-4742-8b7d-e22d1a85cd7d,DISK], DatanodeInfoWithStorage[127.0.0.1:39456,DS-3d4ce52c-6d03-4c49-83ca-05f02afbde5f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38929,DS-410c2ea9-c8fa-4742-8b7d-e22d1a85cd7d,DISK], DatanodeInfoWithStorage[127.0.0.1:39456,DS-3d4ce52c-6d03-4c49-83ca-05f02afbde5f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38929,DS-410c2ea9-c8fa-4742-8b7d-e22d1a85cd7d,DISK], DatanodeInfoWithStorage[127.0.0.1:39456,DS-3d4ce52c-6d03-4c49-83ca-05f02afbde5f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMove
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32858,DS-589bfb9b-b685-47e4-87cc-21bb4bbaaa5c,DISK], DatanodeInfoWithStorage[127.0.0.1:41499,DS-ec6c16ee-5f06-4752-9187-40cf2b6abf08,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32858,DS-589bfb9b-b685-47e4-87cc-21bb4bbaaa5c,DISK], DatanodeInfoWithStorage[127.0.0.1:41499,DS-ec6c16ee-5f06-4752-9187-40cf2b6abf08,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:32858,DS-589bfb9b-b685-47e4-87cc-21bb4bbaaa5c,DISK], DatanodeInfoWithStorage[127.0.0.1:41499,DS-ec6c16ee-5f06-4752-9187-40cf2b6abf08,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:32858,DS-589bfb9b-b685-47e4-87cc-21bb4bbaaa5c,DISK], DatanodeInfoWithStorage[127.0.0.1:41499,DS-ec6c16ee-5f06-4752-9187-40cf2b6abf08,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMove
reconfPoint: -1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35372,DS-8be23452-6570-40d8-9941-d7fa2e401f8c,DISK], DatanodeInfoWithStorage[127.0.0.1:43168,DS-aa8a19ba-36ab-4c95-b026-6f61b654f829,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35372,DS-8be23452-6570-40d8-9941-d7fa2e401f8c,DISK], DatanodeInfoWithStorage[127.0.0.1:43168,DS-aa8a19ba-36ab-4c95-b026-6f61b654f829,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35372,DS-8be23452-6570-40d8-9941-d7fa2e401f8c,DISK], DatanodeInfoWithStorage[127.0.0.1:43168,DS-aa8a19ba-36ab-4c95-b026-6f61b654f829,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35372,DS-8be23452-6570-40d8-9941-d7fa2e401f8c,DISK], DatanodeInfoWithStorage[127.0.0.1:43168,DS-aa8a19ba-36ab-4c95-b026-6f61b654f829,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 2191

reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -3
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39251,DS-f3abe631-1f39-486d-b362-9ef9115c67ae,DISK], DatanodeInfoWithStorage[127.0.0.1:35625,DS-5051a443-7e7c-4876-8868-3d1081e73461,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39251,DS-f3abe631-1f39-486d-b362-9ef9115c67ae,DISK], DatanodeInfoWithStorage[127.0.0.1:35625,DS-5051a443-7e7c-4876-8868-3d1081e73461,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39251,DS-f3abe631-1f39-486d-b362-9ef9115c67ae,DISK], DatanodeInfoWithStorage[127.0.0.1:35625,DS-5051a443-7e7c-4876-8868-3d1081e73461,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39251,DS-f3abe631-1f39-486d-b362-9ef9115c67ae,DISK], DatanodeInfoWithStorage[127.0.0.1:35625,DS-5051a443-7e7c-4876-8868-3d1081e73461,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44641,DS-8a80b29d-6b10-4dda-a78e-0837e3d1a20f,DISK], DatanodeInfoWithStorage[127.0.0.1:37009,DS-24be49bd-ffaa-496e-a44c-1f61b0bb74a7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44641,DS-8a80b29d-6b10-4dda-a78e-0837e3d1a20f,DISK], DatanodeInfoWithStorage[127.0.0.1:37009,DS-24be49bd-ffaa-496e-a44c-1f61b0bb74a7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44641,DS-8a80b29d-6b10-4dda-a78e-0837e3d1a20f,DISK], DatanodeInfoWithStorage[127.0.0.1:37009,DS-24be49bd-ffaa-496e-a44c-1f61b0bb74a7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44641,DS-8a80b29d-6b10-4dda-a78e-0837e3d1a20f,DISK], DatanodeInfoWithStorage[127.0.0.1:37009,DS-24be49bd-ffaa-496e-a44c-1f61b0bb74a7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38372,DS-0ca75e3e-2072-4da9-81eb-23800ddd2dc5,DISK], DatanodeInfoWithStorage[127.0.0.1:39188,DS-e44ba4ac-4874-47bc-acc9-8f1839794b70,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38372,DS-0ca75e3e-2072-4da9-81eb-23800ddd2dc5,DISK], DatanodeInfoWithStorage[127.0.0.1:39188,DS-e44ba4ac-4874-47bc-acc9-8f1839794b70,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:38372,DS-0ca75e3e-2072-4da9-81eb-23800ddd2dc5,DISK], DatanodeInfoWithStorage[127.0.0.1:39188,DS-e44ba4ac-4874-47bc-acc9-8f1839794b70,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:38372,DS-0ca75e3e-2072-4da9-81eb-23800ddd2dc5,DISK], DatanodeInfoWithStorage[127.0.0.1:39188,DS-e44ba4ac-4874-47bc-acc9-8f1839794b70,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34646,DS-f86d509d-ecfe-44af-8082-bbac5599812c,DISK], DatanodeInfoWithStorage[127.0.0.1:35638,DS-8ab83064-2f0b-47cf-84e7-fe44f4443206,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34646,DS-f86d509d-ecfe-44af-8082-bbac5599812c,DISK], DatanodeInfoWithStorage[127.0.0.1:35638,DS-8ab83064-2f0b-47cf-84e7-fe44f4443206,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34646,DS-f86d509d-ecfe-44af-8082-bbac5599812c,DISK], DatanodeInfoWithStorage[127.0.0.1:35638,DS-8ab83064-2f0b-47cf-84e7-fe44f4443206,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34646,DS-f86d509d-ecfe-44af-8082-bbac5599812c,DISK], DatanodeInfoWithStorage[127.0.0.1:35638,DS-8ab83064-2f0b-47cf-84e7-fe44f4443206,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44809,DS-ff8642bb-4b8b-449d-8d8d-4efad55f3d08,DISK], DatanodeInfoWithStorage[127.0.0.1:45190,DS-d5ac6b74-f611-4fb2-b9ca-8d2a621114d8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44809,DS-ff8642bb-4b8b-449d-8d8d-4efad55f3d08,DISK], DatanodeInfoWithStorage[127.0.0.1:45190,DS-d5ac6b74-f611-4fb2-b9ca-8d2a621114d8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44809,DS-ff8642bb-4b8b-449d-8d8d-4efad55f3d08,DISK], DatanodeInfoWithStorage[127.0.0.1:45190,DS-d5ac6b74-f611-4fb2-b9ca-8d2a621114d8,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:44809,DS-ff8642bb-4b8b-449d-8d8d-4efad55f3d08,DISK], DatanodeInfoWithStorage[127.0.0.1:45190,DS-d5ac6b74-f611-4fb2-b9ca-8d2a621114d8,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40072,DS-64932509-0855-4d0a-9636-7699d9bae5d5,DISK], DatanodeInfoWithStorage[127.0.0.1:33081,DS-5af61986-ae60-4bf8-8951-723368bd8438,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40072,DS-64932509-0855-4d0a-9636-7699d9bae5d5,DISK], DatanodeInfoWithStorage[127.0.0.1:33081,DS-5af61986-ae60-4bf8-8951-723368bd8438,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40072,DS-64932509-0855-4d0a-9636-7699d9bae5d5,DISK], DatanodeInfoWithStorage[127.0.0.1:33081,DS-5af61986-ae60-4bf8-8951-723368bd8438,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40072,DS-64932509-0855-4d0a-9636-7699d9bae5d5,DISK], DatanodeInfoWithStorage[127.0.0.1:33081,DS-5af61986-ae60-4bf8-8951-723368bd8438,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45170,DS-0e45be24-49ff-45a7-8a40-05043715cfb4,DISK], DatanodeInfoWithStorage[127.0.0.1:46793,DS-e8218240-37b5-4f39-9c3d-d9c6fef18991,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45170,DS-0e45be24-49ff-45a7-8a40-05043715cfb4,DISK], DatanodeInfoWithStorage[127.0.0.1:46793,DS-e8218240-37b5-4f39-9c3d-d9c6fef18991,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45170,DS-0e45be24-49ff-45a7-8a40-05043715cfb4,DISK], DatanodeInfoWithStorage[127.0.0.1:46793,DS-e8218240-37b5-4f39-9c3d-d9c6fef18991,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45170,DS-0e45be24-49ff-45a7-8a40-05043715cfb4,DISK], DatanodeInfoWithStorage[127.0.0.1:46793,DS-e8218240-37b5-4f39-9c3d-d9c6fef18991,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36859,DS-5742cd2e-ead1-4411-b455-8dcf4cc3e03d,DISK], DatanodeInfoWithStorage[127.0.0.1:43398,DS-fb7bb849-e5cf-4717-9cdb-c7942fc26d0e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43398,DS-fb7bb849-e5cf-4717-9cdb-c7942fc26d0e,DISK], DatanodeInfoWithStorage[127.0.0.1:36859,DS-5742cd2e-ead1-4411-b455-8dcf4cc3e03d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36859,DS-5742cd2e-ead1-4411-b455-8dcf4cc3e03d,DISK], DatanodeInfoWithStorage[127.0.0.1:43398,DS-fb7bb849-e5cf-4717-9cdb-c7942fc26d0e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43398,DS-fb7bb849-e5cf-4717-9cdb-c7942fc26d0e,DISK], DatanodeInfoWithStorage[127.0.0.1:36859,DS-5742cd2e-ead1-4411-b455-8dcf4cc3e03d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -3
result: -1
failureMessage: Waiting timed out after [30,000] msec Not enough entries replicated
stackTrace: java.lang.AssertionError: Waiting timed out after [30,000] msec Not enough entries replicated
	at org.junit.Assert.fail(Assert.java:88)
	at org.apache.hadoop.hbase.Waiter.waitFor(Waiter.java:203)
	at org.apache.hadoop.hbase.Waiter.waitFor(Waiter.java:137)
	at org.apache.hadoop.hbase.HBaseCommonTestingUtility.waitFor(HBaseCommonTestingUtility.java:251)
	at org.apache.hadoop.hbase.replication.SerialReplicationTestBase.waitUntilReplicationDone(SerialReplicationTestBase.java:190)
	at org.apache.hadoop.hbase.replication.SerialReplicationTestBase.enablePeerAndWaitUntilReplicationDone(SerialReplicationTestBase.java:214)
	at org.apache.hadoop.hbase.replication.TestSerialReplication.testRegionMerge(TestSerialReplication.java:166)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:748)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HRegionServer
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestSerialReplication#testRegionMerge
reconfPoint: -3
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44259,DS-f3122fa7-2003-48d2-8e7d-2eca4be21cb7,DISK], DatanodeInfoWithStorage[127.0.0.1:41822,DS-7de131d7-ac8a-429d-ba34-4de08c2fd8ad,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41822,DS-7de131d7-ac8a-429d-ba34-4de08c2fd8ad,DISK], DatanodeInfoWithStorage[127.0.0.1:44259,DS-f3122fa7-2003-48d2-8e7d-2eca4be21cb7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44259,DS-f3122fa7-2003-48d2-8e7d-2eca4be21cb7,DISK], DatanodeInfoWithStorage[127.0.0.1:41822,DS-7de131d7-ac8a-429d-ba34-4de08c2fd8ad,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41822,DS-7de131d7-ac8a-429d-ba34-4de08c2fd8ad,DISK], DatanodeInfoWithStorage[127.0.0.1:44259,DS-f3122fa7-2003-48d2-8e7d-2eca4be21cb7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 2508

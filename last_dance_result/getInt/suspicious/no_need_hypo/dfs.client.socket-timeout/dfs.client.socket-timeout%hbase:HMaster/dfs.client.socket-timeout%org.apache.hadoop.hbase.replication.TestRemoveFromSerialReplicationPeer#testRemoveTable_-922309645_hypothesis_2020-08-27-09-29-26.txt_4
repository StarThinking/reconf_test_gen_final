reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestRemoveFromSerialReplicationPeer#testRemoveTable
reconfPoint: 1
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestRemoveFromSerialReplicationPeer#testRemoveTable
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39285,DS-64d91e52-281a-436e-89da-54f3411b590f,DISK], DatanodeInfoWithStorage[127.0.0.1:33740,DS-b7628eba-9c03-4a3d-8254-ecfb14a21087,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33740,DS-b7628eba-9c03-4a3d-8254-ecfb14a21087,DISK], DatanodeInfoWithStorage[127.0.0.1:39285,DS-64d91e52-281a-436e-89da-54f3411b590f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39285,DS-64d91e52-281a-436e-89da-54f3411b590f,DISK], DatanodeInfoWithStorage[127.0.0.1:33740,DS-b7628eba-9c03-4a3d-8254-ecfb14a21087,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33740,DS-b7628eba-9c03-4a3d-8254-ecfb14a21087,DISK], DatanodeInfoWithStorage[127.0.0.1:39285,DS-64d91e52-281a-436e-89da-54f3411b590f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestRemoveFromSerialReplicationPeer#testRemoveTable
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44856,DS-ff97da65-5521-4dc8-91ac-5a291ceba6f0,DISK], DatanodeInfoWithStorage[127.0.0.1:35238,DS-196dbdac-e488-4a89-9f46-660f4ac33241,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35238,DS-196dbdac-e488-4a89-9f46-660f4ac33241,DISK], DatanodeInfoWithStorage[127.0.0.1:44856,DS-ff97da65-5521-4dc8-91ac-5a291ceba6f0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:44856,DS-ff97da65-5521-4dc8-91ac-5a291ceba6f0,DISK], DatanodeInfoWithStorage[127.0.0.1:35238,DS-196dbdac-e488-4a89-9f46-660f4ac33241,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35238,DS-196dbdac-e488-4a89-9f46-660f4ac33241,DISK], DatanodeInfoWithStorage[127.0.0.1:44856,DS-ff97da65-5521-4dc8-91ac-5a291ceba6f0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestRemoveFromSerialReplicationPeer#testRemoveTable
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46872,DS-78607a5e-1a1d-41cf-b3df-3352cd6329d5,DISK], DatanodeInfoWithStorage[127.0.0.1:35076,DS-b425bf40-a70c-430e-8816-f6d2d0fad9e6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46872,DS-78607a5e-1a1d-41cf-b3df-3352cd6329d5,DISK], DatanodeInfoWithStorage[127.0.0.1:35076,DS-b425bf40-a70c-430e-8816-f6d2d0fad9e6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46872,DS-78607a5e-1a1d-41cf-b3df-3352cd6329d5,DISK], DatanodeInfoWithStorage[127.0.0.1:35076,DS-b425bf40-a70c-430e-8816-f6d2d0fad9e6,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46872,DS-78607a5e-1a1d-41cf-b3df-3352cd6329d5,DISK], DatanodeInfoWithStorage[127.0.0.1:35076,DS-b425bf40-a70c-430e-8816-f6d2d0fad9e6,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestRemoveFromSerialReplicationPeer#testRemoveTable
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35237,DS-e3e978c3-c44c-45fc-ab38-f9cc77454d67,DISK], DatanodeInfoWithStorage[127.0.0.1:39946,DS-8f5d2e56-caf9-4825-8574-ed3b7cda4353,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35237,DS-e3e978c3-c44c-45fc-ab38-f9cc77454d67,DISK], DatanodeInfoWithStorage[127.0.0.1:39946,DS-8f5d2e56-caf9-4825-8574-ed3b7cda4353,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35237,DS-e3e978c3-c44c-45fc-ab38-f9cc77454d67,DISK], DatanodeInfoWithStorage[127.0.0.1:39946,DS-8f5d2e56-caf9-4825-8574-ed3b7cda4353,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35237,DS-e3e978c3-c44c-45fc-ab38-f9cc77454d67,DISK], DatanodeInfoWithStorage[127.0.0.1:39946,DS-8f5d2e56-caf9-4825-8574-ed3b7cda4353,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestRemoveFromSerialReplicationPeer#testRemoveTable
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37556,DS-b77cc4f2-8785-475d-97eb-2faf4bbce04b,DISK], DatanodeInfoWithStorage[127.0.0.1:36983,DS-704b1cb3-214f-4cb4-af43-c0d0f9d853cf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37556,DS-b77cc4f2-8785-475d-97eb-2faf4bbce04b,DISK], DatanodeInfoWithStorage[127.0.0.1:36983,DS-704b1cb3-214f-4cb4-af43-c0d0f9d853cf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37556,DS-b77cc4f2-8785-475d-97eb-2faf4bbce04b,DISK], DatanodeInfoWithStorage[127.0.0.1:36983,DS-704b1cb3-214f-4cb4-af43-c0d0f9d853cf,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37556,DS-b77cc4f2-8785-475d-97eb-2faf4bbce04b,DISK], DatanodeInfoWithStorage[127.0.0.1:36983,DS-704b1cb3-214f-4cb4-af43-c0d0f9d853cf,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestRemoveFromSerialReplicationPeer#testRemoveTable
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37205,DS-344e425e-3c78-43ea-ba69-3b019e3c9f63,DISK], DatanodeInfoWithStorage[127.0.0.1:44116,DS-633254d9-b5d9-460b-9269-3716c51fd94f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37205,DS-344e425e-3c78-43ea-ba69-3b019e3c9f63,DISK], DatanodeInfoWithStorage[127.0.0.1:44116,DS-633254d9-b5d9-460b-9269-3716c51fd94f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37205,DS-344e425e-3c78-43ea-ba69-3b019e3c9f63,DISK], DatanodeInfoWithStorage[127.0.0.1:44116,DS-633254d9-b5d9-460b-9269-3716c51fd94f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37205,DS-344e425e-3c78-43ea-ba69-3b019e3c9f63,DISK], DatanodeInfoWithStorage[127.0.0.1:44116,DS-633254d9-b5d9-460b-9269-3716c51fd94f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestRemoveFromSerialReplicationPeer#testRemoveTable
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45138,DS-dd3a88a7-ef35-421b-8660-21e636fc948c,DISK], DatanodeInfoWithStorage[127.0.0.1:42222,DS-1252b2a2-26d6-4b13-b2f8-dceee0b11023,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45138,DS-dd3a88a7-ef35-421b-8660-21e636fc948c,DISK], DatanodeInfoWithStorage[127.0.0.1:42222,DS-1252b2a2-26d6-4b13-b2f8-dceee0b11023,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45138,DS-dd3a88a7-ef35-421b-8660-21e636fc948c,DISK], DatanodeInfoWithStorage[127.0.0.1:42222,DS-1252b2a2-26d6-4b13-b2f8-dceee0b11023,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45138,DS-dd3a88a7-ef35-421b-8660-21e636fc948c,DISK], DatanodeInfoWithStorage[127.0.0.1:42222,DS-1252b2a2-26d6-4b13-b2f8-dceee0b11023,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestRemoveFromSerialReplicationPeer#testRemoveTable
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43378,DS-64947396-f9f8-42d6-8df8-8add9a9c1cb9,DISK], DatanodeInfoWithStorage[127.0.0.1:33161,DS-ecf04e0a-0e11-4b16-a624-daf2ad538010,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43378,DS-64947396-f9f8-42d6-8df8-8add9a9c1cb9,DISK], DatanodeInfoWithStorage[127.0.0.1:33161,DS-ecf04e0a-0e11-4b16-a624-daf2ad538010,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:43378,DS-64947396-f9f8-42d6-8df8-8add9a9c1cb9,DISK], DatanodeInfoWithStorage[127.0.0.1:33161,DS-ecf04e0a-0e11-4b16-a624-daf2ad538010,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:43378,DS-64947396-f9f8-42d6-8df8-8add9a9c1cb9,DISK], DatanodeInfoWithStorage[127.0.0.1:33161,DS-ecf04e0a-0e11-4b16-a624-daf2ad538010,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestRemoveFromSerialReplicationPeer#testRemoveTable
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34036,DS-21f172b1-aa56-4163-a3eb-b8202f665ca3,DISK], DatanodeInfoWithStorage[127.0.0.1:38227,DS-e02552c9-dfd7-43ee-b37c-fd00d4ff263f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34036,DS-21f172b1-aa56-4163-a3eb-b8202f665ca3,DISK], DatanodeInfoWithStorage[127.0.0.1:38227,DS-e02552c9-dfd7-43ee-b37c-fd00d4ff263f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:34036,DS-21f172b1-aa56-4163-a3eb-b8202f665ca3,DISK], DatanodeInfoWithStorage[127.0.0.1:38227,DS-e02552c9-dfd7-43ee-b37c-fd00d4ff263f,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:34036,DS-21f172b1-aa56-4163-a3eb-b8202f665ca3,DISK], DatanodeInfoWithStorage[127.0.0.1:38227,DS-e02552c9-dfd7-43ee-b37c-fd00d4ff263f,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hbase:HMaster
v1: 600
v2: 60000
testProject: hbase
unitTest: org.apache.hadoop.hbase.replication.TestRemoveFromSerialReplicationPeer#testRemoveTable
reconfPoint: 1
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42319,DS-1e75e0f5-f4e1-488f-ab6d-54c3be445c19,DISK], DatanodeInfoWithStorage[127.0.0.1:40977,DS-e5a0a3ae-0d56-48f2-8d03-d943f47e4787,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40977,DS-e5a0a3ae-0d56-48f2-8d03-d943f47e4787,DISK], DatanodeInfoWithStorage[127.0.0.1:42319,DS-1e75e0f5-f4e1-488f-ab6d-54c3be445c19,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42319,DS-1e75e0f5-f4e1-488f-ab6d-54c3be445c19,DISK], DatanodeInfoWithStorage[127.0.0.1:40977,DS-e5a0a3ae-0d56-48f2-8d03-d943f47e4787,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40977,DS-e5a0a3ae-0d56-48f2-8d03-d943f47e4787,DISK], DatanodeInfoWithStorage[127.0.0.1:42319,DS-1e75e0f5-f4e1-488f-ab6d-54c3be445c19,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1281)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1353)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1568)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1469)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeError(DataStreamer.java:1237)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:657)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 10
v1v1v2v2 failed with probability 0 out of 10
result: might be true error
Total execution time in seconds : 2262

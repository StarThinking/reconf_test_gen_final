reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testReloadOnEditReplayFailure
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testReloadOnEditReplayFailure
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35617,DS-4d4681db-309c-4f87-b9ca-881602eb7d73,DISK], DatanodeInfoWithStorage[127.0.0.1:33035,DS-e267c634-3b36-4c14-a0c9-88665bdd78b7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35617,DS-4d4681db-309c-4f87-b9ca-881602eb7d73,DISK], DatanodeInfoWithStorage[127.0.0.1:33035,DS-e267c634-3b36-4c14-a0c9-88665bdd78b7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35617,DS-4d4681db-309c-4f87-b9ca-881602eb7d73,DISK], DatanodeInfoWithStorage[127.0.0.1:33035,DS-e267c634-3b36-4c14-a0c9-88665bdd78b7,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35617,DS-4d4681db-309c-4f87-b9ca-881602eb7d73,DISK], DatanodeInfoWithStorage[127.0.0.1:33035,DS-e267c634-3b36-4c14-a0c9-88665bdd78b7,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testReloadOnEditReplayFailure
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39604,DS-0fad87a7-1d9b-44c3-ba37-9dccce5b4a11,DISK], DatanodeInfoWithStorage[127.0.0.1:39077,DS-4adc2f32-7dff-4bd9-b3a1-e0ca1134b3b2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39077,DS-4adc2f32-7dff-4bd9-b3a1-e0ca1134b3b2,DISK], DatanodeInfoWithStorage[127.0.0.1:39604,DS-0fad87a7-1d9b-44c3-ba37-9dccce5b4a11,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39604,DS-0fad87a7-1d9b-44c3-ba37-9dccce5b4a11,DISK], DatanodeInfoWithStorage[127.0.0.1:39077,DS-4adc2f32-7dff-4bd9-b3a1-e0ca1134b3b2,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39077,DS-4adc2f32-7dff-4bd9-b3a1-e0ca1134b3b2,DISK], DatanodeInfoWithStorage[127.0.0.1:39604,DS-0fad87a7-1d9b-44c3-ba37-9dccce5b4a11,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testReloadOnEditReplayFailure
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33997,DS-04e5a306-38e7-4ddd-b044-14d8c539df25,DISK], DatanodeInfoWithStorage[127.0.0.1:41647,DS-a4219e0f-772f-498d-9bb4-f229b2466b57,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33997,DS-04e5a306-38e7-4ddd-b044-14d8c539df25,DISK], DatanodeInfoWithStorage[127.0.0.1:41647,DS-a4219e0f-772f-498d-9bb4-f229b2466b57,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33997,DS-04e5a306-38e7-4ddd-b044-14d8c539df25,DISK], DatanodeInfoWithStorage[127.0.0.1:41647,DS-a4219e0f-772f-498d-9bb4-f229b2466b57,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33997,DS-04e5a306-38e7-4ddd-b044-14d8c539df25,DISK], DatanodeInfoWithStorage[127.0.0.1:41647,DS-a4219e0f-772f-498d-9bb4-f229b2466b57,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testReloadOnEditReplayFailure
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40707,DS-2bed8438-ac5f-445e-8b1d-6572b8a9b9b6,DISK], DatanodeInfoWithStorage[127.0.0.1:35704,DS-32cfe590-15c3-433f-868d-3342e16c9771,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40707,DS-2bed8438-ac5f-445e-8b1d-6572b8a9b9b6,DISK], DatanodeInfoWithStorage[127.0.0.1:35704,DS-32cfe590-15c3-433f-868d-3342e16c9771,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40707,DS-2bed8438-ac5f-445e-8b1d-6572b8a9b9b6,DISK], DatanodeInfoWithStorage[127.0.0.1:35704,DS-32cfe590-15c3-433f-868d-3342e16c9771,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40707,DS-2bed8438-ac5f-445e-8b1d-6572b8a9b9b6,DISK], DatanodeInfoWithStorage[127.0.0.1:35704,DS-32cfe590-15c3-433f-868d-3342e16c9771,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testReloadOnEditReplayFailure
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33505,DS-09ab37e6-dcde-48e8-aec1-e97ce5bb5c67,DISK], DatanodeInfoWithStorage[127.0.0.1:37249,DS-7493a33e-e9e4-4c36-91a4-713b74422b25,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33505,DS-09ab37e6-dcde-48e8-aec1-e97ce5bb5c67,DISK], DatanodeInfoWithStorage[127.0.0.1:37249,DS-7493a33e-e9e4-4c36-91a4-713b74422b25,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:33505,DS-09ab37e6-dcde-48e8-aec1-e97ce5bb5c67,DISK], DatanodeInfoWithStorage[127.0.0.1:37249,DS-7493a33e-e9e4-4c36-91a4-713b74422b25,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:33505,DS-09ab37e6-dcde-48e8-aec1-e97ce5bb5c67,DISK], DatanodeInfoWithStorage[127.0.0.1:37249,DS-7493a33e-e9e4-4c36-91a4-713b74422b25,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testReloadOnEditReplayFailure
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42637,DS-e105b947-fd80-4621-977c-26602df5e648,DISK], DatanodeInfoWithStorage[127.0.0.1:38075,DS-1bdf4adf-c17a-4054-9b15-bba700438f67,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42637,DS-e105b947-fd80-4621-977c-26602df5e648,DISK], DatanodeInfoWithStorage[127.0.0.1:38075,DS-1bdf4adf-c17a-4054-9b15-bba700438f67,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42637,DS-e105b947-fd80-4621-977c-26602df5e648,DISK], DatanodeInfoWithStorage[127.0.0.1:38075,DS-1bdf4adf-c17a-4054-9b15-bba700438f67,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:42637,DS-e105b947-fd80-4621-977c-26602df5e648,DISK], DatanodeInfoWithStorage[127.0.0.1:38075,DS-1bdf4adf-c17a-4054-9b15-bba700438f67,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testReloadOnEditReplayFailure
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39507,DS-718fd952-ec5a-41a5-ba6f-92b7f53dc65a,DISK], DatanodeInfoWithStorage[127.0.0.1:37202,DS-2af81750-6736-4fda-a988-bda5f7cd3816,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39507,DS-718fd952-ec5a-41a5-ba6f-92b7f53dc65a,DISK], DatanodeInfoWithStorage[127.0.0.1:37202,DS-2af81750-6736-4fda-a988-bda5f7cd3816,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:39507,DS-718fd952-ec5a-41a5-ba6f-92b7f53dc65a,DISK], DatanodeInfoWithStorage[127.0.0.1:37202,DS-2af81750-6736-4fda-a988-bda5f7cd3816,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:39507,DS-718fd952-ec5a-41a5-ba6f-92b7f53dc65a,DISK], DatanodeInfoWithStorage[127.0.0.1:37202,DS-2af81750-6736-4fda-a988-bda5f7cd3816,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testReloadOnEditReplayFailure
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35449,DS-b14a7dbb-be8b-42a9-9cdb-25c819c0b2ca,DISK], DatanodeInfoWithStorage[127.0.0.1:39417,DS-8e3a7e0a-089d-45a6-a694-5d4963924408,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35449,DS-b14a7dbb-be8b-42a9-9cdb-25c819c0b2ca,DISK], DatanodeInfoWithStorage[127.0.0.1:39417,DS-8e3a7e0a-089d-45a6-a694-5d4963924408,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35449,DS-b14a7dbb-be8b-42a9-9cdb-25c819c0b2ca,DISK], DatanodeInfoWithStorage[127.0.0.1:39417,DS-8e3a7e0a-089d-45a6-a694-5d4963924408,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35449,DS-b14a7dbb-be8b-42a9-9cdb-25c819c0b2ca,DISK], DatanodeInfoWithStorage[127.0.0.1:39417,DS-8e3a7e0a-089d-45a6-a694-5d4963924408,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testReloadOnEditReplayFailure
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37006,DS-0c31f19b-6928-41da-b92a-3770248f003f,DISK], DatanodeInfoWithStorage[127.0.0.1:36733,DS-a21047b1-9fe1-4a29-bd9e-b6a394439c49,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37006,DS-0c31f19b-6928-41da-b92a-3770248f003f,DISK], DatanodeInfoWithStorage[127.0.0.1:36733,DS-a21047b1-9fe1-4a29-bd9e-b6a394439c49,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:37006,DS-0c31f19b-6928-41da-b92a-3770248f003f,DISK], DatanodeInfoWithStorage[127.0.0.1:36733,DS-a21047b1-9fe1-4a29-bd9e-b6a394439c49,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:37006,DS-0c31f19b-6928-41da-b92a-3770248f003f,DISK], DatanodeInfoWithStorage[127.0.0.1:36733,DS-a21047b1-9fe1-4a29-bd9e-b6a394439c49,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60
v2: 60000
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testReloadOnEditReplayFailure
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40075,DS-9797d889-cdc7-4dfa-a40a-52930feeb903,DISK], DatanodeInfoWithStorage[127.0.0.1:38471,DS-066b3541-1761-4edb-973c-98329a29e9ce,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40075,DS-9797d889-cdc7-4dfa-a40a-52930feeb903,DISK], DatanodeInfoWithStorage[127.0.0.1:38471,DS-066b3541-1761-4edb-973c-98329a29e9ce,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40075,DS-9797d889-cdc7-4dfa-a40a-52930feeb903,DISK], DatanodeInfoWithStorage[127.0.0.1:38471,DS-066b3541-1761-4edb-973c-98329a29e9ce,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40075,DS-9797d889-cdc7-4dfa-a40a-52930feeb903,DISK], DatanodeInfoWithStorage[127.0.0.1:38471,DS-066b3541-1761-4edb-973c-98329a29e9ce,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 22
v1v1v2v2 failed with probability 0 out of 22
result: might be true error
Total execution time in seconds : 1543

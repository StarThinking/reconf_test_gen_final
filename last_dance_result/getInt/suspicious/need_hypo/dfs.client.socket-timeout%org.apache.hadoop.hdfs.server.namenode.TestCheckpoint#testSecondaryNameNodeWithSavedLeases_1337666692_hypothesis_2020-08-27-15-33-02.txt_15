reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -2
result: -1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35581,DS-9d714c00-a061-411c-a1c5-86f29ffe6fad,DISK], DatanodeInfoWithStorage[127.0.0.1:33916,DS-4a68416f-2b46-4b82-8c88-b8b56024fbfa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35581,DS-9d714c00-a061-411c-a1c5-86f29ffe6fad,DISK], DatanodeInfoWithStorage[127.0.0.1:33916,DS-4a68416f-2b46-4b82-8c88-b8b56024fbfa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:35581,DS-9d714c00-a061-411c-a1c5-86f29ffe6fad,DISK], DatanodeInfoWithStorage[127.0.0.1:33916,DS-4a68416f-2b46-4b82-8c88-b8b56024fbfa,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:35581,DS-9d714c00-a061-411c-a1c5-86f29ffe6fad,DISK], DatanodeInfoWithStorage[127.0.0.1:33916,DS-4a68416f-2b46-4b82-8c88-b8b56024fbfa,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36511,DS-63eafcc5-fc9f-46c2-a6a9-19c53781e26e,DISK], DatanodeInfoWithStorage[127.0.0.1:42428,DS-127342f6-3492-4447-924e-285f058584a0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36511,DS-63eafcc5-fc9f-46c2-a6a9-19c53781e26e,DISK], DatanodeInfoWithStorage[127.0.0.1:42428,DS-127342f6-3492-4447-924e-285f058584a0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:36511,DS-63eafcc5-fc9f-46c2-a6a9-19c53781e26e,DISK], DatanodeInfoWithStorage[127.0.0.1:42428,DS-127342f6-3492-4447-924e-285f058584a0,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:36511,DS-63eafcc5-fc9f-46c2-a6a9-19c53781e26e,DISK], DatanodeInfoWithStorage[127.0.0.1:42428,DS-127342f6-3492-4447-924e-285f058584a0,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41943,DS-0d550c28-ad8c-46ec-a149-02f59ed2d503,DISK], DatanodeInfoWithStorage[127.0.0.1:41023,DS-9dd42326-b1af-45b1-9c60-dfd7eadc3a1e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41943,DS-0d550c28-ad8c-46ec-a149-02f59ed2d503,DISK], DatanodeInfoWithStorage[127.0.0.1:41023,DS-9dd42326-b1af-45b1-9c60-dfd7eadc3a1e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41943,DS-0d550c28-ad8c-46ec-a149-02f59ed2d503,DISK], DatanodeInfoWithStorage[127.0.0.1:41023,DS-9dd42326-b1af-45b1-9c60-dfd7eadc3a1e,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41943,DS-0d550c28-ad8c-46ec-a149-02f59ed2d503,DISK], DatanodeInfoWithStorage[127.0.0.1:41023,DS-9dd42326-b1af-45b1-9c60-dfd7eadc3a1e,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42824,DS-66f2887c-e3c5-41c3-b3c1-307036e6c62a,DISK], DatanodeInfoWithStorage[127.0.0.1:40605,DS-69c12671-be20-40e4-8f46-83eeb0063022,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40605,DS-69c12671-be20-40e4-8f46-83eeb0063022,DISK], DatanodeInfoWithStorage[127.0.0.1:42824,DS-66f2887c-e3c5-41c3-b3c1-307036e6c62a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:42824,DS-66f2887c-e3c5-41c3-b3c1-307036e6c62a,DISK], DatanodeInfoWithStorage[127.0.0.1:40605,DS-69c12671-be20-40e4-8f46-83eeb0063022,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40605,DS-69c12671-be20-40e4-8f46-83eeb0063022,DISK], DatanodeInfoWithStorage[127.0.0.1:42824,DS-66f2887c-e3c5-41c3-b3c1-307036e6c62a,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40356,DS-7f6114bc-a67f-46b7-88d2-3312caaf5542,DISK], DatanodeInfoWithStorage[127.0.0.1:39501,DS-fa76f3b1-c651-427a-b44d-1bb7f4100380,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40356,DS-7f6114bc-a67f-46b7-88d2-3312caaf5542,DISK], DatanodeInfoWithStorage[127.0.0.1:39501,DS-fa76f3b1-c651-427a-b44d-1bb7f4100380,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40356,DS-7f6114bc-a67f-46b7-88d2-3312caaf5542,DISK], DatanodeInfoWithStorage[127.0.0.1:39501,DS-fa76f3b1-c651-427a-b44d-1bb7f4100380,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40356,DS-7f6114bc-a67f-46b7-88d2-3312caaf5542,DISK], DatanodeInfoWithStorage[127.0.0.1:39501,DS-fa76f3b1-c651-427a-b44d-1bb7f4100380,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is 1

Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45712,DS-afa24a24-40f4-4a62-9388-82eba0230071,DISK], DatanodeInfoWithStorage[127.0.0.1:43801,DS-cc06ddcb-c115-430b-935b-d71c256cab24,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45712,DS-afa24a24-40f4-4a62-9388-82eba0230071,DISK], DatanodeInfoWithStorage[127.0.0.1:43801,DS-cc06ddcb-c115-430b-935b-d71c256cab24,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:45712,DS-afa24a24-40f4-4a62-9388-82eba0230071,DISK], DatanodeInfoWithStorage[127.0.0.1:43801,DS-cc06ddcb-c115-430b-935b-d71c256cab24,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:45712,DS-afa24a24-40f4-4a62-9388-82eba0230071,DISK], DatanodeInfoWithStorage[127.0.0.1:43801,DS-cc06ddcb-c115-430b-935b-d71c256cab24,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41381,DS-1d715a97-9bbf-4809-b17f-13e467704864,DISK], DatanodeInfoWithStorage[127.0.0.1:36573,DS-f9206e71-7306-4f5c-a6fe-355fac40d881,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41381,DS-1d715a97-9bbf-4809-b17f-13e467704864,DISK], DatanodeInfoWithStorage[127.0.0.1:36573,DS-f9206e71-7306-4f5c-a6fe-355fac40d881,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:41381,DS-1d715a97-9bbf-4809-b17f-13e467704864,DISK], DatanodeInfoWithStorage[127.0.0.1:36573,DS-f9206e71-7306-4f5c-a6fe-355fac40d881,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:41381,DS-1d715a97-9bbf-4809-b17f-13e467704864,DISK], DatanodeInfoWithStorage[127.0.0.1:36573,DS-f9206e71-7306-4f5c-a6fe-355fac40d881,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46005,DS-0aa322db-1989-48b1-8153-14114913df06,DISK], DatanodeInfoWithStorage[127.0.0.1:34487,DS-2f09365f-94d1-41e9-946d-b09057a3c853,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46005,DS-0aa322db-1989-48b1-8153-14114913df06,DISK], DatanodeInfoWithStorage[127.0.0.1:34487,DS-2f09365f-94d1-41e9-946d-b09057a3c853,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:46005,DS-0aa322db-1989-48b1-8153-14114913df06,DISK], DatanodeInfoWithStorage[127.0.0.1:34487,DS-2f09365f-94d1-41e9-946d-b09057a3c853,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:46005,DS-0aa322db-1989-48b1-8153-14114913df06,DISK], DatanodeInfoWithStorage[127.0.0.1:34487,DS-2f09365f-94d1-41e9-946d-b09057a3c853,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40462,DS-ca990dce-0ea7-4cef-ba48-8d4889570af5,DISK], DatanodeInfoWithStorage[127.0.0.1:35236,DS-91f1070a-671e-480d-a5d9-89ad523e956d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40462,DS-ca990dce-0ea7-4cef-ba48-8d4889570af5,DISK], DatanodeInfoWithStorage[127.0.0.1:35236,DS-91f1070a-671e-480d-a5d9-89ad523e956d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40462,DS-ca990dce-0ea7-4cef-ba48-8d4889570af5,DISK], DatanodeInfoWithStorage[127.0.0.1:35236,DS-91f1070a-671e-480d-a5d9-89ad523e956d,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40462,DS-ca990dce-0ea7-4cef-ba48-8d4889570af5,DISK], DatanodeInfoWithStorage[127.0.0.1:35236,DS-91f1070a-671e-480d-a5d9-89ad523e956d,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1

Test vvMode=v1v2
tr.result is -1
v1v2 test failed !!!
reconf_parameter: dfs.client.socket-timeout
component: hdfs:DataNode
v1: 60000
v2: 600
testProject: hdfs
unitTest: org.apache.hadoop.hdfs.server.namenode.TestCheckpoint#testSecondaryNameNodeWithSavedLeases
reconfPoint: -2
result: -1
failureMessage: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40429,DS-258bcc35-c58b-494a-a852-961012189468,DISK], DatanodeInfoWithStorage[127.0.0.1:40804,DS-0f76dba9-f5f5-4b28-af4e-77b0a1972504,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40804,DS-0f76dba9-f5f5-4b28-af4e-77b0a1972504,DISK], DatanodeInfoWithStorage[127.0.0.1:40429,DS-258bcc35-c58b-494a-a852-961012189468,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
stackTrace: java.io.IOException: Failed to replace a bad datanode on the existing pipeline due to no more good datanodes being available to try. (Nodes: current=[DatanodeInfoWithStorage[127.0.0.1:40429,DS-258bcc35-c58b-494a-a852-961012189468,DISK], DatanodeInfoWithStorage[127.0.0.1:40804,DS-0f76dba9-f5f5-4b28-af4e-77b0a1972504,DISK]], original=[DatanodeInfoWithStorage[127.0.0.1:40804,DS-0f76dba9-f5f5-4b28-af4e-77b0a1972504,DISK], DatanodeInfoWithStorage[127.0.0.1:40429,DS-258bcc35-c58b-494a-a852-961012189468,DISK]]). The current failed datanode replacement policy is DEFAULT, and a client may configure this via 'dfs.client.block.write.replace-datanode-on-failure.policy' in its configuration.
	at org.apache.hadoop.hdfs.DataStreamer.findNewDatanode(DataStreamer.java:1304)
	at org.apache.hadoop.hdfs.DataStreamer.addDatanode2ExistingPipeline(DataStreamer.java:1372)
	at org.apache.hadoop.hdfs.DataStreamer.handleDatanodeReplacement(DataStreamer.java:1598)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineInternal(DataStreamer.java:1499)
	at org.apache.hadoop.hdfs.DataStreamer.setupPipelineForAppendOrRecovery(DataStreamer.java:1481)
	at org.apache.hadoop.hdfs.DataStreamer.processDatanodeOrExternalError(DataStreamer.java:1256)
	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:667)



Test vvMode=v1v1
tr.result is 1

Test vvMode=v2v2
tr.result is 1
early stop after 10 is satisfied
v1v2 failed with probability 10 out of 12
v1v1v2v2 failed with probability 0 out of 12
result: might be true error
Total execution time in seconds : 825
